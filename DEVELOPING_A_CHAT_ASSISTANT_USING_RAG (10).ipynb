{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOw9-gE9GBmd"
      },
      "source": [
        "**DEVELOPING A CHAT ASSISTANT USING RETRIEVAL AUGMENTED GENERATION (RAG)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "CwH1D_Y77lrn",
        "outputId": "f3f49739-ebe6-4724-b848-898f64aa41b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb==0.5.5\n",
            "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain-chroma==0.1.2\n",
            "  Downloading langchain_chroma-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting langchain==0.2.11\n",
            "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-community==0.2.10\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-text-splitters==0.2.2\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langchain-groq==0.1.6\n",
            "  Downloading langchain_groq-0.1.6-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting transformers==4.43.2\n",
            "  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers==3.0.1\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting unstructured==0.15.0\n",
            "  Downloading unstructured-0.15.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-settings\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (1.2.2)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (2.9.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb==0.5.5)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb==0.5.5)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.5.5)\n",
            "  Downloading posthog-3.6.6-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb==0.5.5)\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (1.27.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==0.5.5)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==0.5.5)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (1.27.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb==0.5.5)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (4.66.5)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.5.5)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==0.5.5)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (0.12.5)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb==0.5.5)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb==0.5.5)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb==0.5.5)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.27.0 (from chromadb==0.5.5)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.3,>=0.1.40 (from langchain-chroma==0.1.2)\n",
            "  Downloading langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (3.10.8)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (4.0.3)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.11)\n",
            "  Downloading langsmith-0.1.130-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.32.3)\n",
            "Collecting tenacity>=8.2.3 (from chromadb==0.5.5)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.2.10)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq==0.1.6)\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (10.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (5.2.0)\n",
            "Collecting filetype (from unstructured==0.15.0)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured==0.15.0)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (4.12.3)\n",
            "Collecting emoji (from unstructured==0.15.0)\n",
            "  Downloading emoji-2.13.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting python-iso639 (from unstructured==0.15.0)\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured==0.15.0)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured==0.15.0)\n",
            "  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured==0.15.0)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured==0.15.0)\n",
            "  Downloading unstructured_client-0.25.9-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (1.16.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (5.9.5)\n",
            "Collecting onnx (from unstructured[pdf]==0.15.0)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting pdf2image (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pdfminer.six (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pikepdf (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pikepdf-9.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Collecting pillow-heif (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pillow_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
            "Collecting pypdf (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pypdf-5.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pytesseract (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting google-cloud-vision (from unstructured[pdf]==0.15.0)\n",
            "  Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting effdet (from unstructured[pdf]==0.15.0)\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting unstructured-inference==0.7.36 (from unstructured[pdf]==0.15.0)\n",
            "  Downloading unstructured_inference-0.7.36-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf]==0.15.0)\n",
            "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting layoutparser (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting python-multipart (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (3.7.1)\n",
            "Collecting timm (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (2.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.13.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb==0.5.5) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb==0.5.5) (2.0.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi>=0.95.2->chromadb==0.5.5)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.6) (1.7.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb==0.5.5) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb==0.5.5)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb==0.5.5)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (3.2.2)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==0.5.5)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.1.40->langchain-chroma==0.1.2)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.11)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (3.1.4)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.5.5)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.5.5) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.5.5) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.5.5) (1.13.3)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb==0.5.5) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb==0.5.5) (8.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.5) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5) (0.48b0)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5) (71.0.4)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.5.5)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb==0.5.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb==0.5.5) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.11) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb==0.5.5) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb==0.5.5) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb==0.5.5) (13.8.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured==0.15.0) (2.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]==0.15.0) (0.19.1+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]==0.15.0) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet->unstructured[pdf]==0.15.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]==0.15.0) (2.19.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]==0.15.0) (1.24.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.15.0) (1.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]==0.15.0) (43.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==3.0.1) (3.5.0)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->unstructured==0.15.0)\n",
            "  Downloading deepdiff-8.0.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured==0.15.0)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client->unstructured==0.15.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured==0.15.0) (1.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]==0.15.0) (1.17.1)\n",
            "Collecting orderly-set==5.2.2 (from deepdiff>=6.0->unstructured-client->unstructured==0.15.0)\n",
            "  Downloading orderly_set-5.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]==0.15.0) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.5) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.5) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.5) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.5.5) (3.20.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.40->langchain-chroma==0.1.2)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[pdf]==0.15.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.5.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.5.5) (2.18.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.5.5)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting iopath (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.5.5) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]==0.15.0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.5.5) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.5) (0.6.1)\n",
            "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pdfminer.six (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.1.2-py3-none-any.whl (9.3 kB)\n",
            "Downloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langchain_groq-0.1.6-py3-none-any.whl (14 kB)\n",
            "Downloading transformers-4.43.2-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-0.7.36-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.41-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.0/397.0 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.130-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.13.2-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.5/467.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pikepdf-9.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading pypdf-5.0.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.25.9-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepdiff-8.0.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orderly_set-5.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: pypika, langdetect, antlr4-python3-runtime, iopath\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=ce08ca094e82aed081f99a6a796200f3eddd01570c9c543016339a2d463d4822\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=f279d3f3c31a6ed74f4f047e96f66f641be667165bf7a399b777bf7dee189bb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=e03bd5658aadf1d392a41bc7f4cd6aedcea03a3688b039f1fdf94ddfab847cad\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=d35025c2baee20f23a56200d3815173fc3cbc4c5b57e38fc3ccb6d6515d4c7ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built pypika langdetect antlr4-python3-runtime iopath\n",
            "Installing collected packages: pypika, pydub, monotonic, filetype, durationpy, antlr4-python3-runtime, websockets, uvloop, unstructured.pytesseract, tomlkit, tenacity, semantic-version, ruff, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, pytesseract, pypdfium2, pypdf, portalocker, pillow-heif, pdf2image, overrides, orjson, orderly-set, opentelemetry-util-http, opentelemetry-proto, onnx, omegaconf, mypy-extensions, mmh3, marshmallow, langdetect, jsonpointer, jsonpath-python, humanfriendly, httptools, h11, ffmpy, emoji, chroma-hnswlib, bcrypt, backoff, asgiref, aiofiles, watchfiles, uvicorn, typing-inspect, starlette, requests-toolbelt, posthog, pikepdf, opentelemetry-exporter-otlp-proto-common, jsonpatch, iopath, httpcore, deepdiff, coloredlogs, pydantic-settings, pdfminer.six, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, fastapi, dataclasses-json, unstructured-client, transformers, timm, pdfplumber, opentelemetry-instrumentation-asgi, langsmith, groq, gradio-client, unstructured, sentence-transformers, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, layoutparser, langchain-core, gradio, google-cloud-vision, effdet, unstructured-inference, langchain-text-splitters, langchain-groq, chromadb, langchain-chroma, langchain, langchain-community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed aiofiles-23.2.1 antlr4-python3-runtime-4.9.3 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.5 coloredlogs-15.0.1 dataclasses-json-0.6.7 deepdiff-8.0.1 durationpy-0.9 effdet-0.4.1 emoji-2.13.2 fastapi-0.115.0 ffmpy-0.4.0 filetype-1.2.0 google-cloud-vision-3.7.4 gradio-4.44.1 gradio-client-1.3.0 groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httptools-0.6.1 httpx-0.27.2 humanfriendly-10.0 iopath-0.1.10 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-3.0.0 kubernetes-31.0.0 langchain-0.2.11 langchain-chroma-0.1.2 langchain-community-0.2.10 langchain-core-0.2.41 langchain-groq-0.1.6 langchain-text-splitters-0.2.2 langdetect-1.0.9 langsmith-0.1.130 layoutparser-0.3.4 marshmallow-3.22.0 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 omegaconf-2.3.0 onnx-1.17.0 onnxruntime-1.19.2 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-util-http-0.48b0 orderly-set-5.2.2 orjson-3.10.7 overrides-7.7.0 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pikepdf-9.3.0 pillow-heif-0.18.0 portalocker-2.10.1 posthog-3.6.6 pydantic-settings-2.5.2 pydub-0.25.1 pypdf-5.0.1 pypdfium2-4.30.0 pypika-0.48.9 pytesseract-0.3.13 python-dotenv-1.0.1 python-iso639-2024.4.27 python-magic-0.4.27 python-multipart-0.0.12 rapidfuzz-3.10.0 requests-toolbelt-1.0.0 ruff-0.6.8 semantic-version-2.10.0 sentence-transformers-3.0.1 starlette-0.38.6 tenacity-8.5.0 timm-1.0.9 tomlkit-0.12.0 transformers-4.43.2 typing-inspect-0.9.0 unstructured-0.15.0 unstructured-client-0.25.9 unstructured-inference-0.7.36 unstructured.pytesseract-0.3.13 uvicorn-0.31.0 uvloop-0.20.0 watchfiles-0.24.0 websockets-12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "f944a275425248a5b87730562d8552ad"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install relevant libraries\n",
        "!pip install chromadb==0.5.5 langchain-chroma==0.1.2 langchain==0.2.11 langchain-community==0.2.10 langchain-text-splitters==0.2.2 langchain-groq==0.1.6 transformers==4.43.2 sentence-transformers==3.0.1 unstructured==0.15.0 unstructured[pdf]==0.15.0 gradio pydantic-settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVZqjcCkFpKY",
        "outputId": "759450db-b5dd-41d1-f930-c207b3595dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ca_NF_vOENwJ"
      },
      "outputs": [],
      "source": [
        "# Import relevant libraries\n",
        "\n",
        "import time\n",
        "import textwrap\n",
        "import gradio as gr\n",
        "\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "from config import settings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zdf_IO3pIpGE"
      },
      "outputs": [],
      "source": [
        "# Create a variable for your api key\n",
        "groq_api_key = settings.groq_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IySaVzyiMAAp",
        "outputId": "8b558ddf-479e-497e-a26a-b707bb1870f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (1,768 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123620 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "# Installing Poppler-utils, which includes tools like pdfinfo, pdftotext, and pdfimages.\n",
        "# This package is required for handling PDF files in the pdf2image library.\n",
        "# The 'pdfinfo' utility is specifically needed to retrieve page counts and metadata from PDF files.\n",
        "# Make sure to run this command in environments where Poppler is not already installed (e.g., Google Colab).\n",
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "yDon9I5pLdeZ",
        "outputId": "f4de8f8b-db90-4858-831c-ffb9ad1d486a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4222c0bd-62e8-4935-a92f-75ce1a5e8fbf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4222c0bd-62e8-4935-a92f-75ce1a5e8fbf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ai and the future of humans.pdf to ai and the future of humans.pdf\n",
            "Saving Artificial Intelligence Accelerates Human Learning_ Discussion Data Analytics ( PDFDrive ).pdf to Artificial Intelligence Accelerates Human Learning_ Discussion Data Analytics ( PDFDrive ).pdf\n",
            "Saving artificial intelligence.pdf to artificial intelligence.pdf\n",
            "Saving future of A.I intelligence.pdf to future of A.I intelligence.pdf\n",
            "Saving future of ai.pdf to future of ai.pdf\n",
            "Saving Introduction to Artificial Intelligence ( PDFDrive ).pdf to Introduction to Artificial Intelligence ( PDFDrive ).pdf\n",
            "Saving preparing for future of ai.pdf to preparing for future of ai.pdf\n",
            "Saving Stahl_2021_Artificial_intelligence_for_human_f.pdf to Stahl_2021_Artificial_intelligence_for_human_f.pdf\n",
            "Saving the future of AI.pdf to the future of AI.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# You will be prompted to select and upload files from your local machine\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4YgHbZwL0H1",
        "outputId": "c54f7121-09a2-4297-d610-287e65668b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file: ai and the future of humans.pdf\n",
            "Uploaded file: Artificial Intelligence Accelerates Human Learning_ Discussion Data Analytics ( PDFDrive ).pdf\n",
            "Uploaded file: artificial intelligence.pdf\n",
            "Uploaded file: future of A.I intelligence.pdf\n",
            "Uploaded file: future of ai.pdf\n",
            "Uploaded file: Introduction to Artificial Intelligence ( PDFDrive ).pdf\n",
            "Uploaded file: preparing for future of ai.pdf\n",
            "Uploaded file: Stahl_2021_Artificial_intelligence_for_human_f.pdf\n",
            "Uploaded file: the future of AI.pdf\n"
          ]
        }
      ],
      "source": [
        "# List all files uploaded\n",
        "for file_name in uploaded.keys():\n",
        "    print(f\"Uploaded file: {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0JfiO_HPv5B",
        "outputId": "00d0c8c1-089a-40b6-993d-e11d5bd52a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: /content/Introduction to Artificial Intelligence ( PDFDrive ).pdf\n",
            "Loaded: /content/Artificial Intelligence Accelerates Human Learning_ Discussion Data Analytics ( PDFDrive ).pdf\n",
            "Loaded: /content/Stahl_2021_Artificial_intelligence_for_human_f.pdf\n",
            "Loaded: /content/ai and the future of humans.pdf\n",
            "Loaded: /content/preparing for future of ai.pdf\n",
            "Loaded: /content/artificial intelligence.pdf\n",
            "Loaded: /content/future of ai.pdf\n",
            "Loaded: /content/future of A.I intelligence.pdf\n",
            "Loaded: /content/the future of AI.pdf\n"
          ]
        }
      ],
      "source": [
        "# Create a variable for the file paths in the /content/ directory (where files are uploaded in Colab)\n",
        "file_path = [\n",
        "    \"/content/Introduction to Artificial Intelligence ( PDFDrive ).pdf\",\n",
        "    \"/content/Artificial Intelligence Accelerates Human Learning_ Discussion Data Analytics ( PDFDrive ).pdf\",\n",
        "    \"/content/Stahl_2021_Artificial_intelligence_for_human_f.pdf\",\n",
        "    \"/content/ai and the future of humans.pdf\",\n",
        "    \"/content/preparing for future of ai.pdf\",\n",
        "    \"/content/artificial intelligence.pdf\",\n",
        "    \"/content/future of ai.pdf\",\n",
        "    \"/content/future of A.I intelligence.pdf\",\n",
        "    \"/content/the future of AI.pdf\"\n",
        "]\n",
        "\n",
        "# List to store the documents\n",
        "documents = []\n",
        "\n",
        "# Step 3: Iterate through each PDF file, check if the file exists, and load the document\n",
        "for path in file_path:\n",
        "    if os.path.exists(path):  # Check if the file exists in the /content/ directory\n",
        "        try:\n",
        "            loader = UnstructuredFileLoader(path)  # Load the file using your PDF loader (replace with your actual loader function if needed)\n",
        "            doc = loader.load()\n",
        "            documents.append(doc)\n",
        "            print(f\"Loaded: {path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {path}: {e}\")\n",
        "    else:\n",
        "        print(f\"File not found: {path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhbyBsIFRYbQ",
        "outputId": "57595028-ab1c-4f04-980a-913c19fb6493"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(metadata={'source': '/content/Introduction to Artificial Intelligence ( PDFDrive ).pdf'}, page_content='Undergraduate Topics in Computer Science\\n\\nWolfgang Ertel\\n\\nIntroduction to Artificial Intelligence\\n\\nSecond Edition\\n\\nUndergraduate Topics in Computer Science\\n\\nSeries editor Ian Mackie\\n\\nAdvisory Board Samson Abramsky, University of Oxford, Oxford, UK Karin Breitman, Pontiﬁcal Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil Chris Hankin, Imperial College London, London, UK Dexter Kozen, Cornell University, Ithaca, USA Andrew Pitts, University of Cambridge, Cambridge, UK Hanne Riis Nielson, Technical University of Denmark, Kongens Lyngby, Denmark Steven Skiena, Stony Brook University, Stony Brook, USA Iain Stewart, University of Durham, Durham, UK\\n\\nUndergraduate Topics in Computer Science (UTiCS) delivers high-quality instruc- tional content for undergraduates studying in all areas of computing and information science. From core foundational and theoretical material to ﬁnal-year topics and applications, UTiCS books take a fresh, concise, and modern approach and are ideal for self-study or for a one- or two-semester course. The texts are all authored by established experts in their ﬁelds, reviewed by an international advisory board, and contain numerous examples and problems. Many include fully worked solutions.\\n\\nMore information about this series at http://www.springer.com/series/7592\\n\\nWolfgang Ertel\\n\\nIntroduction to Artificial Intelligence\\n\\nSecond Edition\\n\\nTranslated by Nathanael Black With illustrations by Florian Mast\\n\\n123\\n\\nWolfgang Ertel Hochschule Ravensburg-Weingarten Weingarten Germany\\n\\nISSN 1863-7310 Undergraduate Topics in Computer Science ISBN 978-3-319-58486-7 DOI 10.1007/978-3-319-58487-4\\n\\nISSN 2197-1781\\n\\n(electronic)\\n\\nISBN 978-3-319-58487-4\\n\\n(eBook)\\n\\nLibrary of Congress Control Number: 2017943187\\n\\n1st edition: © Springer-Verlag London Limited 2011 2nd edition: © Springer International Publishing AG 2017 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.\\n\\ntrademarks, service marks, etc.\\n\\nPrinted on acid-free paper\\n\\nThis Springer imprint is published by Springer Nature The registered company is Springer International Publishing AG The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland\\n\\nPreface to the Second Edition\\n\\nAfter 60 years, Artiﬁcial Intelligence (AI) has now reached industry and the con- sciousness of the population. The impressive successes and new AI methods are now so relevant that they should be taught even in a basic course. In about 30 new pages, I report mainly on deep learning, a consistent further development of neural networks, which ﬁnally enables image processing systems to recognize almost any object in pixel images. Among other beneﬁts, this lead to the ﬁrst computer pro- gram that could beat one of the world’s best Go players.\\n\\nIn the new section on Deep Learning, we must not leave out a short report about the fascinating new subarea of creativity. For the ﬁrst time neural networks can creatively generate texts, music pieces, and even paintings in the style of the old masters. These achievements are based on many years of research on neural net- works and machine learning. Practical AI has developed into an engineering dis- cipline in which programs are developed in large industrial teams by experts from various specializations.\\n\\nSelf-driving cars, service robots, and smart homes—which are all applications of AI—will greatly change our lives. However, in addition to great rays of hope, there will be a dark side. Though we live in a time of rapid technological progress, we have long since exceeded the limits of growth. We must therefore think about sustainability when implementing each new invention. In Chap. 1, I would like to give you some food for thought about this topic.\\n\\nOther new additions to the book include a section on performance evaluation of clustering algorithms and two practical examples explaining Bayes’ theorem and its relevance in everyday life. Finally, in a section on search algorithms, we analyze the cycle check, explain route planning for car navigation systems, and brieﬂy intro- duce Monte Carlo Tree Search.\\n\\nAll known errors have been corrected and updates have been made in many\\n\\nplaces.\\n\\nI would like to sincerely thank the readers who have given me feedback and all those who contributed to this new edition through proofreading and suggestions.\\n\\nv\\n\\nvi\\n\\nPreface to the Second Edition\\n\\nI would especially like to thank Adrian Batzill for the route planning measurements and graphs, as well as Nate Black, Nicole Dathe, Markus Schneider, Robin Leh- mann, Ankita Agrawal, Wenzel Massag, Lars Berge, Jonas Lang, and Richard Cubek.\\n\\nRavensburg March 2017\\n\\nWolfgang Ertel\\n\\nPreface to the First Edition\\n\\nArtiﬁcial Intelligence (AI) has the deﬁnite goal of understanding intelligence and building intelligent systems. However, the methods and formalisms used on the way to this goal are not ﬁrmly set, which has resulted in AI consisting of a multitude of subdisciplines today. The difﬁculty in an introductory AI course lies in conveying as many branches as possible without losing too much depth and precision.\\n\\nRussell and Norvig’s book [RN10] is more or less the standard introduction into AI. However, since this book has 1,152 pages, and since it is too extensive and costly for most students, the requirements for writing this book were clear: it should be an accessible introduction to modern AI for self-study or as the foundation of a four-hour lecture, with at most 300 pages. The result is in front of you.\\n\\nIn the space of 300 pages, a ﬁeld as extensive as AI cannot be fully covered. To avoid turning the book into a table of contents, I have attempted to go into some depth and to introduce concrete algorithms and applications in each of the following branches: agents, logic, search, reasoning with uncertainty, machine learning, and neural networks.\\n\\nThe ﬁelds of image processing, fuzzy logic, and natural language processing are not covered in detail. The ﬁeld of image processing, which is important for all of computer science, is a stand-alone discipline with very good textbooks, such as [GW08]. Natural language processing has a similar status. In recognizing and generating text and spoken language, methods from logic, probabilistic reasoning, and neural networks are applied. In this sense this ﬁeld is part of AI. On the other hand, computer linguistics is its own extensive branch of computer science and has much in common with formal languages. In this book we will point to such appropriate systems in several places, but not give a systematic introduction. For a ﬁrst introduction in this ﬁeld, we refer to Chaps. 22 and 23 in [RN10]. Fuzzy logic, or fuzzy set theory, has developed into a branch of control theory due to its primary application in automation technology and is covered in the corresponding books and lectures. Therefore we will forego an introduction here.\\n\\nThe dependencies between chapters of the book are coarsely sketched in the graph shown below. To keep it simple, Chap. 1, with the fundamental introduction for all further chapters, is left out. As an example, the thicker arrow from 2 to 3 means that propositional logic is a prerequisite for understanding predicate logic.\\n\\nvii\\n\\nviii\\n\\nPreface to the First Edition\\n\\nThe thin arrow from 9 to 10 means that neural networks are helpful for under- standing reinforcement learning, but not absolutely necessary. Thin backward arrows should make clear that later chapters can give more depth of understanding to topics which have already been learned.\\n\\nThis book is applicable to students of computer science and other technical natural sciences and, for the most part, requires high school level knowledge of mathe- matics. In several places, knowledge from linear algebra and multidimensional analysis is needed. For a deeper understanding of the contents, actively working on the exercises is indispensable. This means that the solutions should only be con- sulted after intensive work with each problem, and only to check one’s solutions, true to Leonardo da Vinci’s motto “Study without devotion damages the brain”. Somewhat more difﬁcult problems are marked with ❄, and especially difﬁcult ones with ❄❄. Problems which require programming or special computer science knowledge are labeled with ➳.\\n\\nOn the book’s web site at http://www.hs-weingarten.de/*ertel/aibook digital materials for the exercises such as training data for learning algorithms, a page with references to AI programs mentioned in the book, a list of links to the covered topics, a clickable list of the bibliography, an errata list, and presentation slides for lecturers can be found. I ask the reader to please send suggestions, criticisms, and tips about errors directly to ertel@hs-weingarten.de.\\n\\nThis book is an updated translation of my German book “Grundkurs Künstliche Intelligenz” published by Vieweg Verlag. My special thanks go to the translator Nathan Black who in an excellent trans-Atlantic cooperation between Germany and California via SVN, Skype and Email produced this text. I am grateful to Franz Kurfeß, who introduced me to Nathan; to MatthewWight for proofreading the translated book and to Simon Rees from Springer Verlag for his patience.\\n\\nI would like to thank my wife Evelyn for her support and patience during this time consuming project. Special thanks go to Wolfgang Bibel and Chris Loben- schuss, who carefully corrected the German manuscript. Their suggestions and discussions lead to many improvements and additions. For reading the corrections and other valuable services, I would like to thank Richard Cubek, Celal Döven, Joachim Feßler, Nico Hochgeschwender, Paul Kirner, Wilfried Meister, Norbert Perk, Peter Radtke, Markus Schneider, Manfred Schramm, Uli Stärk, Michel Tokic, Arne Usadel and all interested students. My thanks also go out to Florian Mast for the priceless cartoons and very effective collaboration.\\n\\nI hope that during your studies this book will help you share my fascination with\\n\\nArtiﬁcial Intelligence.\\n\\nRavensburg February 2011\\n\\nWolfgang Ertel\\n\\nContents\\n\\n1\\n\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . What Is Artiﬁcial Intelligence? . . . . . . . . . . . . . . . . . . . . . . . . . 1.1 Brain Science and Problem Solving. . . . . . . . . . . . . . . 1.1.1 1.1.2 The Turing Test and Chatterbots . . . . . . . . . . . . . . . . . The History of AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The First Beginnings . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.1 Logic Solves (Almost) All Problems . . . . . . . . . . . . . . 1.2.2 The New Connectionism . . . . . . . . . . . . . . . . . . . . . . . 1.2.3 Reasoning Under Uncertainty . . . . . . . . . . . . . . . . . . . 1.2.4 Distributed, Autonomous and Learning Agents . . . . . . 1.2.5 AI Grows Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.6 1.2.7 The AI Revolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . AI and Society . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Does AI Destroy Jobs? . . . . . . . . . . . . . . . . . . . . . . . . 1.3.1 AI and Transportation . . . . . . . . . . . . . . . . . . . . . . . . . 1.3.2 1.3.3 Service Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Knowledge-Based Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n1.2\\n\\n1.3\\n\\n1.4 1.5 1.6\\n\\n2\\n\\nPropositional Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Syntax. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Proof Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Resolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Horn Clauses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5 Computability and Complexity . . . . . . . . . . . . . . . . . . . . . . . . . 2.6 Applications and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . 2.7 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.8\\n\\n1 1 3 5 5 7 8 9 9 10 11 11 11 11 14 15 17 19 20\\n\\n23 23 24 26 30 33 36 37 37\\n\\nix\\n\\nx\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\nContents\\n\\nFirst-order Predicate Logic. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Syntax. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Quantiﬁers and Normal Forms . . . . . . . . . . . . . . . . . . . . . . . . . Proof Calculi. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Resolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.1 Resolution Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.2 Automated Theorem Provers. . . . . . . . . . . . . . . . . . . . . . . . . . . Mathematical Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n3.3 3.4 3.5\\n\\n3.6 3.7 3.8 3.9 3.10\\n\\n39 40 41 45 45 49 51 55 55 56 57 60 63 63\\n\\nLimitations of Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The Search Space Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 Decidability and Incompleteness . . . . . . . . . . . . . . . . . . . . . . . . 4.2 The Flying Penguin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Modeling Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5\\n\\n65 65 67 69 71 73\\n\\nLogic Programming with PROLOG . . . . . . . . . . . . . . . . . . . . . . . . . PROLOG Systems and Implementations. . . . . . . . . . . . . . . . . . 5.1 Simple Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Execution Control and Procedural Elements . . . . . . . . . . . . . . . 5.3 Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Self-modifying Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5 A Planning Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6 Constraint Logic Programming . . . . . . . . . . . . . . . . . . . . . . . . . 5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.8 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.9\\n\\n75 76 76 79 81 82 83 85 87 88\\n\\nSearch, Games and Problem Solving . . . . . . . . . . . . . . . . . . . . . . . . . 91 91 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1 97 Uninformed Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Breadth-First Search . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.1 97 99 Depth-First Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.2 Iterative Deepening . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 6.2.3 Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 6.2.4 6.2.5 Cycle Check . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 Heuristic Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 Greedy Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 6.3.1 ★-Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 A 6.3.2 Route Planning with the A★ Search Algorithm . . . . . . 109 6.3.3\\n\\n6.3\\n\\nContents\\n\\n6.4\\n\\n6.5\\n\\n6.6\\n\\nIDA★-Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 6.3.4 Empirical Comparison of the Search Algorithms . . . . . 111 6.3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 6.3.6 Games with Opponents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 6.4.1 Minimax Search. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 Alpha-Beta-Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 6.4.2 6.4.3 Non-deterministic Games. . . . . . . . . . . . . . . . . . . . . . . 117 Heuristic Evaluation Functions . . . . . . . . . . . . . . . . . . . . . . . . . 118 Learning of Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . 118 6.5.1 State of the Art . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 Chess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 6.6.1 Go . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 6.6.2 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n\\n6.7\\n\\n7\\n\\nReasoning with Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 Computing with Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . 127 7.1 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . 130 7.1.1 The Principle of Maximum Entropy . . . . . . . . . . . . . . . . . . . . . 136 7.2.1 An Inference Rule for Probabilities . . . . . . . . . . . . . . . 136 7.2.2 Maximum Entropy Without Explicit Constraints . . . . . 141 7.2.3\\n\\n7.2\\n\\nConditional Probability Versus Material Implication. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 7.2.4 MaxEnt-Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 The Tweety Example. . . . . . . . . . . . . . . . . . . . . . . . . . 144 7.2.5 LEXMED, an Expert System for Diagnosing Appendicitis . . . . . . 145 Appendicitis Diagnosis with Formal Methods . . . . . . . 145 7.3.1 Hybrid Probabilistic Knowledge Base . . . . . . . . . . . . . 146 7.3.2 Application of LEXMED. . . . . . . . . . . . . . . . . . . . . . . . . 149 7.3.3 Function of LEXMED . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 7.3.4 Risk Management Using the Cost Matrix . . . . . . . . . . 153 7.3.5 Performance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 7.3.6 Application Areas and Experiences . . . . . . . . . . . . . . . 157 7.3.7 Reasoning with Bayesian Networks . . . . . . . . . . . . . . . . . . . . . 158 Independent Variables . . . . . . . . . . . . . . . . . . . . . . . . . 158 7.4.1 Graphical Representation of Knowledge as a 7.4.2 Bayesian Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 Conditional Independence . . . . . . . . . . . . . . . . . . . . . . 160 7.4.3 Practical Application . . . . . . . . . . . . . . . . . . . . . . . . . . 162 7.4.4 Software for Bayesian Networks . . . . . . . . . . . . . . . . . 163 7.4.5 Development of Bayesian Networks . . . . . . . . . . . . . . 165 7.4.6 Semantics of Bayesian Networks . . . . . . . . . . . . . . . . . 168 7.4.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\\n\\n7.3\\n\\n7.4\\n\\n7.5 7.6\\n\\nxi\\n\\nxii\\n\\nContents\\n\\n8 Machine Learning and Data Mining . . . . . . . . . . . . . . . . . . . . . . . . . 175 Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180 The Perceptron, a Linear Classiﬁer . . . . . . . . . . . . . . . . . . . . . . 183 The Learning Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 8.2.1 Optimization and Outlook . . . . . . . . . . . . . . . . . . . . . . 188 8.2.2 The Nearest Neighbor Method . . . . . . . . . . . . . . . . . . . . . . . . . 189 Two Classes, Many Classes, Approximation . . . . . . . . 193 8.3.1 Distance Is Relevant . . . . . . . . . . . . . . . . . . . . . . . . . . 194 8.3.2 Computation Times . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 8.3.3 Summary and Outlook. . . . . . . . . . . . . . . . . . . . . . . . . 196 8.3.4 Case-Based Reasoning. . . . . . . . . . . . . . . . . . . . . . . . . 197 8.3.5 Decision Tree Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 A Simple Example. . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 8.4.1 Entropy as a Metric for Information Content . . . . . . . . 200 8.4.2 Information Gain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 8.4.3 Application of C4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 8.4.4 Learning of Appendicitis Diagnosis . . . . . . . . . . . . . . . 207 8.4.5 Continuous Attributes . . . . . . . . . . . . . . . . . . . . . . . . . 210 8.4.6 Pruning—Cutting the Tree. . . . . . . . . . . . . . . . . . . . . . 211 8.4.7 8.4.8 Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 8.4.9 Cross-Validation and Overﬁtting. . . . . . . . . . . . . . . . . . . . . . . . 213 Learning of Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . 215 Learning the Network Structure. . . . . . . . . . . . . . . . . . 215 8.6.1 The Naive Bayes Classiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218 Text Classiﬁcation with Naive Bayes . . . . . . . . . . . . . 220 8.7.1 One-Class Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222 Nearest Neighbor Data Description . . . . . . . . . . . . . . . 223 8.8.1 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 Distance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 8.9.1 k-Means and the EM Algorithm . . . . . . . . . . . . . . . . . 226 8.9.2 Hierarchical Clustering . . . . . . . . . . . . . . . . . . . . . . . . 228 8.9.3 How is the Number of Clusters Determined? . . . . . . . 230 8.9.4 Data Mining in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 8.10.1 The Data Mining Tool KNIME . . . . . . . . . . . . . . . . . . 233 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\\n\\n8.1 8.2\\n\\n8.3\\n\\n8.4\\n\\n8.5 8.6\\n\\n8.7\\n\\n8.8\\n\\n8.9\\n\\n8.10\\n\\n8.11 8.12\\n\\n9\\n\\nNeural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 From Biology to Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 9.1 The Mathematical Model . . . . . . . . . . . . . . . . . . . . . . . 247 9.1.1 Hopﬁeld Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\\n\\n9.2\\n\\nContents\\n\\n9.3\\n\\n9.4\\n\\n9.5\\n\\n9.6 9.7\\n\\n9.8 9.9 9.10 9.11\\n\\nApplication to a Pattern Recognition Example . . . . . . 251 9.2.1 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252 9.2.2 Summary and Outlook. . . . . . . . . . . . . . . . . . . . . . . . . 255 9.2.3 Neural Associative Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . 256 Correlation Matrix Memory . . . . . . . . . . . . . . . . . . . . . 257 9.3.1 The Binary Hebb Rule. . . . . . . . . . . . . . . . . . . . . . . . . 259 9.3.2 9.3.3 A Spelling Correction Program . . . . . . . . . . . . . . . . . . 261 Linear Networks with Minimal Errors . . . . . . . . . . . . . . . . . . . 263 Least Squares Method . . . . . . . . . . . . . . . . . . . . . . . . . 264 9.4.1 Application to the Appendicitis Data . . . . . . . . . . . . . . 265 9.4.2 The Delta Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266 9.4.3 Comparison to the Perceptron . . . . . . . . . . . . . . . . . . . 268 9.4.4 The Backpropagation Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 269 NETtalk: A Network Learns to Speak . . . . . . . . . . . . . 272 9.5.1 Learning of Heuristics for Theorem Provers . . . . . . . . 273 9.5.2 Problems and Improvements . . . . . . . . . . . . . . . . . . . . 274 9.5.3 Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 Nature as Example. . . . . . . . . . . . . . . . . . . . . . . . . . . . 278 9.7.1 Stacked Denoising Autoencoder . . . . . . . . . . . . . . . . . 279 9.7.2 Other Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280 9.7.3 Systems and Implementations . . . . . . . . . . . . . . . . . . . 281 9.7.4 Applications of Deep Learning . . . . . . . . . . . . . . . . . . 281 9.7.5 Creativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 Applications of Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 284 Summary and Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286\\n\\n10 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289 The Task. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 Uninformed Combinatorial Search . . . . . . . . . . . . . . . . . . . . . . 293 Value Iteration and Dynamic Programming . . . . . . . . . . . . . . . 295 A Learning Walking Robot and Its Simulation . . . . . . . . . . . . . 298 Q-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300 10.6.1 Q-Learning in a Nondeterministic Environment. . . . . . 303 Exploration and Exploitation. . . . . . . . . . . . . . . . . . . . . . . . . . . 304 10.7 Approximation, Generalization and Convergence . . . . . . . . . . . 305 10.8 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306 10.9 10.10 AlphaGo, the Breakthrough in Go . . . . . . . . . . . . . . . . . . . . . . 306 10.11 Curse of Dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309 10.12 Summary and Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310 10.13 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\\n\\n10.1 10.2 10.3 10.4 10.5 10.6\\n\\nxiii\\n\\nxiv\\n\\n11 Solutions for the Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 11.1 Propositional Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314 11.2 First-Order Predicate Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 11.3 Limitations of Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 11.4 PROLOG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 11.5 Search, Games and Problem Solving . . . . . . . . . . . . . . . . . . . . 319 11.6 Reasoning with Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . 322 11.7 11.8 Machine Learning and Data Mining . . . . . . . . . . . . . . . . . . . . . 329 11.9 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335 11.10 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\\n\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .\\n\\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .\\n\\nContents\\n\\n339\\n\\n351\\n\\nIntroduction\\n\\n1.1 What Is Artificial Intelligence?\\n\\nThe term artiﬁcial intelligence stirs emotions. For one thing there is our fascination with intelligence, which seemingly imparts to us humans a special place among life forms. Questions arise such as “What is intelligence?”, “How can one measure intelligence?” or “How does the brain work?”. All these questions are meaningful when trying to understand artiﬁcial intelligence. However, the central question for the engineer, especially for the computer scientist, is the question of the intelligent machine that behaves like a person, showing intelligent behavior.\\n\\nThe attribute artiﬁcial might awaken much different associations. It brings up fears of intelligent cyborgs. It recalls images from science ﬁction novels. It raises the question of whether our highest good, the soul, is something we should try to understand, model, or even reconstruct.\\n\\nWith such different offhand interpretations, it becomes difﬁcult to deﬁne the term artiﬁcial intelligence or AI simply and robustly. Nevertheless I would like to try, using examples and historical deﬁnitions, to characterize the ﬁeld of AI. In 1955, John McCarthy, one of the pioneers of AI, was the ﬁrst to deﬁne the term artiﬁcial intelligence, roughly as follows:\\n\\nThe goal of AI is to develop machines that behave as though they were intelligent.\\n\\nTo test this deﬁnition, the reader might imagine the following scenario. Fifteen or so small robotic vehicles are moving on an enclosed four by four meter square surface. One can observe various behavior patterns. Some vehicles form small groups with relatively little movement. Others move peacefully through the space and gracefully avoid any collision. Still others appear to follow a leader. Aggressive behaviors are also observable. Is what we are seeing intelligent behavior?\\n\\nAccording to McCarthy’s deﬁnition the aforementioned robots can be described as intelligent. The psychologist Valentin Braitenberg has shown that this seemingly\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_1\\n\\n1\\n\\n1\\n\\n2\\n\\n1 Introduction\\n\\nFig. 1.1 Two very simple Braitenberg vehicles and their reactions to a light source\\n\\ncomplex behavior can be produced by very simple electrical circuits [Bra84]. So-called Braitenberg vehicles have two wheels, each of which is driven by an independent electric motor. The speed of each motor is inﬂuenced by a light sensor on the front of the vehicle as shown in Fig. 1.1. The more light that hits the sensor, the faster the motor runs. Vehicle 1 in the left part of the ﬁgure, according to its conﬁguration, moves away from a point light source. Vehicle 2 on the other hand moves toward the light source. Further small modiﬁcations can create other behavior patterns, such that with these very simple vehicles we can realize the impressive behavior described above.\\n\\nClearly the above deﬁnition is insufﬁcient because AI has the goal of solving difﬁcult practical problems which are surely too demanding for the Braitenberg vehicle. In the Encyclopedia Britannica [Bri91] one ﬁnds a Deﬁnition that goes like:\\n\\nAI is the ability of digital computers or computer controlled robots to solve problems that are normally associated with the higher intellectual processing capabilities of humans …\\n\\nBut this deﬁnition also has weaknesses. It would admit for example that a computer with large memory that can save a long text and retrieve it on demand displays intelligent capabilities, for memorization of long texts can certainly be considered a higher intellectual processing capability of humans, as can for example the quick multiplication of two 20-digit numbers. According to this deﬁnition, then, every computer is an AI system. This dilemma is solved elegantly by the following deﬁnition by Elaine Rich [Ric83]:\\n\\nArtiﬁcial Intelligence is the study of how to make computers do things at which, at the moment, people are better.\\n\\nRich, tersely and concisely, characterizes what AI researchers have been doing for the last 50 years. Even in the year 2050, this deﬁnition will be up to date.\\n\\nTasks such as the execution of many computations in a short amount of time are the strong points of digital computers. In this regard they outperform humans by many multiples. In many other areas, however, humans are far superior to machines. For instance, a person entering an unfamiliar room will recognize the surroundings within fractions of a second and, if necessary, just as swiftly make decisions and plan actions. To date, this task is too demanding for autonomous1\\n\\n1An autonomous robot works independently, without manual support, in particular without remote control.\\n\\n1.1 What Is Artificial Intelligence?\\n\\nrobots. According to Rich’s deﬁnition, this is therefore a task for AI. In fact, research on autonomous robots is an important, current theme in AI. Construction of chess computers, on the other hand, has lost relevance because they already play at or above the level of grandmasters.\\n\\nIt would be dangerous, however, to conclude from Rich’s deﬁnition that AI is only concerned with the pragmatic implementation of intelligent processes. Intel- ligent systems, in the sense of Rich’s deﬁnition, cannot be built without a deep understanding of human reasoning and intelligent action in general, because of which neuroscience (see Sect. 1.1.1) is of great importance to AI. This also shows that the other cited deﬁnitions reﬂect important aspects of AI.\\n\\nA particular strength of human intelligence is adaptivity. We are capable of adjusting to various environmental conditions and change our behavior accordingly through learning. Precisely because our learning ability is so vastly superior to that of computers, machine learning is, according to Rich’s deﬁnition, a central subﬁeld of AI.\\n\\n1.1.1 Brain Science and Problem Solving\\n\\nThrough research of intelligent systems we can try to understand how the human brain works and then model or simulate it on the computer. Many ideas and principles in the ﬁeld of neural networks (see Chap. 9) stem from brain science with the related ﬁeld of neuroscience.\\n\\nA very different approach results from taking a goal-oriented line of action, starting from a problem and trying to ﬁnd the most optimal solution. How humans solve the problem is treated as unimportant here. The method, in this approach, is secondary. First and foremost is the optimal intelligent solution to the problem. Rather than employing a ﬁxed method (such as, for example, predicate logic) AI has as its constant goal the creation of intelligent agents for as many different tasks as possible. Because the tasks may be very different, it is unsurprising that the methods currently employed in AI are often also quite different. Similar to medi- cine, which encompasses many different, often life-saving diagnostic and therapy procedures, AI also offers a broad palette of effective solutions for widely varying applications. For mental inspiration, consider Fig. 1.2 on page 4. Just as in medi- cine, there is no universal method for all application areas of AI, rather a great number of possible solutions for the great number of various everyday problems, big and small.\\n\\nCognitive science is devoted to research into human thinking at a somewhat higher level. Similarly to brain science, this ﬁeld furnishes practical AI with many important ideas. On the other hand, algorithms and implementations lead to further important conclusions about how human reasoning functions. Thus these three ﬁelds beneﬁt from a fruitful interdisciplinary exchange. The subject of this book, however, is primarily problem-oriented AI as a subdiscipline of computer science. There are many interesting philosophical questions surrounding intelligence and artiﬁcial intelligence. We humans have consciousness; that is, we can think about\\n\\n3\\n\\n4\\n\\n1 Introduction\\n\\nFig. 1.2 A small sample of the solutions offered by AI\\n\\nourselves and even ponder that we are able to think about ourselves. How does consciousness come to be? Many philosophers and neurologists now believe that the mind and consciousness are linked with matter, that is, with the brain. The\\n\\n1.1 What Is Artificial Intelligence?\\n\\nquestion of whether machines could one day have a mind or consciousness could at some point in the future become relevant. The mind-body problem in particu- lar concerns whether or not the mind is bound to the body. We will not discuss these questions here. The interested reader may consult [Spe98, Spe97] and is invited, in the course of AI technology studies, to form a personal opinion about these questions.\\n\\n1.1.2 The Turing Test and Chatterbots\\n\\nAlan Turing made a name for himself as an early pioneer of AI with his deﬁnition of an intelligent machine, in which the machine in question must pass the following test. The test person Alice sits in a locked room with two computer terminals. One terminal is connected to a machine, the other with a non-malicious person Bob. Alice can type questions into both terminals. She is given the task of deciding, after ﬁve minutes, which terminal belongs to the machine. The machine passes the test if it can trick Alice at least 30% of the time [Tur50].\\n\\nWhile the test is very interesting philosophically, for practical AI, which deals with problem solving, it is not a very relevant test. The reasons for this are similar to those mentioned above related to Braitenberg vehicles (see Exercise 1.3 on page 21).\\n\\nThe AI pioneer and social critic Joseph Weizenbaum developed a program named Eliza, which is meant to answer a test subject’s questions like a human psychologist [Wei66]. He was in fact able to demonstrate success in many cases. Supposedly his secretary often had long discussions with the program. Today in the internet there are many so-called chatterbots, some of whose initial responses are quite impressive. After a certain amount of time, however, their artiﬁcial nature becomes apparent. Some of these programs are actually capable of learning, while others possess extraordinary knowledge of various subjects, for example geography or software development. There are already commercial applications for chatterbots in online customer support and there may be others in the ﬁeld of e-learning. It is conceivable that the learner and the e-learning system could communicate through a chatterbot. The reader may wish to compare several chatterbots and evaluate their intelligence in Exercise 1.1 on page 20.\\n\\n1.2 The History of AI\\n\\nAI draws upon many past scientiﬁc achievements which are not mentioned here, for AI as a science in its own right has only existed since the middle of the Twentieth Century. Table 1.1 on page 6, with the most important AI milestones, and a graphical representation of the main movements of AI in Fig. 1.3 on page 8 complement the following text.\\n\\n5\\n\\n6\\n\\n1 Introduction\\n\\nTable 1.1 Milestones in the development of AI from Gödel to today\\n\\n1931\\n\\n1937\\n\\nThe Austrian Kurt Gödel shows that in ﬁrst-order predicate logic all true statements are derivable [Göd31a]. In higher-order logics, on the other hand, there are true statements that are unprovable [Göd31b]. (In [Göd31b] Gödel showed that predicate logic extended with the axioms of arithmetic is incomplete.) Alan Turing points out the limits of intelligent machines with the halting problem [Tur37].\\n\\n1943 McCulloch and Pitts model neural networks and make the connection to propositional\\n\\n1950\\n\\nlogic. Alan Turing deﬁnes machine intelligence with the Turing test and writes about learning machines and genetic algorithms [Tur50].\\n\\n1951 Marvin Minsky develops a neural network machine. With 3000 vacuum tubes he\\n\\n1955\\n\\nsimulates 40 neurons. Arthur Samuel (IBM) builds a learning checkers program that plays better than its developer [Sam59].\\n\\n1956 McCarthy organizes a conference in Dartmouth College. Here the name Artiﬁcial\\n\\nIntelligence was ﬁrst introduced. Newell and Simon of Carnegie Mellon University (CMU) present the Logic Theorist, the ﬁrst symbol-processing computer program [NSS83].\\n\\n1958 McCarthy invents at MIT (Massachusetts Institute of Technology) the high-level\\n\\n1959 1961\\n\\nlanguage LISP. He writes programs that are capable of modifying themselves. Gelernter (IBM) builds the Geometry Theorem Prover. The General Problem Solver (GPS) by Newell and Simon imitates human thought [NS61].\\n\\n1963 McCarthy founds the AI Lab at Stanford University. 1965 1966 Weizenbaum’s program Eliza carries out dialog with people in natural\\n\\nRobinson invents the resolution calculus for predicate logic [Rob65] (Sect. 3.5).\\n\\nlanguage [Wei66] (Sect. 1.1.2).\\n\\n1969 Minsky and Papert show in their book Perceptrons that the perceptron, a very simple\\n\\n1972\\n\\n1976\\n\\n1981\\n\\n1982\\n\\n1986\\n\\n1990\\n\\nneural network, can only represent linear functions [MP69] (Sect. 1.1.2). French scientist Alain Colmerauer invents the logic programming language PROLOG (Chap. 5). British physician de Dombal develops an expert system for diagnosis of acute abdominal pain [dDLS+72]. It goes unnoticed in the mainstream AI community of the time (Sect. 7.3). Shortliffe and Buchanan develop MYCIN, an expert system for diagnosis of infectious diseases, which is capable of dealing with uncertainty (Chap. 7). Japan begins, at great expense, the “Fifth Generation Project” with the goal of building a powerful PROLOG machine. R1, the expert system for conﬁguring computers, saves Digital Equipment Corporation 40 million dollars per year [McD82]. Renaissance of neural networks through, among others, Rumelhart, Hinton and Sejnowski [RM86]. The system Nettalk learns to read texts aloud [SR86] (Chap. 9). Pearl [Pea88], Cheeseman [Che85], Whittaker, Spiegelhalter bring probability theory into AI with Bayesian networks (Sect. 7.4). Multi-agent systems become popular.\\n\\n(continued)\\n\\n1.2 The History of AI\\n\\nTable 1.1 (continued)\\n\\n1992\\n\\nTesauros TD-gammon program demonstrates the advantages of reinforcement learning.\\n\\n1993 Worldwide RoboCup initiative to build soccer-playing autonomous robots [Roba]. 1995\\n\\nFrom statistical learning theory, Vapnik develops support vector machines, which are very important today. IBM’s chess computer Deep Blue defeats the chess world champion Gary Kasparov. First international RoboCup competition in Japan. The robots in RoboCup demonstrate impressively what AI and robotics are capable of achieving. Service robotics becomes a major AI research area. First Google self-driving car drives on the California freeway. Autonomous robots begin to improve their behavior through learning. IBM’s “Watson” beats two human champions on the television game show “Jeopardy!”. Watson understands natural language and can answer difﬁcult questions very quickly (Sect. 1.4). Daimler premiers the ﬁrst autonomous truck on the Autobahn. Google self-driving cars have driven over one million miles and operate within cities. Deep learning (Sect. 11.9) enables very good image classiﬁcation. Paintings in the style of the Old Masters can be automatically generated with deep learning. AI becomes creative! The Go program AlphaGo by Google DeepMind [SHM+16] beats the European champion 5:0 in January and Korean Lee Sedol, one of the world’s best Go players, 4:1 in March. Deep learning techniques applied to pattern recognition, as well as reinforcement learning and Monte Carlo tree search lead to this success.\\n\\n1997\\n\\n2003\\n\\n2006 2009 2010 2011\\n\\n2015\\n\\n2016\\n\\n1.2.1 The First Beginnings\\n\\nIn the 1930s Kurt Gödel, Alonso Church, and Alan Turing laid important foundations for logic and theoretical computer science. Of particular interest for AI are Gödel’s theorems. The completeness theorem states that ﬁrst-order predicate logic is com- plete. This means that every true statement that can be formulated in predicate logic is provable using the rules of a formal calculus. On this basis, automatic theorem provers could later be constructed as implementations of formal calculi. With the incom- pleteness theorem, Gödel showed that in higher-order logics there exist true state- ments that are unprovable.2 With this he uncovered painful limits of formal systems. Alan Turing’s proof of the undecidability of the halting problem also falls into this time period. He showed that there is no program that can decide whether a given arbitrary program (and its respective input) will run in an inﬁnite loop. With\\n\\n2Higher-order logics are extensions of predicate logic, in which not only variables, but also function symbols or predicates can appear as terms in a quantiﬁcation. Indeed, Gödel only showed that any system that is based on predicate logic and can formulate Peano arithmetic is incomplete.\\n\\n7\\n\\n8\\n\\n1 Introduction\\n\\npower of representation\\n\\nc i l o b m y s\\n\\nGödel\\n\\nTuring\\n\\nfirst order logic\\n\\nDartmounth conference\\n\\nLISP\\n\\nresolution\\n\\nGPS\\n\\nautomated theorem provers PTTP, Otter, SETHEO, E−prover\\n\\nPROLOG\\n\\nheuristic search\\n\\nplanning in robotics\\n\\nJaynes\\n\\nBayesian networks\\n\\nprobabilistic reasoning\\n\\ndecision tree learning\\n\\nHunt\\n\\nID3, CART\\n\\nC4.5\\n\\nZadeh\\n\\nfuzzy logic\\n\\npropositional logic\\n\\nDavis/Putnam\\n\\nhybrid systems\\n\\ndeep learning\\n\\nc i r e m u n\\n\\nneural networks\\n\\nneuro− hardware\\n\\nMinsky/Papert book\\n\\nbackpropagation\\n\\nsupport vector machines\\n\\n1930\\n\\n1940\\n\\n1950\\n\\n1960\\n\\n1970 year\\n\\n1980\\n\\n1990\\n\\n2000\\n\\n2010\\n\\nFig. 1.3 History of the various AI areas. The width of the bars indicates prevalence of the method’s use\\n\\nthis Turing also identiﬁed a limit for intelligent programs. It follows, for example, that there will never be a universal program veriﬁcation system.3\\n\\nIn the 1940s, based on results from neuroscience, McCulloch, Pitts and Hebb designed the ﬁrst mathematical models of neural networks. However, computers at that time lacked sufﬁcient power to simulate simple brains.\\n\\n1.2.2 Logic Solves (Almost) All Problems\\n\\nAI as a practical science of thought mechanization could of course only begin once there were programmable computers. This was the case in the 1950s. Newell and Simon introduced Logic Theorist, the ﬁrst automatic theorem prover, and thus also showed that with computers, which actually only work with numbers, one can also process symbols. At the same time McCarthy introduced, with the language LISP, a programming language specially created for the processing of symbolic structures. Both of these systems were introduced in 1956 at the historic Dartmouth Conference, which is considered the birthday of AI.\\n\\nIn the US, LISP developed into the most important tool for the implementation of symbol-processing AI systems. Thereafter the logical inference rule known as resolution developed into a complete calculus for predicate logic.\\n\\n3This statement applies to “total correctness”, which implies a proof of correct execution as well as a proof of termination for every valid input.\\n\\n1.2 The History of AI\\n\\nIn the 1970s the logic programming language PROLOG was introduced as the European counterpart to LISP. PROLOG offers the advantage of allowing direct programming using Horn clauses, a subset of predicate logic. Like LISP, PROLOG has data types for convenient processing of lists.\\n\\nUntil well into the 1980s, a breakthrough spirit dominated AI, especially among many logicians. The reason for this was the string of impressive achievements in symbol processing. With the Fifth Generation Computer Systems project in Japan and the ESPRIT program in Europe, heavy investment went into the construction of intelligent computers.\\n\\nFor small problems, automatic provers and other symbol-processing systems sometimes worked very well. The combinatorial explosion of the search space, however, deﬁned a very narrow window for these successes. This phase of AI was described in [RN10] as the “Look, Ma, no hands!” era.\\n\\nBecause the economic success of AI systems fell short of expectations, funding for logic-based AI research in the United States fell dramatically during the 1980s.\\n\\n1.2.3 The New Connectionism\\n\\nDuring this phase of disillusionment, computer scientists, physicists, and Cognitive scientists were able to show, using computers which were now sufﬁciently pow- erful, that mathematically modeled neural networks are capable of learning using training examples, to perform tasks which previously required costly programming. Because of the fault-tolerance of such systems and their ability to recognize pat- terns, considerable successes became possible, especially in pattern recognition. Facial recognition in photos and handwriting recognition are two example appli- cations. The system Nettalk was able to learn speech from example texts [SR86]. Under the name connectionism, a new subdiscipline of AI was born.\\n\\nConnectionism boomed and the subsidies ﬂowed. But soon even here feasibility limits became obvious. The neural networks could acquire impressive capabilities, but it was usually not possible to capture the learned concept in simple formulas or logical rules. Attempts to combine neural nets with logical rules or the knowledge of human experts met with great difﬁculties. Additionally, no satisfactory solution to the structuring and modularization of the networks was found.\\n\\n1.2.4 Reasoning Under Uncertainty\\n\\nAI as a practical, goal-driven science searched for a way out of this crisis. One wished to unite logic’s ability to explicitly represent knowledge with neural net- works’ strength in handling uncertainty. Several alternatives were suggested.\\n\\nThe most promising, probabilistic reasoning, works with conditional probabil- ities for propositional calculus formulas. Since then many diagnostic and expert systems have been built for problems of everyday reasoning using Bayesian\\n\\n9\\n\\n10\\n\\n1 Introduction\\n\\nnetworks. The success of Bayesian networks stems from their intuitive compre- hensibility, the clean semantics of conditional probability, and from the centuries-old, mathematically grounded probability theory.\\n\\nThe weaknesses of logic, which can only work with two truth values, can be solved by fuzzy logic, which pragmatically introduces inﬁnitely many values between zero and one. Though even today its theoretical foundation is not totally ﬁrm, it is being successfully utilized, especially in control engineering.\\n\\nA much different path led to the successful synthesis of logic and neural net- works under the name hybrid systems. For example, neural networks were employed to learn heuristics for reduction of the huge combinatorial search space in proof discovery [SE90].\\n\\nMethods of decision tree learning from data also work with probabilities. Systems like CART, ID3 and C4.5 can quickly and automatically build very accurate decision trees which can represent propositional logic concepts and then be used as expert systems. Today they are a favorite among machine learning techniques (Sect. 8.4).\\n\\nSince about 1990, data mining has developed as a subdiscipline of AI in the area of statistical data analysis for extraction of knowledge from large databases. Data mining brings no new techniques to AI, rather it introduces the requirement of using large databases to gain explicit knowledge. One application with great market potential is steering ad campaigns of big businesses based on analysis of many millions of purchases by their customers. Typically, machine learning techniques such as decision tree learning come into play here.\\n\\n1.2.5 Distributed, Autonomous and Learning Agents\\n\\nDistributed artiﬁcial intelligence, DAI, has been an active area research since about 1985. One of its goals is the use of parallel computers to increase the efﬁciency of problem solvers. It turned out, however, that because of the high computational complexity of most problems, the use of “intelligent” systems is more beneﬁcial than parallelization itself.\\n\\nA very different conceptual approach results from the development of autonomous software agents and robots that are meant to cooperate like human teams. As with the aforementioned Braitenberg vehicles, there are many cases in which an individual agent is not capable of solving a problem, even with unlimited resources. Only the cooperation of many agents leads to the intelligent behavior or to the solution of a problem. An ant colony or a termite colony is capable of erecting buildings of very high architectural complexity, despite the fact that no single ant comprehends how the whole thing ﬁts together. This is similar to the situation of provisioning bread for a large city like New York [RN10]. There is no central planning agency for bread, rather there are hundreds of bakers that know their respective areas of the city and bake the appropriate amount of bread at those locations.\\n\\nActive skill acquisition by robots is an exciting area of current research. There are robots today, for example, that independently learn to walk or to perform\\n\\n1.2 The History of AI\\n\\nvarious motorskills related to soccer (Chap. 10). Cooperative learning of multiple robots to solve problems together is still in its infancy.\\n\\n1.2.6 AI Grows Up\\n\\nThe above systems offered by AI today are not a universal recipe, but a workshop with a manageable number of tools for very different tasks. Most of these tools are well-developed and are available as ﬁnished software libraries, often with conve- nient user interfaces. The selection of the right tool and its sensible use in each individual case is left to the AI developer or knowledge engineer. Like any other artisanship, this requires a solid education, which this book is meant to promote. More than nearly any other science, AI is interdisciplinary, for it draws upon interesting discoveries from such diverse ﬁelds as logic, operations research, statistics, control engineering, image processing, linguistics, philosophy, psychol- ogy, and neurobiology. On top of that, there is the subject area of the particular application. To successfully develop an AI project is therefore not always so simple, but almost always extremely exciting.\\n\\n1.2.7 The AI Revolution\\n\\nAround the year 2010 after about 25 years of research on neural networks, scientists could start harvesting the fruits of their research. The very powerful deep learning networks can for example learn to classify images with very high arruracy. Since image classiﬁcation is of crucial importance for all types of smart robots, this initiated the AI revolution which in turn leads to smart self-driving cars and service robots.\\n\\n1.3 AI and Society\\n\\nThere have been many scientiﬁc books and science ﬁction novels written on all aspects of this subject. Due to great advances in AI research, we have been on the brink of the age of autonomous robots and the Internet of Things since roughly 2005. Thus we are increasingly confronted with AI in everyday life. The reader, who may soon be working as an AI developer, must also deal with the social impact of this work. As an author of a book on AI techniques, I have the crucial task of examining this topic. I would like to deal with some particularly important aspects of AI which are of great practical relevance for our lives.\\n\\n1.3.1 Does AI Destroy Jobs?\\n\\nIn January 2016, the World Econonic Forum published a study [SS16], frequently cited by the German press, predicting that “industry 4.0 ” would destroy over ﬁve\\n\\n11\\n\\n12\\n\\n1 Introduction\\n\\nmillion jobs in the next ﬁve years. This forecast is hardly surprising because auto- mation in factories, ofﬁces, administration, transportation, in the home and in many other areas has led to continually more work being done by computers, machines and robots. AI has been one of the most important factors in this trend since about 2010. Presumably, the majority of people would gladly leave physically hard, dirty and unhealthy jobs and tasks to machines. Thus automation is a complete blessing for humanity, assuming it does not result in negative side effects, such as harm to the environment. Many of the aforementioned unpleasant jobs can be done faster, more precisely, and above all cheaper by machines. This seems almost like a trend towards paradise on Earth, where human beings do less and less unpleasant work and have correspondingly more time for the good things in life. This seems almost like a trend towards paradise on earth. We have to do less and less unpleasant work and in turn have more time for the good things in life.4 All the while, we would enjoy the same (or potentially even increasing) prosperity, for the economy would not employ these machines if they did not markedly raise productivity.\\n\\nUnfortunately we are not on the road to paradise. For several decades, we have worked more than 40 hours per week, have been stressed, complained of burnout and other sicknesses, and suffered a decline in real wages. How can this be, if productivity is continually increasing? Many economists say that the reason for this is competitive pressure. In an effort to compete and deliver the lowest priced goods to market, companies need to lower production costs and thus lay off workers. This results in the aforementioned unemployment. In order to avoid a drop in sales volume due to reduced prices, more products need to be manufactured and sold. The economy must grow!\\n\\nIf the economy continues to grow in a country in which the population is no longer growing (as is the case in most modern industrialized countries), each citizen must necessarily consume more. For that to happen, new markets must be created,5 and marketing has the task of convincing us that we want the new products. This is—allegedly—the only way to “sustainably” ensure prosperity. Apparently there seems to be no escape from this growth/consumption spiral. This has two fatal consequences. For one thing, this increase in consumption should make people happier, but it is having quite the opposite effect: mental illness is increasing.\\n\\nEven more obvious and, above all, fatal, are economic growth’s effects on our living conditions. It is no secret that the earth’s growth limit has long been excee- ded [MMZM72, Ran12], and that we are overexploiting nature’s nonrenewable resources. We are therefore living at the expense of our children and grandchildren, who consequently will have poorer living conditions than we have today. It is also known that every additional dollar of economic growth is an additional burden on the environment—for example through additional CO2 concentration in the atmo- sphere and the resulting climate change [Pae16]. We are destroying our own basis of\\n\\n4Those of us, such as scientists, computer scientists and engineers, who enjoy it may of course continue our work. 5Many EU and German Ministry of Education and Research funding programs for example require that scientists who submit proposals show evidence that their research will open up new markets.\\n\\n1.3 AI and Society\\n\\nexistence. Thus it is obvious that we should abandon this path of growth for the sake of a livable future. But how?\\n\\nLet’s think back to the road to paradise that AI is supposedly preparing for us. Apparently, as we practice it, it does not lead to paradise. Understanding this problem and ﬁnding the right path is one of the central tasks of today. Because of inherent complexities, this problem can not be fully dealt with in an introductory AI textbook. However, I would like to provide the reader with a little food for thought. Although productivity is growing steadily in almost all areas of the economy, workers are required to work as hard as ever. They do not beneﬁt from the increase in productivity. So, we must ask, where do the proﬁts go? Evidently not to the people to whom they are owed, i.e. the workers. Instead, part of the proﬁts is spent on investment and thus on further growth and the rest is taken by the capital owners, while employees work the same hours for declining real wages [Pik14]. This leads to ever-increasing capital concentration among a few rich individuals and private banks, while on the other hand increasing poverty around the world is creating political tensions that result in war, expulsion and ﬂight.\\n\\nWhat is missing is a fair and just distribution of proﬁts. How can this be achieved? Politicians and economists are continually trying to optimize our eco- nomic system, but politics has not offered a sustainable solution, and too few economists are investigating this highly exciting economic question. Obviously the attempt to optimize the parameters of our current capitalist economic system has not lead to a more equitable distribution of wealth, but to the opposite.\\n\\nThis is why economists and ﬁnancial scientists must begin to question the system and look for alternatives. We should ask ourselves how to change the rules and laws of the economy so that all people proﬁt from increased productivity. A growing community of economists and sustainability scientists have offered interesting solutions, a few of which I will brieﬂy describe here.\\n\\nProblem Number One is the creation of ﬁat money by the banks. New money— which is needed, among other things, to keep our growing economy going—is now being created by private banks. This is made possible by the fact that banks have to own only a small part, namely the minimum cash reserve ratio, of the money they give as loans. In the EU in 2016, the minimum cash reserve ratio is one percent. States then borrow this money from private banks in the form of government bonds and thus fall into debt. This is how our current government debt crises have developed. This problem can be solved easily by prohibiting creation of money by the banks by increasing the minimum cash reserve ratio to 100%. State central banks will then get back the monopoly on creating money, and the newly created money can be used directly by the state for the purposes of social welfare. It should be evident that this simple measure would signiﬁcantly ease the problem of public debt. Further interesting components of such an economic reform could be the conversion of the current interest rate system to the so-called natural economic order [GP58], and the introduction of the “economy for the common good” [Fel14] and the biophysical economy [GK09, Küm11]. The practical implementation of the econ- omy for the common good would involve a tax reform, the most important elements of which would be the abolition of the income tax and substantially increased value\\n\\n13\\n\\n14\\n\\n1 Introduction\\n\\nadded tax on energy and resource consumption. We would thus arrive at a highly prosperous, more sustainable human world with less environmental damage and more local trade. The reader may study the literature and assess whether the ideas quoted here are interesting and, if necessary, help to make the required changes.\\n\\nTo conclude this section, I would like to quote the famous physicist Stephen Hawking. In a community-driven interview on www.reddit.com he gave the following answer to whether he had any thoughts about unemployment caused by automation:\\n\\nIf machines produce everything we need, the outcome will depend on how things are distributed. Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequality.\\n\\nAnother Hawking quotation is also ﬁtting. During the same interview,6 to an AI professor’s question about which moral ideas he should impart to his students, Hawking answered:\\n\\n… Please encourage your students to think not only about how to create AI, but also about how to ensure its beneﬁcial use.\\n\\nAs a consequence we should question the reasonableness of AI applications such as the export of intelligent cruise missiles to “allied” Arab states, the deployment of humanoid combat robots, etc.\\n\\n1.3.2 AI and Transportation\\n\\nIn the past 130 years, automotive industry engineers have made great strides. In Germany, one out of every two people owns their own car. These cars are highly reliable. This makes us very mobile and we use this very convenient mobility in work, everyday life and leisure. Moreover, we are dependent on it. Today, we can not get by without a motor vehicle, especially in rural areas with weak public transportation infrastructure, as for instance in Upper Swabia, where the author and his students live. The next stage of increased convenience in road transportation is now imminent. In a few years, we will be able to buy electric self-driving cars, i.e. robotic cars, which will autonomously bring us to almost any destination. All passengers in the robotic car would be able to read, work or sleep during the trip. This is possible on public transit already, but passengers in a robotic car would be able to do this at any time and on any route.\\n\\nAutonomous vehicles that can operate independently could also travel without passengers. This will lead to yet another increase in convenience: robotic taxis. Via a smartphone app, we will be able to order the optimal taxi, in terms of size and equipment, for any conceivable transportation purpose. We will be able to choose whether we want to travel alone in the taxi or whether we are willing to share a ride with\\n\\n6https://www.reddit.com/user/Prof-Stephen-Hawking.\\n\\n1.3 AI and Society\\n\\nother passengers. We will not need our own car anymore. All associated responsibil- ities and expenses, such as refueling, technical service, cleaning, searching for parking, buying and selling, garage rent, etc. are void, which saves us money and effort.\\n\\nBesides the immediate gains in comfort and convenience, robotic cars will offer other signiﬁcant advantages. For example, according to a McKinsey study [GHZ14], we will need far fewer cars and, above all, far fewer parking places in the era of self-driving cars, which will lead to an immense reduction in resource consumption. According to a Lawrence Berkeley National Laboratory study [GS15], electric self-driving cars will cause a 90% reduction in green house emissions per passenger mile due to the vehicles’ energy efﬁciency and the optimized ﬁt between the vehicle and its purpose. Due to their optimal resource utilization, robotic taxis will be much more environmentally friendly than, for example, heavy buses, which often run at low capacity, especially in rural areas. Overall, robot taxis will contribute dramati- cally to energy savings and thus, among other things, to a signiﬁcant improvement in CO2 and climate problems.\\n\\nPassenger safety will be much higher than it is today. Experts currently estimate future accident rates between zero and ten percent compared to today. Emotional driving (“road rage”), distracted driving and driving under the inﬂuence of drugs and alcohol will no longer exist.\\n\\nTaxi drivers losing their jobs is often cited as a disadvantage of robotic cars. It is almost certain that there will no longer be taxi drivers from about 2030 onwards, but that is not necessarily a problem. As explained in the previous section, our society just needs to deal with the newly gained productivity properly.\\n\\nIn addition to the many advantages mentioned above, robotic cars have two critical problems. Firstly, the so-called rebound effect will nullify at least some of the gains in resource, energy and time savings. Shorter driving times as well as more comfortable and cheaper driving will tempt us to drive more. We can only deal with this problem by rethinking our attitude towards consumption and quality of life. Do we have to use the entire time saved for more activities? Here we are all invited to critical reﬂection. Another problem we should take seriously is that the robotic cars will need to be networked. In principle, this gives hackers and terrorists the ability to access and manipulate the vehicles’ controls through security holes in their network protocols. If a hacker manages to do this once, he could repeat the attack on a grand scale, potentially bringing entire vehicle ﬂeets to a halt, causing accidents, spying on vehicle occupants, or initiating other criminal actions. Here, as in other areas such as home automation and the Internet of Things, IT security experts will be needed to ensure the highest possible security guarantees using tools of the trade such as cryptographic methods. By the way, improved machine learning algorithms will be useful in detecting hacking attacks.\\n\\n1.3.3 Service Robotics\\n\\nIn a few years, shortly after self-driving cars, the next bit of consumption bait on the shelves of electronics stores will be service robots. Recently the Google subsidiary\\n\\n15\\n\\n16\\n\\n1 Introduction\\n\\nFig. 1.4 The assistance robot Marvin, deployed in the AsRoBe research project\\n\\nBoston Dynamics provided an impressive example in its humanoid robot Atlas.7 Like the new cars, service robots offer a large gain in comfort and convenience which we would probably like to enjoy. One need only imagine such a robot dutifully cleaning and scrubbing after a party from night until morning without a grumble. Or think of the help that an assistance robot like Marvin, shown in Fig. 1.4, could provide to the elderly8 or to people with disabilities [SPR+16].\\n\\nthese beneﬁts come with costlier trade-offs. Completely new markets would be created, more natural resources and more energy would be consumed, and it is not even certain that people’s lives would be simpliﬁed by the use of service robots in all areas. One of the ﬁrst applications for robots like Atlas, developed by Boston Dynamics in contract with Google, will probably be military combat.\\n\\nIn contrast to the robotic cars, however,\\n\\nIt is therefore all the more important that, before these robots come to market, we engage in social discourse on this topic. Science ﬁction ﬁlms, such as “Ex Machina” (2015) with its female androids, the chilling “I, Robot” (2004) or the humorous “Robot and Frank” (2012), which depicts the pleasant side of a service robot as an old man’s helper, can also contribute to such a discussion.\\n\\n7https://youtu.be/rVlhMGQgDkY. 8In the coming demographic shift, assistance robots could become important for the elderly and thus for our whole society.\\n\\n1.4 Agents\\n\\n1.4 Agents\\n\\nAlthough the term intelligent agents is not new to AI, only in recent years has it gained prominence through [RN10], among others. Agent denotes rather generally a system that processes information and produces an output from an input. These agents may be classiﬁed in many different ways.\\n\\nsoftware agents are primarily employed (Fig. 1.5). In this case the agent consists of a program that calculates a result from user input.\\n\\nIn classical computer\\n\\nscience,\\n\\nIn robotics, on the other hand, hardware agents (also called autonomous robots) are employed, which additionally have sensors and actuators at their disposal (Fig. 1.6). The agent can perceive its environment with the sensors. With the actuators it carries out actions and changes its environment.\\n\\nWith respect to the intelligence of the agent, there is a distinction between reﬂex agents, which only react to input, and agents with memory, which can also include the past in their decisions. For example, a driving robot that through its sensors knows its exact position (and the time) has no way, as a reﬂex agent, of determining its velocity. If, however, it saves the position, at short, discrete time steps, it can thus easily calculate its average velocity in the previous time interval.\\n\\nIf a reﬂex agent is controlled by a deterministic program, it represents a function of the set of all inputs to the set of all outputs. An agent with memory, on the other hand, is in general not a function. Why? (See Exercise 1.5 on page 21.) Reﬂex agents are sufﬁcient in cases where the problem to be solved involves a Markov decision process. This is a process in which only the current state is needed to determine the optimal next action (see Chap. 10).\\n\\nA mobile robot which should move from room 112 to room 179 in a building takes actions different from those of a robot that should move to room 105. In other words, the actions depend on the goal. Such agents are called goal-based.\\n\\nFig. 1.5 A software agent with user interaction\\n\\nFig. 1.6 A hardware agent\\n\\n17\\n\\n18\\n\\n1 Introduction\\n\\nExample 1.1 A spam ﬁlter is an agent that puts incoming emails into wanted or unwanted (spam) categories, and deletes any unwanted emails. Its goal as a goal- based agent is to put all emails in the right category. In the course of this not-so-simple task, the agent can occasionally make mistakes. Because its goal is to classify all emails correctly, it will attempt to make as few errors as possible. However, that is not always what the user has in mind. Let us compare the fol- lowing two agents. Out of 1,000 emails, Agent 1 makes only 12 errors. Agent 2 on the other hand makes 38 errors with the same 1,000 emails. Is it therefore worse than Agent 1? The errors of both agents are shown in more detail in the following table, the so-called “confusion matrix”:\\n\\nAgent 1:\\n\\nAgent 2:\\n\\ncorrect class wanted spam\\n\\ncorrect class wanted spam\\n\\nspam ﬁlter decides\\n\\nwanted\\n\\n189 spam 11\\n\\n1 799\\n\\nspam ﬁlter decides\\n\\nwanted spam\\n\\n200 0\\n\\n38 762\\n\\nAgent 1 in fact makes fewer errors than Agent 2, but those few errors are severe because the user loses 11 potentially important emails. Because there are in this case two types of errors of differing severity, each error should be weighted with the appropriate cost factor (see Sect. 7.3.5 and Exercise 1.7 on page 21).\\n\\nThe sum of all weighted errors gives the total cost caused by erroneous decisions.The goal of a cost-based agent is to minimize the cost of erroneous decisions in the long term, that is, on average. In Sect. 7.3 we will become familiar with the medical diagnosis system LEXMED as an example of a cost-based agent.\\n\\nAnalogously, the goal of a utility-based agent is to maximize the utility derived from correct decisions in the long term, that is, on average. The sum of all decisions weighted by their respective utility factors gives the total utility.\\n\\nOf particular interest in AI are Learning agents, which are capable of changing themselves given training examples or through positive or negative feedback, such that the average utility of their actions grows over time (see Chap. 8).\\n\\nAs mentioned in Sect. 1.2.5, distributed agents are increasingly coming into use, whose intelligence are not localized in one agent, but rather can only be seen through cooperation of many agents.\\n\\nThe design of an agent is oriented, along with its objective, strongly toward its environment, or alternately its picture of the environment, which strongly depends on it sensors. The environment is observable if the agent always knows the complete state of the world. Otherwise the environment is only partially observable. If an action always leads to the same result, then the environment is deterministic. Otherwise it is nondeterministic. In a discrete environment only ﬁnitely many states and actions occur, whereas a continuous environment boasts inﬁnitely many states or actions.\\n\\n1.5 Knowledge-Based Systems\\n\\n1.5 Knowledge-Based Systems\\n\\nAn agent is a program that implements a mapping from perceptions to actions. For the problem is sufﬁcient. For complex simple agents this way of looking at applications in which the agent must be able to rely on a large amount of infor- mation and is meant to do a difﬁcult task, programming the agent can be very costly and unclear how to proceed. Here AI provides a clear path to follow that will greatly simplify the work.\\n\\nFirst we separate knowledge from the system or program, which uses the knowledge to, for example, reach conclusions, answer queries, or come up with a plan. This system is called the inference mechanism. The knowledge is stored in a knowledge base (KB). Acquisition of knowledge in the knowledge base is denoted Knowledge Engineering and is based on various knowledge sources such as human experts, the knowledge engineer, and databases. Active learning systems can also acquire knowledge through active exploration of the world (see Chap. 10). In Fig. 1.7 the general architecture of knowledge-based systems is presented.\\n\\nMoving toward a separation of knowledge and inference has several crucial advantages. The separation of knowledge and inference can allow inference systems to be implemented in a largely application-independent way. For example, application of a medical expert system to other diseases is much easier by replacing the knowledge base rather than by programming a whole new system.\\n\\nThrough the decoupling of the knowledge base from inference, knowledge can be stored declaratively. In the knowledge base there is only a description of the knowledge, which is independent from the inference system in use. Without this clear separation, knowledge and processing of inference steps would be interwoven, and any changes to the knowledge would be very costly.\\n\\nFig. 1.7 Structure of a classic knowledge-processing system\\n\\n19\\n\\n20\\n\\n1 Introduction\\n\\nFormal language as a convenient interface between man and machine lends itself to the representation of knowledge in the knowledge base. In the following chapters we will get to know a whole series of such languages. First, in Chaps. 2 and 3 there are propositional calculus and ﬁrst-order predicate logic (PL1). But other for- malisms such as probabilistic logic and decision trees are also presented. We start with propositional calculus and the related inference systems. Building on that, we will present predicate logic, a powerful language that is accessible by machines and very important in AI.\\n\\nAs an example for a large scale knowledge based system we want to refer to the software agent “Watson”. Developed at IBM together with a number of universities, Watson is a question answering program, that can be fed with clues given in natural language. It works on a knowledge base comprising four terabytes of hard disk storage, including the full text of Wikipedia [FNA+09]. Watson was developed within IBM’s DeepQA project which is characterized in [Dee11] as follows:\\n\\nThe DeepQA project at IBM shapes a grand challenge in Computer Science that aims to illustrate how the wide and growing accessibility of natural language content and the integration and advancement of Natural Language Processing, Information Retrieval, Machine Learning, Knowledge Representation and Reasoning, and massively parallel computation can drive open-domain automatic Question Answering technology to a point where it clearly and consistently rivals the best human performance.\\n\\nIn the U.S. television quiz show “Jeopardy!”, in February 2011, Watson defeated the two human champions Brad Rutter and Ken Jennings in a two-game, combined-point match and won the one million dollar price. One of Watson’s particular strengths was its very fast reaction to the questions with the result that Watson often hit the buzzer (using a solenoid) faster than its human competitors and then was able to give the ﬁrst answer to the question.\\n\\nThe high performance and short reaction times of Watson were due to an implementation on 90 IBM Power 750 servers, each of which contains 32 processors, resulting in 2880 parallel processors.\\n\\n1.6 Exercises\\n\\nExercise 1.1 Test some of the chatterbots available on the internet. Start for example with www.hs-weingarten.de/*ertel/aibook in the collection of links under Turingtest/Chatterbots, or at www.simonlaven.com or www.alicebot.org. Write down a starting question and measure the time it takes, for each of the various programs, until you know for certain that it is not a human.\\n\\n❄ ❄ Exercise 1.2 At www.pandorabots.com you will ﬁnd a server on which you can build a chatterbot with the markup language AIML quite easily. Depending on your level, develop a simple or complex chatterbot, or change an interest existing one.\\n\\n1.6 Exercises\\n\\nExercise 1.3 Give reasons for the unsuitability of the Turing test as a deﬁnition of “artiﬁcial intelligence” in practical AI.\\n\\n➳ Exercise 1.4 Many well-known inference processes, learning processes, etc. are\\n\\nNP-complete or even undecidable. What does this mean for AI?\\n\\nExercise 1.5 (a) Why is a deterministic agent with memory not a function from the set of all\\n\\ninputs to the set of all outputs, in the mathematical sense?\\n\\n(b) How can one change the agent with memory, or model it, such that it becomes\\n\\nequivalent to a function but does not lose its memory?\\n\\nExercise 1.6 Let there be an agent with memory that can move within a plane. From its sensors, it receives at clock ticks of a regular interval Dt its exact position (x, y) in Cartesian coordinates. (a) Give a formula with which the agent can calculate its velocity from the current\\n\\ntime t and the previous measurement of t − Dt.\\n\\n(b) How must the agent be changed so that it can also calculate its acceleration?\\n\\nProvide a formula here as well.\\n\\n❄ Exercise 1.7\\n\\n(a) Determine for both agents in Example 1.1 on page 18 the costs created by the errors and compare the results. Assume here that having to manually delete a spam email costs one cent and retrieving a deleted email, or the loss of an email, costs one dollar.\\n\\n(b) Determine for both agents the proﬁt created by correct classiﬁcations and compare the results. Assume that for every desired email recognized, a proﬁt of one dollar accrues and for every correctly deleted spam email, a proﬁt of one cent.\\n\\n21\\n\\nPropositional Logic\\n\\nIn propositional logic, as the name suggests, propositions are connected by logical operators. The statement “the street is wet” is a proposition, as is “it is raining”. These two propositions can be connected to form the new proposition\\n\\nif it is raining the street is wet.\\n\\nWritten more formally\\n\\nit is raining ) the street is wet.\\n\\nThis notation has the advantage that the elemental propositions appear again in unaltered form. So that we can work with propositional logic precisely, we will begin with a deﬁnition of the set of all propositional logic formulas.\\n\\n2.1 Syntax\\n\\nDeﬁnition 2.1 Let Op = {¬, ^, _, ),,,(,)} be the set of logical operators and R a set of symbols. The sets Op, R and {t, f} are pairwise disjoint. R is called the signature and its elements are the proposition variables. The set of propositional logic formulas is now recursively deﬁned: (cid:129) t and f are (atomic) formulas. (cid:129) All proposition variables,\\n\\nis all elements from R, are (atomic)\\n\\nthat\\n\\nformulas.\\n\\n(cid:129) If A and B are formulas, then ¬A, (A), A ^ B, A _ B, A ) B, A , B are\\n\\nalso formulas.\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_2\\n\\n23\\n\\n2\\n\\n24\\n\\n2 Propositional Logic\\n\\nThis elegant recursive deﬁnition of the set of all formulas allows us to generate\\n\\ninﬁnitely many formulas. For example, given R = {A, B, C},\\n\\nA ^ B; A ^ B ^ C; A ^ A ^ A; C ^ B _ A;\\n\\nð:A ^ BÞ ) ð:C _ AÞ\\n\\nare formulas. (((A)) _ B) is also a syntactically correct formula.\\n\\nDeﬁnition 2.2 We read the symbols and operators in the following way:\\n\\nt: “true” f : “false” ¬A: “not A” A ∧ B: “A and B” A ∨ B: “A or B” A ⇒ B: “if A then B” A ⇔ B: “A if and only if B” (equivalence)\\n\\n(negation) (conjunction) (disjunction) (implication (also called material implication))\\n\\nThe formulas deﬁned in this way are so far purely syntactic constructions\\n\\nwithout meaning. We are still missing the semantics.\\n\\n2.2 Semantics\\n\\nIn propositional logic there are two truth values: t for “true” and f for “false”. We begin with an example and ask ourselves whether the formula A ^ B is true. The answer is: it depends on whether the variables A and B are true. For example, if A stands for “It is raining today” and B for “It is cold today” and these are both true, then A ^ B is true. If, however, B represents “It is hot today” (and this is false), then A ^ B is false.\\n\\nWe must obviously assign truth values that reﬂect the state of the world to\\n\\nproposition variables. Therefore we deﬁne\\n\\nDeﬁnition 2.3 A mapping I : R ! {t, f}, which assigns a truth value to every proposition variable, is called an interpretation.\\n\\nBecause every proposition variable can take on two truth values, every propo- sitional logic formula with n different variables has 2n different interpretations. We deﬁne the truth values for the basic operations by showing all possible inter- pretations in a truth table (see Table 2.1 on page 25).\\n\\n2.2 Semantics\\n\\nTable 2.1 Deﬁnition of the logical operators by truth table\\n\\nA\\n\\nt t f f\\n\\nB( t f t f\\n\\nt t f f\\n\\nA)\\n\\n¬A f f t t\\n\\nA ^ B A _ B A ) B A,B t t f f t f t f\\n\\nt t t f\\n\\nt f f t\\n\\nThe empty formula is true for all interpretations. In order to determine the truth value for complex formulas, we must also deﬁne the order of operations for logical operators. If expressions are parenthesized, the term in the parentheses is evaluated ﬁrst. For unparenthesized formulas, the priorities are ordered as follows, beginning with the strongest binding: ¬, ^, _, ),,.\\n\\nTo clearly differentiate between the equivalence of formulas and syntactic\\n\\nequivalence, we deﬁne\\n\\nDeﬁnition 2.4 Two formulas F and G are called semantically equivalent if they take on the same truth value for all interpretations. We write F (cid:2) G.\\n\\nSemantic equivalence serves above all to be able to use the meta-language, that is, natural language, to talk about the object language, namely logic. The statement “A (cid:2) B” conveys that the two formulas A and B are semantically equivalent. The statement “A, B” on the other hand is a syntactic object of the formal language of propositional logic.\\n\\nAccording to the number of interpretations in which a formula is true, we can\\n\\ndivide formulas into the following classes:\\n\\nDeﬁnition 2.5 A formula is called (cid:129) Satisﬁable if it is true for at least one interpretation. (cid:129) Logically valid or simply valid if it is true for all interpretations. True\\n\\nformulas are also called tautologies.\\n\\n(cid:129) Unsatisﬁable if it is not true for any interpretation. Every interpretation that satisﬁes a formula is called a model of the formula.\\n\\nClearly the negation of every generally valid formula is unsatisﬁable. The\\n\\nnegation of a satisﬁable, but not generally valid formula F is satisﬁable.\\n\\nWe are now able to create truth tables for complex formulas to ascertain their truth values. We put this into action immediately using equivalences of formulas which are important in practice.\\n\\n25\\n\\n26\\n\\n2 Propositional Logic\\n\\nTheorem 2.1 The operations ^, _ are commutative and associative, and the following equivalences are generally valid:\\n\\n¬A ∨ B ⇔ A ⇒ B A ⇒ B ⇔ ¬B ⇒ ¬A (A ⇒ B) ∧ (B ⇒ A) ⇔ (A ⇔ B) ¬(A ∧ B) ⇔ ¬A ∨ ¬B ¬(A ∨ B) ⇔ ¬A ∧ ¬B\\n\\n(implication) (contraposition) (equivalence) (De Morgan’s law)\\n\\nA ∨ (B ∧ C) ⇔ (A ∨ B) ∧ (A ∨ C) A ∧ (B ∨ C) ⇔ (A ∧ B) ∨ (A ∧ C)\\n\\n(distributive law)\\n\\nA ∨ ¬A ⇔ w A ∧ ¬A ⇔ f A ∨ f ⇔ A A ∨ w ⇔ w A ∧ f ⇔ f A ∧ w ⇔ A\\n\\n(tautology) (contradiction)\\n\\nProof To show the ﬁrst equivalence, we calculate the truth table for ¬A _ B and A ) B and see that the truth values for both formulas are the same for all interpretations. The formulas are therefore equivalent, and thus all the values of the last column are “t”s.\\n\\nA\\n\\nt t f f\\n\\nB¬A t f t f\\n\\nf f t t\\n\\n¬A _ B t f t t\\n\\nA ) B t f t t\\n\\n(¬A _ B) , (A ) B) t t t t\\n\\nThe proofs for the other equivalences are similar and are recommended as exercises □ for the reader (Exercise 2.2 on page 37).\\n\\n2.3 Proof Systems\\n\\nIn AI we are interested in taking existing knowledge and from that deriving new knowledge or answering questions. In propositional logic this means showing that a knowledge base KB—that is, a (possibly extensive) propositional logic formula— a formula Q1 follows. Thus, we ﬁrst deﬁne the term “entailment”.\\n\\n1Here Q stands for query.\\n\\n2.3 Proof Systems\\n\\nDeﬁnition 2.6 A formula KB entails a formula Q (or Q follows from KB) if every model of KB is also a model of Q. We write KB (cid:3) Q.\\n\\nIn other words, in every interpretation in which KB is true, Q is also true. More succinctly, whenever KB is true, Q is also true. Because, for the concept of entailment, in, we are dealing with a semantic concept.\\n\\ninterpretations of variables are brought\\n\\nEvery formula that is not valid chooses so to speak a subset of the set of all interpretations as its model. Tautologies such as A _ ¬A, for example, do not restrict the number of satisfying interpretations because their proposition is empty. The empty formula is therefore true in all interpretations. For every tautology T then ; (cid:3) T. Intuitively this means that tautologies are always true, without restriction of the interpretations by a formula. For short we write (cid:3) T. Now we show an important connection between the semantic concept of entailment and syntactic implication.\\n\\nTheorem 2.2 (Deduktionstheorem)\\n\\nA (cid:3) B if and only if (cid:3) A ) B:\\n\\nProof Observe the truth table for implication:\\n\\nA\\n\\nt t f f\\n\\nB\\n\\nt f t f\\n\\nA ) B t f t t\\n\\nAn arbitrary implication A ) B is clearly always true except with the interpretation A ↦ t, B ↦ f. Assume that A (cid:3) B holds. This means that for every interpretation that makes A true, B is also true. The critical second row of the truth table does not even apply in that case. Therefore A ) B is true, which means that A ) B is a tautology. Thus one direction of the statement has been shown.\\n\\nNow assume that A ) B holds. Thus the critical second row of the truth table is also locked out. Every model of A is then also a model of B. Then A (cid:3) B holds. □\\n\\nIf we wish to show that KB entails Q, we can also demonstrate by means of the truth table method that KB ) Q is a tautology. Thus we have our ﬁrst proof system for propositional logic, which is easily automated. The disadvantage of this method is the very long computation time in the worst case. Speciﬁcally, in the worst case\\n\\n27\\n\\n28\\n\\n2 Propositional Logic\\n\\nwith n proposition variables, for all 2n interpretations of the variables the formula KB ) Q must be evaluated. The computation time grows therefore exponentially with the number of variables. Therefore this process is unusable for large variable counts, at least in the worst case.\\n\\nIf a formula KB entails a formula Q, then by the deduction theorem KB ) Q is a\\n\\ntautology. Therefore the negation ¬(KB ) Q) is unsatisﬁable. We have\\n\\n:ðKB ) QÞ (cid:2) :ð:KB _ QÞ (cid:2) KB ^ :Q:\\n\\nTherefore, KB ^ ¬Q is also unsatisﬁable. We formulate this simple, but important consequence of the deduction theorem as a theorem.\\n\\nTheorem 2.3 (Proof by contradiction) KB (cid:3) Q if and only if KB ^ ¬Q is unsatisﬁable.\\n\\nTo show that the query Q follows from the knowledge base KB, we can also add the negated query ¬Q to the knowledge base and derive a contradiction. Because of the equivalence A ^ ¬A , f from Theorem 2.1 on page 26 we know that a contradiction is unsatisﬁable. Therefore, Q has been proved. This procedure, which is frequently used in mathematics, is also used in various automatic proof calculi such as the resolution calculus and in the processing of PROLOG programs.\\n\\nOne way of avoiding having to test all interpretations with the truth table method is the syntactic manipulation of the formulas KB and Q by application of inference rules with the goal of greatly simplifying them, such that in the end we can instantly see that KB (cid:3) Q. We call this syntactic process derivation and write KB ⊢ Q. Such syntactic proof systems are called calculi. To ensure that a calculus does not generate errors, we deﬁne two fundamental properties of calculi.\\n\\nDeﬁnition 2.7 A calculus is called sound if every derived proposition fol- lows semantically. That is, if it holds for formulas KB and Q that\\n\\nif KB ‘ Q then KB (cid:3) Q:\\n\\nA calculus is called complete if all semantic consequences can be derived. That is, for formulas KB and Q the following holds:\\n\\nif KB (cid:3) Q then KB ‘ Q:\\n\\nThe soundness of a calculus ensures that all derived formulas are in fact semantic consequences of the knowledge base. The calculus does not produce any “false consequences”. The completeness of a calculus, on the other hand, ensures that the calculus does not overlook anything. A complete calculus always ﬁnds a proof if\\n\\n2.3 Proof Systems\\n\\nKB\\n\\nderivation\\n\\nQ\\n\\nsyntactic level (formula)\\n\\ni n t e r p r e t a t i o n\\n\\ni n t e r p r e t a t i o n\\n\\nMod(KB)\\n\\nentailment\\n\\nMod(Q)\\n\\nsemantic level (interpretation)\\n\\nFig. 2.1 Syntactic derivation and semantic entailment. Mod(X) represents the set of models of a formula X\\n\\nthe formula to be proved follows from the knowledge base. If a calculus is sound and complete, then syntactic derivation and semantic entailment are two equivalent relations (see Fig. 2.1).\\n\\nTo keep automatic proof systems as simple as possible, these are usually made to\\n\\noperate on formulas in conjunctive normal form.\\n\\nDeﬁnition 2.8 A formula is in conjunctive normal form (CNF) if and only if it consists of a conjunction\\n\\nK1 ^ K2 ^ (cid:4) (cid:4) (cid:4) ^ Km\\n\\nof clauses. A clause Ki consists of a disjunction\\n\\nðLi1 _ Li2 _ (cid:4) (cid:4) (cid:4) _ Lini\\n\\nÞ\\n\\nof literals. Finally, a literal is a variable (positive literal) or a negated variable (negative literal).\\n\\nThe formula (A _ B _ ¬C) ^ (A _ B) ^ (¬B _ ¬C) is in conjunctive normal form. The conjunctive normal form does not place a restriction on the set of formulas because:\\n\\nTheorem 2.4 Every propositional logic formula can be transformed into an equivalent conjunctive normal form.\\n\\n29\\n\\n30\\n\\n2 Propositional Logic\\n\\nExample 2.1 We put A _ B ) C ^ D into conjunctive normal form by using the equivalences from Theorem 2.1 on page 26:\\n\\nA _ B ) C ^ D\\n\\nðimplicationÞ (cid:2) :ðA _ BÞ _ ðC ^ DÞ ðde MorganÞ (cid:2) ð:A ^ :BÞ _ ðC ^ DÞ (cid:2) ð:A _ ðC ^ DÞÞ ^ ð:B _ ðC ^ DÞÞ ðdistributive lawÞ (cid:2) ðð:A _ CÞ ^ ð:A _ DÞÞ ^ ðð:B _ CÞ ^ ð:B _ DÞÞ ðdistributive lawÞ ðassociative lawÞ (cid:2) ð:A _ CÞ ^ ð:A _ DÞ ^ ð:B _ CÞ ^ ð:B _ DÞ\\n\\nWe are now only missing a calculus for syntactic proof of propositional logic formulas. We start with the modus ponens, a simple, intuitive rule of inference, which, from the validity of A and A ) B, allows the derivation of B. We write this formally as\\n\\nA; A ) B B\\n\\n:\\n\\nThis notation means that we can derive the formula(s) below the line from the comma-separated formulas above the line. Modus ponens as a rule by itself, while sound, is not complete. If we add additional rules we can create a complete calculus, which, however, we do not wish to consider here. Instead we will investigate the resolution rule\\n\\nA _ B; :B _ C A _ C\\n\\nð2:1Þ\\n\\nas an alternative. The derived clause is called resolvent. Through a simple trans- formation we obtain the equivalent form\\n\\nA _ B; B ) C A _ C\\n\\n:\\n\\nIf we set A to f, we see that the resolution rule is a generalization of the modus ponens. The resolution rule is equally usable if C is missing or if A and C are missing. In the latter case the empty clause can be derived from the contradiction B ^ ¬B (Exercise 2.7 on page 38).\\n\\n2.4 Resolution\\n\\nWe now generalize the resolution rule again by allowing clauses with an arbitrary number of literals. With the literals A1, …, Am, B, C1, …, Cn the general resolution rule reads\\n\\n2.4 Resolution\\n\\nðA1 _ (cid:4) (cid:4) (cid:4) _ Am _ BÞ;\\n\\nð:B _ C1 _ (cid:4) (cid:4) (cid:4) _ CnÞ\\n\\nðA1 _ (cid:4) (cid:4) (cid:4) _ Am _ C1 _ (cid:4) (cid:4) (cid:4) _ CnÞ\\n\\n:\\n\\nWe call the literals B and ¬B complementary. The resolution rule deletes a pair of complementary literals from the two clauses and combines the rest of the literals into a new clause.\\n\\nTo prove that from a knowledge base KB, a query Q follows, we carry out a proof by contradiction. Following Theorem 2.3 on page 28 we must show that a contradiction can be derived from KB ^ ¬Q. In formulas in conjunctive normal form, a contradiction appears in the form of two clauses (A) and (¬A), which lead to the empty clause as their resolvent. The following theorem ensures us that this process really works as desired.\\n\\nFor the calculus to be complete, we need a small addition, as shown by the following example. Let the formula (A _ A) be given as our knowledge base. To show by the resolution rule that from there we can derive (A ^ A), we must show that the empty clause can be derived from (A _ A) ^ (¬A _ ¬A). With the resolution rule alone, this is impossible. With factorization, which allows deletion of copies of literals from clauses, this problem is eliminated. In the example, a double application of factorization leads to (A) ^ (¬A), and a resolution step to the empty clause.\\n\\nTheorem 2.5 The resolution calculus for the proof of unsatisﬁability of formulas in conjunctive normal form is sound and complete.\\n\\nBecause it is the job of the resolution calculus to derive a contradiction from\\n\\nKB ^ ¬Q, it is very important that the knowledge base KB is consistent:\\n\\nDeﬁnition 2.9 A formula KB is called consistent if it is impossible to derive from it a contradiction, that is, a formula of the form / ^ ¬/.\\n\\nOtherwise anything can be derived from KB (see Exercise 2.8 on page 38). This\\n\\nis true not only of resolution, but also for many other calculi.\\n\\nOf the calculi for automated deduction, resolution plays an exceptional role. Thus we wish to work a bit more closely with it. In contrast to other calculi, resolution has only two inference rules, and it works with formulas in conjunctive normal form. This makes its implementation simpler. A further advantage com- pared to many calculi lies in its reduction in the number of possibilities for the application of inference rules in every step of the proof, whereby the search space is reduced and computation time decreased.\\n\\nAs an example, we start with a simple logic puzzle that allows the important\\n\\nsteps of a resolution proof to be shown.\\n\\n31\\n\\nð2:2Þ\\n\\n32\\n\\n2 Propositional Logic\\n\\nExample 2.2 Logic puzzle number 7, entitled A charming English family, from the German book [Ber89] reads (translated to English):\\n\\nDespite studying English for seven long years with brilliant success, I must admit that when I hear English people speaking English I’m totally perplexed. Recently, moved by noble feelings, I picked up three hitchhikers, a father, mother, and daughter, who I quickly realized were English and only spoke English. At each of the sentences that follow I wavered between two possible interpretations. They told me the following (the second possible meaning is in parentheses): The father: “We are going to Spain (we are from Newcastle).” The mother: “We are not going to Spain and are from Newcastle (we stopped in Paris and are not going to Spain).” The daughter: “We are not from Newcastle (we stopped in Paris).” What about this charming English family?\\n\\nTo solve this kind of problem we proceed in three steps: formalization, trans- formation into normal form, and proof. In many cases formalization is by far the most difﬁcult step because it is easy to make mistakes or forget small details. (Thus practical exercise is very important. See Exercises 2.9–2.11 on page 38.)\\n\\nHere we use the variables S for “We are going to Spain”, N for “We are from Newcastle”, and P for “We stopped in Paris” and obtain as a formalization of the three propositions of father, mother, and daughter\\n\\nðS _ NÞ ^ ½ð:S ^ NÞ _ ðP ^ :SÞ(cid:5) ^ ð:N _ PÞ:\\n\\nFactoring out ¬S in the middle sub-formula brings the formula into CNF in one step. Numbering the clauses with subscripted indices yields\\n\\nKB (cid:2) ðS _ NÞ\\n\\n1\\n\\n^ ð:SÞ\\n\\n2\\n\\n^ ðP _ NÞ\\n\\n3\\n\\n^ ð:N _ PÞ\\n\\n4\\n\\n:\\n\\nNow we begin the resolution proof, at ﬁrst still without a query Q. An expression of the form “Res(m, n): 〈clause〉k” means that 〈clause〉 is obtained by resolution of clause m with clause n and is numbered k.\\n\\nResð1; 2Þ: Resð3; 4Þ: Resð1; 4Þ:\\n\\n(N)5 (P)6 (S _ P)7\\n\\nWe could have derived clause P also from Res(4, 5) or Res(2, 7). Every further resolution step would lead to the derivation of clauses that are already available. Because it does not allow the derivation of the empty clause, it has therefore been shown that the knowledge base is non-contradictory. So far we have derived N and P. To show that ¬S holds, we add the clause (S)8 to the set of clauses as a negated query. With the resolution step\\n\\nResð2; 8Þ :\\n\\nðÞ\\n\\n9\\n\\nthe proof is complete. Thus ¬S ^ N ^ P holds. The “charming English family” evidently comes from Newcastle, stopped in Paris, but is not going to Spain.\\n\\n2.4 Resolution\\n\\nExample 2.3 Logic puzzle number 28 from [Ber89], entitled The High Jump, reads\\n\\nThree girls practice high jump for their physical education ﬁnal exam. The bar is set to 1.20 meters. “I bet”, says the ﬁrst girl to the second, “that I will make it over if, and only if, you don’t”. If the second girl said the same to the third, who in turn said the same to the ﬁrst, would it be possible for all three to win their bets?\\n\\nWe show through proof by resolution that not all three can win their bets. Formalization:\\n\\nThe first girl’s jump succeeds: A; the second girl’s jump succeeds: B; the third girl’s jump succeeds: C:\\n\\nFirst girl’s bet: A , :B Þ; second girl’s bet: B , :C Þ: third girl’s bet: C , :A\\n\\nð\\n\\nð\\n\\nð\\n\\nÞ;\\n\\nClaim: the three cannot all win their bets:\\n\\nQ (cid:2) :ððA , :BÞ ^ ðB , :CÞ ^ ðC , :AÞÞ\\n\\nIt must now be shown by resolution that ¬Q is unsatisﬁable. Transformation into CNF: First girl’s bet:\\n\\nðA , :BÞ (cid:2) ðA ) :BÞ ^ ð:B ) AÞ (cid:2) ð:A _ :BÞ ^ ðA _ BÞ\\n\\nThe bets of the other two girls undergo analogous transformations, and we obtain the negated claim\\n\\n:Q (cid:2) :A _ :B ð Þ ð ^ C _ A 6\\n\\nÞ\\n\\n1 :\\n\\n^ A _ B\\n\\nð\\n\\nÞ\\n\\n2\\n\\n^ :B _ :C\\n\\nð\\n\\nÞ\\n\\n3\\n\\n^ B _ C\\n\\nð\\n\\nÞ\\n\\n4\\n\\n^ :C _ :A\\n\\nð\\n\\nFrom there we derive the empty clause using resolution:\\n\\nResð1; 6Þ : Resð4; 7Þ : Resð2; 5Þ : Resð3; 9Þ : Resð8; 10Þ :\\n\\nðC _ :BÞ ðCÞ 8 ðB _ :CÞ ð:CÞ ðÞ\\n\\n10\\n\\n7\\n\\n9\\n\\nThus the claim has been proved.\\n\\n2.5 Horn Clauses\\n\\nA clause in conjunctive normal form contains positive and negative literals and can be represented in the form\\n\\nÞ\\n\\n5\\n\\n33\\n\\n34\\n\\n2 Propositional Logic\\n\\nð:A1 _ (cid:4) (cid:4) (cid:4) _ :Am _ B1 _ (cid:4) (cid:4) (cid:4) _ BnÞ\\n\\nwith the variables A1, …, Am and B1, …, Bn. This clause can be transformed in two simple steps into the equivalent form\\n\\nA1 ^ (cid:4) (cid:4) (cid:4) ^ Am ) B1 _ (cid:4) (cid:4) (cid:4) _ Bn:\\n\\nThis implication contains the premise, a conjunction of variables and the conclu- sion, a disjunction of variables. For example, “If the weather is nice and there is snow on the ground, I will go skiing or I will work.” is a proposition of this form. The receiver of this message knows for certain that the sender is not going swimming. A signiﬁcantly clearer statement would be “If the weather is nice and there is snow on the ground, I will go skiing.”. The receiver now has deﬁnite information. Thus we call clauses with at most one positive literal deﬁnite clauses. These clauses have the advantage that they only allow one conclusion and are thus distinctly simpler to interpret. Many relations can be described by clauses of this type. We therefore deﬁne\\n\\nDeﬁnition 2.10 Clauses with at most one positive literal of the form\\n\\nð:A1 _ (cid:4) (cid:4) (cid:4) _ :Am _ BÞ or\\n\\nð:A1 _ (cid:4) (cid:4) (cid:4) _ :AmÞ or B\\n\\nor (equivalently)\\n\\nA1 ^ (cid:4) (cid:4) (cid:4) ^ Am ) B or A1 ^ (cid:4) (cid:4) (cid:4) ^ Am ) f\\n\\nor B:\\n\\nare named Horn clauses (after their inventor). A clause with a single positive literal is a fact. In clauses with negative and one positive literal, the positive literal is called the head.\\n\\nTo better understand the representation of Horn clauses, the reader may derive them from the deﬁnitions of the equivalences we have currently been using (Exercise 2.12 on page 38).\\n\\nHorn clauses are easier to handle not only in daily life, but also in formal reasoning, as we can see in the following example. Let the knowledge base consist of the following clauses (the “^” binding the clauses is left out here and in the text that follows):\\n\\nð nice weather Þ ð snowfall 2 snowfall ) snow Þ nice weather ^ snow ) skiing\\n\\nÞ\\n\\n1\\n\\nð ð\\n\\nÞ\\n\\n3\\n\\n4\\n\\n2.5 Horn Clauses\\n\\nIf we now want to know whether skiing holds, this can easily be derived. A slightly generalized modus ponens sufﬁces here as an inference rule:\\n\\nA1 ^ (cid:4) (cid:4) (cid:4) ^ Am; A1 ^ (cid:4) (cid:4) (cid:4) ^ Am ) B B\\n\\n:\\n\\nThe proof of “skiing” has the following form (MP(i1, …, ik) represents application of the modus ponens on clauses i1 to ik:\\n\\nMPð2; 3Þ : MPð1; 5; 4Þ :\\n\\nðsnowÞ 5 ðskiingÞ\\n\\n6\\n\\n:\\n\\nWith modus ponens we obtain a complete calculus for formulas that consist of propositional logic Horn clauses. In the case of large knowledge bases, however, modus ponens can derive many unnecessary formulas if one begins with the wrong clauses. Therefore, in many cases it is better to use a calculus that starts with the query and works backward until the facts are reached. Such systems are designated backward chaining, in contrast to forward chaining systems, which start with facts and ﬁnally derive the query, as in the above example with the modus ponens.\\n\\nFor backward chaining of Horn clauses, SLD resolution is used. SLD stands for “Selection rule driven linear resolution for deﬁnite clauses”. In the above example, augmented by the negated query (skiing ) f)\\n\\nð ð\\n\\nð ð\\n\\nð\\n\\nÞ\\n\\nnice weather Þ snowfall 2 snowfall ) snow nice weather ^ snow ) skiing Þ skiing ) f\\n\\n1\\n\\nÞ\\n\\n3\\n\\nÞ\\n\\n5\\n\\n4\\n\\nwe carry out SLD resolution beginning with the resolution steps that follow from this clause\\n\\nResð5; 4Þ : Resð6; 1Þ : Resð7; 3Þ : Resð8; 2Þ :\\n\\nð\\n\\nnice weather ^ snow ) f snow ) f 7 snowfall ) f ðÞ\\n\\nð ð\\n\\nÞ\\n\\nÞ\\n\\n8\\n\\nÞ\\n\\n6\\n\\nand derive a contradiction with the empty clause. Here we can easily see “linear resolution”, which means that further processing is always done on the currently derived clause. This leads to a great reduction of the search space. Furthermore, the\\n\\n35\\n\\n36\\n\\n2 Propositional Logic\\n\\nliterals of the current clause are always processed in a ﬁxed order (for example, from right to left) (“Selection rule driven”). The literals of the current clause are called subgoal. The literals of the negated query are the goals. The inference rule for one step reads\\n\\nA1 ^ (cid:4) (cid:4) (cid:4) ^ Am ) B1; B1 ^ B2 ^ (cid:4) (cid:4) (cid:4) ^B n ) f A1 ^ (cid:4) (cid:4) (cid:4) ^ Am ^ B2 ^ (cid:4) (cid:4) (cid:4) ^ Bn ) f\\n\\n:\\n\\nBefore application of the inference rule, B1, B2, …, Bn—the current subgoals—must be proved. After the application, B1 is replaced by the new subgoal A1 ^ (cid:4)(cid:4)(cid:4) ^ Am. To show that B1 is true, we must now show that A1 ^ (cid:4)(cid:4)(cid:4) ^ Am are true. This process continues until the list of subgoals of the current clauses (the so-called goal stack) is empty. With that, a contradiction has been found. If, for a subgoal ¬Bi, there is no clause with the complementary literal Bi as its clause head, the proof terminates and no contradiction can be found. The query is thus unprovable.\\n\\nSLD resolution plays an important role in practice because programs in the logic programming language PROLOG consist of predicate logic Horn clauses, and their processing is achieved by means of SLD resolution (see Exercise 2.13 on page 38, or Chap. 5).\\n\\n2.6 Computability and Complexity\\n\\nThe truth table method, as the simplest semantic proof system for propositional logic, represents an algorithm that can determine every model of any formula in ﬁnite time. Thus the sets of unsatisﬁable, satisﬁable, and valid formulas are decidable. The computation time of the truth table method for satisﬁability grows in the worst case exponentially with the number n of variables because the truth table has 2n rows. An optimization, the method of semantic trees, avoids looking at variables that do not occur in clauses, and thus saves computation time in many cases, but in the worst case it is likewise exponential.\\n\\nIn resolution, in the worst case the number of derived clauses grows exponen- tially with the number of initial clauses. To decide between the two processes, we can therefore use the rule of thumb that in the case of many clauses with few variables, the truth table method is preferable, and in the case of few clauses with many variables, resolution will probably ﬁnish faster.\\n\\nThe question remains: can proof in propositional logic go faster? Are there better algorithms? The answer: probably not. After all, S. Cook, the founder of com- plexity theory, has shown that the 3-SAT problem is NP-complete. 3-SAT is the set of all CNF formulas whose clauses have exactly three literals. Thus it is clear that there is probably (modulo the P/NP problem) no polynomial algorithm for 3-SAT, and thus probably not a general one either. For Horn clauses, however, there is an algorithm in which the computation time for testing satisﬁability grows only lin- early as the number of literals in the formula increases.\\n\\n2.7 Applications and Limitations\\n\\n2.7 Applications and Limitations\\n\\nTheorem provers for propositional logic are part of the developer’s everyday toolset in digital technology. For example, the veriﬁcation of digital circuits and the generation of test patterns for testing of microprocessors in fabrication are some of these tasks. Special proof systems that work with binary decision dia- grams (BDD) are also employed as a data structure for processing propositional logic formulas.\\n\\nIn AI, propositional logic is employed in simple applications. For example, simple expert systems can certainly work with propositional logic. However, the variables must all be discrete, with only a few values, and there may not be any cross-relations between variables. Complex logical connections can be expressed much more elegantly using predicate logic.\\n\\nProbabilistic logic is a very interesting and current combination of propositional logic and probabilistic computation that allows modeling of uncertain knowledge. It is handled thoroughly in Chap. 7.\\n\\n2.8 Exercises\\n\\n➳ Exercise 2.1 Give a Backus–Naur form grammar for the syntax of propositional\\n\\nlogic. Exercise 2.2 Show that the following formulas are tautologies: (a) ¬(A ^ B) , ¬A _ ¬B (b) A ) B , ¬B ) ¬A (c) (d)\\n\\n((A ) B) ^ (B ) A)) , (A , B) (A _ B) ^ (¬B _ C) ) (A _ C)\\n\\nExercise 2.3 Transform the following formulas into conjunctive normal form: (a) A , B (b) A ^ B , A _ B (c) A ^ (A ) B) ) B\\n\\nExercise 2.4 Check the following statements for satisﬁability or validity. (play_lottery ^ six_right) ) winner (a) (play_lottery ^ six_right ^ (six_right ) win)) ) win (b) (c) ¬(¬gas_in_tank ^ (gas_in_tank _ ¬car_starts) ) ¬car_starts)\\n\\n❄ ❄Exercise 2.5 Using the programming language of your choice, program a theorem prover for propositional logic using the truth table method for formulas in conjunctive normal form. To avoid a costly syntax check of the formulas, you may represent clauses as lists or sets of literals, and the formulas as lists or sets of clauses. The program should indicate whether the formula is unsatisﬁable, satisﬁ- able, or true, and output the number of different interpretations and models.\\n\\n37\\n\\n38\\n\\n2 Propositional Logic\\n\\nExercise 2.6 (a) Show that modus ponens is a valid inference rule by showing that\\n\\nA ^ ðA ) BÞ (cid:3) B.\\n\\n(b) Show that the resolution rule (2.1) is a valid inference rule.\\n\\n❄ Exercise 2.7 Show by application of the resolution rule that, in conjunctive normal\\n\\nform, the empty clause is equivalent to the false statement.\\n\\n❄ Exercise 2.8 Show that, with resolution, one can “derive” any arbitrary clause from\\n\\na knowledge base that contains a contradiction. Exercise 2.9 Formalize the following logical functions with the logical operators and show that your formula is valid. Present the result in CNF. (a) The XOR operation (exclusive or) between two variables. (b) The statement at least two of the three variables A, B, C are true.\\n\\n❄ Exercise 2.10 Solve the following case with the help of a resolution proof: “If the criminal had an accomplice, then he came in a car. The criminal had no accomplice and did not have the key, or he had the key and an accomplice. The criminal had the key. Did the criminal come in a car or not?” Exercise 2.11 Show by resolution that the formula from (a) Exercise 2.2(d) is a tautology. (b) Exercise 2.4(c) is unsatisﬁable.\\n\\nExercise 2.12 Prove the following equivalences, which are important for working with Horn clauses: (a) (b) (c) A (cid:2) w ) A\\n\\n(¬A1_ (cid:4)(cid:4)(cid:4) _ ¬Am _ B) (cid:2) A1 ^ (cid:4)(cid:4)(cid:4) ^ Am ) B (¬A1 _ (cid:4)(cid:4)(cid:4) _ ¬Am) (cid:2) A1 ^ (cid:4)(cid:4)(cid:4) ^ Am ) f\\n\\nExercise 2.13 Show by SLD resolution that the following Horn clause set is unsatisﬁable.\\n\\nðAÞ ðBÞ 2 ðCÞ\\n\\n1\\n\\n3\\n\\nðDÞ ðEÞ ðA ^ B ^ C ) FÞ 6\\n\\n4\\n\\n5\\n\\nðA ^ D ) GÞ ðC ^ F ^ E ) HÞ ðH ) f Þ\\n\\n7\\n\\n9\\n\\n8\\n\\n➳ Exercise 2.14 In Sect. 2.6 it says: “Thus it is clear that there is probably (modulo the P/NP problem) no polynomial algorithm for 3-SAT, and thus probably not a general one either.” Justify the “probably” in this sentence.\\n\\nFirst-order Predicate Logic\\n\\nMany practical, relevant problems cannot be or can only very inconveniently be formulated in the language of propositional logic, as we can easily recognize in the following example. The statement\\n\\n“Robot 7 is situated at the xy position (35, 79)”\\n\\ncan in fact be directly used as the propositional logic variable\\n\\n“Robot_7_is_situated_at_xy_position_(35, 79)”\\n\\nfor reasoning with propositional logic, but reasoning with this kind of proposition is very inconvenient. Assume 100 of these robots can stop anywhere on a grid of 100 (cid:1) 100 points. To describe every position of every robot, we would need 100 ⋅ 100 ⋅ 100 = 1 000 000 = 106 different variables. The deﬁnition of relationships between objects (here robots) becomes truly difﬁcult. The relation\\n\\n“Robot A is to the right of robot B.”\\n\\nis semantically nothing more than a set of pairs. Of the 10 000 possible pairs of x-coordinates there are (99 ⋅ 98)/2 = 4851 ordered pairs. Together with all 10 000 combinations of possible y-values for both robots, there are (100 ⋅ 99) = 9900 formulas of the type\\n\\nRobot 7 is to the right of robot 12 ,\\n\\nRobot 7 is situated at xy position ð35; 79Þ ^ Robot 12 is situated at xy position ð10; 93Þ _ . . .\\n\\ndeﬁning these relations, each of them with (104)2 ⋅ 0.485 = 0.485 ⋅ 108 alternatives In ﬁrst-order predicate logic, we can deﬁne a predicate on the right side.\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_3\\n\\n39\\n\\n3\\n\\n40\\n\\n3 First-order Predicate Logic\\n\\nPosition(number, xPosition, yPosition). The above relation must no longer be enumerated as a huge number of pairs, rather it is described abstractly with a rule of the form\\n\\n8u 8v is further rightðu; vÞ ,\\n\\n9xu 9yu 9xv 9yv positionðu; xu; yuÞ ^ positionðv; xv; yvÞ ^ xu [ xv;\\n\\nWhere 8u is read as “for every u” and 9v as “there exists v”.\\n\\nIn this chapter we will deﬁne the syntax and semantics of ﬁrst-order predicate logic (PL1), show that many applications can be modeled with this language and that there is a complete and sound calculus for this language.\\n\\n3.1 Syntax\\n\\nFirst we solidify the syntactic structure of terms.\\n\\nDeﬁnition 3.1 Let V be a set of variables, K a set of constants, and F a set of function symbols. The sets V, K and F are pairwise disjoint. We deﬁne the set of terms recursively: (cid:129) All variables and constants are (atomic) terms. (cid:129) If t1, … , tn are terms and f an n-place function symbol, then f (t1, … , tn) is\\n\\nalso a term.\\n\\nSome examples of terms are f (sin(ln(3)), exp(x)) and g(g(g(x))). To be able to\\n\\nestablish logical relationships between terms, we build formulas from terms.\\n\\nDeﬁnition 3.2 Let P be a set of predicate symbols. Predicate logic formulas are built as follows: (cid:129) If t1, … , tn are terms and p an n-place predicate symbol, then p(t1, …, tn)\\n\\nis an (atomic) formula.\\n\\n(cid:129) If A and B are formulas, then ¬A, (A), A ^ B, A _ B, A ) B, A , B are\\n\\nalso formulas.\\n\\n(cid:129) If x is a variable and A a formula, then 8x A and 9x A are also formulas.\\n\\n8 is the universal quantiﬁer and 9 the existential quantiﬁer.\\n\\n(cid:129) p(t1, … , tn) and ¬p(t1, … , tn) are called literals.\\n\\n3.1 Syntax\\n\\nTable 3.1 Examples of formulas in ﬁrst-order predicate logic. Please note that mother here is a function symbol\\n\\nFormula 8x frog(x) ) green(x) All frogs are green 8x frog(x) ^ brown(x) ) big(x) All brown frogs are big 8x likes(x, cake) Everyone likes cake ¬8x likes(x, cake) Not everyone likes cake ¬9x likes(x, cake) No one likes cake 9x 8y likes(y, x) There is something that everyone likes 9x 8y likes(x, y) There is someone who likes everything 8x 9y likes(y, x) Everything is loved by someone 8x 9y likes(x, y) Everyone likes something 8x customer(x) ) likes(bob, x) Bob likes every customer 9x customer(x) ^ likes(x, bob) There is a customer whom bob likes 9x baker(x) ^ 8y customer(y) ) likes(x, y) There is a baker who likes all of his\\n\\nDescription\\n\\n8x older(mother(x), x) 8x older(mother(mother(x)), x)\\n\\n8x 8y 8z rel(x, y) ^ rel(y, z) ) rel(x, z)\\n\\ncustomers Every mother is older than her child Every grandmother is older than her daughter’s child rel is a transitive relation\\n\\n(cid:129) Formulas in which every variable is in the scope of a quantiﬁer are called ﬁrst-order sentences or closed formulas. Variables which are not in the scope of a quantiﬁer are called free variables.\\n\\n(cid:129) Deﬁnitions 2.8 (CNF) and 2.10 (Horn clauses) hold for formulas of\\n\\npredicate logic literals analogously.\\n\\nIn Table 3.1 several examples of PL1 formulas are given along with their\\n\\nintuitive interpretations.\\n\\n3.2 Semantics\\n\\nIn propositional logic, every variable is directly assigned a truth value by an interpretation. In predicate logic, the meaning of formulas is recursively deﬁned over the construction of the formula, in that we ﬁrst assign constants, variables, and function symbols to objects in the real world.\\n\\n41\\n\\n42\\n\\n3 First-order Predicate Logic\\n\\nDeﬁnition 3.3 An interpretation I is deﬁned as (cid:129) A mapping from the set of constants and variables K [ V to a set W of\\n\\nnames of objects in the world.\\n\\n(cid:129) A mapping from the set of function symbols to the set of functions in the world. Every n-place function symbol is assigned an n-place function. (cid:129) A mapping from the set of predicate symbols to the set of relations in the world. Every n-place predicate symbol is assigned an n-place relation.\\n\\nExample 3.1 Let c1, c2, c3 be constants, “plus” a two-place function symbol, and “gr” a two-place predicate symbol. The truth of the formula\\n\\nF (cid:3) grðplusðc1; c3Þ; c2Þ\\n\\ndepends on the interpretation I. We ﬁrst choose the following obvious interpretation of constants, the function, and of the predicates in the natural numbers:\\n\\nI1: c1 7! 1; c2 7! 2; c3 7! 3;\\n\\nplus 7! þ ;\\n\\ngr 7![ :\\n\\nThus the formula is mapped to\\n\\n1 þ 3 [ 2;\\n\\nor after evaluation\\n\\n4 [ 2:\\n\\nThe greater-than relation on the set {1, 2, 3, 4} is the set of all pairs (x, y) of numbers with x > y, meaning the set G = {(4, 3), (4, 2), (4, 1), (3, 2), (3, 1), (2, 1)}. Because (4, 2) 2 G, the formula F is true under the interpretation I1. However, if we choose the interpretation\\n\\nI2: c1 7! 2; c2 7! 3; c3 7! 1;\\n\\nplus 7! (cid:4);\\n\\ngr 7![ ;\\n\\nwe obtain\\n\\n2 (cid:4) 1 [ 3;\\n\\nor\\n\\n1 [ 3:\\n\\nThe pair (1, 3) is not a member of G. The formula F is false under the interpretation I2. Obviously, the truth of a formula in PL1 depends on the interpretation. Now, after this preview, we deﬁne truth.\\n\\n3.2 Semantics\\n\\nDeﬁnition 3.4 (cid:129) An atomic formula p(t1,…, tn ) is true (or valid) under the interpretation I if, after interpretation and evaluation of all terms t1, … , tn and interpre- tation of the predicate p through the n-place relation r, it holds that\\n\\nðIðt1Þ; . . . ; IðtnÞÞ 2 r:\\n\\n(cid:129) The truth of quantiﬁerless formulas follows from the truth of atomic formulas—as in propositional calculus—through the semantics of the logical operators deﬁned in Table 2.1 on page 25.\\n\\n(cid:129) A formula 8x F is true under the interpretation I exactly when it is true given an arbitrary change of the interpretation for the variable x (and only for x)\\n\\n(cid:129) A formula 9x F is true under the interpretation I exactly when there is an\\n\\ninterpretation for x which makes the formula true.\\n\\nThe deﬁnitions of semantic equivalence of formulas, for the concepts satis- ﬁable, true, unsatisﬁable, and model, along with semantic entailment (Deﬁ- nitions 2.4, 2.5, 2.6) carry over unchanged from propositional calculus to predicate logic.\\n\\nTheorem 3.1 Theorems 2.2 (deduction theorem) and 2.3 (proof by contra- diction) hold analogously for PL1.\\n\\nExample 3.2 The family tree given in Fig. 3.1 graphically represents (in the semantic level) the relation\\n\\nFig. 3.1 A family tree. The edges going from Clyde B. upward to Mary B. and Oscar B. represent the element (Clyde B., Mary B., Oscar B.) as a child relationship\\n\\n43\\n\\n44\\n\\n3 First-order Predicate Logic\\n\\nChild ¼ fðOscar A., Karen A., Frank A.Þ; (Henry A., Anne A., Oscar A.), (Isabelle A., Anne A., Oscar A.),\\n\\nðMary B., Karen A., Frank A.Þ; (Eve A., Anne A., Oscar A.), (Clyde B., Mary B., Oscar B.)g\\n\\nFor example, the triple (Oscar A., Karen A., Frank A.) stands for the proposition “Oscar A. is a child of Karen A. and Frank A.”. From the names we read off the one-place relation\\n\\nFemale ¼ fKaren A., Anne A., Mary B., Eve A., Isabelle A.g\\n\\nof the women. We now want to establish formulas for family relationships. First we deﬁne a three-place predicate child(x, y, z) with the semantic\\n\\nIðchildðx; y; zÞÞ ¼ w (cid:3) ðIðxÞ; IðyÞ; IðzÞÞ 2 Kind:\\n\\nUnder the interpretation IðoscarÞ ¼ Oscar A., IðeveÞ ¼ Eve A., IðanneÞ ¼ Anne A., it is also true that child(eve, anne, oscar). For child(eve, oscar, anne) to be true, we require, with\\n\\n8x 8y 8z childðx; y; zÞ ,child ðx; z; yÞ;\\n\\nsymmetry of the predicate child in the last two arguments. For further deﬁnitions we refer to Exercise 3.1 on page 63 and deﬁne the predicate descendant recursively as\\n\\n8x 8y descendantðx; yÞ , 9z childðx; y; zÞ _\\n\\nð9u 9v childðx; u; vÞ ^ descendantðu; yÞÞ:\\n\\nNow we build a small knowledge base with rules and facts. Let\\n\\nKB (cid:3) femaleðkarenÞ ^ femaleðanneÞ ^ femaleðmaryÞ\\n\\n^ femaleðeveÞ ^ femaleðisabelleÞ ^ childðoscar; karen; franzÞ ^ childðmary; karen; franzÞ ^ childðeve; anne; oscarÞ ^ childðhenry; anne; oscarÞ ^ childðisabelle; anne; oscarÞ ^ childðclyde; mary; oscarbÞ ^ ð8x 8y 8z childðx; y; zÞ )child ðx; z; yÞÞ ^ ð8x 8y descendantðx; yÞ , 9z childðx; y; zÞ _ ð9u 9v childðx; u; vÞ ^ descendantðu; yÞÞÞ:\\n\\nWe can now ask, for example, whether the propositions child(eve, oscar, anne) or descendant(eve, franz) are derivable. To that end we require a calculus.\\n\\n3.2 Semantics\\n\\n3.2.1 Equality\\n\\nTo be able to compare terms, equality is a very important relation in predicate logic. The equality of terms in mathematics is an equivalence relation, meaning it is reﬂexive, symmetric and transitive. If we want to use equality in formulas, we must either incorporate these three attributes as axioms in our knowledge base, or we must integrate equality into the calculus. We take the easy way and deﬁne a predicate “=” which, deviating from Deﬁnition 3.2 on page 40, is written using inﬁx notation as is customary in mathematics. (An equation x = y could of course also be written in the form eq(x, y).) Thus, the equality axioms have the form\\n\\n8x 8x 8y 8x 8y 8z\\n\\nx ¼ x x ¼ y ) y ¼ x x ¼ y ^ y ¼ z ) x ¼ z\\n\\nðreflexivityÞ ðsymmetryÞ ðtransitivityÞ:\\n\\nTo guarantee the uniqueness of functions, we additionally require\\n\\n8x 8y x ¼ y ) f ðxÞ ¼ f ðyÞ\\n\\nðsubstitution axiomÞ\\n\\nfor every function symbol. Analogously we require for all predicate symbols\\n\\n8x 8y x ¼ y ) pðxÞ , pðyÞ\\n\\nðsubstitution axiomÞ:\\n\\nWe formulate other mathematical relations, such as the “<” relation, by similar means (Exercise 3.4 on page 64).\\n\\nOften a variable must be replaced by a term. To carry this out correctly and\\n\\ndescribe it simply, we give the following deﬁnition.\\n\\nDeﬁnition 3.5 We write u[x/t] for the formula that results when we replace every free occurrence of the variable x in u with the term t. Thereby we do not allow any variables in the term t that are quantiﬁed in u. In those cases variables must be renamed to ensure this.\\n\\nExample 3.3 If, in the formula 8x x ¼ y, the free variable y is replaced by the term x + 1, the result is 8x x ¼ x þ 1. With correct substitution we obtain the formula 8x x ¼ y þ 1, which has a very different semantic.\\n\\n3.3 Quantifiers and Normal Forms\\n\\nBy Deﬁnition 3.4 on page 43, the formula 8x p(x) is true if and only if it is true for all interpretations of the variable x. Instead of the quantiﬁer, one could write p(a1) ^ ⋅⋅⋅ ^ p(an) for all constants a1⋅⋅⋅ an in K. For 9x p(x) one could write p(a1) _ ⋅⋅⋅ _ p(an). From this it follows with de Morgan’s law that\\n\\n45\\n\\nð3:1Þ\\n\\nð3:2Þ\\n\\nð3:3Þ\\n\\n46\\n\\n3 First-order Predicate Logic\\n\\n8x u (cid:3) :9x:u:\\n\\nThrough this equivalence, universal, and existential quantiﬁers are mutually replaceable.\\n\\nExample 3.4 The proposition “Everyone wants to be loved” is equivalent to the proposition “Nobody does not want to be loved”.\\n\\nQuantiﬁers are an important component of predicate logic’s expressive power. However, they are disruptive for automatic inference in AI because they make the structure of formulas more complex and increase the number of applicable inference rules in every step of a proof. Therefore our next goal is to ﬁnd, for every predicate logic formula, an equivalent formula in a standardized normal form with as few quantiﬁers as possible. As a ﬁrst step we bring universal quantiﬁers to the beginning of the formula and thus deﬁne\\n\\nDeﬁnition 3.6 A predicate logic formula u is in prenex normal form if it holds that (cid:129) u = Q1x1 ⋅⋅⋅ Qnxn w. (cid:129) w is a quantiﬁerless formula. (cid:129) Qi 2{8, 9} for i = 1, … , n.\\n\\nCaution is advised if a quantiﬁed variable appears outside the scope of its\\n\\nquantiﬁer, as for example x in\\n\\n8x pðxÞ ) 9x qðxÞ:\\n\\nHere one of the two variables must be renamed, and in\\n\\n8x pðxÞ ) 9y qðyÞ\\n\\nthe quantiﬁer can easily be brought to the front, and we obtain as output the equivalent formula\\n\\n8x 9y pðxÞ )q ðyÞ:\\n\\nIf, however, we wish to correctly bring the quantiﬁer to the front of\\n\\nð8x pðxÞÞ ) 9y qðyÞ\\n\\nð3:4Þ\\n\\n3.3 Quantifiers and Normal Forms\\n\\nwe ﬁrst write the formula in the equivalent form\\n\\n:ð8x pðxÞÞ _ 9y qðyÞ:\\n\\nThe ﬁrst universal quantiﬁer now turns into\\n\\nð9x:pðxÞÞ _ 9y qðyÞ\\n\\nand now the two quantiﬁers can ﬁnally be pulled forward to\\n\\n9x 9y:pðxÞ _ qðyÞ;\\n\\nwhich is equivalent to\\n\\n9x 9ypðxÞ ) qðyÞ:\\n\\nWe see then that in (3.4) on page 46 we cannot simply pull both quantiﬁers to the front. Rather, we must ﬁrst eliminate the implications so that there are no negations on the quantiﬁers. It holds in general that we may only pull quantiﬁers out if negations only exist directly on atomic sub-formulas.\\n\\nExample 3.5 As is well known in analysis, convergence of a series (an)n2ℕ to a limit a is deﬁned by\\n\\n8e [ 0 9n0 2 N 8n [ n0jan (cid:4) aj \\\\ e:\\n\\nWith the function abs(x) for |x|, a(n) for an, minus(x, y) for x–y and the predicates el(x, y) for x 2 y, gr(x, y) for x > y, the formula reads\\n\\n8e ðgrðe; 0Þ ) 9n0 ðelðn0; NÞ ) 8n ðgrðn; n0Þ )gr ðe; absðminusðaðnÞ; aÞÞÞÞÞÞ:\\n\\nThis is clearly not in prenex normal form. Because the variables of the inner quantiﬁers 9n0 and 8n do not occur to the left of their respective quantiﬁers, no variables must be renamed. Next we eliminate the implications and obtain\\n\\n8e ð:grðe; 0Þ _ 9n0 ð:elðn0; NÞ _ 8n ð:grðn; n0Þ _ grðe; absðminusðaðnÞ; aÞÞÞÞÞÞ:\\n\\nBecause every negation is in front of an atomic formula, we bring the quantiﬁers forward, eliminate the redundant parentheses, and with\\n\\n8e 9n0 8n ð:grðe; 0Þ _ :elðn0; NÞ _ :grðn; n0Þ _ grðe; absðminusðaðnÞ; aÞÞÞÞ\\n\\nit becomes a quantiﬁed clause in conjunctive normal form.\\n\\n47\\n\\nð3:5Þ\\n\\n48\\n\\n3 First-order Predicate Logic\\n\\nThe transformed formula is equivalent to the output formula. The fact that this\\n\\ntransformation is always possible is guaranteed by\\n\\nTheorem 3.2 Every predicate logic formula can be transformed into an equivalent formula in prenex normal form.\\n\\nIn addition, we can eliminate all existential quantiﬁers. However, the formula resulting from the so-called Skolemization is no longer equivalent to the output formula. Its satisﬁability, however, remains unchanged. In many cases, especially when one wants to show the unsatisﬁability of KB ^ ¬Q, this is sufﬁcient. The following formula in prenex normal form will now be skolemized:\\n\\n8x1 8x2 9y1 8x3 9y2 pðf ðx1Þ; x2; y1Þ _ qðy1; x3; y2Þ:\\n\\nBecause the variable y1 apparently depends on x1 and x2, every occurrence of y1 is replaced by a Skolem function g(x1, x2). It is important that g is a new function symbol that has not yet appeared in the formula. We obtain\\n\\n8x1 8x2 8x3 9y2 pðf ðx1Þ; x2; gðx1; x2ÞÞ _ qðgðx1; x2Þ; x3; y2Þ\\n\\nand replace y2 analogously by h(x1, x2, x3), which leads to\\n\\n8x1 8x2 8x3 pðf ðx1Þ; x2; gðx1; x2ÞÞ _ qðgðx1; x2Þ; x3; hðx1; x2; x3ÞÞ:\\n\\nBecause now all the variables are universally quantiﬁed, the universal quantiﬁers can be left out, resulting in\\n\\npðf ðx1Þ; x2; gðx1; x2ÞÞ _ qðgðx1; x2Þ; x3; hðx1; x2; x3ÞÞ:\\n\\nNow we can eliminate the existential quantiﬁer (and thereby also the universal quantiﬁer) in (3.5) on page 47 by introducing the Skolem function n0(e). The skolemized prenex and conjunctive normal form of (3.5) on page 47 thus reads\\n\\n:grðe; 0Þ _ :elðn0ðeÞ; NÞ _ :grðn; n0ðeÞÞ _ grðe; absðminusðaðnÞ; aÞÞÞ:\\n\\nBy dropping the variable n0, the Skolem function can receive the name n0.\\n\\nWhen skolemizing a formula in prenex normal form, all existential quanti- ﬁers are eliminated from the outside inward, where a formula of the form 8x1 … 8xn 9y u is replaced by 8x1 … 8xn u[y/f (x1, … , xn)], during which f may not appear in u. If an existential quantiﬁer is on the far outside, such as in 9y p(y), then y must be replaced by a constant (that is, by a zero-place function symbol).\\n\\n3.3 Quantifiers and Normal Forms\\n\\nNORMALFORMTRANSFORMATION(Formula): 1. Transformation into prenex normal form:\\n\\nTransformation into conjunctive normal form (Theorem 2.1):\\n\\nElimination of equivalences. Elimination of implications. Repeated application of de Morgan’s law and distributive law.\\n\\nRenaming of variables if necessary. Factoring out universal quantiﬁers.\\n\\n2. Skolemization:\\n\\nReplacement of existentially quantiﬁed variables by new Skolem functions. Deletion of resulting universal quantiﬁers.\\n\\nFig. 3.2 Transformation of predicate logic formulas into normal form\\n\\nThe procedure for transforming a formula in conjunctive normal form is sum- marized in the pseudocode represented in Fig. 3.2. Skolemization has polynomial runtime in the number of literals. When transforming into normal form, the number of literals in the normal form can grow exponentially, which can lead to exponential computation time and exponential memory usage. The reason for this is the repe- ated application of the distributive law. The actual problem, which results from a large number of clauses, is the combinatorial explosion of the search space for a subsequent resolution proof. However, there is an optimized transformation algo- rithm which only spawns polynomially many literals [Ede91].\\n\\n3.4 Proof Calculi\\n\\nFor reasoning in predicate logic, various calculi of natural reasoning such as Gentzen calculus or sequent calculus, have been developed. As the name suggests, these calculi are meant to be applied by humans, since the inference rules are more or less intuitive and the calculi work on arbitrary PL1 formulas. In the next section we will primarily concentrate on the resolution calculus, which is in practice the most important efﬁcient, automatizable calculus for formulas in conjunctive normal form. Here, using Example 3.2 on page 43 we will give a very small “natural” proof. We use the inference rule\\n\\nA; A ) B B\\n\\nðmodus ponens, MPÞ\\n\\nand\\n\\n8x A A½x=t(cid:5)\\n\\nð8-elimination; 8EÞ:\\n\\nThe modus ponens is already familiar from propositional logic. When eliminating universal quantiﬁers one must keep in mind that the quantiﬁed variable x must be\\n\\n49\\n\\n50\\n\\n3 First-order Predicate Logic\\n\\nTable 3.2 Simple proof with modus ponens and quantiﬁer elimination\\n\\nWB: WB: 8E(2): x/eve, y/anne, z/oscar MP(1, 3)\\n\\n1 2 3 4\\n\\nchild(eve, anne, oscar) 8x 8y 8z child(x, y, z) ) child(x, z, y) child(eve, anne, oscar) ) child(eve, oscar, anne) child(eve, oscar, anne)\\n\\nreplaced by a ground term t, meaning a term that contains no variables. The proof of child(eve, oscar, anne) from an appropriately reduced knowledge base is presented in Table 3.2.\\n\\nThe two formulas of the reduced knowledge base are listed in rows 1 and 2. In row 3 the universal quantiﬁers from row 2 are eliminated, and in row 4 the claim is derived with modus ponens.\\n\\nThe calculus consisting of the two given inference rules is not complete. However, it can be extended into a complete procedure by addition of further inference rules. This nontrivial fact is of fundamental importance for mathematics and AI. The Austrian logician Kurt Gödel proved in 1931 that [Göd31a].\\n\\nTheorem 3.3 (Gödel’s completeness theorem) First-order predicate logic is complete. That is, there is a calculus with which every proposition that is a consequence of a knowledge base KB can be proved. If KB (cid:6) u, then it holds that KB ⊢ u.\\n\\nEvery true proposition in ﬁrst-order predicate logic is therefore provable. But is the reverse also true? Is everything we can derive syntactically actually true? The answer is “yes”:\\n\\nTheorem 3.4 (Correctness) There are calculi with which only true propo- sitions can be proved. That is, if KB ⊢ u holds, then KB (cid:6) u.\\n\\nIn fact, nearly all known calculi are correct. After all, it makes little sense to work with incorrect proof methods. Provability and semantic consequence are therefore equivalent concepts, as long as correct and complete calculus is being used. Thereby ﬁrst-order predicate logic becomes a powerful tool for mathematics and AI. The aforementioned calculi of natural deduction are rather unsuited for automatization. Only resolution calculus, which was introduced in 1965 and essentially works with only one simple inference rule, enabled the construction of powerful automated theorem provers, which later were employed as inference machines for expert systems.\\n\\n3.5 Resolution\\n\\n3.5 Resolution\\n\\nIndeed, the correct and complete resolution calculus triggered a logic euphoria during the 1970s. Many scientists believed that one could formulate almost every task of knowledge representation and reasoning in PL1 and then solve it with an automated prover. Predicate logic, a powerful, expressive language, together with a complete proof calculus seemed to be the universal intelligent machine for repre- senting knowledge and solving many difﬁcult problems (Fig. 3.3).\\n\\nIf one feeds a set of axioms (that is, a knowledge base) and a query into such a logic machine as input, the machine searches for a proof and returns it—for one exists and will be found—as output. With Gödel’s completeness theorem and the work of Herbrand as a foundation, much was invested into the mechanization of logic. The vision of a machine that could, with an arbitrary non-contradictory PL1 knowledge base, prove any true query was very enticing. Accordingly, until now many proof calculi for PL1 are being developed and realized in the form of theorem provers. As an example, here we describe the historically important and widely used resolution calculus and show its capabilities. The reason for selecting reso- lution as an example of a proof calculus in this book is, as stated, its historical and\\n\\nFig. 3.3 The universal logic machine\\n\\n51\\n\\n52\\n\\n3 First-order Predicate Logic\\n\\ndidactic importance. Today, resolution represents just one of many calculi used in high-performance provers.\\n\\nWe begin by trying to compile the proof in Table 3.2 on page 50 with the knowledge base of Example 3.2 on page 43 into a resolution proof. First the formulas are transformed into conjunctive normal form and the negated query\\n\\n:Q (cid:3) :childðeve; oscar; anneÞ\\n\\nis added to the knowledge base, which gives\\n\\nKB ^ :Q (cid:3) ðchildðeve; anne; oscarÞÞ1 ^\\n\\nð:childðx; y; zÞ _ childðx; z; yÞÞ2 ^ ð:childðeve; oscar; anneÞÞ3\\n\\n:\\n\\nThe proof could then look something like\\n\\nð2Þ x=eve; y=anne; z=oscar : ð:childðeve; anne; oscarÞ _\\n\\nchildðeve; oscar; anneÞÞ4\\n\\nResð3; 4Þ : ð:childðeve; anne; oscarÞÞ5 Resð1; 5Þ : ðÞ6\\n\\n;\\n\\nwhere, in the ﬁrst step, the variables x, y, z are replaced by constants. Then two resolution steps follow under application of the general resolution rule from (2.2), which was taken unchanged from propositional logic.\\n\\nThe circumstances in the following example are somewhat more complex. We assume that everyone knows his own mother and ask whether Henry knows anyone. With the function symbol “mother” and the predicate “knows”, we have to derive a contradiction from\\n\\nðknowsðx; motherðxÞÞÞ1 ^ ð:knowsðhenry; yÞÞ2\\n\\n:\\n\\nBy the replacement x/henry, y/mother(henry) we obtain the contradictory clause pair\\n\\nðknowsðhenry; motherðhenryÞÞÞ1 ^ ð:knowsðhenry; motherðhenryÞÞÞ2\\n\\n:\\n\\nThis replacement step is called uniﬁcation. The two literals are complementary, which means that they are the same other than their signs. The empty clause is now derivable with a resolution step, by which it has been shown that Henry does know someone (his mother). We deﬁne\\n\\n3.5 Resolution\\n\\nDeﬁnition 3.7 Two literals are called uniﬁable if there is a substitution r for all variables which makes the literals equal. Such a r is called a uniﬁer. A uniﬁer is called the most general uniﬁer (MGU) if all other uniﬁers can be obtained from it by substitution of variables.\\n\\nExample 3.6 We want to unify the literals p(f(g(x)), y, z) and p(u, u, f(u)). Several uniﬁers are\\n\\nr1 : r2 : r3 : r4 : r5 :\\n\\nx=hðvÞ; x=hðhðvÞÞ; x=hðaÞ; x=a;\\n\\ny=f ðgðxÞÞ; y=f ðgðhðvÞÞÞ; y=f ðgðhðhðvÞÞÞÞ; y=f ðgðhðaÞÞÞ; y=f ðgðaÞÞ;\\n\\nz=f ðf ðgðxÞÞÞ; z=f ðf ðgðhðvÞÞÞÞ; z=f ðf ðgðhðhðvÞÞÞÞÞ; z=f ðf ðgðhðaÞÞÞÞ; z=f ðf ðgðaÞÞÞ;\\n\\nu=f ðgðxÞÞ; u=f ðgðhðvÞÞÞ u=f ðgðhðhðvÞÞÞÞ u=f ðgðhðaÞÞÞ u=f ðgðaÞÞ\\n\\nwhere r1 is the most general uniﬁer. The other uniﬁers result from r1 through the substitutions x=hðvÞ; x=hðhðvÞÞ; x=hðaÞ; x=a.\\n\\nWe can see in this example that during uniﬁcation of literals, the predicate symbols can be treated like function symbols. That is, the literal is treated like a term. Implementations of uniﬁcation algorithms process the arguments of functions sequentially. Terms are uniﬁed recursively over the term structure. The simplest uniﬁcation algorithms are very fast in most cases. In the worst case, however, the computation time can grow exponentially with the size of the terms. Because for automated provers the overwhelming number of uniﬁcation attempts fail or are very simple, in most cases the worst case complexity has no dramatic effect. The fastest uniﬁcation algorithms have nearly linear complexity even in the worst case [Bib82].\\n\\nWe can now give the general resolution rule for predicate logic:\\n\\nDeﬁnition 3.8 The resolution rule for two clauses in conjunctive normal form reads\\n\\nðA1 _ (cid:7) (cid:7) (cid:7) _ Am _ BÞ;\\n\\nð:B0 _ C1 _ (cid:7) (cid:7) (cid:7) _ CnÞ rðBÞ ¼ rðB0Þ\\n\\nðrðA1Þ _ (cid:7) (cid:7) (cid:7) _ rðAmÞ _ rðC1Þ _ (cid:7) (cid:7) (cid:7) _ rðCnÞÞ\\n\\n;\\n\\nð3:6Þ\\n\\nwhere r is the MGU of B and B′.\\n\\nTheorem 3.5 The resolution rule is correct. That is, the resolvent is a semantic consequence of the two parent clauses.\\n\\n53\\n\\n54\\n\\n3 First-order Predicate Logic\\n\\nFor Completeness, however, we still need a small addition, as is shown in the\\n\\nfollowing example.\\n\\nExample 3.7 The famous Russell paradox reads “There is a barber who shaves everyone who does not shave himself.” This statement is contradictory, meaning it is unsatisﬁable. We wish to show this with resolution. Formalized in PL1, the paradox reads\\n\\n8x shavesðbarber; xÞ , :shavesðx; xÞ\\n\\nand transformation into clause form yields (see Exercise 3.6 on page 64)\\n\\nð:shavesðbarbier; xÞ _ :shavesðx; xÞÞ1 ^ ðshavesðbarbier; xÞ _ shavesðx; xÞÞ2 : ð3:7Þ\\n\\nFrom these two clauses we can derive several tautologies, but no contradiction. Thus resolution is not complete. We need yet a further inference rule.\\n\\nDeﬁnition 3.9 Factorization of a clause is accomplished by\\n\\nðA1 _ A2 _ (cid:7) (cid:7) (cid:7) _ AnÞ rðA1Þ ¼ rðA2Þ ðrðA2Þ _ (cid:7) (cid:7) (cid:7) _ rðAnÞÞ\\n\\n;\\n\\nwhere r is the MGU of A1 and A2.\\n\\nNow a contradiction can be derived from (3.7)\\n\\nFakð1; r : x=barberÞ : Fakð2; r : x=barberÞ : Resð3; 4Þ :\\n\\nð:shavesðbarber; barberÞÞ3 ðshavesðbarber; barberÞÞ4 ðÞ5\\n\\nand we assert:\\n\\nTheorem 3.6 The resolution rule (3.6) together with the factorization rule (3.9) is refutation complete. That is, by application of factorization and resolution steps, the empty clause can be derived from any unsatisﬁable formula in conjunctive normal form.\\n\\n3.5 Resolution\\n\\n3.5.1 Resolution Strategies\\n\\nWhile completeness of resolution is important for the user, the search for a proof can be very frustrating in practice. The reason for this is the immense combinatorial search space. Even if there are only very few pairs of clauses in KB ^ ¬Q in the beginning, the prover generates a new clause with every resolution step, which increases the number of possible resolution steps in the next iteration. Thus it has long been attempted to reduce the search space using special strategies, preferably without losing completeness. The most important strategies are the following.\\n\\nUnit resolution prioritizes resolution steps in which one of the two clauses consists of only one literal, called a unit clause. This strategy preserves com- pleteness and leads in many cases, but not always, to a reduction of the search space. It therefore is a heuristic process (see Sect. 6.3).\\n\\nOne obtains a guaranteed reduction of the search space by application of the set of support strategy. Here a subset of KB ^ ¬Q is deﬁned as the set of support (SOS). Every resolution step must involve a clause from the SOS, and the resolvent is added to the SOS. This strategy is incomplete. It becomes complete when it is ensured that the set of clauses is satisﬁable without the SOS (see Exercise 3.7 on page 64). The negated query ¬Q is often used as the initial SOS.\\n\\nIn input resolution, a clause from the input set KB ^ ¬Q must be involved in every resolution step. This strategy also reduces the search space, but at the cost of completeness.\\n\\nWith the pure literal rule all clauses that contain literals for which there are no complementary literals in other clauses can be deleted. This rule reduces the search space and is complete, and therefore it is used by practically all resolution provers. If the literals of a clause K1 represent a subset of the literals of the clause K2, then\\n\\nK2 can be deleted. For example, the clause\\n\\nðrainingðtodayÞ ) street wetðtodayÞÞ\\n\\nis redundant if street_wet(today) is already valid. This important reduction step is called subsumption. Subsumption, too, is complete.\\n\\n3.5.2 Equality\\n\\nEquality is an especially inconvenient cause of explosive growth of the search space. If we add (3.1) on page 45 and the equality axioms formulated in (3.2) on page 45 to the knowledge base, then the symmetry clause ¬x = y _ y = x can be uniﬁed with every positive or negated equation, for example. This leads to the derivation of new clauses and equations upon which equality axioms can again be applied, and so on. The transitivity and substitution axioms have similar conse- quences. Because of this, special inference rules for equality have been developed which get by without explicit equality axioms and, in particular, reduce the search\\n\\n55\\n\\n56\\n\\n3 First-order Predicate Logic\\n\\nspace. Demodulation, for example, allows substitution of a term t2 for t1, if the equation t1 = t2 exists. An equation t1 = t2 is applied by means of uniﬁcation to a term t as follows:\\n\\nt1 ¼ t2;\\n\\nð. . . t. . .Þ; rðt1Þ ¼ rðtÞ ð. . . rðt2Þ. . .Þ\\n\\n:\\n\\nSomewhat more general is paramodulation, which works with conditional equa- tions [Bib82, Lov78].\\n\\nThe equation t1 = t2 allows the substitution of the term t1 by t2 as well as the substitution t2 by t1. It is usually pointless to reverse a substitution that has already been carried out. On the contrary, equations are frequently used to simplify terms. They are thus often used in one direction only. Equations which are only used in one direction are called directed equations. Efﬁcient processing of directed equa- tions is accomplished by so-called term rewriting systems. For formulas with many equations there exist special equality provers.\\n\\n3.6 Automated Theorem Provers\\n\\nImplementations of proof calculi on computers are called theorem provers. Along with specialized provers for subsets of PL1 or special applications, there exist today a whole line of automated provers for the full predicate logic and higher-order logics, of which only a few will be discussed here. An overview of the most important systems can be found in [McC].\\n\\nOne of the oldest resolution provers was developed at the Argonne National Laboratory in Chicago. Based on early developments starting in 1963, Otter [Kal01], was created in 1984. Above all, Otter was successfully applied in specialized areas of mathematics, as one can learn from its home page:\\n\\n“Currently, the main application of Otter is research in abstract algebra and formal logic. Otter and its predecessors have been used to answer many open questions in the areas of ﬁnite semigroups, ternary Boolean algebra, logic calculi, combinatory logic, group theory, lattice theory, and algebraic geometry.”\\n\\nSeveral years later the University of Technology, Munich, created the high- performance prover SETHEO [LSBB92] based on fast PROLOG technology. With the goal of reaching even higher performance, an implementation for parallel computers was developed under the name PARTHEO. It turned out that it was not worthwhile to use special hardware in theorem provers, as is also the case in other areas of AI, because these computers are very quickly overtaken by faster pro- cessors and more intelligent algorithms. Munich is also the birthplace of E [Sch02], an award-winning modern equation prover, which we will become familiar with in the next example. On E’s homepage one can read the following compact, ironic characterization, whose second part incidentally applies to all automated provers in existence today.\\n\\n3.6 Automated Theorem Provers\\n\\n“E is a purely equational theorem prover for clausal logic. That means it is a program that you can stuff a mathematical speciﬁcation (in clausal logic with equality) and a hypothesis into, and which will then run forever, using up all of your machines resources. Very occasionally it will ﬁnd a proof for the hypothesis and tell you so ;-).”\\n\\nFinding proofs for true propositions is apparently so difﬁcult that the search suc- ceeds only extremely rarely, or only after a very long time—if at all. We will go into this in more detail in Chap. 4. Here it should be mentioned, though, that not only computers, but also most people have trouble ﬁnding strict formal proofs.\\n\\nThough evidently computers by themselves are in many cases incapable of ﬁnding a proof, the next best thing is to build systems that work semi-automatically and allow close cooperation with the user. Thereby the human can better apply his knowledge of special application domains and perhaps limit the search for the proof. One of the most successful interactive provers for higher-order predicate logic is Isabelle [NPW02], a common product of Cambridge University and the University of Technology, Munich.\\n\\nAnyone searching for a high-performance prover should look at the current results of the CASC (CADE ATP System Competition) [SS06].1 Here we ﬁnd that the winner from 2001 to 2006 in the PL1 and clause normal form categories was Manchester’s prover Vampire, which works with a resolution variant and a special approach to equality. The system Waldmeister of the Max Planck Institute in Saarbrücken has been leading for years in equality proving.\\n\\nThe many top positions of German systems at CASC show that German research groups in the area of automated theorem proving are playing a leading role, today as well as in the past.\\n\\n3.7 Mathematical Examples\\n\\nWe now wish to demonstrate the application of an automated prover with the aforementioned prover E [Sch02]. E is a specialized equality prover which greatly shrinks the search space through an optimized treatment of equality.\\n\\nWe want to prove that left- and right-neutral elements in a semigroup are equal.\\n\\nFirst we formalize the claim step by step.\\n\\nDeﬁnition 3.10 A structure (M,⋅) consisting of a set M with a two-place inner operation “⋅” is called a semigroup if the law of associativity\\n\\n8x 8y 8z ðx (cid:7) yÞ (cid:7) z ¼ x (cid:7) ðy (cid:7) zÞ\\n\\nholds. An 8x e (cid:7) x ¼ x ð8x x (cid:7) e ¼ xÞ.\\n\\nelement\\n\\ne2 M is\\n\\ncalled\\n\\nleft-neutral\\n\\n(right-neutral)\\n\\n1CADE is the annual “Conference on Automated Deduction” [CAD] and ATP stands for “Automated Theorem Prover”.\\n\\nif\\n\\n57\\n\\n58\\n\\n3 First-order Predicate Logic\\n\\nIt remains to be shown that\\n\\nTheorem 3.7 If a semigroup has a left-neutral element el and a right-neutral element er, then el = er.\\n\\nFirst we prove the theorem semi-formally by intuitive mathematical reasoning.\\n\\nClearly it holds for all x 2 M that\\n\\nel (cid:7) x ¼ x\\n\\nð3:8Þ\\n\\nand\\n\\nx (cid:7) er ¼ x:\\n\\nð3:9Þ\\n\\nIf we set x = er in (3.8) and x = el in (3.9), we obtain the two equations el ⋅ er = er and el ⋅ er = el. Joining these two equations yields\\n\\nel ¼ el (cid:7) er ¼ er;\\n\\nwhich we want to prove. In the last step, incidentally, we used the fact that equality is symmetric and transitive.\\n\\nBefore we apply the automated prover, we carry out the resolution proof man- ually. First we formalize the negated query and the knowledge base KB, consisting of the axioms as clauses in conjunctive normal form:\\n\\nð:el ¼ erÞ1 ðmðmðx; yÞ; zÞ ¼ mðx; mðy; zÞÞÞ2 ðmðel; xÞ ¼ xÞ3 ðmðx; erÞ ¼ xÞ4\\n\\nnegated query\\n\\nequality axioms: ðx ¼ xÞ5 ð:x ¼ y _ y ¼ xÞ6 ð:x ¼ y _ :y ¼ z _ x ¼ zÞ7 ð:x ¼ y _ mðx; zÞ ¼ mðy; zÞÞ8 ð:x ¼ y _ mðz; xÞ ¼ mðz; yÞÞ9\\n\\n(reflexivity)\\n\\n(symmetry)\\n\\n(transitivity) substitution in m substitution in m;\\n\\nwhere multiplication is represented by the two-place function symbol m. The equality axioms were formulated analogously to (3.1) on page 45 and (3.2) on page 45. A simple resolution proof has the form\\n\\n3.7 Mathematical Examples\\n\\nResð3; 6; x6=mðel; x3Þ; y6=x3Þ : Resð7; 10; x7=x10; y7=mðel; x10ÞÞ : Resð4; 11; x4=el; x11=er; z11=elÞ : Resð1; 12; ;Þ :\\n\\nðx ¼ mðel; xÞÞ10 ð:mðel; xÞ ¼ z _ x ¼ zÞ11 ðer ¼ elÞ12 ðÞ:\\n\\nHere, for example, Res(3, 6, x6/m(el , x3), y6/x3) means that in the resolution of clause 3 with clause 6, the variable x from clause 6 is replaced by m(el, x3) with variable x from clause 3. Analogously, y from clause 6 is replaced by x from clause 3.\\n\\nNow we want to apply the prover E to the problem. The clauses are transformed\\n\\ninto the clause normal form language LOP through the mapping\\n\\n(¬A1_ (cid:7)(cid:7)(cid:7) _ ¬Am _ B1 _ (cid:7)(cid:7)(cid:7) _ Bn)\\n\\n7! B1;(cid:7)(cid:7)(cid:7) ; Bn < –A1,(cid:7)(cid:7)(cid:7) , Am.\\n\\nThe syntax of LOP represents an extension of the PROLOG syntax (see Chap. 5) for non Horn clauses. Thus we obtain as an input ﬁle for E\\n\\n.)re,le(qe-< eq( m(m(X,Y),Z), m(X,m(Y,Z)) ). eq( m(el,X), X ). eq( m(X,er), X ). .)X,X(qe eq(Y,X) <- eq(X,Y). eq(X,Z) <- eq(X,Y), eq(Y,Z). eq( m(X,Z), m(Y,Z) ) <- eq(X,Y). eq( m(Z,X), m(Z,Y) ) <- eq(X,Y).\\n\\nyreuq# # associativity of m # left-neutral element of m # right-neutral element of m ytivixelfer:ytilauqe# # equality: symmetry # equality: transitivity # equality: substitution in m # equality: substitution in m\\n\\nwhere equality is modeled by the predicate symbol eq. Calling the prover delivers\\n\\nunixprompt> eproof halbgr1.lop # Problem status determined, constructing proof object # Evidence for problem status starts 0 : [--eq(el,er)] : initial 1 : [++eq(X1,X2),--eq(X2,X1)] : initial 2 : [++eq(m(el,X1),X1)] : initial 3 : [++eq(m(X1,er),X1)] : initial 4 : [++eq(X1,X2),--eq(X1,X3),--eq(X3,X2)] : initial 5 : [++eq(X1,m(X1,er))] : pm(3,1) 6 : [++eq(X2,X1),--eq(X2,m(el,X1))] : pm(2,4) 7 : [++eq(el,er)] : pm(5,6) 8 : [] : sr(7,0) 9 : [] : 8 : {proof}\\n\\n# Evidence for problem status ends\\n\\n59\\n\\n60\\n\\n3 First-order Predicate Logic\\n\\nPositive literals are identiﬁed by ++ and negative literals by – –. In lines 0 to 4, marked with initial, the clauses from the input data are listed again. pm(a, b) stands for a resolution step between clause a and clause b. We see that the proof found by E is very similar to the manually created proof. Because we explicitly model the equality by the predicate eq, the particular strengths of E do not come into play. Now we omit the equality axioms and obtain\\n\\n.re=le-< m(m(X,Y),Z) = m(X,m(Y,Z)) . m(el,X) = X . m(X,er) = X .\\n\\nyreuq% % associativity of m % left-neutral element of m % right-neutral element of m\\n\\nas input for the prover.\\n\\nThe proof also becomes more compact. We see in the following output of the prover that the proof consists essentially of a single inference step on the two relevant clauses 1 and 2.\\n\\nunixprompt> eproof halbgr1a.lop # Problem status determined, constructing proof object # Evidence for problem status starts 0 : [--equal(el, er)] : initial 1 : [++equal(m(el,X1), X1)] : initial 2 : [++equal(m(X1,er), X1)] : initial 3 : [++equal(el, er)] : pm(2,1) 4 : [--equal(el, el)] : rw(0,3) 5 : [] : cn(4) 6 : [] : 5 : {proof}\\n\\n# Evidence for problem status ends\\n\\nThe reader might now take a closer look at the capabilities of E (Exercise 3.9 on page 64).\\n\\n3.8 Applications\\n\\nIn mathematics automated theorem provers are used for certain specialized tasks. For example, the important four color theorem of graph theory was ﬁrst proved in 1976 with the help of a special prover. However, automated provers still play a minor role in mathematics.\\n\\n3.8 Applications\\n\\nOn the other hand, in the beginning of AI, predicate logic was of great impor- tance for the development of expert systems in practical applications. Due to its problems modeling uncertainty (see Sect. 4.4), expert systems today are most often developed using other formalisms.\\n\\nToday logic plays an ever more important role in veriﬁcation tasks. Automatic program veriﬁcation is currently an important research area between AI and soft- ware engineering. Increasingly complex software systems are now taking over tasks of more and more responsibility and security relevance. Here a proof of certain safety characteristics of a program is desirable. Such a proof cannot be brought about through testing of a ﬁnished program, for in general it is impossible to apply a program to all possible inputs. This is therefore an ideal domain for general or even specialized inference systems. Among other things, cryptographic protocols are in use today whose security characteristics have been automatically veriﬁed [FS97, Sch01]. A further challenge for the use of automated provers is the synthesis of software and hardware. To this end, for example, provers should support the software engineer in the generation of programs from speciﬁcations.\\n\\nSoftware reuse is also of great importance for many programmers today. The programmer looks for a program that takes input data with certain properties and calculates a result with desired properties. A sorting algorithm accepts input data with entries of a certain data type and from these creates a permutation of these entries with the property that every element is less than or equal to the next element. The programmer ﬁrst formulates a speciﬁcation of the query in PL1 consisting of two parts. The ﬁrst part PREQ comprises the preconditions, which must hold before the desired program is applied. The second part POSTQ contains the postconditions, which must hold after the desired program is applied.\\n\\nIn the next step a software database must be searched for modules which fulﬁll these requirements. To check this formally, the database must store a formal description of the preconditions PREM and postconditions POSTM for every mod- ule M. An assumption about the capabilities of the modules is that the preconditions of the module follow from the preconditions of the query. It must hold that\\n\\nPREQ ) PREM:\\n\\nAll conditions that are required as a prerequisite for the application of module M must appear as preconditions in the query. If, for example, a module in the database only accepts lists of integers, then lists of integers as input must also appear as preconditions in the query. An additional requirement in the query that, for example, only even numbers appear, does not cause a problem.\\n\\nFurthermore, it must hold for the postconditions that\\n\\nPOST M ) POST Q:\\n\\nThat is, after application of the module, all attributes that the query requires must be fulﬁlled. We now show the application of a theorem prover to this task in an example from [Sch01].\\n\\n61\\n\\n62\\n\\n3 First-order Predicate Logic\\n\\nExample 3.8 VDM-SL, the Vienna Development Method Speciﬁcation Language, is often used as a language for the speciﬁcation of pre- and postconditions. Assume that in the software database the description of a module ROTATE is available, which moves the ﬁrst list element to the end of the list. We are looking for a module SHUFFLE, which creates an arbitrary permutation of the list. The two speciﬁcations read\\n\\nROTATE(l : List) l : List pre true post\\n\\nSHUFFLE(x : List) x : List pre true post ∀i : Item·\\n\\n(l = [] ⇒ l = [])∧ (l\\n\\nl = (tail l)ˆ[head l])\\n\\n(∃x1, x2 : List · x = x1ˆ[i]ˆx2 ⇔ ∃y1, y2 : List · x = y1ˆ[i]ˆy2)\\n\\nHere “^” stands for the concatenation of lists, and “⋅” separates quantiﬁers with their variables from the rest of the formula. The functions “head l” and “tail l” choose the ﬁrst element and the rest from the list, respectively. The speciﬁcation of SHUFFLE indicates that every list element i that was in the list (x) before the application of SHUFFLE must be in the result (x′) after the application, and vice versa. It must now be shown that the formula (PREQ ) PREM) ^ (POSTM ) POSTQ) is a consequence of the knowledge base containing a description of the data type List. The two VDM-SL speciﬁcations yield the proof task\\n\\n∀l, l , x, x : List · (l = x ∧ l = x ∧ (w ⇒ w)) ∧ (l = x ∧ l = x ∧ ((l = [] ⇒ l = []) ∧ (l\\n\\nl = (tl l)ˆ[hd l])\\n\\n⇒ ∀i : Item · (∃x1, x2 : List · x = x1ˆ[i]ˆx2 ⇔ ∃y1, y2 : List · x = y1ˆ[i]ˆy2))),\\n\\nwhich can then be proven with the prover SETHEO.\\n\\nIn the coming years the semantic web will likely represent an important appli- cation of PL1. The content of the World Wide Web is supposed to become inter- pretable not only for people, but for machines. To this end web sites are being furnished with a description of their semantics in a formal description language. The search for information in the web will thereby become signiﬁcantly more effective than today, where essentially only text building blocks are searchable.\\n\\nDecidable subsets of predicate logic are used as description languages. The development of efﬁcient calculi for reasoning is very important and closely con- nected to the description languages. A query for a future semantically operating search engine could (informally) read: Where in Switzerland next Sunday at ele- vations under 2000 meters will there be good weather and optimally prepared ski slopes? To answer such a question, a calculus is required that is capable of working very quickly on large sets of facts and rules. Here, complex nested function terms are less important.\\n\\n3.8 Applications\\n\\nAs a basic description framework, the World Wide Web Consortium developed the language RDF (Resource Description Framework). Building on RDF, the sig- niﬁcantly more powerful language OWL (Web Ontology Language) allows the description of relations between objects and classes of objects, similarly to PL1 [SET09]. Ontologies are descriptions of relationships between possible objects.\\n\\nA difﬁculty when building a description of the innumerable websites is the expenditure of work and also checking the correctness of the semantic descriptions. Here machine learning systems for the automatic generation of descriptions can be very helpful. An interesting use of “automatic” generation of semantics in the web was introduced by Luis von Ahn of Carnegie Mellon University [vA06]. He developed computer games in which the players, distributed over the network, are supposed to collaboratively describe pictures with key words. Thus the pictures are assigned semantics in a fun way at no cost.\\n\\n3.9 Summary\\n\\nWe have provided the most terms, and procedures of predicate logic and we have shown that even one of the most difﬁcult intellectual tasks, namely the proof of mathematical theorems, can be automated. Automated provers can be employed not only in mathematics, but rather, in particular, in veriﬁcation tasks in computer science. For everyday reasoning, however, predicate logic in most cases is ill-suited. In the next and the following chapters we show its weak points and some interesting modern alternatives. Furthermore, we will show in Chap. 5 that one can program elegantly with logic and its procedural extensions.\\n\\nimportant foundations,\\n\\nAnyone interested in ﬁrst-order logic, resolution and other calculi for automated provers will ﬁnd good advanced instruction in [New00, Fit96, Bib82, Lov78, CL73]. References to Internet resources can be found on this book’s web site.\\n\\n3.10 Exercises\\n\\nExercise 3.1 Let the three-place predicate “child” and the one-place predicate “female” from Example 3.2 on page 43 be given. Deﬁne: (a) A one-place predicate “male”. (b) A two-place predicate “father” and “mother”. (c) A two-place predicate “siblings”. (d) A predicate “parents (x, y, z)”, which is true if and only if x is the father and\\n\\ny is the mother of z.\\n\\n(e) A predicate “uncle (x, y)”, which is true if and only if x is the uncle of y (use\\n\\nthe predicates that have already been deﬁned).\\n\\n(f) A two-place predicate “ancestor” with the meaning: ancestors are parents,\\n\\ngrandparents, etc. of arbitrarily many generations.\\n\\n63\\n\\n64\\n\\n3 First-order Predicate Logic\\n\\nExercise 3.2 Formalize the following statements in predicate logic: (a) Every person has a father and a mother. (b) Some people have children. (c) All birds ﬂy. (d) There is an animal that eats (some) grain-eating animals. (e) Every animal eats plants or plant-eating animals which are much smaller than\\n\\nitself.\\n\\nExercise 3.3 Adapt Exercise 3.1 on page 63 by using one-place function symbols and equality instead of “father” and “mother”.\\n\\nExercise 3.4 Give predicate logic axioms for the two-place relation “<” as a total order. For a total order we must have (1) Any two elements are comparable. (2) It is symmetric. (3) It is transitive.\\n\\nExercise 3.5 Unify (if possible) the following terms and give the MGU and the resulting terms. (a) p(x, f(y)), p(f(z), u) (b) p(x, f(x)), p(y, y) (c) x = 4 − 7 (cid:7) x, cos y = z (d) x < 2 (cid:7) x, 3 < 6 (e) q(f(x, y, z), f (gðw; wÞ, g(x, x), g(y, y))), q(u, u)\\n\\nExercise 3.6 (a) Transform Russell’s Paradox from Example 3.7 on page 54 into CNF. (b) Show that\\n\\nthe empty clause cannot be derived using resolution without\\n\\nfactorization from (3.7) on page 54. Try to understand this intuitively.\\n\\nExercise 3.7\\n\\n(a) Why is resolution with the set of support strategy incomplete? (b) Justify (without proving) why the set of support strategy becomes complete if\\n\\n(KB ^ ¬Q)\\\\SOS is satisﬁable.\\n\\n(c) Why is resolution with the pure literal rule complete?\\n\\n❄ Exercise 3.8 Formalize and prove with resolution that in a semigroup with at least two different elements a, b, a left-neutral element e, and a left null element n, these two elements have to be different, that is, that n 6¼ e. Use demodulation, which allows replacement of “like with like”.\\n\\nExercise 3.9 Obtain the theorem prover E [Sch02] or another prover and prove the following statements. Compare these proofs with those in the text. (a) The claim from Example 2.3 on page 33. (b) Russell’s paradox from Example 3.7 on page 54. (c) The claim from Exercise 3.8.\\n\\nLimitations of Logic\\n\\n4.1 The Search Space Problem\\n\\nAs already mentioned in several places, in the search for a proof there are almost always many (depending on the calculus, potentially inﬁnitely many) possibilities for the application of inference rules at every step. The result is the aforementioned explosive growth of the search space (Fig. 4.1 on page 66). In the worst case, all of these possibilities must be tried in order to ﬁnd the proof, which is usually not possible in a reasonable amount of time.\\n\\nIf we compare automated provers or inference systems with mathematicians or human experts who have experience in special domains, we make interesting observations. For one thing, experienced mathematicians can prove theorems which are far out of reach for automated provers. On the other hand, automated provers perform tens of thousands of inferences per second. A human in contrast performs maybe one inference per second. Although human experts are much slower on the object level (that is, in carrying out inferences), they apparently solve difﬁcult problems much faster.\\n\\nThere are several reasons for this. We humans use intuitive calculi that work on a higher level and often carry out many of the simple inferences of an automated prover in one step. Furthermore, we use lemmas, that is, derived true formulas that we already know and therefore do not need to re-prove them each time. Meanwhile there are also machine provers that work with such methods. But even they cannot yet compete with human experts.\\n\\nA further, much more important advantage of us humans is intuition, without which we could not solve any difﬁcult problems [PS09]. The attempt to formalize intuition causes problems. Experience in applied AI projects shows that in complex domains such as medicine (see Sect. 7.3) or mathematics, most experts are unable to formulate this intuitive meta-knowledge verbally, much less to formalize it. Therefore we cannot program this knowledge or integrate it into calculi in the form of heuristics. Heuristics are methods that in many cases can greatly simplify or shorten the way to the goal, but in some cases (usually rarely) can greatly lengthen\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_4\\n\\n65\\n\\n4\\n\\n66\\n\\n4 Limitations of Logic\\n\\nFig. 4.1 Possible consequences of the explosion of a search space\\n\\nthe way to the goal. Heuristic search is important not only to logic, but generally to problem solving in AI and will therefore be thoroughly handled in Chap. 6.\\n\\nAn interesting approach, which has been pursued since about 1990, is the application of machine learning techniques to the learning of heuristics for directing the search of inference systems, which we will brieﬂy sketch now. A resolution prover has, during the search for a proof, hundreds or more possibilities for reso- lution steps at each step, but only a few lead to the goal. It would be ideal if the prover could ask an oracle which two clauses it should use in the next step to quickly ﬁnd the proof. There are attempts to build such proof-directing modules, which evaluate the various alternatives for the next step and then choose the alternative with the best rating. In the case of resolution, the rating of the available clauses could be computed by a function that calculates a value based on the number of positive literals, the complexity of the terms, etc., for every pair of resolvable clauses.\\n\\nHow can this function be implemented? Because this knowledge is “intuitive”, the programmer is not familiar with it. Instead, one tries to copy nature and uses machine learning algorithms to learn from successful proofs [ESS89, SE90].\\n\\n4.1 The Search Space Problem\\n\\nThe attributes of all clause pairs participating in successful resolution steps are stored as positive, and the attributes of all unsuccessful resolutions are stored as negative. Then, using this training data and a machine learning system, a program is generated which can rate clause pairs heuristically (see Sect. 9.5).\\n\\nA different, more successful approach to improving mathematical reasoning is followed with interactive systems that operate under the control of the user. Here one could name computer algebra programs such as Mathematica, Maple, or Maxima, which can automatically carry out difﬁcult symbolic mathematical manipulations. The search for the proof, however, is left fully to the human. The aforementioned interactive prover Isabelle [NPW02] provides distinctly more support during the proof search. There are at present several projects, such as Omega [SB04] and MKM,1 for the development of systems for supporting math- ematicians during proofs.\\n\\nIn summary, one can say that, because of the search space problem, automated provers today can only prove relatively simple theorems in special domains with few axioms.\\n\\n4.2 Decidability and Incompleteness\\n\\nFirst-order predicate logic provides a powerful tool for the representation of knowledge and reasoning. We know that there are correct and complete calculi and theorem provers. When proving a theorem, that is, a true statement, such a prover is very helpful because, due to completeness, one knows after ﬁnite time that the statement really is true. What if the statement is not true? The completeness theorem (Theorem 3.3 on page 50) does not answer this question.2 Speciﬁcally, there is no process that can prove or refute any formula from PL1 in ﬁnite time, for it holds that\\n\\nTheorem 4.1 The set of valid formulas in ﬁrst-order predicate logic is semi- decidable.\\n\\nThis theorem implies that there are programs (theorem provers) which, given a true (valid) formula as input, determine its truth in ﬁnite time. If the formula is not valid, however, it may happen that the prover never halts. (The reader may grapple with this question in Exercise 4.1 on page 73.) Propositional logic is decidable because the truth table method provides all models of a formula in ﬁnite time. Evidently predicate logic with quantiﬁers and nested function symbols is a language somewhat too powerful to be decidable.\\n\\n1www.mathweb.org/mathweb/demo.html. 2Just this case is especially important in practice, because if I already know that a statement is true, I no longer need a prover.\\n\\n67\\n\\n68\\n\\n4 Limitations of Logic\\n\\nOn the other hand, predicate logic is not powerful enough for many purposes. One often wishes to make statements about sets of predicates or functions. This does not work in PL1 because it only knows quantiﬁers for variables, but not for predicates or functions.\\n\\nKurt Gödel showed, shortly after his completeness theorem for PL1, that com- pleteness is lost if we extend PL1 even minimally to construct a higher-order logic. A ﬁrst-order logic can only quantify over variables. A second-order logic can also quantify over formulas of the ﬁrst order, and a third-order logic can quantify over formulas of the second order. Even adding only the induction axiom for the natural numbers makes the logic incomplete. The statement “If a predicate p(n) holds for n, then p(n + 1) also holds”, or\\n\\n8p pðnÞ )pð n þ 1Þ\\n\\nis a second-order proposition because it quantiﬁes over a predicate. Gödel proved the following theorem:\\n\\nTheorem 4.2 (Gödel’s incompleteness theorem) Every axiom system for the natural numbers with addition and multiplication (arithmetic) is incomplete. That is, there are true statements in arithmetic that are not provable.\\n\\nGödel’s proof works with what is called Gödelization, in which every arithmetic formula is encoded as a number. It obtains a unique Gödel number. Gödelization is now used to formulate the proposition\\n\\nF ¼ “I am not provable.”\\n\\nin the language of arithmetic. This formula is true for the following reason. Assume F is false. Then we can prove F and therefore show that F is not provable. This is a contradiction. Thus F is true and therefore not provable.\\n\\nThe deeper background of this theorem is that mathematical theories (axiom languages become incomplete if the language systems) and, more generally, becomes too powerful. A similar example is set theory. This language is so powerful that one can formulate paradoxes with it. These are statements that contradict themselves, such as the statement we already know from Example 3.7 on page 54 about the barbers who all shave those who do not shave themselves (see Exercise 4.2 on page 73). The dilemma consists therein that with languages which are powerful enough to describe mathematics and interesting applications, we are smuggling contradictions and incompletenesses through the back door. This does not mean, however, that higher-order logics are wholly unsuited for formal methods. There are certainly formal systems as well as provers for higher-order logics.\\n\\n4.3 The Flying Penguin\\n\\n69\\n\\n4.3 The Flying Penguin\\n\\nWith a simple example we will demonstrate a fundamental problem of logic and possible solution approaches. Given the statements 1. Tweety is a penguin 2. Penguins are birds 3. Birds can ﬂy Formalized in PL1, the knowledge base KB results:\\n\\npenguinðtweetyÞ penguinðxÞ )bird ðxÞ birdðxÞ )fly ðxÞ\\n\\nFrom there (for example with resolution) ﬂy(tweety) can be derived (Fig. 4.2 on page 70).3 Evidently the formalization of the ﬂight attributes of penguins is insufﬁcient. We try the additional statement Penguins cannot ﬂy, that is\\n\\npenguinðxÞ ) : flyðxÞ\\n\\nFrom there ¬ﬂy(tweety) can be derived. But ﬂy(tweety) is still true. The knowledge base is therefore inconsistent. Here we notice an important characteristic of logic, namely monotony. Although we explicitly state that penguins cannot ﬂy, the opposite can still be derived.\\n\\nDeﬁnition 4.1 A logic is called monotonic if, for an arbitrary knowledge base KB and an arbitrary formula /, the set of formulas derivable from KB is a subset of the formulas derivable from KB [ /.\\n\\nIf a set of formulas is extended, then, after the extension, all previously derivable statements can still be proved, and additional statements can potentially also be proved. The set of provable statements thus grows monotonically when the set of formulas is extended. For our example this means that the extension of the knowledge base will never lead to our goal. We thus modify KB by replacing the obviously false statement “(all) birds can ﬂy” with the more exact statement “(all) birds except penguins can ﬂy” and obtain as KB2 the following clauses:\\n\\npenguinðtweetyÞ penguinðxÞ )bird ðxÞ birdðxÞ ^ : penguinðxÞ ) flyðxÞ penguinðxÞ ) : flyðxÞ\\n\\n3The formal execution of this and the following simple proof may be left (Exercise 4.3 on page 73).\\n\\nto the reader\\n\\n70\\n\\n4 Limitations of Logic\\n\\nFig. 4.2 The ﬂying penguin Tweety\\n\\nNow the world is apparently in order again. We can derive ¬ﬂy(tweety), but not ﬂy (tweety), because for that we would need ¬ penguin(x), which, however, is not derivable. As long as there are only penguins in this world, peace reigns. Every normal bird, however, immediately causes problems. We wish to add the raven Abraxas (from the German book “The Little Witch”) and obtain\\n\\nravenðabraxasÞ ravenðxÞ )bird ðxÞ penguinðtweetyÞ penguinðxÞ )bird ðxÞ birdðxÞ ^ : penguinðxÞ ) flyðxÞ penguinðxÞ ) : flyðxÞ\\n\\nWe cannot say anything about the ﬂight attributes of Abraxas because we forgot to formulate that ravens are not penguins. Thus we extend KB3 to KB4:\\n\\nravenðabraxasÞ ravenðxÞ )bird ðxÞ ravenðxÞ ) : pinguinðxÞ penguinðtweetyÞ penguinðxÞ )bird ðxÞ birdðxÞ ^ : penguinðxÞ ) flyðxÞ penguinðxÞ ) : flyðxÞ\\n\\nThe fact that ravens are not penguins, which is self-evident to humans, must be explicitly added here. For the construction of a knowledge base with all 9,800 or so\\n\\n4.3 The Flying Penguin\\n\\ntypes of birds worldwide, it must therefore be speciﬁed for every type of bird (except for penguins) that it is not a member of penguins. We must proceed analogously for all other exceptions such as the ostrich.\\n\\nFor every object in the knowledge base, in addition to its attributes, all of the\\n\\nattributes it does not have must be listed.\\n\\nTo solve this problem, various forms of non-monotonic logic have been developed, which allow knowledge (formulas) to be removed from the knowledge base. Under the name default logic, logics have been developed which allow objects to be assigned attributes which are valid as long as no other rules are available. In the Tweety example, the rule birds can ﬂy would be such a default rule. Despite great effort, these logics have at present, due to semantic and practical problems, not succeeded.\\n\\nMonotony can be especially inconvenient in complex planning problems in which the world can change. If for example a blue house is painted red, then afterwards it is red. A knowledge base such as\\n\\ncolorðhouse; blueÞ paintðhouse; redÞ paintðx; yÞ ) colorðx; yÞ\\n\\nleads to the conclusion that, after painting, the house is red and blue. The problem that comes up here in planning is known as the frame problem. A solution for this is the situation calculus presented in Sect. 5.6.\\n\\nAn interesting approach for modeling problems such as the Tweety example is probability theory. The statement “all birds can ﬂy” is false. A statement something like “almost all birds can ﬂy” is correct. This statement becomes more exact if we give a probability for “birds can ﬂy”. This leads to probabilistic logic, which today represents an important sub-area of AI and an important tool for modeling uncer- tainty (see Chap. 7).\\n\\n4.4 Modeling Uncertainty\\n\\nTwo-valued logic can and should only model circumstances in which there is true, false, and no other truth values. For many tasks in everyday reasoning, two-valued logic is therefore not expressive enough. The rule\\n\\nbirdðxÞ ) flyðxÞ\\n\\nis true for almost all birds, but for some it is false. As was already mentioned, working with probabilities allows exact formulation of uncertainty. The statement “99% of all birds can ﬂy” can be formalized by the expression\\n\\nPðbirdðxÞ )fly ðxÞÞ ¼ 0:99:\\n\\n71\\n\\n72\\n\\n4 Limitations of Logic\\n\\nFig. 4.3 Probability density of the continuous variable rainfall\\n\\nIn Chap. 7 we will see that here it is better to work with conditional probabilities such as\\n\\nPðflyjbirdÞ ¼ 0:99:\\n\\nWith the help of Bayesian networks, complex applications with many variables can also be modeled.\\n\\nA different model is needed for the statement “The weather is nice”. Here it often makes no sense to speak in terms of true and false. The variable weather_is_nice should not be modeled as binary, rather continuously with values, for example, in the interval [0, 1]. weather_is_nice = 0.7 then means “The weather is fairly nice”. Fuzzy logic was developed for this type of continuous (fuzzy) variable.\\n\\nProbability theory also offers the possibility of making statements about the probability of continuous variables. A statement in the weather report such as “There is a high probability that there will be some rain” could for example be exactly formulated as a probability density of the form\\n\\nPðrainfall ¼ XÞ ¼ Y\\n\\nand represented graphically something like in Fig. 4.3.\\n\\nThis very general and even visualizable representation of both types of uncertainty we have discussed, together with inductive statistics and the theory of Bayesian networks, makes it possible, in principle, to answer arbitrary probabilistic queries. Probability theory as well as fuzzy logic are not directly comparable to predicate logic because they do not allow variables or quantiﬁers. They can thus be seen as extensions of propositional logic as shown in the following table.\\n\\nFormalism\\n\\nPropositional logic Fuzzy logic Discrete probabilistic logic Continuous probabilistic logic\\n\\nNumber of truth values\\n\\n2 ∞\\n\\nn ∞\\n\\nProbabilities expressible – –\\n\\nyes yes\\n\\n4.5 Exercises\\n\\n4.5 Exercises\\n\\n❄ Exercise 4.1\\n\\n(a) With the following (false) argument, one could claim that PL1 is decidable: We take a complete proof calculus for PL1. With it we can ﬁnd a proof for any true formula in ﬁnite time. For every other formula / I proceed as follows: I apply the calculus to ¬/ and show that ¬/ is true. Thus / is false. Thus I can prove or refute every formula in PL1. Find the mistake in the argument and change it so it becomes correct.\\n\\n(b) Construct a decision process for the set of true and unsatisﬁable formulas in\\n\\nPL1.\\n\\nExercise 4.2 (a) Given the statement “There is a barber who shaves every person who does not\\n\\nshave himself.” Consider whether this barber shaves himself.\\n\\n(b) Let M = {x | x 62 x}. Describe this set and consider whether M contains itself.\\n\\nExercise 4.3 Use an automated theorem prover (for example E [Sch02]) and apply it to all ﬁve different axiomatizations of the Tweety example from Sect. 4.3. Validate the example’s statements.\\n\\n73\\n\\nLogic Programming with PROLOG\\n\\nCompared to classical programming languages such as C or Pascal, Logic makes it possible to express relationships elegantly, compactly, and declaratively. Auto- mated theorem provers are even capable of deciding whether a knowledge base logically entails a query. Proof calculus and knowledge stored in the knowledge base are strictly separated. A formula described in clause normal form can be used as input data for any theorem prover, independent of the proof calculus used. This is of great value for reasoning and the representation of knowledge.\\n\\nIf one wishes to implement algorithms, which inevitably have procedural components, a purely declarative description is often insufﬁcient. Robert Kowalski, one of the pioneers of logic programming, made this point with the formula\\n\\nAlgorithm ¼ Logic þ Control:\\n\\nThis idea was brought to fruition in the language PROLOG. PROLOG is used in many projects, primarily in AI and computational linguistics. We will now give a short introduction to this language, present the most important concepts, show its strengths, and compare it with other programming languages and theorem provers. Those looking for a complete programming course are directed to textbooks such as [Bra11, CM94] and the handbooks [Wie04, Dia04].\\n\\nThe syntax of the language PROLOG only allows Horn clauses. Logical nota-\\n\\ntion and PROLOG’s syntax are juxtaposed in the following table:\\n\\nPL1 / clause normal form (¬A1 _ … _ ¬ Am _ B) (A1 ^ … ^ Am ) ) B A (¬A1 _ … _ ¬Am) ¬(A1 ^ … ^ Am)\\n\\nPROLOG B:- A1, … , Am. B:- A1, … , Am. A. ?- A1, … , Am. ?- A1, … , Am.\\n\\nDescription\\n\\nRule Rule Fact Query Query\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_5\\n\\n75\\n\\n5\\n\\n76\\n\\n5 Logic Programming with PROLOG\\n\\nHere A1, … , Am, A, B are literals. The literals are, as in PL1, constructed from predicate symbols with terms as arguments. As we can see in the above table, in PROLOG there are no negations in the strict logical sense because the sign of a literal is determined by its position in the clause.\\n\\n5.1 PROLOG Systems and Implementations\\n\\nAn overview of current PROLOG systems is available in the collection of links on this book’s home page. To the reader we recommend the very powerful and freely available (under GNU public licenses) systems GNU-PROLOG [Dia04] and SWI-PROLOG. For the following examples, SWI-PROLOG [Wie04] was used.\\n\\nMost modern PROLOG systems work with an interpreter based on the Warren abstract machine (WAM). PROLOG source code is compiled into so-called WAM code, which is then interpreted by the WAM. The fastest implementations of a WAM manage up to 10 million logical inferences per second (LIPS) on a 1 GHz PC.\\n\\n5.2 Simple Examples\\n\\nWe begin with the family relationships from Example 3.2 on page 43. The small knowledge base KB is coded—without the facts for the predicate female—as a PROLOG program named rel.pl in Fig. 5.1.\\n\\nThe program can be loaded and compiled in the PROLOG interpreter with the\\n\\ncommand\\n\\n?- [rel].\\n\\n1 child(oscar,karen,frank). 2 child(mary,karen,frank). 3 child(eve,anne,oscar). 4 child(henry,anne,oscar). 5 child(isolde,anne,oscar). 6 child(clyde,mary,oscarb). 7 8 child(X,Z,Y) :- child(X,Y,Z). 9\\n\\n10 descendant(X,Y) :- child(X,Y,Z). 11 descendant(X,Y) :- child(X,U,V), descendant(U,Y).\\n\\nFig. 5.1 PROLOG program with family relationships\\n\\n5.2 Simple Examples\\n\\nAn initial query returns the dialog\\n\\n?- child(eve,oscar,anne).\\n\\nYes\\n\\nwith the correct answer Yes. How does this answer come about? For the query “?- child(eve,oscar,anne).” there are six facts and one rule with the same predicate in its clause head. Now uniﬁcation is attempted between the query and each of the complementary literals in the input data in order of occurrence. If one of the alternatives fails, this results in backtracking to the last branching point, and the next alternative is tested. Because uniﬁcation fails with every fact, the query is uniﬁed with the recursive rule in line 8. Now the system attempts to solve the subgoal child(eve,anne,oscar), which succeeds with the third alternative. The query\\n\\n?- descendant(X,Y).\\n\\nX = oscar Y = karen\\n\\nYes\\n\\nis answered with the ﬁrst solution found, as is\\n\\n?- descendant(clyde,Y).\\n\\nY = mary\\n\\nYes\\n\\nThe query\\n\\n?- descendant(clyde,karen).\\n\\nis not answered, however. The reason for this is the clause in line 8, which speciﬁes symmetry of the child predicate. This clause calls itself recursively without the possibility of termination. This problem can be solved with the following new program (facts have been omitted here).\\n\\n1 descendant(X,Y) :- child(X,Y,Z). 2 descendant(X,Y) :- child(X,Z,Y). 3 descendant(X,Y) :- child(X,U,V), descendant(U,Y).\\n\\nBut now the query\\n\\n?- child(eve,oscar,anne).\\n\\n77\\n\\n78\\n\\n5 Logic Programming with PROLOG\\n\\nis no longer correctly answered because the symmetry of child in the last two variables is no longer given. A solution to both problems is found in the program\\n\\n1 child_fact(oscar,karen,franz). 2 child_fact(mary,karen,franz). 3 child_fact(eva,anne,oscar). 4 child_fact(henry,anne,oscar). 5 child_fact(isolde,anne,oscar). 6 child_fact(clyde,mary,oscarb). 7 8 child(X,Z,Y) :- child_fact(X,Y,Z). 9 child(X,Z,Y) :- child_fact(X,Z,Y).\\n\\n10 11 descendant(X,Y) :- child(X,Y,Z). 12 descendant(X,Y) :- child(X,U,V), descendant(U,Y).\\n\\nBy introducing the new predicate child_fact for the facts, the predicate child is no longer recursive. However, the program is no longer as elegant and simple as the—logically correct—ﬁrst variant in Fig. 5.1 on page 76, which leads to the inﬁnite loop. The PROLOG programmer must, just as in other languages, pay attention to processing and avoid inﬁnite loops. PROLOG is just a programming language and not a theorem prover.\\n\\nWe must distinguish here between declarative and procedural semantics of PROLOG programs. The declarative semantics is given by the logical interpretation of the horn clauses. The procedural semantics, in contrast, is deﬁned by the exe- cution of the PROLOG program, which we wish to observe in more detail now. The execution of the program from Fig. 5.1 on page 76 with the query child(eve, oscar,anne) is represented in Fig. 5.2 as a search tree.1 Execution begins at the top left with the query. Each edge represents a possible SLD resolution step with a complementary uniﬁable literal. While the search tree becomes inﬁnitely deep by the recursive rule, the PROLOG execution terminates because the facts occur before the rule in the input data.\\n\\nFig. 5.2 PROLOG search tree for child(eve,oscar,anne)\\n\\n1The constants have been abbreviated to save space.\\n\\n5.2 Simple Examples\\n\\nFig. 5.3 And-or tree for desc(clyde,karen)\\n\\nWith the query descendant(clyde,karen), in contrast, the PROLOG execution does not terminate. We can see this clearly in the and-or tree pre- represented by sented in Fig. 5.3. , lead from the head of a clause to the subgoals. Because all subgoals of a clause must be solved, these are and branches. All other branches are or branches, of which at least one must be uniﬁable with its parent nodes. The two outlined facts represent the solution to the query. The PROLOG interpreter does not terminate here, however, because it works by using a depth-ﬁrst search with backtracking (see Sect. 6.2.2) and thus ﬁrst chooses the inﬁnitely deep path to the far left.\\n\\nIn this\\n\\nrepresentation the branches,\\n\\n5.3 Execution Control and Procedural Elements\\n\\nAs we have seen in the family relationship example, it is important to control the execution of PROLOG. Avoiding unnecessary backtracking especially can lead to large increases in efﬁciency. One means to this end is the cut. By inserting an exclamation mark into a clause, we can prevent backtracking over this point. In the following program, the predicate max(X,Y,Max) computes the maximum of the two numbers X and Y.\\n\\n1 max(X,Y,X) :- X >= Y. 2 max(X,Y,Y) :- X < Y.\\n\\nIf the ﬁrst case (ﬁrst clause) applies, then the second will not be reached. On the other hand, if the ﬁrst case does not apply, then the condition of the second case is true, which means that it does not need to be checked. For example, in the query\\n\\n79\\n\\n80\\n\\n5 Logic Programming with PROLOG\\n\\n?- max(3,2,Z), Z > 10.\\n\\nbacktracking is employed because Z = 3, and the second clause is tested for max, which is doomed to failure. Thus backtracking over this spot is unnecessary. We can optimize this with a cut:\\n\\n1 max(X,Y,X) :- X >= Y, !. 2 max(X,Y,Y).\\n\\nThus the second clause is only called if it is really necessary, that is, if the ﬁrst clause fails. However, to understand.\\n\\nthis optimization makes the program harder\\n\\nAnother possibility for execution control is the built-in predicate fail, which is never true. In the family relationship example we can quite simply print out all children and their parents with the query\\n\\n?- child\\\\_fact(X,Y,Z), write(X), write(’ is a child of ’), write(Y), write(’ and ’), write(Z), write(’.’), nl, fail.\\n\\nThe corresponding output is\\n\\noscar is a child of karen and frank. mary is a child of karen and frank. eve is a child of anne and oscar. ... No.\\n\\nwhere the predicate nl causes a line break in the output. What would be the output in the end without use of the fail predicate?\\n\\nWith the same knowledge base, the query “?- child_fact(ulla,X,Y).” would result in the answer No because there are no facts about ulla. This answer is not logically correct. Speciﬁcally, it is not possible to prove that there is no object with the name ulla. Here the prover E would correctly answer “No proof found.” Thus if PROLOG answers No, this only means that the query Q cannot be proved. For this, however, ¬Q must not necessarily be proved. This behavior is called negation as failure.\\n\\nRestricting ourselves to Horn clauses does not cause a big problem in most cases. However, it is important for procedural execution using SLD-resolution (Sect. 2.5). Through the singly determined positive literal per clause, SLD resolution, and therefore the execution of PROLOG programs, have a unique entry point into the clause. This is the only way it is possible to have repro- therefore, well-deﬁned procedural ducible execution of semantics.\\n\\nlogic programs and,\\n\\nIndeed, there are certainly problem statements which cannot be described by Horn clauses. An example is Russell’s paradox from Example 3.7 on page 54, which contains the non-Horn clause (shaves(barber, X) _ shaves(X, X)).\\n\\n5.4 Lists\\n\\n5.4 Lists\\n\\nAs a high-level language, PROLOG has, like the language LISP, the convenient generic list data type. A list with the elements A, 2, 2, B, 3, 4, 5 has the form\\n\\n[A,2,2,B,3,4,5]\\n\\nThe construct [Head|Tail] separates the ﬁrst element (Head) from the rest (Tail) of the list. With the knowledge base\\n\\nlist([A,2,2,B,3,4,5]).\\n\\nPROLOG displays the dialog\\n\\n?- list([H|T]).\\n\\nH = A T = [2, 2, B, 3, 4, 5]\\n\\nYes\\n\\nBy using nested lists, we can create arbitrary tree structures. For example, the two trees\\n\\nb\\n\\n(cid:129)\\n\\nc\\n\\nand\\n\\nb\\n\\na\\n\\nc\\n\\ncan be represented by the lists [b,c] and [a,b,c], respectively, and the two trees\\n\\n(cid:129)\\n\\na\\n\\n(cid:129)\\n\\n(cid:129) d\\n\\nand\\n\\nb\\n\\nc\\n\\nd\\n\\ne f g h\\n\\ne f\\n\\ng h\\n\\nby the lists [[e,f,g],[h],d] and [a,[b,e,f,g],[c,h],d], respec- tively. In the trees where the inner nodes contain symbols, the symbol is the head of the list and the child nodes are the tail.\\n\\nA nice, elegant example of list processing is the deﬁnition of the predicate append(X,Y,Z) for appending list Y to the list X. The result is saved in Z. The corresponding PROLOG program reads\\n\\n1 append([],L,L). 2 append([X|L1],L2,[X|L3]) :- append(L1,L2,L3).\\n\\nThis is a declarative (recursive) logical description of the fact that L3 results from appending L2 to L1. At the same time, however, this program also does the work when it is called. The call\\n\\n?- append([a,b,c],[d,1,2],Z).\\n\\n81\\n\\n82\\n\\n5 Logic Programming with PROLOG\\n\\nreturns the substitution Z = [a, b, c, d, 1, 2], just as the call\\n\\n?- append(X,[1,2,3],[4,5,6,1,2,3]).\\n\\nyields the substitution X = [4, 5, 6]. Here we observe that append is not a two-place function, but a three-place relationship. Actually, we can also input the “output parameter” Z and ask whether it can be created.\\n\\nReversing the order of a list’s elements can also be elegantly described and\\n\\nsimultaneously programmed by the recursive predicate\\n\\n1 nrev([],[]). 2 nrev([H|T],R) :- nrev(T,RT), append(RT,[H],R).\\n\\nwhich reduces the reversal of a list down to the reversal of a list that is one element smaller. Indeed, this predicate is very inefﬁcient due to calling append. This program is known as naive reverse and is often used as a PROLOG benchmark (see Exercise 5.6 on page 89). Things go better when one proceeds using a temporary store, known as the accumulator, as follows:\\n\\nList [a,b,c,d] [b,c,d] [c,d] [d] []\\n\\nAccumulator [] [a] [b,a] [c,b,a] [d,c,b,a]\\n\\nThe corresponding PROLOG program reads\\n\\n1 accrev([],A,A). 2 accrev([H|T],A,R) :- accrev(T,[H|A],R).\\n\\n5.5 Self-modifying Programs\\n\\nPROLOG programs are not fully compiled, rather, they are interpreted by the WAM. Therefore it is possible to modify programs at runtime. A program can even modify itself. With commands such as assert and retract, facts and rules can be added to the knowledge base or taken out of it.\\n\\nA simple application of the variant asserta is the addition of derived facts to the beginning of the knowledge base with the goal of avoiding a repeated, poten- tially time-expensive derivation (see Exercise 5.8 on page 89). If in our family relationship example we replace the two rules for the predicate descendant with\\n\\n5.5 Self-modifying Programs\\n\\n1 :- dynamic descendant/2. 2 descendant(X,Y) :- child(X,Y,Z), asserta(descendant(X,Y)). 3 descendant(X,Y) :- child(X,U,V), descendant(U,Y), 4\\n\\nasserta(descendant(X,Y)).\\n\\nthen all derived facts for this predicate are saved in the knowledge base and thus in the future are not re-derived. The query\\n\\n?- descendant(clyde, karen).\\n\\nleads to the addition of the two facts\\n\\ndescendant(clyde, karen). descendant(mary, karen).\\n\\nBy manipulating rules with assert and retract, even programs that change themselves completely can be written. This idea became known under the term genetic programming. It allows the construction of arbitrarily ﬂexible learning programs. In practice, however, it turns out that, due to the huge number of senseless possible changes, changing the code by trial and error rarely leads to a performance increase. Systematic changing of rules, on the other hand, makes programming so much more complex that, so far, such programs that extensively modify their own code have not been successful. In Chap. 8 we will show how machine learning has been quite successful. However, only very limited modiﬁ- cations of the program code are being conducted here.\\n\\n5.6 A Planning Example\\n\\nExample 5.1 The following riddle serves as a problem statement for a typical PROLOG program.\\n\\nA farmer wants to bring a cabbage, a goat, and a wolf across a river, but his boat is so small that he can only take them across one at a time. The farmer thought it over and then said to himself: “If I ﬁrst bring the wolf to the other side, then the goat will eat the cabbage. If I transport the cabbage ﬁrst, then the goat will be eaten by the wolf. What should I do?”\\n\\nThis is a planning task which we can quickly solve with a bit of thought. The PROLOG program given in Fig. 5.4 on page 84 is not created quite as fast. The program works on terms of the form state(Farmer,Wolf,Goat, Cabbage), which describe the current state of the world. The four variables with possible values left, right give the location of the objects. The central\\n\\n83\\n\\n84\\n\\n5 Logic Programming with PROLOG\\n\\n1 start :- action(state(left,left,left,left), 2 3 4 action(Start,Goal):- 5 6 7 8 % 9\\n\\n.))thgir,thgir,thgir,thgir(etats\\n\\nplan(Start,Goal,[Start],Path), nl,write(’Solution:’),nl, write_path(Path).\\n\\nwrite_path(Path), fail.\\n\\n% all solutions output\\n\\n10 plan(Start,Goal,Visited,Path):- go(Start,Next), 11 safe(Next), 12 \\\\+ member(Next,Visited), 13 14 plan(Next,Goal,[Next|Visited],Path). 15 plan(Goal,Goal,Path,Path). 16 17 go(state(X,X,Z,K),state(Y,Y,Z,K)):-across(X,Y). % farmer, wolf 18 go(state(X,W,X,K),state(Y,W,Y,K)):-across(X,Y). % farmer, goat 19 go(state(X,W,Z,X),state(Y,W,Z,Y)):-across(X,Y). % farmer, cabbage 20 go(state(X,W,Z,K),state(Y,W,Z,K)):-across(X,Y). % farmer 21 22 across(left,right). 23 across(right,left). 24 25 safe(state(B,W,Z,K)):- across(W,Z), across(Z,K). 26 safe(state(B,B,B,K)). 27 safe(state(B,W,B,B)).\\n\\n% not(member(...))\\n\\nFig. 5.4 PROLOG program for the farmer–wolf–goat–cabbage problem\\n\\nrecursive predicate plan ﬁrst creates a successor state Next using go, tests its safety with safe, and repeats this recursively until the start and goal states are the same (in program line 15). The states which have already been visited are stored in the third argument of plan. With the built-in predicate member it is tested whether the state Next has already been visited. If yes, is not attempted.\\n\\nit\\n\\nThe deﬁnition of the predicate write_path for the task of outputting the plan found is missing here. It is suggested as an exercise for the reader (Exercise 5.2 on page 88). For initial program tests the literal write_path(Path) can be replaced with write(Path). For the query “?- start.” we get the answer\\n\\nSolution: Farmer and goat from left to right Farmer from right to left Farmer and wolf from left to right Farmer and goat from right to left Farmer and cabbage from left to right Farmer from right to left Farmer and goat from left to right\\n\\nYes\\n\\n5.6 A Planning Example\\n\\nFor better understanding we describe the deﬁnition of plan in logic:\\n\\n8z planðz; zÞ ^ 8s 8z 8n ½goðs; nÞ ^ safeðnÞ ^ planðn; zÞ )planð s; zÞ(cid:2)\\n\\nThis deﬁnition comes out signiﬁcantly more concise than in PROLOG. There are two reasons for this. For one thing, the output of the discovered plan is unimportant for logic. Furthermore, it is not really necessary to check whether the next state was already visited if unnecessary trips do not bother the farmer. If, however, \\\\+ member(...) is left out of the PROLOG program, then there is an inﬁnite loop and PROLOG might not ﬁnd a schedule even if there is one. The cause of this is PROLOG’s backward chaining search strategy, which, according to the depth-ﬁrst search (Sect. 6.2.2) principle, always works on subgoals one at a time without restricting recursion depth, and is therefore incomplete. This would not happen to a theorem prover with a complete calculus.\\n\\nAs in all planning tasks, the state of the world changes as actions are carried out from one step to the next. This suggests sending the state as a variable to all predicates that depend on the state of the world, such as in the predicate safe. The state transitions occur in the predicate go. This approach is called situation cal- culus [RN10]. We will become familiar with an interesting extension to learning action sequences in partially observable, non-deterministic worlds in Chap. 10.\\n\\nPredicate logic and simpler sub-languages for the description and solution of planning tasks play an increasingly important role for intelligent robots. To solve complex tasks, robots often work on symbolic relational descriptions of the world. The classic application for a service robot is recognizing an audible command linguisti- cally and then converting it into a logical formula. Then a planner has the task of ﬁnding a plan which eventually reaches a state in which the formula becomes true. Even more interesting is when the robot learns to meet the goals of its trainer by observing them and then use the symbolic planner to achieve the goals [CEP15b]. One very exciting thing about this is that the robot is able to take numerical sensor values such as pixel images and turn them into a symbolic description of situations. This generally very difﬁcult task is known in AI as symbol grounding. Robotics developers have found interesting solutions to this problem for certain tasks [CEP15a].\\n\\n5.7 Constraint Logic Programming\\n\\nThe programming of scheduling systems, in which many (sometimes complex) logical and numerical conditions must be fulﬁlled, can be very expensive and difﬁcult with conventional programming languages. This is precisely where logic could be useful. One simply writes all logical conditions in PL1 and then enters a query. Usually this approach fails miserably. The reason is the penguin problem discussed in Sect. 4.3. The fact penguin(tweety) does ensure that penguin (tweety) is true. However, it does not rule out that raven(tweety) is also true. To rule this out with additional axioms is very inconvenient (Sect. 4.3).\\n\\n85\\n\\n86\\n\\n5 Logic Programming with PROLOG\\n\\nConstraint Logic Programming (CLP), which allows the explicit formulation of constraints for variables, offers an elegant and very efﬁcient mechanism for solving this problem. The interpreter constantly monitors the execution of the program for adherence to all of its constraints. The programmer is fully relieved of the task of controlling the constraints, which in many cases can greatly simplify programming. This is expressed in the following quotation by Eugene C. Freuder from [Fre97]:\\n\\nConstraint programming represents one of the closest approaches computer science has yet made to the Holy Grail of programming: the user states the problem, the computer solves it.\\n\\nWithout going into the theory of the constraint satisfaction problem (CSP), we will apply the CLP mechanism of GNU-PROLOG to the following example.\\n\\nExample 5.2 The secretary of Albert Einstein High School has to come up with a plan for allocating rooms for ﬁnal exams. He has the following information: the four teachers Mayer, Hoover, Miller and Smith give tests for the subjects German, English, Math, and Physics in the ascendingly numbered rooms 1, 2, 3 and 4. Every teacher gives a test for exactly one subject in exactly one room. Besides that, he knows the following about the teachers and their subjects. 1. Mr. Mayer never tests in room 4. 2. Mr. Miller always tests German. 3. Mr. Smith and Mr. Miller do not give tests in neighboring rooms. 4. Mrs. Hoover tests Mathematics. 5. Physics is always tested in room number 4. 6. German and English are not tested in room 1. Who gives a test in which room?\\n\\nA GNU-PROLOG program for solving this problem is given in Fig. 5.5. This program works with the variables Mayer,Hoover,Miller,Smith as well as\\n\\n1 start :- 2 3 4 5 6 7 8 9 01 11 12 13 14 15 16 17 18 19\\n\\nfd_domain([Mayer, Hoover, Miller, Smith],1,4), fd_all_different([Mayer, Miller, Hoover, Smith]),\\n\\nfd_domain([German, English, Math, Physics],1,4), fd_all_different([German, English, Math, Physics]),\\n\\nfd_labeling([Mayer, Hoover, Miller, Smith]),\\n\\n,4=\\\\#reyaM Miller #= German, dist(Miller,Smith) #>= 2, Hoover #= Math, Physics #= 4, German #\\\\=1, English #\\\\=1, nl, write([Mayer, Hoover, Miller, Smith]), nl, write([German, English, Math, Physics]), nl.\\n\\n4moornitonreyaM% % Miller tests German % Distance Miller/Smith >= 2 % Hoover tests mathematics % Physics in room 4 % German not in room 1 % English not in room 1\\n\\nFig. 5.5 CLP program for the room scheduling problem\\n\\n5.7 Constraint Logic Programming\\n\\nGerman,English,Math,Physics, which can each take on an integer value from 1 to 4 as the room number (program lines 2 and 5). A binding Mayer = 1 and German = 1 means that Mr. Mayer gives the German test in room 1. Lines 3 and 6 ensure that the four particular variables take on different values. Line 8 ensures that all variables are assigned a concrete value in the case of a solution. This line is not absolutely necessary here. If there were multiple solutions, however, only intervals would be output. In lines 10 to 16 the constraints are given, and the remaining lines output the room numbers for all teachers and all subjects in a simple format.\\n\\nThe program is loaded into GNU-PROLOG with “[′raumplan.pl′].”, and\\n\\nwith “start.” we obtain the output\\n\\n[3,1,2,4] [2,3,1,4]\\n\\nRepresented somewhat more conveniently, we have the following room schedule:\\n\\nRoom num.\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\nTeacher Subject\\n\\nHoover Math\\n\\nMiller German\\n\\nMayer English\\n\\nSmith Physics\\n\\nGNU-PROLOG has, like most other CLP languages, a so-called ﬁnite domain constraint solver, with which variables can be assigned a ﬁnite range of integers. This need not necessarily be an interval as in the example. We can also input a list of values. As an exercise the user is invited, in Exercise 5.9 on page 89, to create a CLP program, for example with GNU-PROLOG, for a not-so-simple logic puzzle. This puzzle, supposedly created by Einstein, can very easily be solved with a CLP system. If we tried using PROLOG without constraints, on the other hand, we could easily grind our teeth out. Anyone who ﬁnds an elegant solution with PROLOG or a prover, please let it ﬁnd its way to the author.\\n\\n5.8 Summary\\n\\nUniﬁcation, lists, declarative programming, and the relational view of procedures, in which an argument of a predicate can act as both input and output, allow the development of short, elegant programs for many problems. Many programs would be signiﬁcantly longer and thus more difﬁcult to understand if written in a proce- dural language. Furthermore, these language features save the programmer time. Therefore PROLOG is also an interesting tool for rapid prototyping, particularly for AI applications. The CLP extension of PROLOG is helpful not only for logic puzzles, but also for many optimization and scheduling tasks.\\n\\n87\\n\\n88\\n\\n5 Logic Programming with PROLOG\\n\\nSince its invention in 1972, in Europe PROLOG has developed into one of Europe’s leading programming languages in AI, along with procedural languages. In the U.S., on the other hand, the natively invented language LISP dominates the AI market.\\n\\nPROLOG is not a theorem prover. This is intentional, because a programmer must be able to easily and ﬂexibly control processing, and would not get very far with a theorem prover. On the other hand, PROLOG is not very helpful on its own for proving mathematical theorems. However, there are certainly interesting theo- rem provers which are programmed in PROLOG.\\n\\nRecommended as advanced literature are [Bra11] and [CM94], as well as the\\n\\nhandbooks [Wie04, Dia04] and, on the topic of CLP, [Bar98].\\n\\n5.9 Exercises\\n\\nExercise 5.1 Try to prove the theorem from Sect. 3.7 about the equality of left- and right-neutral elements of semi-groups with PROLOG. Which problems come up? What is the cause of this?\\n\\nExercise 5.2 (a) Write a predicate write_move(+State1, +State2),\\n\\nthat outputs a sentence like “Farmer and wolf cross from left to right” for each boat crossing. State1 and State2 are terms of the form state(Farmer, Wolf, Goat, Cabbage).\\n\\n(b) Write a recursive predicate write_path(+Path), which calls the predi- cate write_move(+State1, +State2) and outputs all of the farmer’s actions.\\n\\nExercise 5.3 (a) At ﬁrst glance the variable Path in the predicate plan of the PROLOG program from Example 5.1 on page 83 is unnecessary because it is apparently not changed anywhere. What is it needed for? If we add a fail to the end of action in the example, then all solutions will be given as output. Why is every solution now printed twice? How can you prevent this?\\n\\n(b)\\n\\nExercise 5.4 (a) Show by testing out that the theorem prover E (in contrast to PROLOG), given the knowledge base from Fig. 5.1 on page 76, answers the query “?- descendant(clyde, karen).” correctly. Why is that?\\n\\n(b) Compare the answers of PROLOG and E for the query “?- descendant\\n\\n(X, Y).”.\\n\\nExercise 5.5 Write as short a PROLOG program as possible that outputs 1024 ones.\\n\\n5.9 Exercises\\n\\n❄ Exercise 5.6 Investigate the runtime behavior of the naive reverse predicate.\\n\\n(a) Run PROLOG with the trace option and observe the recursive calls of nrev,\\n\\nappend, and accrev.\\n\\n(b) Compute the asymptotic time complexity of append(L1,L2,L3), that is, the dependency of the running time on the length of the list for large lists. Assume that access to the head of an arbitrary list takes constant time.\\n\\n(c) Compute the time complexity of nrev(L,R). (d) Compute the time complexity of accrev(L,R). (e) Experimentally determine the time complexity of the predicates nrev, append, and accrev, for example by carrying out time measurements (time(+Goal) gives inferences and CPU time.).\\n\\nExercise 5.7 Use function symbols instead of lists to represent the trees given in Sect. 5.4 on page 81.\\n\\n❄ Exercise 5.8 The Fibonacci sequence is deﬁned recursively by ﬁb(0) = 1, ﬁb(1) = 1\\n\\nand ﬁb(n) = ﬁb(n − 1) + ﬁb(n − 2). (a) Deﬁne a recursive PROLOG predicate fib(N,R) which calculates ﬁb(N) and\\n\\nreturns it in R.\\n\\n(b) Determine the runtime complexity of the predicate fib theoretically and by\\n\\nmeasurement.\\n\\n(c) Change your program by using asserta such that unnecessary inferences\\n\\nare no longer carried out.\\n\\n(d) Determine the runtime complexity of the modiﬁed predicate theoretically and by measurement (notice that this depends on whether fib was previously called).\\n\\n(e) Why is fib with asserta also faster when it is started for the ﬁrst time right\\n\\nafter PROLOG is started?\\n\\n❄ Exercise 5.9 The following typical logic puzzle was supposedly written by Albert Einstein. Furthermore, he supposedly claimed that only 2% of the world’s population is capable of solving it. The following statements are given. (cid:129) There are ﬁve houses, each painted a different color. (cid:129) Every house is occupied by a person with a different nationality. (cid:129) Every resident prefers a speciﬁc drink, smokes a speciﬁc brand of cigarette, and\\n\\nhas a speciﬁc pet.\\n\\n(cid:129) None of the ﬁve people drinks the same thing, smokes the same thing, or has the\\n\\nsame pet.\\n\\n(cid:129) Hints:\\n\\n– The Briton lives in the red house. – The Swede has a dog. – The Dane likes to drink tea. – The green house is to the left of the white house.\\n\\n89\\n\\n90\\n\\n5 Logic Programming with PROLOG\\n\\n– The owner of the green house drinks coffee. – The person who smokes Pall Mall has a bird. – The man who lives in the middle house drinks milk. – The owner of the yellow house smokes Dunhill. – The Norwegian lives in the ﬁrst house. – The Marlboro smoker lives next to the one who has a cat. – The man with the horse lives next to the one who smokes Dunhill. – The Winﬁeld smoker likes to drink beer. – The Norwegian lives next to the blue house. – The German smokes Rothmanns. – The Marlboro smoker has a neighbor who drinks water.\\n\\nQuestion: To whom does the ﬁsh belong? (a) First solve the puzzle manually. (b) Write a CLP program (for example with GNU-PROLOG) to solve the puzzle. Orient yourself with the room scheduling problem in Fig. 5.5 on page 86.\\n\\nSearch, Games and Problem Solving\\n\\n6.1 Introduction\\n\\nThe search for a solution in an extremely large search tree presents a problem for nearly all inference systems. From the starting state there are many possibilities for the ﬁrst inference step. For each of these possibilities there are again many possi- bilities in the next step, and so on. Even in the proof of a very simple formula from [Ert93] with three Horn clauses, each with at most three literals, the search tree for SLD resolution has the following shape:\\n\\nThe tree was cut off at a depth of 14 and has a solution in the leaf node marked by (cid:1). It is only possible to represent it at all because of the small branching factor of at most two and a cutoff at depth 14. For realistic problems, the branching factor and depth of the ﬁrst solution may become signiﬁcantly bigger.\\n\\nAssume the branching factor is a constant equal to 30 and the ﬁrst solution is at depth 50. The search tree has 3050 (cid:3) 7.2 (cid:4) 1073 leaf nodes. But the number of inference steps is even bigger because not only every leaf node, but also every inner node of the tree corresponds to an inference step. Therefore we must add up the nodes over all levels and obtain the total number of nodes of the search tree\\n\\nX50\\n\\nd¼0\\n\\n30d ¼ 1 (cid:5) 3051 1 (cid:5) 30\\n\\n¼ 7:4 (cid:4) 1073;\\n\\nwhich does not change the node count by much. Evidently, nearly all of the nodes of this search tree are on the last level. As we will see, this is generally the case. But now back to the search tree with the 7.4 (cid:4) 1073 nodes. Assume we had 10,000\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_6\\n\\n91\\n\\n6\\n\\n92\\n\\n6 Search, Games and Problem Solving\\n\\ncomputers which can each perform a billion inferences per second, and that we could distribute the work over all of the computers with no cost. The total com- putation time for all 7.4 (cid:4) 1073 inferences would be approximately equal to\\n\\n7:4 (cid:4) 1073 inferences 10000 (cid:4) 109 inferences=sec\\n\\n¼ 7:4 (cid:4) 1060 sec (cid:3) 2:3 (cid:4) 1053 years;\\n\\nwhich is about 1043 times as much time as the age of our universe. By this simple thought exercise, we can quickly recognize that there is no realistic chance of searching this kind of search space completely with the means available to us in this world. Moreover, the assumptions related to the size of the search space were completely realistic. In chess for example, there are over 30 possible moves for a typical situation, and a game lasting 50 half-turns is relatively short.\\n\\nHow can it be then, that there are good chess players—and these days also good chess computers? How can it be that mathematicians ﬁnd proofs for theorems in which the search space is even much bigger? Evidently we humans use intelligent strategies which dramatically reduce the search space. The experienced chess player, just like the experienced mathematician, will, by mere observation of the situation, immediately rule out many actions as senseless. Through his experience, he has the ability to evaluate various actions for their utility in reaching the goal. Often a person will go by feel. If one asks a mathematician how he found a proof, he may answer that the intuition came to him in a dream. In difﬁcult cases, many doctors ﬁnd a diagnosis purely by feel, based on all known symptoms. Especially in difﬁcult situations, there is often no formal theory for solution-ﬁnding that guarantees an optimal solution. In everyday problems, such as the search for a runaway cat in Fig. 6.1 on page 93, intuition plays a big role. We will deal with this kind of heuristic search method in Sect. 6.3 and additionally describe processes with which computers can, similarly to humans, improve their heuristic search strategies by learning.\\n\\nFirst, however, we must understand how uninformed search, that is, blindly\\n\\ntrying out all possibilities, works. We begin with a few examples.\\n\\nExample 6.1 With the 8-puzzle, a classic example for search algorithms [Nil98, RN10], the various algorithms can be very visibly illustrated. Squares with the numbers 1 to 8 are distributed in a 3 (cid:4) 3 matrix like the one in Fig. 6.2 on page 93. The goal is to reach a certain ordering of the squares, for example in ascending order by rows as represented in Fig. 6.2 on page 93. In each step a square can be moved left, right, up, or down into the empty space. The empty space therefore moves in the corresponding opposite direction. For analysis of the search space, it is convenient to always look at the possible movements of the empty ﬁeld.\\n\\nThe search tree for a starting state is represented in Fig. 6.3 on page 94. We can see that the branching factor alternates between two, three, and four. Averaged over (cid:3) 2:83. We see two levels at a time, we obtain an average branching factor1 of\\n\\np\\n\\nﬃﬃﬃ 8\\n\\n1The average branching factor of a tree is the branching factor that a tree with a constant branching factor, equal depth, and an equal amount of leaf nodes would have.\\n\\n6.1 Introduction\\n\\nFig. 6.1 A heavily trimmed search tree—or: “Where is my cat?”\\n\\nFig. 6.2 Possible starting and goal states of the 8-puzzle\\n\\n93\\n\\n94\\n\\n6 Search, Games and Problem Solving\\n\\nFig. 6.3 Search tree for the 8-puzzle. Bottom right a goal state in depth 3 is represented. To save space the other nodes at this level have been omitted\\n\\nFig. 6.4 Search tree for an 8-puzzle without cycles of length 2\\n\\nthat each state is repeated multiple times two levels deeper because in a simple uninformed search, every action can be reversed in the next step.\\n\\nIf we disallow cycles of length 2, then for the same starting state we obtain the search tree represented in Fig. 6.4. The average branching factor is reduced by about 1 and becomes 1.8.2\\n\\nBefore we begin describing the search algorithms, a few new terms are needed. We are dealing with discrete search problems here. Being in state s, an action a1 leads to a new state s′. Thus s′ = a1(s). A different action may lead to state s″, in\\n\\n2For an 8-puzzle the average branching factor depends on the starting state (see Exercise 6.2 on page 122).\\n\\n6.1 Introduction\\n\\nother words: s″ = a2(s). Recursive application of all possible actions to all states, beginning with the starting state, yields the search tree.\\n\\nDeﬁnition 6.1 A search problem is deﬁned by the following values State: Description of the state of the world in which the search agent ﬁnds\\n\\nitself.\\n\\nStarting state: The initial state in which the search agent is started. Goal state: If the agent reaches a goal state, then it terminates and outputs a\\n\\nsolution (if desired).\\n\\nActions: All of the agents allowed actions. Solution: The path in the search tree from the starting state to the goal state. Cost function: Assigns a cost value to every action. Necessary for ﬁnding a\\n\\ncost-optimal solution. State space: Set of all states.\\n\\nApplied to the 8-puzzle, we get\\n\\nState: 3 (cid:4) 3 matrix S with the values 1, 2, 3, 4, 5, 6, 7, 8 (once each) and one\\n\\nempty square.\\n\\nStarting state: An arbitrary state. Goal state: An arbitrary state, e.g. the state given to the right in Fig. 6.2 on page 93. Actions: Movement of the empty square Sij to the left (if j 6¼ 1), right (if j 6¼ 3),\\n\\nup (if i 6¼ 1), down (if i 6¼ 3).\\n\\nCost function: The constant function 1, since all actions have equal cost. State space: The state space is degenerate in domains that are mutually unreachable\\n\\n(Exercise 6.4 on page 122). Thus there are unsolvable 8-puzzle problems.\\n\\nFor analysis of the search algorithms, the following terms are needed:\\n\\nDeﬁnition 6.2 (cid:129) The number of successor states of a state s is called the branching factor\\n\\nb(s), or b if the branching factor is constant.\\n\\n(cid:129) The effective branching factor of a tree of depth d with n total nodes is deﬁned as the branching factor that a tree with constant branching factor, equal depth, and equal n would have (see Exercise 6.3 on page 122). (cid:129) A search algorithm is called complete if it ﬁnds a solution for every solvable problem. If a complete search algorithm terminates without ﬁnding a solution, then the problem is unsolvable.\\n\\nFor a given depth d and node count n, the effective branching factor can be\\n\\ncalculated by solving the equation\\n\\n95\\n\\n96\\n\\n6 Search, Games and Problem Solving\\n\\nn ¼ bd þ 1 (cid:5) 1 b (cid:5) 1\\n\\nð6:1Þ\\n\\nfor b because a tree with constant branching factor and depth d has a total of\\n\\nn ¼\\n\\nXd\\n\\ni¼0\\n\\nbi ¼ bd þ 1 (cid:5) 1 b (cid:5) 1\\n\\nð6:2Þ\\n\\nnodes.\\n\\nFor the practical application of search algorithms for ﬁnite search trees, the last\\n\\nlevel is especially important because\\n\\nTheorem 6.1 For heavily branching ﬁnite search trees with a large constant branching factor, almost all nodes are on the last level.\\n\\nThe simple proof of this theorem is recommended to the reader as an exercise\\n\\n(Exercise 6.1 on page 122).\\n\\nExample 6.2 We are given a map, such as the one represented in Fig. 6.5, as a graph with cities as nodes and highway connections between the cities as weighted edges with distances. We are looking for an optimal route from city A to city B. The description of the corresponding schema reads State: A city as the current location of the traveler. Starting state: An arbitrary city. Goal state: An arbitrary city. Actions: Travel from the current city to a neighboring city. Cost function: The distance between the cities. Each action corresponds to an edge\\n\\nin the graph with the distance as the weight.\\n\\nFig. 6.5 The graph of southern Germany as an example of a search task with a cost function\\n\\n6.1 Introduction\\n\\nState space: All cities, that is, nodes of the graph. To ﬁnd the route with minimal length, the costs must be taken into account because they are not constant as they were in the 8-puzzle.\\n\\nDeﬁnition 6.3 A search algorithm is called optimal if it, if a solution exists, always ﬁnds the solution with the lowest cost.\\n\\nThe 8-puzzle problem is deterministic, which means that every action leads from a state to a unique successor state. It is furthermore observable, that is, the agent always knows which state it is in. In route planning in real applications both characteristics are not always given. The action “Drive from Munich to Ulm” may—for example because of an accident—lead to the successor state “Munich”. It can also occur that the traveler no longer knows where he is because he got lost. We want to ignore these kinds of complications at ﬁrst. Therefore in this chapter we will only look at problems that are deterministic and observable.\\n\\nProblems like the 8-puzzle, which are deterministic and observable, make action planning relatively simple because, due to having an abstract model, it is possible to ﬁnd action sequences for the solution of the problem without actually carrying out the actions in the real world. In the case of the 8-puzzle, it is not necessary to actually move the squares in the real world to ﬁnd the solution. We can ﬁnd optimal solutions with so-called ofﬂine algorithms. One faces much different challenges when, for example, building robots that are supposed to play soccer. Here there will never be an exact abstract model of the actions. For example, a robot that kicks the ball in a speciﬁc direction cannot predict with certainty where the ball will move because, among other things, it does not know whether an opponent will catch or deﬂect the ball. Here online algorithms are then needed, which make decisions based on sensor signals in every situation. Reinforcement learning, described in Chap. 10, works toward optimization of these decisions based on experience.\\n\\n6.2 Uninformed Search\\n\\n6.2.1 Breadth-First Search\\n\\nIn breadth-ﬁrst search, the search tree is explored from top to bottom according to the algorithm given in Fig. 6.6 on page 98 until a solution is found. First every node in the node list is tested for whether it is a goal node, and in the case of success, the program is stopped. Otherwise all successors of the node are generated. The search is then continued recursively on the list of all newly generated nodes. The whole thing repeats until no more successors are generated.\\n\\nThis algorithm is generic. That is, it works for arbitrary applications if the two application-speciﬁc functions “GoalReached” and “Successors” are provided.\\n\\n97\\n\\n98\\n\\n6 Search, Games and Problem Solving\\n\\nBREADTHFIRSTSEARCH(NodeList, Goal) NewNodes H ; 2 For all Node NodeList\\n\\nIf GoalReached(Node, Goal)\\n\\nReturn(“Solution found”, Node)\\n\\nNewNodes H Append(NewNodes, Successors(Node))\\n\\nIf NewNodes\\n\\nReturn(BREADTH-FIRST-SEARCH(NewNodes, Goal))\\n\\nElse\\n\\nReturn(“No solution”)\\n\\nFig. 6.6 The algorithm for breadth-ﬁrst search\\n\\nFig. 6.7 Breadth-ﬁrst search during the expansion of the third-level nodes. The nodes are numbered according to the order they were generated. The successors of nodes 11 and 12 have not yet been generated\\n\\n“GoalReached” calculates whether the argument is a goal node, and “Successors” calculates the list of all successor nodes of its argument. Figure 6.7 shows a snapshot of breadth-ﬁrst search. Analysis Since breadth-ﬁrst search completely searches through every depth and reaches every depth in ﬁnite time, it is complete if the branching factor b is ﬁnite. The optimal (that is, the shortest) solution is found if the costs of all actions are the same (see Exercise 6.7 on page 123). Computation time and memory space grow exponentially with the depth of the tree. For a tree with constant branching factor b and depth d, the total compute time is thus given by\\n\\nc (cid:6)\\n\\nXd\\n\\ni¼0\\n\\nbi ¼ bd þ 1 (cid:5) 1 b (cid:5) 1\\n\\n¼ OðbdÞ:\\n\\nAlthough only the last level is saved in memory, the memory space requirement is also O(bd).\\n\\n6.2 Uninformed Search\\n\\nWith the speed of today’s computers, which can generate billions of nodes within minutes, main memory quickly ﬁlls up and the search ends. The problem of the shortest solution not always being found can be solved by the so-called Uniform Cost Search, in which the node with the lowest cost from the list of nodes (which is sorted ascendingly by cost) is always expanded, and the new nodes sorted in. Thus we ﬁnd the optimal solution. The memory problem is not yet solved, however. A solution for this problem is provided by depth-ﬁrst search.\\n\\n6.2.2 Depth-First Search\\n\\nIn depth-ﬁrst search only a few nodes are stored in memory at one time. After the expansion of a node only its successors are saved, and the ﬁrst successor node is immediately expanded. Thus the search quickly becomes very deep. Only when a node has no successors and the search fails at that depth is the next open node expanded via backtracking to the last branch, and so on. We can best perceive this in the elegant recursive algorithm in Fig. 6.8 and in the search tree in Fig. 6.9 on page 100. Analysis Depth-ﬁrst search requires much less memory than breadth-ﬁrst search because at most b nodes are saved at each depth. Thus we need at most b ⋅ d memory cells.\\n\\nHowever, depth-ﬁrst search is not complete for inﬁnitely deep trees because depth-ﬁrst search runs into an inﬁnite loop when there is no solution in the far left branch. Therefore the question of ﬁnding the optimal solution is obsolete. Because of the inﬁnite loop, no bound on the computation time can be given. In the case of a ﬁnitely deep search tree with depth d, a total of about bd nodes are generated. Thus the computation time grows, just as in breadth-ﬁrst search, exponentially with depth. We can make the search tree ﬁnite by setting a depth limit. Now if no solution is found in the pruned search tree, there can nonetheless be solutions outside the limit.\\n\\nDEPTHFIRSTSEARCH(Node, Goal)\\n\\nIf GoalReached(Node, Goal) Return(“Solution found”) NewNodes D Successors(Node) While NewNodes\\n\\nResult = DEPTH-FIRST-SEARCH(First(NewNodes), Goal) If Result = “Solution found” Return(“Solution found”) NewNodes = Rest(NewNodes)\\n\\nReturn(“No solution”)\\n\\nFig. 6.8 The algorithm for depth-ﬁrst search. The function “First” returns the ﬁrst element of a list, and “Rest” the rest of the list\\n\\n99\\n\\n100\\n\\n6 Search, Games and Problem Solving\\n\\nFig. 6.9 Execution of depth-ﬁrst search. All nodes at depth three are unsuccessful and cause backtracking. The nodes are numbered in the order they were generated\\n\\nFig. 6.10 Schematic representation of the development of the search tree in iterative deepening with limits from 1 to 7. The breadth of the tree corresponds to a branching factor of 2\\n\\nThus the search becomes incomplete. There are obvious ideas, however, for getting the search to completeness.\\n\\n6.2.3 Iterative Deepening\\n\\nWe begin the depth-ﬁrst search with a depth limit of 1. If no solution is found, we raise the limit by 1 and start searching from the beginning, and so on, as shown in Fig. 6.10. This iterative raising of the depth limit is called iterative deepening.\\n\\nWe must augment the depth-ﬁrst search program given in Fig. 6.8 on page 99 with the two additional parameters “Depth” and “Limit”. “Depth” is raised by one at the recursive call, and the head line of the while loop is replaced by “While NewNodes 6¼ ; And Depth < Limit”. The modiﬁed algorithm is represented in Fig. 6.11 on page 101.\\n\\nAnalysis The memory requirement is the same as in depth-ﬁrst search. One could argue that repeatedly re-starting depth-ﬁrst search at depth zero causes a lot of redundant work. For large branching factors this is not the case. We now show that\\n\\n6.2 Uninformed Search\\n\\nITERATIVEDEEPENING(Node, Goal)\\n\\nDepthLimit = 0 Repeat\\n\\nResult = DEPTHFIRSTSEARCH-B(Node, Goal, 0, DepthLimit) DepthLimit = DepthLimit + 1\\n\\nUntil Result = “Solution found”\\n\\nDEPTHFIRSTSEARCH-B(Node, Goal, Depth, Limit)\\n\\nIf GoalReached(Node, Goal) Return(“Solution found”) NewNodes = Successors(Node) While NewNodes Result =\\n\\nAnd Depth < Limit\\n\\nDEPTHFIRSTSEARCH-B(First(NewNodes), Goal, Depth + 1, Limit)\\n\\nIf Result = “Solution found” Return(“Solution found”) NewNodes = Rest(NewNodes)\\n\\nReturn(“No solution”)\\n\\nFig. 6.11 The algorithm for iterative deepening, which calls the slightly modiﬁed depth-ﬁrst search with a depth limit (TIEFENSUCHE-B)\\n\\nthe sum of the number of nodes of all depths up to the one before last dmax − 1 in all trees searched is much smaller than the number of nodes in the last tree searched.\\n\\nLet Nb(d) be the number of nodes of a search tree with branching factor b and\\n\\ndepth d and dmax be the last depth searched. The last tree searched contains\\n\\nNbðdmaxÞ ¼\\n\\nXdmax\\n\\ni¼0\\n\\nbi ¼ bdmax þ 1 (cid:5) 1\\n\\nb (cid:5) 1\\n\\nnodes. All trees searched beforehand together have\\n\\nXdmax(cid:5)1\\n\\nd¼1\\n\\nXdmax(cid:5)1\\n\\nNbðdÞ ¼\\n\\nd¼1 ¼ 1\\n\\nb (cid:5) 1\\n\\n¼ 1\\n\\nb (cid:5) 1\\n\\n(cid:3) 1\\n\\nb (cid:5) 1\\n\\nbd þ 1 (cid:5) 1 b (cid:5) 1 Xdmax\\n\\n¼ 1\\n\\nb (cid:5) 1\\n\\n!\\n\\nXdmax(cid:5)1\\n\\nd¼1 !\\n\\nbd þ 1\\n\\nbd\\n\\n(cid:5) dmax þ 1\\n\\n(cid:3)\\n\\n(cid:3)\\n\\nd¼2\\n\\nbdmax þ 1 (cid:5) 1 b (cid:5) 1 bdmax þ 1 (cid:5) 1 b (cid:5) 1\\n\\n(cid:5) 1 (cid:5) b (cid:5) dmax þ 1 (cid:4)\\n\\n¼ 1\\n\\nb (cid:5) 1\\n\\nNbðdmaxÞ\\n\\n!\\n\\n(cid:4)\\n\\n(cid:5) dmax þ 1\\n\\n!\\n\\n101\\n\\n102\\n\\n6 Search, Games and Problem Solving\\n\\nnodes. For b > 2 this is less than the number Nb(dmax) of nodes in the last tree. For b = 20 the ﬁrst dmax − 1 trees together contain only about 1 ¼ 1=19 of the number of b(cid:5)1 nodes in the last tree. The computation time for all iterations besides the last can be ignored.\\n\\nJust like breadth-ﬁrst search, this method is complete, and given a constant cost\\n\\nfor all actions, it ﬁnds the shortest solution.\\n\\n6.2.4 Comparison\\n\\nThe described search algorithms have been put side-by-side in Table 6.1.\\n\\nWe can clearly see that iterative deepening is the winner of this test because it gets the best grade in all categories. In fact, of all four algorithms presented it is the only practically usable one.\\n\\nWe do indeed have a winner for this test, although for realistic applications it is usually not successful. Even for the 15-puzzle, the 8-puzzle’s big brother (see Exercise 6.4 on page 122), there are about 2 (cid:4) 1013 different states. For non-trivial inference systems the state space is many orders of magnitude bigger. As shown in Sect. 6.1, all the computing power in the world will not help much more. Instead what is needed is an intelligent search that only explores a tiny fraction of the search space and ﬁnds a solution there.\\n\\n6.2.5 Cycle Check\\n\\nAs shown in Sect. 6.1, nodes may be repeatedly visited during a search. In the 8-puzzle, for example, every move can be immediately undone, which leads to unnecessary cycles of length two. Such cycles can be prevented by recording within each node all of its predecessors and, when expanding a node, comparing the newly created successor nodes with the predecessor nodes. All of the duplicates found can be removed from the list of successor nodes. This simple check costs only a small constant factor of additional memory space and increases the constant computation time c by an additional constant d for the check itself for a total of c þ d. This overhead for the cycle check is (hopefully) offset by a reduction in the cost of the\\n\\nTable 6.1 Comparison of the uninformed search algorithms. (*) means that the statement is only true given a constant action cost. ds is the maximal depth for a ﬁnite search tree\\n\\nBreadth-ﬁrst search\\n\\nUniform cost search\\n\\nDepth-ﬁrst search\\n\\nIterative deepening\\n\\nCompleteness Optimal solution Computation time Memory use\\n\\nYes Yes (*) bd bd\\n\\nYes Yes bd bd\\n\\nNo No 1 or bd bd\\n\\nYes Yes (*) bd bd\\n\\n6.2 Uninformed Search\\n\\nsearch. The reduction depends, of course, on the particular application and therefore cannot be given in general terms.\\n\\nFor the 8-puzzle we obtain the result as follows. If, for example, during breadth-ﬁrst search with effective branching factor b on a ﬁnite tree of depth d, the computation time without the cycle check is c (cid:6) bd, the required time with the cycle check becomes\\n\\nðc þ dÞ (cid:6) ðb (cid:5) 1Þd:\\n\\nThe check thus practically always results in a clear gain because reducing the branching factor by one has an exponentially growing effect as the depth increases, whereas the additional computation time d only somewhat increases the constant factor.\\n\\nNow the question arises as to how a check on cycles of arbitrary length would affect the search performance. The list of all predecessors must now be stored for each node, which can be done very efﬁciently (see Exercise 6.8 on page 123). During the search, each newly created node must now be compared with all its predecessors. The computation time of depth-ﬁrst search or breadth-ﬁrst search is given by\\n\\nXd\\n\\nXd\\n\\nc1 (cid:6)\\n\\nbi þ c2 (cid:6)\\n\\ni (cid:6) bi:\\n\\ni¼0\\n\\ni¼0\\n\\nHere, the ﬁrst term is the already-known cost of generating the nodes, and the second term is the cost of the cycle check. We can show that for large values of b and d,\\n\\nXd\\n\\ni (cid:6) bi (cid:3) d (cid:6) bd:\\n\\ni¼0\\n\\nThe complexity of the search with the full cycle check therefore only increases by a factor of d faster than for the search without a cycle check. In search trees that are not very deep, this extra complexity is not important. For search tasks with very deep, weakly branching trees, it may be advantageous to use a hash table [CLR90] to store the list of predecessors. Lookups in the table can be done in constant time such that the computation time of the search algorithm only grows by a small constant factor. In summary, we can conclude that the cycle check implies hardly any additional overhead and is therefore worthwhile for applications with repeatedly occurring nodes.\\n\\n6.3 Heuristic Search\\n\\nHeuristics are problem-solving strategies which in many cases ﬁnd a solution faster than uninformed search. However, this is not guaranteed. Heuristic search could require a lot more time and can even result in the solution not being found.\\n\\n103\\n\\n104\\n\\n6 Search, Games and Problem Solving\\n\\nWe humans successfully use heuristic processes for all kinds of things. When buying vegetables at the supermarket, for example, we judge the various options for a pound of strawberries using only a few simple criteria like price, appearance, source of production, and trust in the seller, and then we decide on the best option by gut feeling. It might theoretically be better to subject the strawberries to a basic chemical analysis before deciding whether to buy them. For example, the straw- berries might be poisoned. If that were the case the analysis would have been worth the trouble. However, we do not carry out this kind of analysis because there is a very high probability that our heuristic selection will succeed and will quickly get us to our goal of eating tasty strawberries.\\n\\nHeuristic decisions are closely linked with the need to make real-time decisions with limited resources. In practice a good solution found quickly is preferred over a solution that is optimal, but very expensive to derive.\\n\\nA heuristic evaluation function f(s) for states is used to mathematically model a heuristic. The goal is to ﬁnd, with little effort, a solution to the stated search problem with minimal total cost. Please note that there is a subtle difference between the effort to ﬁnd a solution and the total cost of this solution. For example it may take Google Maps half a second’s worth of effort to ﬁnd a route from the City Hall in San Francisco to Tuolumne Meadows in Yosemite National Park, but the ride from San Francisco to Tuolumne Meadows by car may take four hours and some money for gasoline etc. (total cost).\\n\\nNext we will modify the breadth-ﬁrst search algorithm by adding the evaluation function to it. The currently open nodes are no longer expanded left to right by row, but rather according to their heuristic rating. From the set of open nodes, the node with the minimal rating is always expanded ﬁrst. This is achieved by immediately evaluating nodes as they are expanded and sorting them into the list of open nodes. The list may then contain nodes from different depths in the tree.\\n\\nBecause heuristic evaluation of states is very important for the search, we will differentiate from now on between states and their associated nodes. The node contains the state and further information relevant to the search, such as its depth in the search tree and the heuristic rating of the state. As a result, the function “Successors”, which generates the successors (children) of a node, must also immediately calculate for these successor nodes their heuristic ratings as a com- ponent of each node. We deﬁne the general search algorithm HEURISTICSEARCH in Fig. 6.12 on page 105.\\n\\nThe node list is initialized with the starting nodes. Then, in the loop, the ﬁrst node from the list is removed and tested for whether it is a solution node. If not, it will be expanded with the function “Successors” and its successors added to the list with the function “SortIn”. “SortIn(X,Y)” inserts the elements from the unsorted list X into the ascendingly sorted list Y. The heuristic rating is used as the sorting key. Thus it is guaranteed that the best node (that is, the one with the lowest heuristic value) is always at the beginning of the list.3\\n\\n3When sorting in a new node from the node list, it may be advantageous to check whether the node is already available and, if so, to delete the duplicate.\\n\\n6.3 Heuristic Search\\n\\nHEURISTICSEARCH(Start, Goal)\\n\\nNodeList = [Start] While True\\n\\nIf NodeList = ∅ Return(“No solution”) Node = First(NodeList) NodeList = Rest(NodeList) If GoalReached(Node, Goal) Return(“Solution found”, Node) NodeList = SortIn(Successors(Node),NodeList)\\n\\nFig. 6.12 The algorithm for heuristic search\\n\\nFig. 6.13 He: “Dear, think of the fuel cost! I’ll pluck one for you somewhere else.” She: “No, I want that one over there!”\\n\\nDepth-ﬁrst and breadth-ﬁrst search also happen to be special cases of the function HEURISTICSEARCH. We can easily generate them by plugging in the appropriate evaluation function (Exercise 6.11 on page 123).\\n\\nThe best heuristic would be a function that calculates the actual costs from each node to the goal. To do that, however, would require a traversal of the entire search space, which is exactly what the heuristic is supposed to prevent. Therefore we need a heuristic that is fast and simple to compute. How do we ﬁnd such a heuristic? An interesting idea for ﬁnding a heuristic is simpliﬁcation of the problem. The original task is simpliﬁed enough that it can be solved with little computational cost. The costs from a state to the goal in the simpliﬁed problem then serve as an estimate for the actual problem (see Fig. 6.13). This cost estimate function we denote h.\\n\\n105\\n\\n106\\n\\n6 Search, Games and Problem Solving\\n\\n6.3.1 Greedy Search\\n\\nIt seems sensible to choose the state with the lowest estimated h value (that is, the one with the lowest estimated cost) from the list of currently available states. The cost estimate then can be used directly as the evaluation function. For the evaluation in the function HEURISTICSEARCH we set f (s) = h(s). This can be seen clearly in the trip planning example (Example 6.2 on page 96). We set up the task of ﬁnding the straight line path from city to city (that is, the ﬂying distance) as a simpliﬁcation of the problem. Instead of searching the optimal route, we ﬁrst determine from every node a route with minimal ﬂying distance to the goal. We choose Ulm as the destination. Thus the cost estimate function becomes\\n\\nhðsÞ ¼ flying distance from city s to Ulm.\\n\\nThe ﬂying distances from all cities to Ulm are given in Fig. 6.14 next to the graph. The search tree for starting in Linz is represented in Fig. 6.15 on page 107 left. We can see that the tree is very slender. The search thus ﬁnishes quickly. Unfor- tunately, this search does not always ﬁnd the optimal solution. For example, this algorithm fails to ﬁnd the optimal solution when starting in Mannheim (Fig. 6.15 on page 107 right). The Mannheim–Nürnberg–Ulm path has a length of 401 km. The route Mannheim–Karlsruhe–Stuttgart–Ulm would be signiﬁcantly shorter at 238 km. As we observe the graph, the cause of this problem becomes clear. Nürnberg is in fact somewhat closer than Karlsruhe to Ulm, but the distance from Mannheim to Nürnberg is signiﬁcantly greater than that from Mannheim to Karl- sruhe. The heuristic only looks ahead “greedily” to the goal instead of also taking into account the stretch that has already been laid down to the current node. This is why we give it the name greedy search.\\n\\nFig. 6.14 City graph with ﬂying distances from all cities to Ulm\\n\\n6.3 Heuristic Search\\n\\nFig. 6.15 Greedy search: from Linz to Ulm (left) and from Mannheim to Ulm (right). The node list data structure for the left search tree, sorted by the node rating before the expansion of the node München is given\\n\\n6.3.2 A★-Search\\n\\nWe now want to take into account the costs that have accrued during the search up to the current node s. First we deﬁne the cost function\\n\\ngðsÞ ¼ Sum of accrued costs from the start to the current node;\\n\\nthen add to that the estimated cost to the goal and obtain as the heuristic evaluation function\\n\\nf ðsÞ ¼ gðsÞ þ hðsÞ:\\n\\nNow we add yet another small, but important requirement.\\n\\nDeﬁnition 6.4 A heuristic cost estimate function h(s) that never overestimates the actual cost from state s to the goal is called admissible.\\n\\nfunction f(s) = g(s) + h(s) and an admissible heuristic function h is called A★-algorithm. This famous algorithm is complete and optimal. A★ thus always ﬁnds the shortest solution for every solvable search problem. We will explain and prove this in the following discussion.\\n\\nThe\\n\\nfunction HEURISTICSEARCH\\n\\ntogether with\\n\\nan\\n\\nevaluation\\n\\n107\\n\\n108\\n\\n6 Search, Games and Problem Solving\\n\\nFig. 6.16 Two snapshots of the A★ search tree for the optimal route from Frankfurt to Ulm. In the boxes below the name of the city s we show g(s), h(s), f(s). Numbers in parentheses after the city names show the order in which the nodes have been generated by the “Successor” function\\n\\nFirst we apply the A★-algorithm to the example. We are looking for the shortest\\n\\npath from Frankfurt to Ulm.\\n\\nIn the top part of Fig. 6.16 we see that the successors of Mannheim are generated before the successors of Würzburg. The optimal solution Frankfurt–Würzburg–Ulm is generated shortly thereafter in the eighth step, but it is not yet recognized as such. Thus the algorithm does not terminate yet because the node Karlsruhe (3) has a better (lower) f value and thus is ahead of the node Ulm (8) in line. Only when all f values are greater than or equal to that of the solution node Ulm (8) have we ensured that we have an optimal solution. Otherwise there could potentially be another solution with lower costs. We will now show that this is true generally.\\n\\nTheorem 6.2 The A★ algorithm is optimal. That is, it always ﬁnds the solution with the lowest total cost if the heuristic h is admissible.\\n\\nProof In the HEURISTICSEARCH algorithm, every newly generated node s is sorted in by the function “SortIn” according to its heuristic rating f(s). The node with the\\n\\n6.3 Heuristic Search\\n\\nFig. 6.17 The ﬁrst solution node l found by A★ never has a higher cost than another arbitrary node l′\\n\\nsmallest rating value thus is at the beginning of the list. If the node l at the beginning of the list is a solution node, then no other node has a better heuristic rating. For all other nodes s it is true then that f (l) (cid:7) f (s). Because the heuristic is admissible, no better solution l′ can be found, even after expansion of all other nodes (see Fig. 6.17). Written formally:\\n\\ngðlÞ ¼ gðlÞ þ hðlÞ ¼ f ðlÞ (cid:7) f ðsÞ ¼ gðsÞ þ hðsÞ (cid:7) gðl0Þ:\\n\\nThe ﬁrst equality holds because l is a solution node with h(l) = 0. The second is the deﬁnition of f. The third (in)equality holds because the list of open nodes is sorted in ascending order. The fourth equality is again the deﬁnition of f. Finally, the last (in)equality is the admissibility of the heuristic, which never overestimates the cost from node s to an arbitrary solution. Thus it has been shown that g(l) (cid:7) g(l′), that □ is, that the discovered solution l is optimal.\\n\\n6.3.3 Route Planning with the A★ Search Algorithm\\n\\nMany current car navigation systems use the A★ algorithm. The simplest, but very good heuristic for computing A★ is the straight-line distance from the current node to the destination. The use of 5 to 60 so-called landmarks is somewhat better. For these randomly chosen points the shortest paths to and from all nodes on the map are calculated in a precomputation step. Let l be such a landmark, s the current node, and z the destination node. Also let c★(x, y) be the cost of the shortest path from x to y. Then we obtain for the shortest path from s to l the triangle inequality (see Exercise 6.11 on page 123)\\n\\ncHðs; lÞ (cid:7) cHðs; zÞ þ cHðz; lÞ:\\n\\nSolving for c★(s, z) results in the admissible heuristic\\n\\nhðsÞ ¼ cHðs; lÞ (cid:5) cHðz; lÞ (cid:7) cHðs; zÞ:\\n\\nIn [Bat16], it was shown that this heuristic is better than the straight-line distance for route planning. On one hand, it can be calculated faster than the straight-line dis- tance. Due to precomputation, distances to the landmarks can be quickly retrieved\\n\\n109\\n\\n110\\n\\n6 Search, Games and Problem Solving\\n\\nfrom an array, whereas the Euclidean distances must be computed individually. It turns out that the landmark heuristic shrinks the search space even more. This can be seen in the left image of Fig. 6.18, which illustrates the search tree of A★ search for planning a route from Ravensburg to Biberach (two towns in southern Germany).4 The edges without any heuristic (i.e. with h(s) = 0) are plotted in red colour, dark green lines show the search tree using the straight-line distance heuristic, and the edges of the landmark heuristic with twenty landmarks are plotted in blue.\\n\\nThe right image shows the same route using bidirectional search, where a route from Ravensburg to Biberach and one in the opposite direction are planned effectively in parallel. If the routes meet, given certain conditions of the heuristic, an optimal route has been found [Bat16]. A quantitative analysis of the search tree sizes and the computation time on a PC can be found in Table 6.2.\\n\\nFig. 6.18 A★ search tree without heuristic (red), with straight-line distance (dark green) and with landmarks (blue). The left image shows unidirectional search and the right shows bidirectional search. Note that the green edges are covered by blue and the red edges are covered by green and blue\\n\\nTable 6.2 Comparison of search tree size and computation time for route planning with and without each of the two heuristics. The landmark heuristic is the clear winner\\n\\nUnidirectional\\n\\nBidirectional\\n\\nTree Size [nodes]\\n\\nComp. time [msec.]\\n\\nTree Size [nodes]\\n\\nComp. time [msec.]\\n\\nNo heuristic Straight-line distance Landmark heuristic\\n\\n62000 9380 5260\\n\\n192 86 16\\n\\n41850 12193 7290\\n\\n122 84 16\\n\\n4Both graphs in Fig. 6.18 were generated by A. Batzill using the system described in [Bat16].\\n\\n6.3 Heuristic Search\\n\\nObserving unidirectional search, we see that both heuristics clearly reduce the search space. The computation times are truly interesting. In the case of the land- mark heuristic, we see the computation time and the size of the search space reduced by a factor of about 12. The cost of computing the heuristic is thus insigniﬁcant. The straight-line distance, however, results in a search space reduction of a factor of 6.6, but only an improvement of a factor of 2.2 in run time due to the overhead of computing the euclidean distance.\\n\\nIn the case of bidirectional search, in contrast to unidirectional search, we see a signiﬁcant reduction of the search space even without heuristic. On the other hand, the search space is larger than the unidirectional case for both heuristics. However, because the nodes are partitioned into two sorted lists in bidirectional search (see HEURISTICSEARCH function in Fig. 6.12 on page 105), the lists are handled faster and the resulting computation times are roughly the same [Bat16].\\n\\nWhen planning a route, usually the driver cares more about driving time than the distance driven. We should thus adjust the heuristic accordingly and replace straight-line distance d(s, z) with time tðs; zÞ ¼ dðs; zÞ=vmax. Here we have to divide by the maximum average velocity, which degrades the heuristic because it causes the heuristically estimated times to be much too small. The landmark heuristic, in contrast, builds on precomputed optimal routes and therefore does not degrade. Thus, as shown in [Bat16], the search for a time-optimized route using landmark heuristic is signiﬁcantly faster than with the modiﬁed straight-line distance.\\n\\nalgorithm performs even better than A★ with landmark heuristic. It is based on the idea of combining, in a precompution step, several edges into so-called shortcuts, which are then used to reduce the search space [GSSD08, Bat16].\\n\\nThe contraction hierarchies\\n\\n6.3.4 IDA★-Search\\n\\nThe A★ search inherits a quirk from breadth-ﬁrst search. It has to save many nodes in memory, which can lead to very high memory use. Furthermore, the list of open nodes must be sorted. Thus insertion of nodes into the list and removal of nodes from the list can no longer run in constant time, which increases the algorithm’s complexity slightly. Based on the heapsort algorithm, we can structure the node list as a heap with logarithmic time complexity for insertion and removal of nodes (see [CLR90]).\\n\\nBoth problems can be solved—similarly to breadth-ﬁrst search—by iterative deepening. We work with depth-ﬁrst search and successively raise the limit. However, rather than working with a depth limit, here we use a limit for the heuristic evaluation f (s). This process is called the IDA★-algorithm.\\n\\n6.3.5 Empirical Comparison of the Search Algorithms\\n\\nIn A★, or (alternatively) IDA★, we have a search algorithm with many good properties. It is complete and optimal. It can thus be used without risk. The most\\n\\n111\\n\\n112\\n\\n6 Search, Games and Problem Solving\\n\\nimportant thing, however, is that it works with heuristics, and therefore can sig- niﬁcantly reduce the computation time needed to ﬁnd a solution. We would like to explore this empirically in the 8-puzzle example.\\n\\nFor the 8-puzzle there are two simple admissible heuristics. The heuristic h1 simply counts the number of squares that are not in the right place. Clearly this heuristic is admissible. Heuristic h2 measures the Manhattan distance. For every square the horizontal and vertical distances to that square’s location in the goal state are added together. This value is then summed over all squares. For example, the Manhattan distance of the two states\\n\\nis calculated as\\n\\nh2ðsÞ ¼ 1 þ 1 þ 1 þ 1 þ 2 þ 0 þ 3 þ 1 ¼ 10:\\n\\nThe admissibility of the Manhattan distance is also obvious (see Exercise 6.13 on page 123).\\n\\nThe described algorithms were implemented in Mathematica. For a comparison with uninformed search, the A★ algorithm with the two heuristics h1 and h2 and iterative deepening was applied to 132 randomly generated 8-puzzle problems. The average values for the number of steps and computation time are given in Table 6.3. We see that the heuristics signiﬁcantly reduce the search cost compared to unin- formed search.\\n\\nIf we compare iterative deepening to A★ with h1 at depth 12, for example, it becomes evident that h1 reduces the number of steps by a factor of about 3,000, but\\n\\nTable 6.3 Comparison of the computation cost of uninformed search and heuristic search for solvable 8-puzzle problems with various depths. Measurements are in steps and seconds. All values are averages over multiple runs (see last column)\\n\\nDepth\\n\\nIterative deepening\\n\\nSteps\\n\\nTime [sec]\\n\\nSteps\\n\\nA★ algorithm\\n\\nHeuristic h1\\n\\nTime [sec]\\n\\nSteps\\n\\nHeuristic h2\\n\\nTime [sec]\\n\\nNum. runs\\n\\n2\\n\\n20\\n\\n0.003\\n\\n3.0\\n\\n0.0010\\n\\n3.0\\n\\n0.0010\\n\\n10\\n\\n4\\n\\n81\\n\\n0.013\\n\\n5.2\\n\\n0.0015\\n\\n5.0\\n\\n0.0022\\n\\n24\\n\\n6\\n\\n806\\n\\n0.13\\n\\n10.2\\n\\n0.0034\\n\\n8.3\\n\\n0.0039\\n\\n19\\n\\n8\\n\\n6455\\n\\n1.0\\n\\n17.3\\n\\n0.0060\\n\\n12.2\\n\\n0.0063\\n\\n14\\n\\n10\\n\\n50512\\n\\n7.9\\n\\n48.1\\n\\n0.018\\n\\n22.1\\n\\n0.011\\n\\n15\\n\\n12\\n\\n486751\\n\\n75.7\\n\\n162.2\\n\\n0.074\\n\\n56.0\\n\\n0.031\\n\\n12\\n\\nIDA★\\n\\n14\\n\\n–\\n\\n–\\n\\n10079.2\\n\\n2.6\\n\\n855.6\\n\\n0.25\\n\\n16\\n\\n16\\n\\n–\\n\\n–\\n\\n69386.6\\n\\n19.0\\n\\n3806.5\\n\\n1.3\\n\\n13\\n\\n18\\n\\n–\\n\\n–\\n\\n708780.0\\n\\n161.6\\n\\n53941.5\\n\\n14.1\\n\\n4\\n\\n6.3 Heuristic Search\\n\\nthe computation time by only a factor of 1,023. This is due to the higher cost per step for the computation of the heuristic.\\n\\nCloser examination reveals a jump in the number of steps between depth 12 and depth 14 in the column for h1. This jump cannot be explained solely by the repeated work done by IDA★. It comes about because the implementation of the A★ algo- rithm deletes duplicates of identical nodes and thereby shrinks the search space. This is not possible with IDA★ because it saves almost no nodes. Despite this, A★ can no longer compete with IDA★ beyond depth 14 because the cost of sorting in new nodes pushes up the time per step so much.\\n\\nA computation of the effective branching factor according to (6.1) on page 96 yields values of about 2.8 for uninformed search. This number is consistent with the value from Sect. 6.1. Heuristic h1 reduces the branching factor to values of about 1.5 and h2 to about 1.3. We can see in the table that a small reduction of the branching factor from 1.5 to 1.3 gives us a big advantage in computation time.\\n\\nHeuristic search thus has an important practical signiﬁcance because it can solve\\n\\nproblems which are far out of reach for uninformed search.\\n\\n6.3.6 Summary\\n\\nOf the various search algorithms for uninformed search, iterative deepening is the only practical one because it is complete and can get by with very little memory. However, for difﬁcult combinatorial search problems, even iterative deepening usually fails due to the size of the search space. Heuristic search helps here through its reduction of the effective branching factor. The IDA★-algorithm, like iterative deepening, is complete and requires very little memory.\\n\\nHeuristics naturally only give a signiﬁcant advantage if the heuristic is “good”. When solving difﬁcult search problems, the developer’s actual task consists of designing heuristics which greatly reduce the effective branching factor. In Sect. 6.5 we will deal with this problem and also show how machine learning techniques can be used to automatically generate heuristics.\\n\\nIn closing, it remains to note that heuristics have no performance advantage for unsolvable problems because the unsolvability of a problem can only be estab- lished when the complete search tree has been searched through. For decidable problems such as the 8-puzzle this means that the whole search tree must be traversed up to a maximal depth whether a heuristic is being used or not. The heuristic is always a disadvantage in this case, attributable to the computational cost of evaluating the heuristic. This disadvantage can usually be estimated by a constant factor independent of the size of the problem. For undecidable problems such as the proof of PL1 formulas, the search tree can be inﬁnitely deep. This means that, in the unsolvable case, the search potentially never ends. In summary we can say the following: for solvable problems, heuristics often reduce compu- tation time dramatically, but for unsolvable problems the cost can even be higher with heuristics.\\n\\n113\\n\\n114\\n\\n6 Search, Games and Problem Solving\\n\\n6.4 Games with Opponents\\n\\nGames for two players, such as chess, checkers, Othello, and Go are deterministic because every action (a move) results in the same child state given the same parent state. In contrast, backgammon is non-deterministic because its child state depends on the result of a dice roll. These games are all observable because every player always knows the complete game state. Many card games, such as poker, for example, are only partially observable because the player does not know the other players’ cards, or only has partial knowledge about them.\\n\\nThe problems discussed so far in this chapter were deterministic and observable. In the following we will look at games which, too, are deterministic and observable. Furthermore, we will limit ourselves to zero-sum games. These are games in which every gain one player makes means a loss of the same value for the opponent. The sum of the gain and loss is always equal to zero. This is true of the games chess, checkers, Othello, and Go, mentioned above.\\n\\n6.4.1 Minimax Search\\n\\nThe goal of each player is to make optimal moves that result in victory. In principle it is possible to construct a search tree and completely search through it (like with the 8-puzzle) for a series of moves that will result in victory. However, there are several peculiarities to watch out for: 1. The effective branching factor in chess is around 30 to 35. In a typical game with 50 moves per player, the search tree has more than 30100 (cid:3) 10148 leaf nodes. Thus there is no chance to fully explore the search tree. Additionally, chess is often played with a time limit. Because of this real-time requirement, the search must be limited to an appropriate depth in the tree, for example eight half-moves. Since among the leaf nodes of this depth-limited tree there are normally no solution nodes (that is, nodes which terminate the game) a heuristic evaluation function B for board positions is used. The level of play of the program strongly depends on the quality of this evaluation function. Therefore we will further treat this subject in Sect. 6.5.\\n\\n2. In the following we will call the player whose game we wish to optimize Max, and his opponent Min. The opponent’s (Min’s) moves are not known in advance, and thus neither is the actual search tree. This problem can be elegantly solved by assuming that the opponent always makes the best move he can. The higher the evaluation B(s) for position s, the better position s is for the player Max and the worse it is for his opponent Min. Max tries to maximize the evaluation of his moves, whereas Min makes moves that result in as low an evaluation as possible.\\n\\nA search tree with four half-moves and evaluations of all leaves is given in Fig. 6.19 on page 115. The evaluation of an inner node is derived recursively as the maximum or minimum of its child nodes, depending on the node’s level.\\n\\n6.4 Games with Opponents\\n\\nFig. 6.19 A minimax game tree with look-ahead of four half-moves\\n\\nFig. 6.20 An alpha-beta game tree with look-ahead of four half-moves. The dotted portions of the tree are not traversed because they have no effect on the end result\\n\\n6.4.2 Alpha-Beta-Pruning\\n\\nBy switching between maximization and minimization, we can save ourselves a lot of work in some circumstances. Alpha-beta pruning works with depth-ﬁrst search up to a preset depth limit. In this way the search tree is searched through from left to right. Like in minimax search, in the minimum nodes the minimum is generated from the minimum value of the successor nodes and in the maximum nodes like- wise the maximum. In Fig. 6.20 this process is depicted for the tree from Fig. 6.19. At the node marked a, all other successors can be ignored after the ﬁrst child is evaluated as the value 1 because the minimum is sure to be (cid:7)1. It could even become smaller still, but that is irrelevant since the maximum is already (cid:8)3 one level above. Regardless of how the evaluation of the remaining successors turns out, the maximum will keep the value 3. Analogously the tree will be trimmed at node b. Since the ﬁrst child of b has the value 2, the minimum to be generated for b can only be less than or equal to 2. But the maximum at the root node is already sure to be (cid:8)3. This cannot be changed by values (cid:7)2. Thus the remaining subtrees of b can be pruned.\\n\\nThe same reasoning applies for the node c. However, the relevant maximum\\n\\nnode is not the direct parent, but the root node. This can be generalized.\\n\\n115\\n\\n116\\n\\n6 Search, Games and Problem Solving\\n\\nALPHABETAMAX(Node, α, β)\\n\\nIf DepthLimitReached(Node) Return(Rating(Node)) NewNodes = Successors(Node) While NewNodes\\n\\nα = Maximum(α, ALPHABETAMIN(First(NewNodes), α, β)) If α ≥ β Return(β) NewNodes = Rest(NewNodes)\\n\\nReturn(αα)\\n\\nALPHABETAMIN(Node, α, β)\\n\\nIf DepthLimitReached(Node) Return(Rating(Node)) NewNodes = Successors(Node) While NewNodes\\n\\nβ = Minimum(β, ALPHABETAMAX(First(NewNodes), α, β)) If β ≤ α Return(α) NewNodes = Rest(NewNodes)\\n\\nReturn(β)\\n\\nFig. 6.21 The algorithm for alpha-beta search with the two functions ALPHABETAMIN and ALPHABETAMAX\\n\\n(cid:129) At every leaf node the evaluation is calculated. (cid:129) For every maximum node the current largest child value is saved in a. (cid:129) For every minimum node the current smallest child value is saved in b. (cid:129) If at a minimum node k the current value b (cid:7) a, then the search under k can end. Here a is the largest value of a maximum node in the path from the root to k.\\n\\n(cid:129) If at a maximum node l the current value a (cid:8) b, then the search under l can end. Here b is the smallest value of a minimum node in the path from the root to l.\\n\\nThe algorithm given in Fig. 6.21 is an extension of depth-ﬁrst search with two functions which are called in alternation. It uses the values deﬁned above for a and b.\\n\\nThe initial alpha-beta pruning call is done with the command ALPHABETAMAX(RootNode, –∞, ∞).\\n\\nComplexity The computation time saved by alpha-beta pruning heavily depends on the order in which child nodes are traversed. In the worst case, alpha-beta\\n\\n6.4 Games with Opponents\\n\\npruning does not offer any advantage. For a constant branching factor b the number nd of leaf nodes to evaluate at depth d is equal to\\n\\nnd ¼ bd:\\n\\nIn the best case, when the successors of maximum nodes are descendingly sorted and the successors of minimum nodes are ascendingly sorted, the effective branching factor is reduced to . In chess this means a substantial reduction of the effective branching factor from 35 to about 6. Then only\\n\\np\\n\\nﬃﬃﬃ b\\n\\nnd ¼\\n\\np d ¼ bd=2\\n\\nﬃﬃﬃﬃ b\\n\\nleaf nodes would be created. This means that the depth limit and thus also the search horizon are doubled with alpha-beta pruning. However, this is only true in the case of optimally sorted successors because the child nodes’ ratings are unknown at the time when they are created. If the child nodes are randomly sorted, then the branching factor is reduced to b3/4 and the number of leaf nodes to\\n\\nnd ¼ b\\n\\n3 4 d:\\n\\nWith the same computing power a chess computer using alpha-beta pruning can, for example, compute eight half-moves ahead instead of six, with an effective branching factor of about 14. A thorough analysis with a derivation of these parameters can be found in [Pea84].\\n\\nTo double the search depth as mentioned above, we would need the child nodes to be optimally ordered, which is not the case in practice. Otherwise the search would be unnecessary. With a simple trick we can get a relatively good node ordering. We connect alpha-beta pruning with iterative deepening over the depth limit. Thus at every new depth limit we can access the ratings of all nodes of previous levels and order the successors at every branch. Thereby we reach an effective branching factor of roughly 7 to 8, which is not far from the theoretical optimum of\\n\\np\\n\\nﬃﬃﬃﬃﬃ 35\\n\\n[Nil98].\\n\\n6.4.3 Non-deterministic Games\\n\\nMinimax search can be generalized to all games with non-deterministic actions, such as backgammon. Each player rolls before his move, which is inﬂuenced by the result of the dice roll. In the game tree there are now therefore three types of levels in the sequence\\n\\nMax, dice, Min, dice, … ,\\n\\nwhere each dice roll node branches six ways. Because we cannot predict the value of the die, we average the values of all rolls and conduct the search as described with the average values from [RN10].\\n\\n117\\n\\n118\\n\\n6 Search, Games and Problem Solving\\n\\n6.5 Heuristic Evaluation Functions\\n\\nHow do we ﬁnd a good heuristic evaluation function for the task of searching? Here there are fundamentally two approaches. The classical way uses the knowledge of human experts. The knowledge engineer is given the usually difﬁcult task of for- malizing the expert’s implicit knowledge in the form of a computer program. We now want to show how this process can be simpliﬁed in the chess program example.\\n\\nIn the ﬁrst step, experts are questioned about the most important factors in the selection of a move. Then it is attempted to quantify these factors. We obtain a list of relevant features or attributes. These are then (in the simplest case) combined into a linear evaluation function B(s) for positions, which could look like:\\n\\nBðsÞ ¼ a1 (cid:6) material þ a2 (cid:6) pawn structure þ a3 (cid:6) king safety\\n\\nþ a4 (cid:6) knight in center þ a5 (cid:6) bishop diagonal coverage þ (cid:6) (cid:6) (cid:6);\\n\\nð6:3Þ\\n\\nwhere “material” is by far the most important feature and is calculated by\\n\\nmaterial ¼ material(own teamÞ (cid:5) material(opponent)\\n\\nwith\\n\\nmaterial(team) ¼ num pawnsðteamÞ (cid:6) 100 þ num knightsðteamÞ (cid:6) 300\\n\\nþ num bishopsðteamÞ (cid:6) 300 þ num rooksðteamÞ (cid:6) 500 þ num queensðteamÞ (cid:6) 900\\n\\nNearly all chess programs make a similar evaluation for material. However, there are big differences for all other features, which we will not go into here [Fra05, Lar00]. In the next step the weights ai of all features must be determined. These are set intuitively after discussion with experts, then changed after each game based on positive and negative experience. The fact that this optimization process is very expensive and furthermore that the linear combination of features is very limited suggests the use of machine learning.\\n\\n6.5.1 Learning of Heuristics\\n\\nWe now want to automatically optimize the weights ai of the evaluation function B(s) from (6.3). In this approach the expert is only asked about the relevant features f1(s), … , fn(s) for game state s. Then a machine learning process is used with the goal of ﬁnding an evaluation function that is as close to optimal as possible. We start with an initial pre-set evaluation function (determined by the learning process), and then let the chess program play. At the end of the game a rating is derived from the result (victory, defeat, or draw). Based on this rating, the evaluation function is\\n\\n6.5 Heuristic Evaluation Functions\\n\\nFig. 6.22 In this sketch of a search tree, several MCTS paths to leaf nodes are shown in red. Notice that only a small part of the tree is searched\\n\\nchanged with the goal of making fewer mistakes next time. In principle, the same thing that is done by the developer is now being taken care of automatically by the learning process.\\n\\nAs easy as this sounds, it is very difﬁcult in practice. A central problem with improving the position rating based on won or lost matches is known today as the credit assignment problem. We do in fact have a rating at the end of the game, but no ratings for the individual moves. Thus the agent carries out many actions but does not receive any positive or negative feedback until the very end. How should it then assign this feedback to the many actions taken in the past? And how should it improve its actions in that case? The exciting ﬁeld of reinforcement learning deals with these questions (see Chap. 10).\\n\\nMonte Carlo tree search (MCTS) [KS06] works quite similarly. To improve the heuristic rating of a game state s, a random number of search tree branches starting from this state are either explored to the end and evaluated, or stopped at a certain depth and then the leaf nodes are evaluated heuristically. The evaluation B(s) of state s is given as the mean of all leaf node scores. The use of MCTS paths requires only a small part of the entire exponentially exploding tree to be searched. This is illustrated in Fig. 6.22. For many computer-simulated games, such as chess, this algorithm can be used to achieve better play for the same computational effort or to reduce computational effort for the same difﬁculty level [KS06]. This method was used together with machine learning algorithms in 2016 by the program AlphaGo, described in Sect. 10.10, which was the ﬁrst Go program to defeat world-class human players [SHM+16].\\n\\n6.6 State of the Art\\n\\nFor evaluation of the quality of the heuristic search processes, I would like to repeat Elaine Rich’s deﬁnition [Ric83]:\\n\\nArtiﬁcial Intelligence is the study of how to make computers do things at which, at the moment, people are better.\\n\\n119\\n\\n120\\n\\n6 Search, Games and Problem Solving\\n\\nThere is hardly a better suited test for deciding whether a computer program is intelligent as the direct comparison of computer and human in a game like chess, checkers, backgammon or Go.\\n\\nIn 1950, Claude Shannon, Konrad Zuse, and John von Neumann introduced the ﬁrst chess programs, which, however, could either not be implemented or would take a great deal of time to implement. Just a few years later, in 1955, Arthur Samuel wrote a program that played checkers and could improve its own param- eters through a simple learning process. To do this he used the ﬁrst programmable logic computer, the IBM 701. Compared to the chess computers of today, however, it had access to a large number of archived games, for which every individual move had been rated by experts. Thus the program improved its evaluation function. To achieve further improvements, Samuel had his program play against itself. He solved the credit assignment problem in a simple manner. For each individual position during a game it compares the evaluation by the function B(s) with the one calculated by alpha-beta pruning and changes B(s) accordingly. In 1961 his checkers program beat the fourth-best checkers player in the USA. With this ground-breaking work, Samuel was surely nearly 30 years ahead of his time.\\n\\nOnly at the beginning of the nineties, as reinforcement learning emerged, did Gerald Tersauro build a learning backgammon program named TD-Gammon, which played at the world champion level (see Chap. 10).\\n\\n6.6.1 Chess\\n\\nToday many chess programs exist that play above grandmaster level. The break- through came in 1997, as IBM’s Deep Blue defeated the chess world champion Gary Kasparov with a score of 3.5 games to 2.5. Deep Blue could on average compute 12 half-moves ahead with alpha-beta pruning and heuristic position evaluation.\\n\\nAround the year 2005 one of the most powerful chess computers was Hydra, a parallel computer owned by a company in the United Arab Emirates. The software was developed by the scientists Christian Donninger (Austria) and Ulf Lorenz (Germany), as well as the German chess grand champion Christopher Lutz. Hydra uses 64 parallel Xeon processors with about 3 GHz computing power and 1 GByte memory each. For the position evaluation function each processor has an FPGA (ﬁeld programmable gate array) co-processor. Thereby it becomes possible to evaluate 200 million positions per second even with an expensive evaluation function.\\n\\nWith this technology Hydra can on average compute about 18 moves ahead. In special, critical situations the search horizon can even be stretched out to 40 half-moves. Clearly this kind of horizon is beyond what even grand champions can do, for Hydra often makes moves which grand champions cannot comprehend, but which in the end lead to victory. In 2005 Hydra defeated seventh ranked grand- master Michael Adams with 5.5–0.5 games.\\n\\nHydra uses little special textbook knowledge about chess, rather alpha-beta search with relatively general, well-known heuristics and a good hand-coded\\n\\n6.6 State of the Art\\n\\nposition evaluation. In particular, Hydra is not capable of learning. Improvements are carried out between games by the developers. As a consequence, Hydra was soon outperformed by machines that used smart learning algorithms rather than expensive hardware.\\n\\nIn 2009 the system Pocket Fritz 4, running on a PDA, won the Copa Mercosur chess tournament in Buenos Aires with nine wins and one draw against 10 excellent human chess players, three of them grandmasters. Even though not much infor- mation about the internal structure of the software is available, this chess machine represents a trend away from raw computing power toward more intelligence. This machine plays at grandmaster level, and is comparable to, if not better than Hydra. According to Pocket Fritz developer Stanislav Tsukrov [Wik13], Pocket Fritz with its chess search engine HIARCS 13 searches less than 20,000 positions per second, which is slower than Hydra by a factor of about 10,000. This leads to the con- clusion that HIARCS 13 deﬁnitely uses better heuristics to decrease the effective branching factor than Hydra and can thus well be called more intelligent than Hydra. By the way, HIARCS is a short hand for Higher Intelligence Auto Response Chess System.\\n\\n6.6.2 Go\\n\\nEven though today no human stands a chance against the best chess computers, there are still many challenges for AI. For example Go. In this ancient Japanese game, played on a square board of 361 spaces with 181 white and 180 black stones, the effective branching factor is about 250. After 8 half-moves there are already 1:5 (cid:6) 1019 possible positions. Given this complexity, none of the classic, well-known game tree search algorithms have a chance against a good human Go player. Yet in the most recent previous edition of this book, it was stated that:\\n\\nThe experts agree that “truly intelligent” algorithms are needed here. Combinatoric enu- meration of all possibilities is the wrong approach. Rather, procedures are needed that recognize patterns on the board, track gradual developments, and make rapid “intuitive” decisions. Similar to object recognition in complex images, we humans are still far superior to today’s computer programs. We process the image as a whole in a highly parallel manner, whereas the computer processes the millions of pixels successively and has great difﬁculty recognizing the essentials in the abundance of pixels. The program “The Many Faces of Go” recognizes 1100 different patterns and knows 200 different playing strategies. All Go programs, however, still have great difﬁculty recognizing whether a group of stones is dead or alive, or where in between to classify them.\\n\\nThis statement is now obsolete. In January of 2016, Google [SHM+16] and Facebook [TZ16] published the breakthrough concurrently. That same month, the program AlphaGo, developed and presented in [SHM+16] by Google DeepMind, defeated European Go champion Fan Hui 5:0. Two months later, Korean player Lee Sedol, one of the best in the world, was defeated 4:1. Deep Learning for pattern recognition (see Sect. 9.7), reinforcement learning (see Chap. 10) and Monte Carlo tree search (MCTS, see Sect. 6.5.1) lead to this successful result.\\n\\n121\\n\\n122\\n\\n6 Search, Games and Problem Solving\\n\\nThe program plays hundreds of thousands of games against itself and uses the results (win, loss, draw) to learn the best possible heuristic score for a given position. Monte Carlo tree search is used as a replacement for Minimax search, which is not suitable for Go. In Sect. 10.10, after we have gained familiarity with the necessary learning algorithms, we will introduce AlphaGo.\\n\\n6.7 Exercises\\n\\nExercise 6.1 (a) Prove Theorem 6.1 on page 96, in other words, prove that for a tree with large constant branching factor b, almost all nodes are on the last level at depth d. (b) Show that this is not always true when the effective branching factor is large\\n\\nand not constant.\\n\\nExercise 6.2 (a) Calculate the average branching factor for the 8-puzzle without a check for cycles. The average branching factor is the branching factor that a tree with an equal number of nodes on the last level, constant branching factor, and equal depth would have.\\n\\n(b) Calculate the average branching factor for the 8-puzzle for uninformed search\\n\\nwhile avoiding cycles of length 2.\\n\\nExercise 6.3 (a) What is the difference between the average and the effective branching factor\\n\\n(Deﬁnition 6.2 on page 95)?\\n\\n(b) Why is the effective branching factor better suited to analysis and comparison of the computation time of search algorithms than the average branching factor? (c) Show that for a heavily branching tree with n nodes and depth d the effective branching factor (cid:1)b is approximately equal to the average branching factor and thus equal to\\n\\np ﬃﬃﬃ nd\\n\\n.\\n\\nExercise 6.4 (a) Calculate the size of the state space for the 8-puzzle, for the analogous 3-puzzle (2 (cid:4) 2-matrix), as well as for the 15-puzzle (4 (cid:4) 4-matrix). (b) Prove that the state graph consisting of the states (nodes) and the actions (edges) for the 3-puzzle falls into two connected sub-graphs, between which there are no connections.\\n\\nExercise 6.5 With breadth-ﬁrst search for the 8-puzzle, ﬁnd a path (manually) from\\n\\nthe starting node\\n\\nto the goal node\\n\\n.\\n\\n6.7 Exercises\\n\\n➳ Exercise 6.6\\n\\n(a) Program breadth-ﬁrst search, depth-ﬁrst search, and iterative deepening in the\\n\\nlanguage of your choice and test them on the 8-puzzle example. (b) Why does it make little sense to use depth-ﬁrst search on the 8-puzzle?\\n\\nExercise 6.7 (a) Show that breadth-ﬁrst search given constant cost for all actions is guaranteed\\n\\nto ﬁnd the shortest solution.\\n\\n(b) Show that this is not the case for varying costs.\\n\\nExercise 6.8 The predecessors of all nodes must be stored to check for cycles during depth-ﬁrst search.\\n\\n(a) For depth ﬁrst search develop a data structure (not a hash table) that is as efﬁcient as possible for storing all nodes in the search path of a search tree. (b) For constant branching factor b and depth d, give a formula for the storage space needed by depth-ﬁrst search with and without storing predecessors.\\n\\nP d\\n\\nk¼0 k (cid:6) bk (cid:3) d (cid:6) bd.\\n\\n(c) Show that for large b and d, we have\\n\\nExercise 6.9 Using A★ search for the 8-puzzle, search (manually) for a path from\\n\\nthe starting node\\n\\nto the goal node\\n\\n(a) using the heuristic h1 (Sect. 6.3.4). (b) using the heuristic h2 (Sect. 6.3.4).\\n\\nExercise 6.10 Construct the A★ search tree for the city graph from Fig. 6.14 on page 106 and use the ﬂying distance to Ulm as the heuristic. Start in Bern with Ulm as the destination. Take care that each city only appears once per path.\\n\\nExercise 6.11 (a) Show that the triangle inequality is valid for shortest distances on maps. (b) Using an example, show that it is not always the case that the triangle inequality holds for direct neighbor nodes x and y, where the distance is d(x, y). That is, it is not the case that dðx; yÞ (cid:7) dðx; zÞ þ dðz; yÞ.\\n\\n➳ Exercise 6.12 Program A★ search in the programming language of your choice\\n\\nusing the heuristics h1 and h2 and test these on the 8-puzzle example.\\n\\n❄ Exercise 6.13 Give a heuristic evaluation function for states with which HEURIS- TICSEARCH can be implemented as depth-ﬁrst search, and one for a breadth-ﬁrst search implementation.\\n\\n123\\n\\n124\\n\\n6 Search, Games and Problem Solving\\n\\nExercise 6.14 What is the relationship between the picture of the couple at the canyon from Fig. 6.13 on page 105 and admissible heuristics?\\n\\nExercise 6.15 Show that the heuristics h1 and h2 for the 8-puzzle from Sect. 6.3.4 are admissible.\\n\\nExercise 6.16 (a) The search tree for a two-player game is given in Fig. 6.23 with the ratings of all leaf nodes. Use minimax search with a-b pruning from left to right. Cross out all nodes that are not visited and give the optimal resulting rating for each inner node. Mark the chosen path.\\n\\n(b) Test yourself using P. Winston’s applet [Win].\\n\\nFig. 6.23 Minimax search tree\\n\\nReasoning with Uncertainty\\n\\nWe have already shown in Chap. 4 with the Tweety problem that two-value logic leads to problems in everyday reasoning. In this example, the statements Tweety is a penguin, Penguins are birds, and All birds can ﬂy lead to the (semantically incorrect) inference Tweety can ﬂy. Probability theory provides a language in which we can formalize the statement Nearly all birds can ﬂy and carry out inferences on it. Probability theory is a proven method we can use here because the uncertainty about whether birds can ﬂy can be modeled well by a probability value. We will show, that statements such as 99% of all birds can ﬂy, together with probabilistic logic, lead to correct inferences.\\n\\nReasoning under uncertainty with limited resources plays a big role in everyday situations and also in many technical applications of AI. In these areas heuristic processes are very important, as we have already discussed in Chap. 6. For example, we use heuristic techniques when looking for a parking space in city trafﬁc. Heuristics alone are often not enough, especially when a quick decision is needed given incomplete knowledge, as shown in the following example. A pedestrian crosses the street and an auto quickly approaches. To prevent a serious accident, the pedestrian must react quickly. He is not capable of worrying about complete information about the state of the world, which he would need for the search algorithms discussed in Chap. 6. He must therefore come to an optimal decision under the given constraints (little time and little, potentially uncertain knowledge). If he thinks too long, it will be dangerous. In this and many similar situations (see Fig. 7.1 on page 126), a method for reasoning with uncertain and incomplete knowledge is needed.\\n\\nWe want to investigate the various possibilities of reasoning under uncertainty in a simple medical diagnosis example. If a patient experiences pain in the right lower abdomen and a raised white blood cell (leukocyte) count, this raises the suspicion that it might be appendicitis. We model this relationship using propositional logic with the formula\\n\\nStomach pain right lower ^ Leukocytes [ 10000 ! Appendicitis\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_7\\n\\n125\\n\\n7\\n\\n126\\n\\n7 Reasoning with Uncertainty\\n\\nFig. 7.1 “Let’s just sit back and think about what to do!”\\n\\nIf we then know that\\n\\nStomach pain right lower ^ Leukocytes [ 10000\\n\\nis true, then we can use modus ponens to derive Appendicitis. This model is clearly too coarse. In 1976, Shortliffe and Buchanan recognized this when building their medical expert system MYCIN [Sho76]. They developed a calculus using so-called certainty factors, which allowed the certainty of facts and rules to be represented. A rule A → B is assigned a certainty factor β. The semantic of a rule A → β B is deﬁned via the conditional probability P (B | A) ¼ β. In the above example, the rule could then read\\n\\nStomach pain right lower ^ Leukocytes [ 10000 !0:6 Appendicitis:\\n\\nFor reasoning with this kind of formulas, they used a calculus for connecting the factors of rules. It turned out, however, that with this calculus inconsistent results could be derived.\\n\\n7 Reasoning with Uncertainty\\n\\nAs discussed in Chap. 4, there were also attempts to solve this problem by using non-monotonic logic and default logic, which, however, were unsuccessful in the end. The Dempster–Schäfer theory assigns a belief function Bel(A) to a logical proposition A, whose value gives the degree of evidence for the truth of A. But even this formalism has weaknesses, which is shown in [Pea88] using a variant of the Tweety example. Even fuzzy logic, which above all is successful in control theory, demonstrates considerable weaknesses when reasoning under uncertainty in more complex applications [Elk93].\\n\\nSince about the mid-1980s, probability theory has had more and more inﬂuence in AI [Pea88, Che85, Whi96, Jen01]. In the ﬁeld of reasoning with Bayesian networks, it has secured itself a ﬁrm place among successful or subjective probability, AI techniques. Rather than implication as it is known in logic (material implication), conditional probability is used here, which models everyday causal reasoning sig- niﬁcantly better. Reasoning with probability proﬁts heavily from the fact that probability theory is a hundreds of years old, well-established branch of mathematics. In this chapter we will select an elegant, but for an instruction book somewhat unusual, entry point into this ﬁeld. After a short introduction to the most important foundations needed here for reasoning with probability, we will begin with a simple, but important example for reasoning with uncertain and incomplete knowledge. In a quite natural, almost compelling way, we will be led to the method of maximum entropy (MaxEnt). Then we will show the usefulness of this method in practice using the medical expert system LEXMED. Finally we will introduce the now widespread reasoning with Bayesian networks, and show the relationship between the two methods.\\n\\n7.1 Computing with Probabilities\\n\\nThe reader who is familiar with probability theory can skip this section. For everyone else we will give a quick ramp-up and recommend a few appropriate textbooks such as [Ros09, FPP07].\\n\\nProbability is especially well-suited for modeling reasoning under uncertainty. One reason for this is that probabilities are intuitively easy to interpret, which can be seen in the following elementary example.\\n\\nExample 7.1 For a single roll of a gaming die (experiment), the probability of the event “rolling a six” equals 1/6, whereas the probability of the occurrence “rolling an odd number” is equal to 1/2.\\n\\nDeﬁnition 7.1 Let Ω be the ﬁnite set of events for an experiment. Each event ω 2 Ω represents a possible outcome of the experiment. If these events wi 2 Ω mutually exclude each other, but cover all possible outcomes of the attempt, then they are called elementary events.\\n\\n127\\n\\n128\\n\\n7 Reasoning with Uncertainty\\n\\nExample 7.2 For a single roll of one gaming die\\n\\nX ¼ f1; 2; 3; 4; 5; 6g\\n\\nbecause no two of these events can happen simultaneously. Rolling an even number ({2, 4, 6}) is therefore not an elementary event, nor is rolling a number smaller than ﬁve ({1, 2, 3, 4}) because {2, 4, 6} \\\\ {1, 2, 3, 4} ¼ {2, 4} 6¼ ;.\\n\\nGiven two events A and B, A [ B is also an event. Ω itself is denoted the certain\\n\\nevent, and the empty set ; the impossible event.\\n\\nIn the following we will use the propositional logic notation for set opera- tions. That is, for the set A \\\\ B we write A ∧ B. This is not only a syntactic transformation, rather it is also semantically correct because the intersection of two sets is deﬁned as\\n\\nx 2 A \\\\ B , x 2 A ^ x 2 B:\\n\\nBecause this is the semantic of A ∧ B, we can and will use this notation. This is also true for the other set operations union and complement, and we will, as shown in the following table, use the propositional logic notation for them as well.\\n\\nSet notation A \\\\ B A [ B (cid:1)A X\\n\\n;\\n\\nPropositional logic A ^ B A _ B :A t f\\n\\nDescription\\n\\nintersection / and union / or complement / negation\\n\\ncertain event / true impossible event / false\\n\\nThe variables used here (for example A, B, etc.) are called random variables in probability theory. We will only use discrete chance variables with ﬁnite domains here. The variable face_number for a dice roll is discrete with the values 1, 2, 3, 4, 5, 6. The probability of rolling a ﬁve or a six is equal to 1/3. This can be described by\\n\\nPðface number 2 f5; 6gÞ ¼ Pðface number ¼ 5 _ face number ¼ 6Þ ¼ 1=3:\\n\\nThe concept of probability is supposed to give us a description as objective as possible of our “belief” or “conviction” about the outcome of an experiment. All numbers in the interval [0,1] should be possible, where 0 is the probability of the impossible event and 1 the probability of the certain event. We come to this from the following deﬁnition.\\n\\n7.1 Computing with Probabilities\\n\\nDeﬁnition 7.2 Let Ω = {ω1, ω2, … , ωn} be ﬁnite. There is no preferred elementary event, which means that we assume a symmetry related to the frequency of how often each elementary event appears. The probability P(A) of the event A is then\\n\\nPðAÞ ¼\\n\\njAj jXj\\n\\n¼\\n\\nNumber of favorable cases for A Number of possible cases\\n\\n:\\n\\nIt follows immediately that every elementary event has the probability 1/|Ω|. The requirement that elementary events have equal probability is called the Laplace assumption and the probabilities calculated thereby are called Laplace probabili- ties. This deﬁnition hits its limit when the number of elementary events becomes inﬁnite. Because we are only looking at ﬁnite event spaces here, though, this does not present a problem. To describe events we use variables with the appropriate number of values. For example, a variable eye_color can take on the values green, blue, brown. eye_color ¼ blue then describes an event because we are dealing with a proposition with the truth values t or f. For binary (boolean) variables, the variable itself is already a proposition. Here it is enough, for example, to write P(JohnCalls) instead of P(JohnCalls ¼ t).\\n\\nExample 7.3 By this deﬁnition, the probability of rolling an even number is\\n\\nPðface number 2 f2; 4; 6gÞ ¼\\n\\njf2; 4; 6gj jf1; 2; 3; 4; 5; 6gj\\n\\n¼\\n\\n3 6\\n\\n¼\\n\\n1 2\\n\\n:\\n\\nThe following important rules follow directly from the deﬁnition.\\n\\nTheorem 7.1 1. P(Ω) = 1. 2. P(;) = 0, which means that the impossible event has a probability of 0. 3. For pairwise exclusive events A and B it is true that P(A ∨ B) = P(A) + P(B). 4. For two complementary events A and ¬A it is true that P(A) + P(¬A) = 1. 5. For arbitrary events A and B it is true that P(A ∨ B) = P(A) + P(B) −\\n\\nP(A ∧ B).\\n\\n6. For A (cid:2) B it is true that P(A) ≤ P(B). 7. If A1; . . .; An are the elementary events, then ization condition).\\n\\n6. For A (cid:2) B it is true that P(A) ≤ P(B). 7. If A1; . . .; An are the elementary events, then P\\n\\n129\\n\\n130\\n\\n7 Reasoning with Uncertainty\\n\\nThe expression P(A ∧ B) or equivalently P(A, B) stands for the probability of the events A ∧ B. We are often interested in the probabilities of all elementary events, that is, of all combinations of all values of the variables A and B. For the binary variables A and B these are P(A, B), P(A,¬B), P(¬A, B), P(¬A, ¬B). We call the vector\\n\\nðPðA; BÞ; PðA; :BÞ; Pð:A; BÞ; Pð:A; :BÞÞ\\n\\nconsisting of these four values a distribution or joint probability distribution of the variables A and B. A shorthand for this is P(A, B). The distribution in the case of two variables can be nicely visualized in the form of a table (matrix), represented as follows:\\n\\nP(A, B) A ¼ w A ¼ f\\n\\nB ¼ w P (A, B) P (¬A, B)\\n\\nB ¼ f P (A, ¬B) P (¬A, ¬B)\\n\\nFor the d variables X1; . . . ; Xd with n values each, the distribution has the values PðX1 ¼ x1; . . . ; Xd ¼ xdÞ and x1; . . . ; xd, each of which take on n different values. The distribution can therefore be represented as a d-dimensional matrix with a total of nd elements. Due to the normalization condition from Theorem 7.1 on page 129, however, one of these nd values is redundant and the distribution is characterized by nd −1 unique values.\\n\\n7.1.1 Conditional Probability\\n\\nExample 7.4 On Landsdowne street in Boston, the speed of 100 vehicles is mea- sured. For each measurement it is also noted whether the driver is a student. The results are\\n\\nEvent\\n\\nFrequency\\n\\nRelative frequency\\n\\nVehicle observed Driver is a student (S) Speed too high (G) Driver is a student and speeding (S ∧ G)\\n\\n100 30 10 5\\n\\n1 0.3 0.1 0.05\\n\\nWe pose the question: Do students speed more frequently than the average\\n\\nperson, or than non-students?1\\n\\n1The computed probabilities can only be used for continued propositions if the measured sample (100 vehicles) is representative. Otherwise only propositions about the observed 100 vehicles can be made.\\n\\n7.1 Computing with Probabilities\\n\\nThe answer is given by the probability\\n\\nPðGjSÞ ¼\\n\\njDriver is a student and speedingj jDriver is a studentj\\n\\n¼\\n\\n5 30\\n\\n¼\\n\\n1 6\\n\\n(cid:3) 0:17\\n\\nfor speeding under the condition that the driver is a student. This is obviously different from the a priori probability P(G) = 0.1 for speeding. For the a priori probability, the event space is not limited by additional conditions.\\n\\nDeﬁnition 7.3 For two events A and B, the probability P(A|B) for A under the condition B (conditional probability) is deﬁned by\\n\\nPðAjBÞ ¼\\n\\nPðA ^ BÞ PðBÞ\\n\\n:\\n\\nIn Example 7.4 we see that in the case of a ﬁnite event space, the conditional probability P(A|B) can be understood as the probability of A ∧ B when we only look at the event B, that is, as\\n\\nPðAjBÞ ¼\\n\\njA ^ Bj jBj\\n\\n:\\n\\nThis formula can be easily derived using Deﬁnition 7.2 on page 129\\n\\nPðAjBÞ ¼\\n\\nPðA ^ BÞ PðBÞ\\n\\n¼\\n\\njA^Bj jXj jBj jXj\\n\\n¼\\n\\njA ^ Bj jBj\\n\\n:\\n\\nDeﬁnition 7.4 If, for two events A and B,\\n\\nPðAjBÞ ¼ PðAÞ;\\n\\nthen these events are called independent.\\n\\nThus A and B are independent if the probability of the event A is not inﬂuenced\\n\\nby the event B.\\n\\n131\\n\\n132\\n\\n7 Reasoning with Uncertainty\\n\\nTheorem 7.2 For independent events A and B, it follows from the deﬁnition that\\n\\nPðA ^ BÞ ¼ PðAÞ (cid:4) PðBÞ:\\n\\nExample 7.5 For a roll of two dice, the probability of rolling two sixes is 1/36 if the two dice are independent because\\n\\nPðD1 ¼ 6 ^ D2 ¼ 6Þ ¼ PðD1 ¼ 6Þ (cid:4) PðD2 ¼ 6Þ ¼\\n\\n1 6\\n\\n(cid:4)\\n\\n1 6\\n\\n¼\\n\\n1 36\\n\\n;\\n\\nwhere the ﬁrst equation is only true when the two dice are independent. If for example by some magic power die 2 is always the same as die 1, then\\n\\nPðD1 ¼ 6 ^ D2 ¼ 6Þ ¼\\n\\n1 6\\n\\n:\\n\\nChain Rule Solving the deﬁnition of conditional probability for P(A ∧ B) results in the so-called product rule\\n\\nPðA ^ BÞ ¼ PðAjBÞ PðBÞ;\\n\\nwhich we immediately generalize for the case of n variables. By repeated appli- cation of the above rule we obtain the chain rule\\n\\nPðX1; . . . ; XnÞ\\n\\n¼ PðXnjX1; . . . ; Xn(cid:5)1Þ (cid:4) PðX1; . . . ; Xn(cid:5)1Þ ¼ PðXnjX1; . . . ; Xn(cid:5)1Þ (cid:4) PðXn(cid:5)1jX1; . . . ; Xn(cid:5)2Þ (cid:4) PðX1; . . . ; Xn(cid:5)2Þ ¼ PðXnjX1; . . . ; Xn(cid:5)1Þ (cid:4) PðXn(cid:5)1jX1; . . . ; Xn(cid:5)2Þ (cid:4) . . . (cid:4) PðXnjX1Þ (cid:4) PðX1Þ\\n\\n¼\\n\\nYn\\n\\nPðXnjX1; . . . ; Xi(cid:5)1Þ;\\n\\nð7:1Þ\\n\\ni¼1\\n\\nwith which we can represent a distribution as a product of conditional probabilities. Because the chain rule holds for all values of the variables X1; . . .; Xn, it has been formulated for the distribution using the symbol P.\\n\\nMarginalization Because A ⇔ (A ∧ B) ∨ (A ∧ ¬B) is true for binary variables A and B\\n\\nPðAÞ ¼ PððA ^ BÞ _ ðA ^ :BÞÞ ¼ PðA ^ BÞ þ PðA ^ :BÞ:\\n\\n7.1 Computing with Probabilities\\n\\nBy summation over the two values of B, the variable B is eliminated. Analogously, for arbitrary variables X1; . . . ; Xd, a variable, for example Xd, can be eliminated by summation over all of their variables and we get\\n\\nX\\n\\nPðX1 ¼ x1; . . .; Xd(cid:5)1 ¼ xd(cid:5)1Þ ¼\\n\\nPðX1 ¼ x1; . . .; Xd(cid:5)1 ¼ xd(cid:5)1; Xd ¼ xdÞ:\\n\\nxd\\n\\nThe application of this formula is called marginalization. This summation can continue with the variables X1, … , Xd−1 until just one variable is left. Marginal- ization can also be applied to the distribution P(X1, … , Xd). The resulting distri- bution P(X1, … , Xd−1) is called the marginal distribution. It is comparable to the projection of a rectangular cuboid on a ﬂat surface. Here the three-dimensional object is drawn on the edge or “margin” of the cuboid, i.e. on a two-dimensional set. In both cases the dimensionality is reduced by one.\\n\\nExample 7.6 We observe the set of all patients who come to the doctor with acute stomach pain. For each patient the leukocyte value is measured, which is a metric for the relative abundance of white blood cells in the blood. We deﬁne the variable Leuko, which is true if and only if the leukocyte value is greater than 10,000. This indicates an infection in the body. Otherwise we deﬁne the variable App, which tells us whether the patient has appendicitis, that is, an infected appendix. The distri- bution P(App, Leuko) of these two variables is given in the following table:\\n\\nP(App, Leuko) Leuko ¬Leuko Total\\n\\nApp\\n\\n0.23 0.05 0.28\\n\\n¬App 0.31 0.41 0.72\\n\\nTotal\\n\\n0.54 0.46 1\\n\\nIn the last row the sum over the rows is given, and in the last column the sum of the columns is given. These sums are arrived at by marginalization. For example, we read off\\n\\nPðLeukoÞ ¼ PðApp; LeukoÞ þ Pð:App; LeukoÞ ¼ 0:54:\\n\\nThe given distribution P(App, Leuko) could come from a survey of German doctors, for example. From it we can then calculate the conditional probability\\n\\nPðLeukojAppÞ ¼\\n\\nPðLeuko; AppÞ PðAppÞ\\n\\n¼ 0:82\\n\\nwhich tells us that about 82% of all appendicitis cases lead to a high leukocyte value. Values like this are published in medical literature. However, the conditional\\n\\n133\\n\\n134\\n\\n7 Reasoning with Uncertainty\\n\\nprobability P(App|Leuko), which would actually be much more helpful for diag- nosing appendicitis, is not published. To understand this, we will ﬁrst derive a simple, but very important formula.\\n\\nBayes’ Theorem Swapping A and B in Deﬁnition 7.3 yields\\n\\nPðAjBÞ ¼\\n\\nPðA ^ BÞ PðBÞ\\n\\nand\\n\\nPðBjAÞ ¼\\n\\nPðA ^ BÞ PðAÞ\\n\\n:\\n\\nBy solving both equations for PðA ^ BÞ and equating them we obtain Bayes’ theorem\\n\\nPðAjBÞ ¼\\n\\nPðBjAÞ (cid:4) PðAÞ PðBÞ\\n\\n;\\n\\nð7:2Þ\\n\\nwhose relevance to many applications we will illustrate using three examples. First we apply it to the appendicitis example and obtain\\n\\nExample 7.7\\n\\nPðAppjLeukoÞ ¼\\n\\nPðLeukojAppÞ (cid:4) PðAppÞ PðLeukoÞ\\n\\n¼\\n\\n0:82 (cid:4) 0:28 0:54\\n\\n¼ 0:43:\\n\\nð7:3Þ\\n\\nWhy then is PðLeukojAppÞ published, but not PðAppjLeukoÞ?\\n\\nAssuming that appendicitis affects the biology of all humans the same, regard- less of ethnicity, PðLeukojAppÞ is a universal value that is valid worldwide. In Equation 7.3 we see that PðAppjLeukoÞ is not universal, for this value is inﬂuenced by the a priori probabilities PðAppÞ and PðLeukoÞ. Each of these can vary according to one’s life circumstances. For example, PðLeukoÞ is dependent on whether a population has a high or low rate of exposure to infectious diseases. In the tropics, this value can differ signiﬁcantly from that of cold regions. Bayes’ theorem, however, makes it easy for us to take the universally valid value PðLeukojAppÞ, and compute PðAppjLeukoÞ which is useful for diagnosis.\\n\\nBefore we dive deeper into this example and build a medical expert system for appendicitis in Sect. 7.3 let us ﬁrst apply Bayes’ theorem to another interesting medical example.\\n\\nExample 7.8 In cancer diagnosis, so-called tumor markers are often measured. One example of this is the use of the tumor marker PSA (prostate speciﬁc antigen) for the diagnosis of prostate cancer (PCa = prostate cancer) in men. Assuming that no further tests for PCa have been conducted, the test is considered positive, that is, there is suspected PCa, if the concentration of PSA reaches a level at or above 4 ng/ml. If this occurs, the probability P Cjpos Þ of PCa is of interest to the patient. ð\\n\\n7.1 Computing with Probabilities\\n\\nThe binary variable C is true if the patient has PCa, and pos represents a PSA value (cid:6) 4 ng=ml. Let us now compute the P Cjpos Þ. For reasons similar to those men- tioned for appendicitis diagnosis, this value is not reported. Instead, researchers Þ and the speciﬁcity Pðnegj:CÞ of the test.2 publish the sensitivity P posjC According to [HL04], for a sensitivity of 0.95, the speciﬁcity can be at most 0.25, which is why we proceed from PðposjCÞ ¼ 0:95 and Pðnegj:CÞ ¼ 0:25 below. We apply Bayes’ theorem and obtain\\n\\nð\\n\\nð\\n\\nPðCjposÞ ¼\\n\\n¼\\n\\nPðposjCÞ (cid:4) PðCÞ PðposÞ\\n\\nPðposjCÞ (cid:4) PðCÞ PðposjCÞ (cid:4) PðCÞ þ Pðposj:CÞ (cid:4) Pð:CÞ 0:95 (cid:4) 0:0021 0:75\\n\\n¼\\n\\n0:95 (cid:4) 0:0021 0:95 (cid:4) 0:0021 þ 0:75 (cid:4) 0:99679\\n\\n¼ 0:0027:\\n\\n¼\\n\\nHere we use Pðposj:CÞ ¼ 1 (cid:5) Pðnegj:CÞ ¼ 1 (cid:5) 0:25 ¼ 0:75 and PðCÞ ¼ 0:0021 ¼ 0:21% as the a priori probability of PCa during one year.3 It makes sense to assume that the PSA test is done once per year. This result is somewhat sur- prising from the patient’s perspective because the probability of PCa after a positive test is, at 0:27%, only marginally higher than the probability of 0:21% for PCa for a 55-year-old man. Thus, a PSA value of just over 4 ng/ml is deﬁnitively no reason for the patient to panic. At most it is used as a basis for further examinations, such as biopsy or MRI, leading if necessary to radiation and surgery. The situation is similar for many other tumor markers such as those for colorectal cancer or breast cancer diagnosis by mammography.\\n\\nThe cause of this problem is the very low speciﬁcity Pðnegj:CÞ ¼ 0:25, which leads to 75% of healthy patients (without PCa) getting a false-positive test result and consequently undergoing unnecessary examinations. Because of this, PSA testing has been a controversial discussion topic for years.4\\n\\nAssume we had a better test with a speciﬁcity of 99%, which would only deliver a false-positive result for one percent of healthy men. Then, in the above calcula- tion, we would assign Pðposj:CÞ the value 0.01 and obtain the result PðCjposÞ ¼ 0; 17. Plainly, this test would be much more speciﬁc.\\n\\nExample 7.9 A sales representative who wants to sell an alarm system could make the following argument:\\n\\nIf you buy this very reliable alarm system, it will alert you to any break-in with 99% certainty. Our competitor’s system only offers a certainty of 85%.\\n\\nHearing this, if the buyer concludes that from an alert A he can infer a break-in B with high certainty, he is wrong. Bayes’ theorem shows the reason. What the\\n\\n2For deﬁnitions of sensitivity and speciﬁcity see Eqs. 7.16 and 7.17. 3See http://www.prostata.de/pca_haeuﬁgkeit.html for a 55-year-old man. 4The author is not a medical doctor. Therefore these computations should not be used as a basis for personal medical decisions by potentially afﬂicted individuals. If necessary, please consult a specialist physician or the relevant specialist literature.\\n\\n135\\n\\n136\\n\\n7 Reasoning with Uncertainty\\n\\nrepresentative told us is that PðAjBÞ ¼ 0:99. What he doesn’t say, however, is what it means when we hear the alarm go off. To ﬁnd out, we use Bayes’ theorem to compute P(B|A) and assume that the buyer lives in a relatively safe area in which break-ins are rare, with PðBÞ ¼ 0:001. Additionally, we assume that the alarm system is triggered not only by burglars, but also by animals, such as birds or cats in the yard, which results in PðAÞ ¼ 0:1. Thus we obtain\\n\\nPðBjAÞ ¼\\n\\nPðAjBÞPðBÞ PðAÞ\\n\\n¼\\n\\n0:99 (cid:4) 0:001 0:1\\n\\n¼ 0:01;\\n\\nwhich means that whoever buys this system will not be happy because they will be startled by too many false alarms. When we examine the denominator\\n\\nPðAÞ ¼ PðAjBÞ PðBÞ þ PðAj:BÞ Pð:BÞ ¼ 0:00099 þ PðAj:BÞ (cid:4) 0:999 ¼ 0:1\\n\\nof Bayes’ theorem more closely, we see that PðAj:BÞ (cid:3) 0:1, which means that the alarm will be triggered roughly every tenth day that there is not a break-in.\\n\\nFrom this example we learn, among other things, that it is important to consider which probabilities we are really interested in as a buyer, expecially when it comes to security. When the arguments of a conditional probability are inter- changed, the value can change dramatically when the prior probabilities differ signiﬁcantly.\\n\\n7.2 The Principle of Maximum Entropy\\n\\nWe will now show, using an inference example, that a calculus for reasoning under uncertainty can be realized using probability theory. However, we will soon see that the well-worn probabilistic paths quickly come to an end. Speciﬁcally, when too little knowledge is available to solve the necessary equations, new ideas are needed. The American physicist E.T. Jaynes did pioneering work in this area in the 1950s. He claimed that given missing knowledge, one can maximize the entropy of the desired probability distribution, and applied this principle to many examples in [Jay57, Jay03]. This principle was then further developed [Che83, Nil86, Kan89, KK92] and is now mature and can be applied technologically, which we will show in the example of the LEXMED project in Sect. 7.3.\\n\\n7.2.1 An Inference Rule for Probabilities\\n\\nWe want to derive an inference rule for uncertain knowledge that is analogous to the modus ponens. From the knowledge of a proposition A and a rule A ⇒ B, the conclusion B shall be reached. Formulated succinctly, this reads\\n\\n7.2 The Principle of Maximum Entropy\\n\\nA; A ! B B\\n\\n:\\n\\nThe generalization for probability rules yields\\n\\nPðAÞ ¼ a; PðBjAÞ ¼ b PðBÞ ¼ ?\\n\\n:\\n\\nthe two probability rules α, β be given and the value P(B) desired. By\\n\\nLet marginalization we obtain the desired marginal distribution\\n\\nPðBÞ ¼ PðA; BÞ þ Pð:A; BÞ ¼ PðBjAÞ (cid:4) PðAÞ þ PðBj:AÞ (cid:4) Pð:AÞ:\\n\\nThe three values P(A), P(¬A), P BjAð Þ on the right side are known, but the value P(B | ¬A) is unknown. We cannot make an exact statement about P(B) with classical probability theory, but at the most we can estimate P Bð Þ (cid:6) P BjAð\\n\\nÞ (cid:4) PðAÞ.\\n\\nWe now consider the distribution\\n\\nPðA; BÞ ¼ ðPðA; BÞ; PðA; :BÞ; Pð:A; BÞ; Pð:A; :BÞÞ\\n\\nand introduce for shorthand the four unknowns\\n\\np1 ¼ PðA; BÞ; p2 ¼ PðA; :BÞ; p3 ¼ Pð:A; BÞ; p4 ¼ Pð:A; :BÞ:\\n\\nThese four parameters determine the distribution. If they are all known, then every probability for the two variables A and B can be calculated. To calculate the four unknowns, four equations are needed. One equation is already known in the form of the normalization condition\\n\\np1 þ p2 þ p3 þ p4 ¼ 1:\\n\\nTherefore, three more equations are needed. In our example, however, only two equations are known.\\n\\nFrom the given values PðAÞ ¼ a and PðBjAÞ ¼ b we calculate\\n\\nPðA; BÞ ¼ PðBjAÞ (cid:4) PðAÞ ¼ ab\\n\\nand\\n\\nPðAÞ ¼ PðA; BÞ þ PðA; :BÞ:\\n\\n137\\n\\n138\\n\\n7 Reasoning with Uncertainty\\n\\nFrom this we can set up the following system of equations and solve it as far as is possible:\\n\\np1 ¼ ab; p1 þ p2 ¼ a; p1 þ p2 þ p3 þ p4 ¼ 1;\\n\\nð7:4Þ in (7.5): ð7:5Þ in ð7:6Þ:\\n\\np2 ¼ a (cid:5) ab ¼ að1 (cid:5) bÞ;\\n\\np3 þ p4 ¼ 1 (cid:5) a:\\n\\nð7:4Þ ð7:5Þ ð7:6Þ ð7:7Þ ð7:8Þ\\n\\nThe probabilities p1, p2 for the interpretations (A, B) and (A, ¬B) are thus known, but for the values p3, p4 only one equation still remains. To come to a deﬁnite solution despite this missing knowledge, we change our point of view. We use the given equation as a constraint for the solution of an optimization problem.\\n\\nWe are looking for a distribution p (for the variables p3, p4) which maximizes\\n\\nthe entropy\\n\\nHð pÞ ¼ (cid:5)\\n\\nXn\\n\\npi ln pi ¼ (cid:5)p3 ln p3 (cid:5) p4 ln p4\\n\\nð7:9Þ\\n\\ni¼1\\n\\nunder the constraint p3 + p4 = 1− α (7.8). Why exactly should the entropy function be maximized? Because we are missing information about the distribu- tion, it must somehow be added in. We could ﬁx an ad hoc value, for example p3 = 0.1. Yet it is better to determine the values p3 and p4 such that the infor- mation added is minimal. We can show (Sect. 8.4.2 and [SW76]) that entropy measures the uncertainty of a distribution up to a constant factor. Negative entropy is then a measure of the amount of information a distribution contains. Maxi- mization of entropy minimizes the information content of the distribution. To visualize this, the entropy function for the two-dimensional case is represented graphically in Fig. 7.2 on page 139.\\n\\nTo determine the maximum of the entropy under the constraint p3 + p4 − 1 + α = 0 we use the method of Lagrange multipliers [Ste07]. The Lagrange function reads\\n\\nL ¼ (cid:5)p3 ln p3 (cid:5) p4 ln p4 þ kðp3 þ p4 (cid:5) 1 þ aÞ:\\n\\nTaking the partial derivatives with respect to p3 and p4 we obtain\\n\\n@L @p3 @L @p4\\n\\n¼ (cid:5) ln p3 (cid:5) 1 þ k ¼ 0;\\n\\n¼ (cid:5) ln p4 (cid:5) 1 þ k ¼ 0\\n\\n7.2 The Principle of Maximum Entropy\\n\\nFig. 7.2 Contour line diagram of the two-dimensional entropy function. We see that it is strictly convex in the whole unit square and that it has an isolated global maximum. Also marked is the constraint p3 + p4 = 1 as a special case of the condition p3 + p4 −1 + α = 0 for α = 0 which is relevant here\\n\\nand calculate\\n\\np3 ¼ p4 ¼\\n\\n1 (cid:5) a 2\\n\\n:\\n\\nNow we can calculate the desired value\\n\\nPðBÞ ¼ PðA; BÞ þ Pð:A; BÞ ¼ p1 þ p3 ¼ ab þ\\n\\n1 (cid:5) a 2\\n\\n¼ a\\n\\n(cid:2)\\n\\nb (cid:5)\\n\\n1 2\\n\\n(cid:3)\\n\\nþ\\n\\nSubstituting in α and β yields\\n\\n(cid:2)\\n\\n(cid:3)\\n\\nPðBÞ ¼ PðAÞ\\n\\nPðBjAÞ (cid:5)\\n\\n1 2\\n\\nþ\\n\\n1 2\\n\\n:\\n\\nP(B) is shown in Fig. 7.3 on page 140 for various values of P BjAð Þ. We see that in the two-value edge case, that is, when P(B) and P BjAð Þ take on the values 0 or 1, probabilistic inference returns the same value for P(B) as the modus ponens. When A and B|A are both true, B is also true. An interesting case is P(A) = 0, in which ¬A is true. Modus ponens cannot be applied here, but our formula results in the value 1/2 for P(B) irrespective of P BjAð Þ. When A is false, we know nothing about B, which reﬂects our intuition exactly. The case where P(A) = 1 and P BjAð Þ = 0 is also covered by propositional logic. Here A is true and A ⇒ B false, and thus A ∧ ¬B true. Then B is false. The horizontal line in the ﬁgure means that we cannot make a prediction about B in the case of P BjAð Þ = 1/2. Between these points, P(B) changes linearly for changes to P(A) or P BjAð\\n\\nÞ.\\n\\n1 2\\n\\n:\\n\\n139\\n\\n140\\n\\n7 Reasoning with Uncertainty\\n\\nFig. 7.3 Curve array for P(B) as a function of P(A) for different values of P BjAð\\n\\nÞ\\n\\nTheorem 7.3 Let there be a consistent5 set of linear probabilistic equations. Then there exists a unique maximum for the entropy function with the given equations as constraints. The MaxEnt distribution thereby deﬁned has min- imum information content under the constraints.\\n\\nIt follows from this theorem that there is no distribution which satisﬁes the constraints and has higher entropy than the MaxEnt distribution. A calculus that leads to lower entropy puts in additional ad hoc information, which is not justiﬁed. Looking more closely at the above calculation of P(B), we see that the two values p3 and p4 always occur symmetrically. This means that swapping the two variables does not change the result. Thus the end result is p3 = p4. The so-called indifference of these two variables leads to them being set equal by MaxEnt. This relationship is valid generally:\\n\\nDeﬁnition 7.5 If an arbitrary exchange of two or more variables in the Lagrange equations results in equivalent equations, these variables are called indifferent.\\n\\n5A set of probabilistic equations is called consistent if there is at least one solution, that is, one distribution which satisﬁes all equations.\\n\\n7.2 The Principle of Maximum Entropy\\n\\n; . . .; pik g is indifferent, then the Theorem 7.4 If a set of variables fpi1 maximum of the entropy under the given constraints is at the point where pi1 ¼ pi2 ¼ (cid:4) (cid:4) (cid:4) ¼ pik .\\n\\nWith this knowledge we could have immediately set the two variables p3 and p4\\n\\nequal (without solving the Lagrange equations).\\n\\n7.2.2 Maximum Entropy Without Explicit Constraints\\n\\nWe now look at the case in which no knowledge is given. This means that, other than the normalization condition\\n\\np1 þ p2 þ (cid:4) (cid:4) (cid:4) þp n ¼ 1;\\n\\nthere are no constraints. All variables are therefore indifferent. Therefore we can set them equal and it follows that p1 = p2 = ⋅⋅⋅ = pn = 1/n.6 For reasoning under uncertainty, this means that given a complete lack of knowledge, all worlds are equally probable. That is, the distribution is uniform. For example, in the case of two variables A and B it would be the case that\\n\\nPðA; BÞ ¼ PðA; :BÞ ¼ Pð:A; BÞ ¼ Pð:A; :BÞ ¼ 1=4;\\n\\nfrom which P(A) = P(B) = 1/2 and P(B|A) = 1/2 follow. The result for the two-dimensional case can be seen in Fig. 7.2 on page 139 because the marked condition is exactly the normalization condition. We see that the maximum of the entropy lies on the line at exactly (1/2, 1/2).\\n\\nAs soon as the value of a condition deviates from the one derived from the uniform distribution, the probabilities of the worlds shift. We show this in a further example. With the same descriptions as used above we assume that only\\n\\nPðBjAÞ ¼ b\\n\\nis known. Thus PðA; BÞ ¼ PðBjAÞPðAÞ ¼ bPðAÞ, from which p1 = β(p1 + p2) follows and we derive the two constraints\\n\\nbp2 þ ðb (cid:5) 1Þp1 ¼ 0;\\n\\np1 þ p2 þ p3 þ p4 (cid:5) 1 ¼ 0:\\n\\n6The reader may calculate this result by maximization of the entropy under the normalization condition (Exercise 7.5 on page 132).\\n\\n141\\n\\n142\\n\\n7 Reasoning with Uncertainty\\n\\nFig. 7.4 p1, p2, p3, p4 in dependence on β\\n\\nHere the Lagrange equations can no longer be solved symbolically so easily. A numeric solution of the Lagrange equations yields the picture represented in Fig. 7.4, which shows that p3 = p4. We can already see this in the constraints, in which p3 and p4 are indifferent. For P BjAð Þ = 1/2 we obtain the uniform distribution, which is no surprise. This means that the constraint for this value does not imply a restriction on the distribution. Furthermore, we can see that for small P(B|A), P(A, B) is also small.\\n\\n7.2.3 Conditional Probability Versus Material Implication\\n\\nWe will now show that, for modeling reasoning, conditional probability is better than what is known in logic as material implication (to this end, also see [Ada75]). First we observe the truth table shown in Table 7.1, in which the conditional probability and material implication for the extreme cases of probabilities zero and one are compared. In both cases with false premises (which, intuitively, are critical cases), P BjAð\\n\\nÞ is undeﬁned, which makes sense.\\n\\nTable 7.1 Truth table for material implication and conditional probability for propositional logic limit\\n\\nA\\n\\nt t f f\\n\\nB\\n\\nt f t f\\n\\nA ⇒ B t f t t\\n\\nP(A)\\n\\n1 1 0 0\\n\\nP(B)\\n\\n1 0 1Unde 0Unde\\n\\nP BjAð 1 0\\n\\nÞ\\n\\nﬁned ﬁned\\n\\n7.2 The Principle of Maximum Entropy\\n\\nÞ when arbitrary values P(A) = α and P(B) = γ are given and no other information is known. Again we maximize entropy under the given constraints. As above we set\\n\\nNow we ask ourselves which value is taken on by P BjAð\\n\\np1 ¼ PðA; BÞ;\\n\\np2 ¼ PðA; :BÞ;\\n\\np3 ¼ Pð:A; BÞ;\\n\\np4 ¼ Pð:A; :BÞ\\n\\nand obtain as constraints\\n\\np1 þ p2 ¼ a;\\n\\np1 þ p3 ¼ c;\\n\\np1 þ p2 þ p3 þ p4 ¼ 1:\\n\\nWith this we calculate using entropy maximization (see Exercise 7.8 on page 173)\\n\\np1 ¼ ac;\\n\\np2 ¼ að1 (cid:5) cÞ;\\n\\np3 ¼ cð1 (cid:5) aÞ;\\n\\np4 ¼ ð1 (cid:5) aÞð1 (cid:5) cÞ:\\n\\nFrom p1 = αγ it follows that P(A, B) = P(A) ⋅ P(B), which means that A and B are independent. Because there are no constraints connecting A and B, the MaxEnt principle results in the independence of these variables. The right half of Table 7.1 on page 142 makes this easier to understand. From the deﬁnition\\n\\nPðBjAÞ ¼\\n\\nPðA; BÞ PðAÞ\\n\\nit follows for the case P(A) ≠ 0, that is, when the premise is not false, because A and B are independent, that P BjAð Þ remains undeﬁned.\\n\\nÞ ¼ P Bð Þ. For the case P(A) = 0, P BjAð\\n\\n7.2.4 MaxEnt-Systems\\n\\nAs previously mentioned, due to the nonlinearity of the entropy function, MaxEnt optimization usually cannot be carried out symbolically for non-trivial problems. Thus two systems were developed for numerical entropy maximization. The ﬁrst system, SPIRIT (Symmetrical Probabilistic Intensional Reasoning in Inference Networks in Transition, www.xspirit.de), [RM96] was built at Fernuniversität Hagen. The second, PIT (Probability Induction Tool) was developed at the Munich Technical University [Sch96, ES99, SE00]. We will now brieﬂy introduce PIT.\\n\\nThe PIT system uses the sequential quadratic programming (SQP) method to ﬁnd an extremum of the entropy function under the given constraints. As input, PIT\\n\\n143\\n\\nð7:10Þ\\n\\nð7:11Þ\\n\\nð7:12Þ\\n\\n144\\n\\n7 Reasoning with Uncertainty\\n\\nexpects data containing the constraints. For example, the constraints P(A) = α and P(B|A) = β from Sect. 7.2.1 have the form\\n\\nvar A{t,f}, B{t,f};\\n\\nP([A=t]) = 0.6; P([B=t] | [A=t]) = 0.3;\\n\\nQP([B=t]); QP([B=t] | [A=t]);\\n\\nBecause PIT performs a numerical calculation, we have to input explicit proba- bility values. The second to last row contains the query QP([B = t]). This means that P(B) is the desired value. At www.pit-systems.de under “Examples” we now put this input into a blank input page (“Blank Page”) and start PIT. As a result we get\\n\\nNr.\\n\\n1 2\\n\\nTruth value\\n\\nUNSPECIFIED UNSPECIFIED\\n\\nProbability 3.800e–01 3.000e–01\\n\\nQuery QP([B = t]); QP([A = t]-|> [B = t]);\\n\\nand from there read off P(B) = 0.38 and P BjAð\\n\\nÞ ¼ 0:3.\\n\\n7.2.5 The Tweety Example\\n\\nWe now show, using the Tweety example from Sect. 4.3, that probabilistic rea- soning and in particular MaxEnt are non-monotonic and model everyday reasoning very well. We model the relevant rules with probabilities as follows:\\n\\nP(birdjpenguin) = 1 P( fliesjbird) P( fliesjpenguin) = 0\\n\\n2 [0.95, 1]\\n\\n‘‘penguins are birds’’ ‘‘(almost all) birds can fly’’ ‘‘penguins cannot fly’’\\n\\nThe ﬁrst and third rules represent ﬁrm predictions, which can also be easily formulated in logic. In the second, however, we express our knowledge that almost all birds can ﬂy by means of a probability interval. With the PIT input data\\n\\nvar penguin{yes,no}, bird{yes,no}, flies{yes,no};\\n\\nP([bird=yes] | [penguin=yes]) = 1; P([flies=yes] | [bird=yes]) IN [0.95,1]; P([flies=yes] | [penguin=yes]) = 0;\\n\\nQP([flies=yes]| [penguin=yes]);\\n\\n7.2 The Principle of Maximum Entropy\\n\\nwe get back the correct answer\\n\\nNr.\\n\\nTruthvalue\\n\\nProbability\\n\\nQuery\\n\\n1\\n\\nUNSPECIFIED\\n\\n0.000e+00\\n\\nQP([penguin = yes]-|> [ﬂies = yes]);\\n\\nwith the proposition that penguins cannot ﬂy.7 The explanation for this is very simple. With P fliesjbird Þ 2 [0.95, 1] it is possible that there are non-ﬂying birds. If this rule were replaced by P(ﬂies|bird) = 1, then PIT would not be able to do anything and would output an error message about inconsistent constraints.\\n\\nð\\n\\nIn this example we can easily see that probability intervals are often very helpful for modeling our ignorance about exact probability values. We could have made an even fuzzier formulation of the second rule in the spirit of “normally birds ﬂy” with P(ﬂies|bird) 2 (0.5, 1]. The use of the half-open interval excludes the value 0.5.\\n\\nIt has already been shown in [Pea88] that this example can be solved using probabilistic logic, even without MaxEnt. In [Sch96] it is shown for a number of demanding benchmarks for non-monotonic reasoning that these can be solved elegantly with MaxEnt. In the following section we introduce a successful practical application of MaxEnt in the form of a medical expert system.\\n\\n7.3 LEXMED, an Expert System for Diagnosing Appendicitis\\n\\nThe medical expert system LEXMED, which uses the MaxEnt method, was developed at the Ravensburg-Weingarten University of Applied Sciences by Manfred Schramm, Walter Rampf, and the author, in cooperation with the Weingarten 14-Nothelfer Hospital [SE00, Le999].8 The acronym LEXMED stands for learning expert system for medical diagnosis.\\n\\n7.3.1 Appendicitis Diagnosis with Formal Methods\\n\\nThe most common serious cause of acute abdominal pain [dD91] is appendicitis— an inﬂammation of the appendix, a blind-ended tube connected to the cecum. Even today, diagnosis can be difﬁcult in many cases [OFY+95]. For example, up to about 20% of the removed appendices are without pathological ﬁndings, which means that the operations were unnecessary. Likewise, there are regularly cases in which an inﬂamed appendix is not recognized as such.\\n\\nSince as early as the beginning of the 1970s, there have been attempts to automate reducing the rate of false\\n\\nthe diagnosis of appendicitis, with the goal of\\n\\n7QP([penguin=yes]-|> [ﬂies=yes]) is an alternative form of the PIT syntax for QP([ﬂies=yes] | [penguin=yes]). 8The project was ﬁnanced by the German state of Baden-Württemberg, the health insurance company AOK Baden-Württemberg, the Ravensburg-Weingarten University of Applied Sciences, and the 14 Nothelfer Hospital in Weingarten.\\n\\n145\\n\\n146\\n\\n7 Reasoning with Uncertainty\\n\\ndiagnoses [dDLS+72, OPB94, OFY+95]. Especially noteworthy is the expert sys- tem for diagnosis of acute abdominal pain, developed by de Dombal in Great Britain. It was made public in 1972, thus distinctly earlier than the famous system MYCIN. Nearly all of the formal diagnostic processes used in medicine to date have been based on scores. Score systems are extremely easy to apply: For each value of a symptom (for example fever or lower right stomach pain) the doctor notes a certain number of points. If the sum of the points is over a certain value (threshold), a certain decision is recommended (for example operation). For n symptoms S1, … , Sn a score for appendicitis can be described formally as (cid:4)\\n\\nDiagnose ¼\\n\\nAppendicitis negative\\n\\nif w1S1 þ (cid:4) (cid:4) (cid:4) þw nSn [ H; else.\\n\\nWith scores, a linear combination of symptom values is thus compared with a threshold Θ. The weights of the symptoms are extracted from databases using statistical methods. The advantage of scores is their simplicity of application. The weighted sum of the points can be computed by hand easily and a computer is not needed for the diagnosis.\\n\\nBecause of the linearity of this method, scores are too weak to model complex relationships. Since the contribution wiSi of a symptom Si to the score is calculated independently of the other symptoms, it is clear that score systems cannot take any “context” into account. Principally, they cannot distinguish between combinations of symptoms, for example they cannot distinguish between the white blood cell count of an old patient and that of a young patient.\\n\\nFor a ﬁxed given set of symptoms, conditional probability is much more powerful than scores for making predictions because the latter cannot describe the dependencies between different symptoms. We can show that scores implicitly assume that all symptoms are independent.\\n\\nWhen using scores, yet another problem comes up. To arrive at a good diagnosis quality, we must put strict requirements on the databases used to statistically determine the weights wi. In particular they must be representative of the set of patients in the area in which the diagnosis system is used. This is often difﬁcult, if not impossible, to guarantee. In such cases, scores and other statistical methods either cannot be used, or will have a high rate of errors.\\n\\n7.3.2 Hybrid Probabilistic Knowledge Base\\n\\nComplex probabilistic relationships appear frequently in medicine. With LEXMED, these relationships can be modeled well and calculated quickly. Here the use of probabilistic propositions, with which uncertain and incomplete information can be expressed and processed in an intuitive and mathematically grounded way, is essential. The following question may serve as a typical query against the expert system: “How high is the probability of an inﬂamed appendix if the patient is a 23-year-old man with pain in the right lower abdomen and a white blood cell count\\n\\n7.3 LEXMED, an Expert System for Diagnosing Appendicitis\\n\\nTable 7.2 Symptoms used for the query in LEXMED and their values. The number of values for the each symptom is given in the column marked #\\n\\nSymptom\\n\\nValues\\n\\n#\\n\\nGender\\n\\nAge\\n\\nMale, female 2 0–5, 6–10, 11–15, 16–20, 21–25, 26–35, 36–45, 46–55, 56–65, 65– 10\\n\\nPain 1st Quad.\\n\\nYes, no\\n\\n2\\n\\nPain 2nd Quad.\\n\\nYes, no\\n\\n2\\n\\nPain 3rd Quad.\\n\\nYes, no\\n\\n2\\n\\nPain 4th Quad.\\n\\nYes, no\\n\\n2\\n\\nGuarding\\n\\nLocal, global, none\\n\\n3\\n\\nRebound tenderness\\n\\nYes, no\\n\\n2\\n\\nPain on tapping\\n\\nYes, no\\n\\n2\\n\\nRectal pain\\n\\nYes, no\\n\\n2\\n\\nBowel sounds\\n\\nWeak, normal, increased, none\\n\\n4\\n\\nAbnormal ultrasound\\n\\nYes, no\\n\\n2\\n\\nAbnormal urine sedim. Yes, no\\n\\n2\\n\\nTemperature (rectal)\\n\\nLeukocytes\\n\\n–37.3, 37.4–37.6, 37.7–38.0, 38.1–38.4, 38.5–38.9, 39.0– 0–6k, 6k–8k, 8k–10k, 10k–12k, 12k–15k, 15k–20k, 20k–\\n\\n6\\n\\n7\\n\\nDiagnosis\\n\\nInﬂamed, perforated, negative, other\\n\\n4\\n\\nof 13,000?” Formulated as conditional probability, using the names and value ranges for the symptoms used in Table 7.2, this reads\\n\\nPðDiag4 ¼ inflamed _ Diag4 ¼ perforated j\\n\\nSex2 ¼ male ^ Age10 2 21(cid:5)25 ^ Leuko7 2 12k(cid:5)15kÞ:\\n\\nBy using probabilistic propositions, LEXMED has the ability to use information from non-representative databases because this information can be complemented appropriately from other sources. Underlying LEXMED is a database which only contains data about patients whose appendixes were surgically removed. With statistical methods, (about 400) rules are generated which compile the knowledge contained in the database into an abstracted form [ES99]. Because there are no patients in this database who were suspected of having appendicitis but had neg- ative diagnoses (that is, not requiring treatment),9 there is no knowledge about negative patients in the database. Thus knowledge from other sources must be added in. In LEXMED therefore the rules gathered from the database are comple- mented by (about 100) rules from medical experts and the medical literature. This results in a hybrid probabilistic database, which contains knowledge extracted from data as well as knowledge explicitly formulated by experts. Because both types of rules are formulated as conditional probabilities (see for example (7.14) on page 152), they can be easily combined, as shown in Fig. 7.5 on page 148 and with more details in Fig. 7.7 on page 150.\\n\\n9These negative diagnoses are denoted “non-speciﬁc abdominal pain” (NSAP).\\n\\n147\\n\\nShort\\n\\nSex2\\n\\nAge10\\n\\nP1Q2\\n\\nP2Q2\\n\\nP3Q2\\n\\nP4Q2\\n\\nGua3\\n\\nReb2\\n\\nTapp2\\n\\nRecP2\\n\\nBowS4\\n\\nSono2\\n\\nUrin2\\n\\nTRec6\\n\\nLeuko7\\n\\nDiag4\\n\\n148\\n\\n7 Reasoning with Uncertainty\\n\\nFig. 7.5 Probabilistic rules are generated from data and expert knowledge, which are integrated in a rule base (knowledge base) and ﬁnally made complete using the MaxEnt method\\n\\nLEXMED calculates the probabilities of various diagnoses using the probability distribution of all relevant variables (see Table 7.2 on page 147). Because all 14 symptoms used in LEXMED and the diagnoses are modeled as discrete variables (even continuous variables like the leukocyte value are divided into ranges), the size of the distribution (that is, the size of the event space) can be determined using Table 7.2 on page 147 as the product of the number of values of all symptoms, or\\n\\n210 (cid:4) 10 (cid:4) 3 (cid:4) 4 (cid:4) 6 (cid:4) 7 (cid:4) 4 ¼ 20643840\\n\\nelements. Due to the normalization condition from Theorem 7.1 on page 129, it thus has 20643839 independent values. Every rule set with fewer than 20643839 proba- bility values potentially does not completely describe this event space. To be able to answer any arbitrary query, the expert system needs a complete distribution. The construction of such an extensive, consistent distribution using statistical methods is very difﬁcult.10 To require from a human expert all 20643839 values for the distri- bution (instead of the aforementioned 100 rules) would essentially be impossible.\\n\\nHere the MaxEnt method comes into play. The generalization of about 500 rules to a complete probability model is done in LEXMED by maximizing the entropy with the 500 rules as constraints. An efﬁcient encoding of the resulting MaxEnt distri- bution leads to response times for the diagnosis of around one second.\\n\\n10The task of generating a function from a set of data is known as machine learning. We will cover this thoroughly in Chap. 8.\\n\\n7.3 LEXMED, an Expert System for Diagnosing Appendicitis\\n\\nFig. 7.6 The LEXMED input mask for input of the examined symptoms and below it the output of the resulting diagnosis probabilities\\n\\n7.3.3 Application of LEXMED\\n\\nThe usage of LEXMED is simple and self-explanatory. The doctor visits the LEXMED home page at www.lexmed.de.11 For an automatic diagnosis, the doctor inputs the results of his examination into the input form in Fig. 7.6. After one or two seconds he receives the probabilities for the four different diagnoses as well as a suggestion for a treatment (Sect. 7.3.5). If certain examination results are missing as input (for example the sonogram results), then the doctor chooses the entry not examined. Naturally the certainty of the diagnosis is higher when more symptom values are input.\\n\\n11A version with limited functionality is accessible without a password.\\n\\n149\\n\\n150\\n\\n7 Reasoning with Uncertainty\\n\\nFig. 7.7 Rules are generated from the database as well as from expert knowledge. From these, MaxEnt creates a complete probability distribution. For a user query, the probability of every possible diagnosis is calculated. Using the cost matrix (see Sect. 7.3.5) a decision is then suggested\\n\\nEach registered user has access to a private patient database, in which input data can be archived. Thus data and diagnoses from earlier patients can be easily compared with those of a new patient. An overview of the processes in LEXMED is given in Fig. 7.7.\\n\\n7.3.4 Function of LEXMED\\n\\nKnowledge is formalized using probabilistic propositions. For example, proposition\\n\\nthe\\n\\nPðLeuko7 [ 20000jDiag4 ¼ inflamedÞ ¼ 0:09\\n\\ngives a frequency of 9% for a leukocyte value of more than 20,000 in case of an inﬂamed appendix.12\\n\\nLearning of Rules by Statistical Induction The raw data in LEXMED’s database contain 54 different (anonymized) values for 14,646 patients. As previously mentioned, only patients whose appendixes were surgically removed are included in this database. Of the 54 attributes used in the\\n\\n12Instead of individual numerical values, intervals can also be used here (for example [0.06, 0.12]).\\n\\n7.3 LEXMED, an Expert System for Diagnosing Appendicitis\\n\\nFig. 7.8 Dependency graph computed from the database\\n\\ndatabase, after a statistical analysis the 14 symptoms shown in Table 7.2 on page 147 were used. Now the rules are created from this database in two steps. The ﬁrst step determines the dependency structure of the symptoms. The second step ﬁlls this structure with the respective probability rules.13\\n\\nDetermining the Dependency Graph The graph in Fig. 7.8 contains for each variable (the symptom and the diagnosis) a node and directed edges which connect various nodes. The thickness of the edges between the variables represents a measure of the statistical dependency or correlation of the variables. The correlation of two independent variables is equal to zero. The pair correlation for each of the 14 symptoms with Diag4 was computed and listed in the graph. Furthermore, all triple correlations between the diagnosis and two symptoms were calculated. Of these, only the strongest values have been drawn as additional edges between the two participating symptoms.\\n\\n13For a systematic introduction to machine learning we refer the reader to Chap. 8.\\n\\n151\\n\\n152\\n\\n7 Reasoning with Uncertainty\\n\\n| [Diag4=negativ] * [Age10=16-20]) = [0.132,0.156]; 1 P([Leuco7=0-6k] | [Diag4=negativ] * [Age10=16-20]) = [0.257,0.281]; 2 P([Leuco7=6-8k] 3 P([Leuco7=8-10k] | [Diag4=negativ] * [Age10=16-20]) = [0.250,0.274]; 4 P([Leuco7=10-12k] | [Diag4=negativ] * [Age10=16-20]) = [0.159,0.183]; 5 P([Leuco7=12-15k] | [Diag4=negativ] * [Age10=16-20]) = [0.087,0.112]; 6 P([Leuco7=15-20k] | [Diag4=negativ] * [Age10=16-20]) = [0.032,0.056]; | [Diag4=negativ] * [Age10=16-20]) = [0.000,0.023]; 7 P([Leuco7=20k-] | [Diag4=negativ] * [Age10=21-25]) = [0.132,0.172]; 8 P([Leuco7=0-6k] | [Diag4=negativ] * [Age10=21-25]) = [0.227,0.266]; 9 P([Leuco7=6-8k] | [Diag4=negativ] * [Age10=21-25]) = [0.211,0.250]; 10 P([Leuco7=8-10k] 11 P([Leuco7=10-12k] | [Diag4=negativ] * [Age10=21-25]) = [0.166,0.205]; 12 P([Leuco7=12-15k] | [Diag4=negativ] * [Age10=21-25]) = [0.081,0.120]; 13 P([Leuco7=15-20k] | [Diag4=negativ] * [Age10=21-25]) = [0.041,0.081]; | [Diag4=negativ] * [Age10=21-25]) = [0.004,0.043]; 14 P([Leuco7=20k-]\\n\\nFig. 7.9 Some of the LEXMED rules with probability intervals. “*” stands for “∧” here\\n\\nEstimating the Rule Probabilities The structure of the dependency graph describes the structure of the learned rules.14 The rules here have different complexities: there are rules which only describe the distribution of the possible diagnoses (a priori rules, for example (7.13)), rules which describe the dependency between the diagnosis and a symptom (rules with simple conditions, for example (7.14)), and ﬁnally rules which describe the dependency between the diagnosis and two symptoms, as given in Fig. 7.9 in PIT syntax.\\n\\nPðDiag4 ¼ inflamedÞ ¼ 0:40; PðSono2 ¼ yesjDiag4 ¼ inflamedÞ ¼ 0:43; PðP4Q2 ¼ yesjDiag4 ¼ inflamed ^ P2Q2 ¼ yesÞ ¼ 0:61:\\n\\nð7:13Þ ð7:14Þ ð7:15Þ\\n\\nTo keep the context dependency of the saved knowledge as small as possible, all rules contain the diagnosis in their conditions and not as conclusions. This is quite similar to the construction of many medical books with formulations of the kind “With appendicitis we usually see …”. As previously shown in Example 7.6 on page 133, however, this does not present a problem because, using the Bayesian formula, LEXMED automatically puts these rules into the right form.\\n\\nThe numerical values for these rules are estimated by counting their frequency in the database. For example, the value in (7.14) is given by counting and calculating\\n\\njDiag4 ¼ inflamed ^ Sono2 ¼ yesj jDiag4 ¼ inflamedj\\n\\n¼ 0:43:\\n\\n14The difference between this and a Bayesian network is, for example, that the rules are equipped with probability intervals and that only after applying the principle of maximum entropy is a unique probability model produced.\\n\\n7.3 LEXMED, an Expert System for Diagnosing Appendicitis\\n\\nExpert Rules Because the appendicitis database only contains patients who have undergone the operation, rules for non-speciﬁc abdominal pain (NSAP) receive their values from propositions of medical experts. The experiences in LEXMED conﬁrm that the probabilistic rules are easy to read and can be directly translated into natural lan- guage. Statements by medical experts about frequency relationships of speciﬁc symptoms and the diagnosis, whether from the literature or as the result of an interview, can therefore be incorporated into the rule base with little expense. To model the uncertainty of expert knowledge, the use of probability intervals has proven effective. The expert knowledge was primarily acquired from the partici- pating surgeons, Dr. Rampf and Dr. Hontschik, and their publications [Hon94].\\n\\nOnce the expert rules have been created, the rule base is ﬁnished. Then the complete probability model is calculated with the method of maximum entropy by the PIT-system.\\n\\nDiagnosis Queries Using its efﬁciently stored probability model, LEXMED calculates the probabilities for the four possible diagnoses within a few seconds. For example, we assume the following output:\\n\\nDiagnosis Probability\\n\\nResults of the PIT diagnosis Appendix inﬂamed\\n\\n0.24\\n\\nAppendix perforated\\n\\n0.16\\n\\nNegative\\n\\n0.57\\n\\nOther\\n\\n0.03\\n\\nA decision must be made based on these four probability values to pursue one of the four treatments: operation, emergency operation, stationary observation, or ambulant observation.15 While the probability for a negative diagnosis in this case outweighs the others, sending the patient home as healthy is not a good decision. We can clearly see that, even when the probabilities of the diagnoses have been calculated, the diagnosis is not yet ﬁnished.\\n\\nRather, the task is now to derive an optimal decision from these probabilities.\\n\\nTo this end, the user can have LEXMED calculate a recommended decision.\\n\\n7.3.5 Risk Management Using the Cost Matrix\\n\\nHow can the computed probabilities now be translated optimally into decisions? A naive algorithm would assign a decision to each diagnosis and ultimately select the decision that corresponds to the highest probability. Assume that the computed probabilities are 0.40 for the diagnosis appendicitis (inﬂamed or perforated), 0.55 for the diagnosis negative, and 0.05 for the diagnosis other. A naive algorithm would now choose the (too risky) decision “no operation” because it corresponds to the diagnosis with the higher probability. A better method consists of comparing\\n\\n15Ambulant observation means that the patient is released to stay at home.\\n\\n153\\n\\n154\\n\\n7 Reasoning with Uncertainty\\n\\nTable 7.3 The cost matrix of LEXMED together with a patient’s computed diagnosis probabilities\\n\\nTherapy\\n\\nProbability of various diagnoses Inﬂamed\\n\\nPerforated\\n\\nNegative\\n\\nOther\\n\\n0.25\\n\\n0.15\\n\\n0.55\\n\\n0.05\\n\\nOperation Emergency operation Ambulant observ. Other Stationary observ.\\n\\n0 500 12000 3000 3500\\n\\n500 0 150000 5000 7000\\n\\n5800 6300 0 1300 400\\n\\n6000 6500 16500 0 600\\n\\n3565 3915 26325 2215 2175\\n\\nthe costs of the possible errors that can occur for each decision. The error is quantiﬁed in the form of “(hypothetical) additional cost of the current decision compared to the optimum”. The given values contain the costs to the hospital, to the insurance company, the patient (for example risk of post-operative complications), and to other parties (for example absence from work), taking into account long term consequences. These costs are given in Table 7.3.\\n\\nThe entries are ﬁnally averaged for each decision, that is, summed while taking into account their frequencies. These are listed in the last column in Table 7.3. Finally, the decision with the smallest average cost of error is suggested. In Table 7.3 the matrix is given together with the probability vector calculated for a patient (in this case: (0.25, 0.15, 0.55, 0.05)). The last column of the table contains the result of the calculations of the average expected costs of the errors. The value of Operation in the ﬁrst row is thus calculated as 0.25 ⋅ 0 + 0.15 ⋅ 500 + 0.55 ⋅ 5800 + 0.05 ⋅ 6000 = 3565, a weighted average of all costs. The optimal decisions are entered with (additional) costs of 0. The system decides on the treatment with the minimal average cost. It thus is an example of a cost-oriented agent.\\n\\nCost Matrix in the Binary Case To better understand the cost matrix and risk management we will now restrict the LEXMED system to the two-value decision between the diagnosis appendicitis with probability\\n\\np1 ¼ PðappendicitisÞ ¼ PðDiag4 ¼ inflamedÞ þ PðDiag4 ¼ perforatedÞ\\n\\nand NSAP with the probability\\n\\np2 ¼ PðNSAPÞ ¼ PðDiag4 ¼ negativeÞ þ PðDiag4 ¼ otherÞ\\n\\nThe only available treatments are operation and ambulant observation.\\n\\nThe cost matrix is thus a 2 × 2 matrix of the form\\n\\n(cid:2)\\n\\n0 k1\\n\\nk2 0\\n\\n(cid:3) :\\n\\n7.3 LEXMED, an Expert System for Diagnosing Appendicitis\\n\\nThe two zeroes in the diagonal stand for the correct decision operation in the case of appendicitis and ambulant observation for NSAP. The parameter k2 stands for the expected costs which occur when a patient without an inﬂamed appendix is operated on. This error is called a false positive. On the other hand, the decision ambulant observation in the case of appendicitis is a false negative. The probability vector (p1, p2)T is now multiplied by this matrix and we obtain the vector\\n\\nðk2 p2; k1 p1ÞT\\n\\nwith the average additional cost for the two possible treatments. Because the decision only takes into account the relationship of the two components, the vector can be multiplied by any scalar factor. We choose 1/k1 and obtain ((k2/k1) p2, p1). Thus only the relationship k = k2/k1 is relevant here. The same result is obtained by the simpler cost matrix\\n\\n(cid:2)\\n\\n(cid:3)\\n\\n0 1\\n\\nk 0\\n\\n;\\n\\nwhich only contains the variable k. This parameter is very important because it determines risk management. By changing k we can ﬁt the “working point” of the diagnosis system. For k → ∞ the system is put in an extremely risky setting because no patient will ever be operated on, with the consequence that it gives no false positive classiﬁcations, but many False negatives. In the case of k = 0 the conditions are in exact reverse and all patients are operated upon.\\n\\n7.3.6 Performance\\n\\nLEXMED is intended for use in a medical practice or ambulance. Prerequisites for the use of LEXMED are acute abdominal pain for several hours (but less than ﬁve days). Furthermore, LEXMED is (currently) specialized for appendicitis, which means that for other illnesses the system contains very little information.\\n\\nIn the scope of a prospective study, a representative database with 185 cases was created in the 14 Nothelfer Hospital. It contains the hospital’s patients who came to the clinic after several hours of acute abdominal pain and suspected appendicitis. From these patients, the symptoms and the diagnosis (veriﬁed from a tissue sample in the case of an operation) is noted.\\n\\nIf the patients were released to go home (without operation) after a stay of several hours or 1–2 days with little or no complaint, it was afterwards inquired by telephone whether the patient remained free of symptoms or whether a positive diagnosis was found in subsequent treatment.\\n\\nTo simplify the representation and make for a better comparison to similar studies, LEXMED was restricted to the two-value distinction between appendicitis and NSAP, as described in Sect. 7.3.5. Now k is varied between zero and inﬁnity\\n\\n155\\n\\n156\\n\\n7 Reasoning with Uncertainty\\n\\nand for every value of k the sensitivity and speciﬁcity are measured against the test data. Sensitivity measures\\n\\nPðclassified positivejpositiveÞ ¼\\n\\njpositive and classified positivej jpositivej\\n\\n;\\n\\nð7:16Þ\\n\\nthat is, the relative portion of positive cases which are correctly identiﬁed. It indi- cates how sensitive the diagnostic system is. Speciﬁcity, on the other hand, measures\\n\\nPðclassified negativejnegativeÞ ¼\\n\\njnegative and classified negativej jnegativej\\n\\n;\\n\\nð7:17Þ\\n\\nthat is, the relative portion of negative cases which are correctly identiﬁed.\\n\\nWe give the results of the sensitivity and speciﬁcity in Fig. 7.10 for 0 ≤ k < ∞. This curve is denoted the ROC curve, or receiver operating characteristic. Before we come to the analysis of the quality of LEXMED, a few words about the meaning of the ROC curve. The line bisecting the diagram diagonally is drawn in for orien- tation. All points on this line correspond to a random decision. For example, the point (0.2, 0.2) corresponds to a speciﬁcity of 0.8 with a sensitivity of 0.2. We can arrive at this quite easily by classifying a new case, without looking at it, with probabilities 0.2 for positive and 0.8 for negative. Every knowledge-based diag- nosis system must therefore generate a ROC which clearly lies above the diagonal.\\n\\nSpecificity\\n\\nFig. 7.10 ROC curve from LEXMED compared with the Ohmann score and two additional models\\n\\n7.3 LEXMED, an Expert System for Diagnosing Appendicitis\\n\\nThe extreme values in the ROC curve are also interesting. At point (0, 0) all three curves intersect. The corresponding diagnosis system would classify all cases as negative. The other extreme value (1, 1) corresponds to a system which would decide to do the operation for every patient and thus has a sensitivity of 1. We could call the ROC curve the characteristic curve for two-value diagnostic systems. The ideal diagnostic system would have a characteristic curve which consists only of the point (0, 1), and thus has 100% speciﬁcity and 100% sensitivity.\\n\\nNow let us analyse the ROC curve. At a sensitivity of 88%, LEXMED attains a speciﬁcity of 87% (k = 0.6). For comparison, the Ohmann score, an established, well-known score for appendicitis is given [OMYL96, ZSR+99]. Because LEXMED is above or to the left of the Ohmann score almost everywhere, its average quality of diagnosis is clearly better. This is not surprising because scores are simply too weak to model interesting propositions. In Sect. 8.7 and in Exercise 8.17 on page 242 we will show that scores are equivalent to the special case of naive Bayes, that is, to the assumption that all symptoms are pairwise independent when the diagnosis is known. When comparing LEXMED with scores it should, however, be mentioned that a statistically representative database was used for the Ohmann score, but a non-representative database enhanced with expert knowledge was used for LEXMED. To get an idea of the quality of the LEXMED data in comparison to the Ohmann data, a linear score was calculated using the least squares method (see Sect. 9.4.1), which is also drawn for comparison. Furthermore, a neural network was trained on the LEXMED data with the RProp algorithm (see Sect. 9.5). The strength of combining data and expert knowledge is displayed clearly in the difference between the LEXMED curve and the curves of the score system and the RProp algorithm.\\n\\n7.3.7 Application Areas and Experiences\\n\\nLEXMED should not replace the judgment of an experienced surgeon. However, because a specialist is not always available in a clinical setting, a LEXMED query offers a substantive second opinion. Especially interesting and worthwhile is the application of the system in a clinical ambulance and for general practitioners.\\n\\nThe learning capability of LEXMED, which makes it possible to take into account further symptoms, further patient data, and further rules, also presents new possi- bilities in the clinic. For especially rare groups which are difﬁcult to diagnose, for example children under six years of age, LEXMED can use data from pediatricians or other special databases, to support even experienced surgeons.\\n\\nAside from direct use in diagnosis, LEXMED also supports quality assurance measures. For example, insurance companies can compare the quality of diagnosis of hospitals with that of expert systems. By further developing the cost matrix created in LEXMED (with the consent of doctors, insurance, and patients), the quality of physician diagnoses, computer diagnoses, and other medical institutions will become easier to compare.\\n\\nLEXMED has pointed to a new way of constructing automatic diagnostic systems. Using the language of probability theory and the MaxEnt algorithm, inductively,\\n\\n157\\n\\n158\\n\\n7 Reasoning with Uncertainty\\n\\nstatistically derived knowledge is combined with knowledge from experts and from the literature. The approach based on probabilistic models is theoretically elegant, generally applicable, and has given very good results in a small study.\\n\\nLEXMED has been in practical use in the 14 Nothelfer Hospital in Weingarten since 1999 and has performed there very well. It is also available at www.lexmed.de, without warranty, of course. Its quality of diagnosis is comparable with that of an experienced surgeon and is thus better than that of an average general practitioner, or that of an inexperienced doctor in the clinic.\\n\\nDespite this success it has become evident that it is very difﬁcult to market such a system commercially in the German medical system. One reason for this is that there is no free market to promote better quality (here better diagnoses) through its selection mechanisms. Furthermore, in medicine the time for broad use of intelli- gent techniques is not yet at hand—even in 2010. One cause of this could be conservative teachings in this regard in German medical school faculties.\\n\\nA further issue is the desire of many patients for personal advice and care from the doctor, together with the fear that, with the introduction of expert systems, the patient will only communicate with the machine. This fear, however, is wholly unfounded. Even in the long term, medical expert systems cannot replace the doctor. They can, however, just like laser surgery and magnetic resonance imaging, be used advantageously for all participants. Since the ﬁrst medical computer diagnostic system of de Dombal in 1972, almost 40 years have passed. It remains to be seen whether medicine will wait another 40 years until computer diagnostics becomes an established medical tool.\\n\\n7.4 Reasoning with Bayesian Networks\\n\\nOne problem with reasoning using probability in practice was already pointed out in Sect. 7.1. If d variables X1, … , Xd with n values each are used, then the associated probability distribution has nd total values. This means that in the worst case the memory use and computation time for determining the speciﬁed probabilities grows exponentially with the number of variables.\\n\\nIn practice the applications are usually very structured and the distribution contains many redundancies. This means that it can be heavily reduced with the appropriate methods. The use of Bayesian networks has proved its power here and is one of the AI techniques which have been successfully used in practice. Bayesian networks utilize knowledge about the independence of variables to simplify the model.\\n\\n7.4.1 Independent Variables\\n\\nIn the simplest case, all variables are pairwise independent and it is the case that\\n\\nPðX1; . . .; XdÞ ¼ PðX1Þ (cid:4) PðX2Þ (cid:4) (cid:4) (cid:4) (cid:4) (cid:4) PðXdÞ:\\n\\n7.4 Reasoning with Bayesian Networks\\n\\nAll entries in the distribution can thus be calculated from the d values P(X1), … , P(Xd). Interesting applications, however, can usually not be modeled because conditional probabilities become trivial.16 Because of\\n\\nPðAjBÞ ¼\\n\\nPðA; BÞ PðBÞ\\n\\n¼\\n\\nPðAÞPðBÞ PðBÞ\\n\\n¼ PðAÞ\\n\\nall conditional probabilities are reduced to the a priori probabilities. The situation becomes more interesting when only a portion of the variables are independent or independent under certain conditions. For reasoning in AI, the dependencies between variables happen to be important and must be utilized.\\n\\nWe would like to outline reasoning with Bayesian networks through a simple and very illustrative example by J. Pearl [Pea88], which became well known through [RN10] and is now basic AI knowledge.\\n\\nExample 7.10 (Alarm-Example) Bob, who is single, has had an alarm system installed in his house to protect against burglars. Bob cannot hear the alarm when he is working at the ofﬁce. Therefore he has asked his two neighbors, John in the house next door to the left, and Mary in the house to the right, to call him at his ofﬁce if they hear his alarm. After a few years Bob knows how reliable John and Mary are and models their calling behavior using conditional probability as follows.17\\n\\nPðJjAlÞ ¼ 0:90 PðJj:AlÞ ¼ 0:05\\n\\nPðMjAlÞ ¼ 0:70; PðMj:AlÞ ¼ 0:01:\\n\\nBecause Mary is hard of hearing, she fails to hear the alarm more often than John. However, John sometimes mixes up the alarm at Bob’s house with the alarms at other houses. The alarm is triggered by a burglary, but can also be triggered by a (weak) earthquake, which can lead to a false alarm because Bob only wants to know about burglaries while at his ofﬁce. These relationships are modeled by\\n\\nPðAljBur; EarÞ ¼ 0:95; PðAljBur; :EarÞ ¼ 0:94; PðAlj:Bur; EarÞ ¼ 0:29; PðAlj:Bur; :EarÞ ¼ 0:001;\\n\\nas well as the a priori probabilities P(Bur) = 0.001 and P(Ear) = 0.002. These two variables are independent because earthquakes do not make plans based on the habits of burglars, and conversely there is no way to predict earthquakes, so burglars do not have the opportunity to set their schedule accordingly.\\n\\n16In the naive Bayes method, the independence of all attributes is assumed, and this method has been successfully applied to text classiﬁcation (see Sect. 8.7). 17The binary variables J and M stand for the two events “John calls”, and “Mary calls”, respectively, Al for “alarm siren sounds”, Bur for “burglary” and Ear for “earthquake”.\\n\\n159\\n\\n160\\n\\n7 Reasoning with Uncertainty\\n\\nQueries are now made against this knowledge base. For example, Bob might be interested in P(Bur|J ∨ M), P(J|Bur) orP (M|Bur). That is, he wants to know how sensitively the variables J and M react to a burglary report.\\n\\n7.4.2 Graphical Representation of Knowledge as a Bayesian\\n\\nNetwork\\n\\nWe can greatly simplify practical work by graphically representing knowledge that is formulated as conditional probability. Figure 7.11 shows the Bayesian network for the alarm example. Each node in the network represents a variable and every directed edge a statement of conditional probability. The edge from Al to J for example represents the two values P(J|Al) and P(J | ¬Al), which is given in the form of a table, the so-called CPT (conditional probability table). The CPT of a node lists all the conditional probabilities of the node’s variable conditioned on all the nodes connected by incoming edges.\\n\\nWhile studying the network, we might ask ourselves why there are no other edges included besides the four that are drawn in. The two nodes Bur and Ear are not linked since the variables are independent. All other nodes have a parent node, which makes the reasoning a little more complex. We ﬁrst need the concept of conditional independence.\\n\\n7.4.3 Conditional Independence\\n\\nAnalogously to independence of random variables, we give\\n\\nDeﬁnition 7.6 Two variables A and B are called conditionally independent, given C if\\n\\nPðA; BjCÞ ¼ PðAjCÞ (cid:4) PðBjCÞ:\\n\\nFig. 7.11 Bayesian network for the alarm example with the associated CPTs\\n\\n7.4 Reasoning with Bayesian Networks\\n\\nThis equation is true for all combinations of values for all three variables (that is, for the distribution), which we see in the notation. We now look at nodes J and M in the alarm example, which have the common parent node Al. If John and Mary independently react to an alarm, then the two variables J and M are independent given Al, that is:\\n\\nPðJ; MjAlÞ ¼ PðJjAlÞ (cid:4) PðMjAlÞ:\\n\\nIf the value of Al is known, for example because an alarm was triggered, then the variables J and M are independent (under the condition Al = w). Because of the conditional independence of the two variables J and M, no edge between these two nodes is added. However, J and M are not independent (see Exercise 7.11 on page 173).\\n\\nQuite similar is the relationship between the two variables J and Bur, because John does not react to a burglary, rather the alarm. This could be, for example, because of a high wall that blocks his view on Bob’s property, but he can still hear the alarm. Thus J and Bur are independent given Al and\\n\\nPðJ; BurjAlÞ ¼ PðJjAlÞ (cid:4) PðBurjAlÞ:\\n\\nGiven an alarm, the variables J and Ear, M and Bur, as well as M and Ear are also independent. For computing with conditional independence, the following characterizations, which are equivalent to the above deﬁnition, are helpful:\\n\\nTheorem 7.5 The following equations are pairwise equivalent, which means that each individual equation describes the conditional independence for the variables A and B given C.\\n\\nPðA; BjCÞ ¼ PðAjCÞ (cid:4) PðBjCÞ;\\n\\nð7:18Þ\\n\\nPðAjB; CÞ ¼ PðAjCÞ;\\n\\nð7:19Þ\\n\\nPðBjA; CÞ ¼ PðBjCÞ:\\n\\nð7:20Þ\\n\\nProof On one hand, using conditional independence (7.18) we can conclude that\\n\\nPðA; B; CÞ ¼ PðA; BjCÞPðCÞ ¼ PðAjCÞ PðBjCÞ PðCÞ:\\n\\nOn the other hand, the product rule gives us\\n\\nPðA; B; CÞ ¼ PðAjB; CÞPðBjCÞPðCÞ:\\n\\nThus P(A|B, C) = P(A|C) is equivalent to (7.18). We obtain (7.20) analogously by □ swapping A and B in this derivation.\\n\\n161\\n\\n162\\n\\n7 Reasoning with Uncertainty\\n\\n7.4.4 Practical Application\\n\\nNow we turn again to the alarm example and show how the Bayesian network in Fig. 7.11 can be used for reasoning. Bob is interested, for example, in the sensi- tivity of his two alarm reporters John and Mary, that is, in P JjBur Þ. However, the values P BurjJ Þ are even ð more important to him. We begin with P JjBur\\n\\nð Þ, as well as P BurjJ; M\\n\\nÞ and P MjBur ð\\n\\nÞ and P BurjM ð\\n\\nð Þ and calculate\\n\\nð\\n\\nPðJjBurÞ ¼\\n\\nPðJ; BurÞ PðBurÞ\\n\\n¼\\n\\nPðJ; Bur; AlÞ þ PðJ; Bur; :AlÞ PðBurÞ\\n\\nð7:21Þ\\n\\nand\\n\\nPðJ; Bur; AlÞ ¼ PðJjBur; AlÞPðAljBurÞPðBurÞ ¼ PðJjAlÞPðAljBurÞPðBurÞ;\\n\\nð7:22Þ\\n\\nwhere for the last two equations we have used the product rule and the conditional independence of J and Bur given Al. Inserted in (7.21) we obtain\\n\\nPðJjBurÞ ¼\\n\\nPðJjAlÞPðAljBurÞPðBurÞ þ PðJj:AlÞPð:AljBurÞPðBurÞ PðBurÞ\\n\\nð7:23Þ\\n\\n¼ PðJjAlÞPðAljBurÞ þ PðJj:AlÞPð:AljBurÞ:\\n\\nHere P(Al|Bur) and P(¬Al|Bur) are missing. Therefore we calculate\\n\\nPðAljBurÞ ¼\\n\\n¼\\n\\nPðAl; Bur; EarÞ þ PðAl; Bur; :EarÞ PðBurÞ PðAljBur; EarÞPðBurÞPðEarÞ þ PðAljBur; :EarÞPðBurÞPð:EarÞ PðBurÞ\\n\\nPðAl; BurÞ PðBurÞ\\n\\n¼\\n\\n¼ PðAljBur; EarÞPðEarÞ þ PðAljBur; :EarÞPð:EarÞ ¼ 0:95 (cid:4) 0:002 þ 0:94 (cid:4) 0:998 ¼ 0:94\\n\\nas well as P(¬Al|Bur) = 0.06 and insert this into (7.23) which gives the result\\n\\nPðJjBurÞ ¼ 0:9 (cid:4) 0:94 þ 0:05 (cid:4) 0:06 ¼ 0:849:\\n\\nAnalogously we calculate P(M|Bur) = 0.659. We now know that John calls for about 85% of all break-ins and Mary for about 66% of all break-ins. The probability that both of them call is calculated, due to conditional independence, as\\n\\nPðJ; MjBurÞ ¼ PðJ; MjAlÞPðAljBurÞ þ PðJ; Mj:AlÞPð:AljBurÞ\\n\\n¼ PðJjAlÞPðMjAlÞPðAljBurÞ þ PðJj:AlÞPðMj:AlÞPð:AljBurÞ ¼ 0:9 (cid:4) 0:7 (cid:4) 0:94 þ 0:05 (cid:4) 0:01 (cid:4) 0:06 ¼ 0:5922:\\n\\n7.4 Reasoning with Bayesian Networks\\n\\nMore interesting, however, is the probability of a call from John or Mary\\n\\nPðJ _ MjBurÞ ¼ Pð:ð:J; :MÞjBurÞ ¼ 1 (cid:5) Pð:J; :MjBurÞ\\n\\n¼ 1 (cid:5) ½Pð:JjAlÞPð:MjAlÞPðAljBurÞ þ Pð:Jj:AlÞPð:Mj:AlÞPð:AljBurÞ(cid:7) ¼ 1 (cid:5) ½0:1 (cid:4) 0:3 (cid:4) 0:94 þ 0:95 (cid:4) 0:99 (cid:4) 0:06(cid:7) ¼ 1 (cid:5) 0:085 ¼ 0:915:\\n\\nBob thus receives a notiﬁcation for about 92% of all burglaries. Now to calculate P BurjJ\\n\\nÞ, we apply Bayes’ theorem, which gives us\\n\\nð\\n\\nPðBurjJÞ ¼\\n\\nPðJjBurÞPðBurÞ PðJÞ\\n\\n¼\\n\\n0:849 (cid:4) 0:001 0:052\\n\\n¼ 0:016:\\n\\nEvidently only about 1.6% of all calls from John are actually due to a break-in. Because the probability of false alarms is ﬁve times smaller for Mary, with Þ ¼ 0:056, we have signiﬁcantly higher conﬁdence given a call from P BurjM Mary. Bob should only be seriously concerned about his home if both of them call, Þ ¼ 0:284 (see Exercise 7.11 on page 173). because P BurjJ; M ð\\n\\nð\\n\\nIn (7.23) on page 162 we showed with\\n\\nPðJjBurÞ ¼ PðJjAlÞPðAljBurÞ þ PðJj:AlÞPð:AljBurÞ\\n\\nhow we can “slide in” a new variable. This relationship holds in general for two variables A and B given the introduction of an additional variable C and is called conditioning:\\n\\nX\\n\\nPðAjBÞ ¼\\n\\nPðAjB; C ¼ cÞPðC ¼ cjBÞ:\\n\\nc\\n\\nIf furthermore A and B are conditionally independent given C, this formula sim- pliﬁes to\\n\\nPðAjBÞ ¼\\n\\nX\\n\\nPðAjC ¼ cÞPðC ¼ cjBÞ:\\n\\nc\\n\\n7.4.5 Software for Bayesian Networks\\n\\nWe will give a brief introduction to two tools using the alarm example. We are already familiar with the system PIT. We input the values from the CPTs in PIT syntax into the online input window at www.pit-systems.de. After the input shown in Fig. 7.12 on page 164 we receive the answer:\\n\\nP([Einbruch=t] | [John=t] AND [Mary=t]) = 0.2841.\\n\\n163\\n\\n164\\n\\n7 Reasoning with Uncertainty\\n\\n1 var Alarm{t,f}, Burglary{t,f}, Earthquake{t,f}, John{t,f}, Mary{t,f}; 2 3 P([Earthquake=t]) = 0.002; 4 P([Burglary=t]) = 0.001; 5 P([Alarm=t] | [Burglary=t] AND [Earthquake=t]) = 0.95; 6 P([Alarm=t] | [Burglary=t] AND [Earthquake=f]) = 0.94; 7 P([Alarm=t] | [Burglary=f] AND [Earthquake=t]) = 0.29; 8 P([Alarm=t] | [Burglary=f] AND [Earthquake=f]) = 0.001; 9 P([John=t] | [Alarm=t]) = 0.90; 10 P([John=t] | [Alarm=f]) = 0.05; 11 P([Mary=t] | [Alarm=t]) = 0.70; 12 P([Mary=t] | [Alarm=f]) = 0.01; 13 14 QP([Burglary=t] | [John=t] AND [Mary=t]);\\n\\nFig. 7.12 PIT input for the alarm example\\n\\nFig. 7.13 The user interface of JavaBayes: left the graphical editor and right the console where the answers are given as output\\n\\nWhile PIT is not a classical Bayesian network tool, it can take arbitrary conditional probabilities and queries as input and calculate correct results. It can be shown [Sch96], that on input of CPTs or equivalent rules, the MaxEnt principle implies the same conditional independences and thus also the same answers as a Bayesian network. Bayesian networks are thus a special case of MaxEnt.\\n\\nNext we will look at JavaBayes [Coz98], a classic system also freely available on the Internet with the graphical interface shown in Fig. 7.13. With the graphical network editor, nodes and edges can be manipulated and the values in the CPTs edited. Furthermore, the values of variables can be assigned with “Observe” and the values of other variables called up with “Query”. The answers to queries then appear in the console window.\\n\\nThe professional, commercial system Hugin is signiﬁcantly more powerful and convenient. For example, Hugin can use continuous variables in addition to discrete variables. It can also learn Bayesian networks, that is, generate the network fully automatically from statistical data (see Sect. 8.5).\\n\\n7.4 Reasoning with Bayesian Networks\\n\\n7.4.6 Development of Bayesian Networks\\n\\nA compact Bayesian network is very clear and signiﬁcantly more informative for the reader than a full probability distribution. Furthermore, it requires much less memory. For the variables v1; . . .; vn with jv1j; . . .; jvnj different values each, the distribution has a total of\\n\\nYn\\n\\njvij (cid:5) 1\\n\\ni¼1\\n\\nindependent entries. In the alarm example the variables are all binary. Thus for all variables jvij = 2, and the distribution has 25 − 1 = 31 independent entries. To calculate the number of independent entries for the Bayesian network, the total number of all entries of all CPTs must be determined. For a node vi with ki parent nodes ei1; . . .; eiki, the associated CPT has\\n\\nðjvij (cid:5) 1Þ\\n\\nYki\\n\\njeijj\\n\\nj¼1\\n\\nentries. Then all CPTs in the network together have\\n\\nXn\\n\\nYki\\n\\nðjvij (cid:5) 1Þ\\n\\njeijj\\n\\ni¼1\\n\\nj¼1\\n\\nentries.18 For the alarm example the result is then\\n\\n2 þ 2 þ 4 þ 1 þ 1 ¼ 10\\n\\nindependent entries which uniquely describe the network. The comparison in memory complexity between the full distribution and the Bayesian network becomes clearer when we assume that all n variables have the same number b of values and each node has k parent nodes. Then (7.24) can be simpliﬁed and all CPTs together have n(b − 1)bk entries. The full distribution contains bn − 1 entries. A signiﬁcant gain is only made then if the average number of parent nodes is much smaller than the number of variables. This means that the nodes are only locally connected. Because of the local connection, the network becomes modularized, which—as in software engineering—leads to a reduction in complexity. In the alarm example the alarm node separates the nodes Bur and Ear from the nodes J and M. We can also see this clearly in the LEXMED example.\\n\\n18For the case of a node without ancestors the product in this sum is empty. For this we substitute the value 1 because the CPT for nodes without ancestors contains, with its a priori probability, exactly one value.\\n\\n165\\n\\nð7:24Þ\\n\\n166\\n\\n7 Reasoning with Uncertainty\\n\\nFig. 7.14 Bayesian network for the LEXMED application\\n\\nLEXMED as a Bayesian Network The LEXMED system described in Sect. 7.3 can also be modeled as a Bayesian network. By making the outer, thinly-drawn lines directed (giving them arrows), the independence graph in Fig. 7.8 on page 151 can be interpreted as a Bayesian network. The resulting network is shown in Fig. 7.14.\\n\\nIn Sect. 7.3.2 the size of the distribution for LEXMED was calculated as the value 20 643 839. The Bayesian network on the other hand can be fully described with only 521 values. This value can be determined by entering the variables from Fig. 7.14 into (7.24) on page 165. In the order (Leuko, TRek, Gua, Age, Reb, Sono, Tapp, BowS, Sex, P4Q, P1Q, P2Q, RecP, Urin, P3Q, Diag4) we calculate Xn\\n\\nYki\\n\\nðjvij (cid:5) 1Þ\\n\\njeijj ¼ 6 (cid:4) 6 (cid:4) 4 þ 5 (cid:4) 4 þ 2 (cid:4) 4 þ 9 (cid:4) 7 (cid:4) 4 þ 1 (cid:4) 3 (cid:4) 4 þ 1 (cid:4) 4 þ 1 (cid:4) 2 (cid:4) 4\\n\\ni¼1\\n\\nj¼1\\n\\nþ 3 (cid:4) 3 (cid:4) 4 þ 1 (cid:4) 4 þ 1 (cid:4) 4 (cid:4) 2 þ 1 (cid:4) 4 (cid:4) 2 þ 1 (cid:4) 4 þ 1 (cid:4) 4 þ 1 (cid:4) 4 þ 1 (cid:4) 4 þ 1 ¼ 521:\\n\\n7.4 Reasoning with Bayesian Networks\\n\\nThis example demonstrates that it is practically impossible to build a full distri- bution for real applications. A Bayesian network with 22 edges and 521 probability values on the other hand is still manageable.\\n\\nCausality and Network Structure Construction of a Bayesian network usually proceeds in two stages. 1. Design of the network structure: This step is usually performed manually and\\n\\nwill be described in the following.\\n\\n2. Entering the probabilities in the CPTs: Manually entering the values in the case of many variables is very tedious. If (as for example with LEXMED) a database is available, this step can be automated through estimating the CPT entries by counting frequencies.\\n\\nWe will now describe the construction of the network in the alarm example (see Fig. 7.15). At the beginning we know the two causes Burglary and Earthquake and the two symptoms John and Mary. However, because John and Mary do not directly react to a burglar or earthquake, rather only to the alarm, it is appropriate to add this as an additional variable which is not observable by Bob. The process of adding edges starts with the causes, that is, with the variables that have no parent nodes. First we choose Burglary and next Earthquake. Now we must check whether Earthquake is independent of Burglary. This is given, and thus no edge is added from Burglary to Earthquake. Because Alarm is directly dependent on Burglary and Earthquake, these variables are chosen next and an edge is added from both Bur- glary and Earthquake to Alarm. Then we choose John. Because Alarm and John are not independent, an edge is added from alarm to John. The same is true for Mary. Now we must check whether John is conditionally independent of Burglary given Alarm. If this is not the case, then another edge must be inserted from Burglary to John. We must also check whether edges are needed from Earthquake to John and from Burglary or Earthquake to Mary. Because of conditional independence, these four edges are not necessary. Edges between John and Mary are also unnecessary because John and Mary are conditionally independent given Alarm.\\n\\nThe structure of the Bayesian network heavily depends on the chosen variable ordering. If the order of variables is chosen to reﬂect the causal relationship beginning with the causes and proceeding to the diagnosis variables, then the result will be a simple network. Otherwise the network may contain signiﬁcantly more edges. Such non-causal networks are often very difﬁcult to understand and have a higher complexity for reasoning. The reader may refer to Exercise 7.11 on page 173 for better understanding.\\n\\nFig. 7.15 Stepwise construction of the alarm network considering causality\\n\\n167\\n\\n168\\n\\n7 Reasoning with Uncertainty\\n\\nFig. 7.16 There is no edge between A and B if they are independent (left) or conditionally independent (middle, right)\\n\\n7.4.7 Semantics of Bayesian Networks\\n\\nAs we have seen in the previous section, no edge is added to a Bayesian network between two variables A and B when A and B are independent or conditionally independent given a third variable C. This situation is represented in Fig. 7.16.\\n\\nWe now require the Bayesian network to have no cycles and we assume that the variables are numbered such that no variable has a lower number than any variable that precedes it. This is always possible when the network has no cycles.19 Then, when using all conditional independencies, we have\\n\\nPðXnjX1; . . .; Xn(cid:5)1Þ ¼ PðXnjParentsðXnÞÞ:\\n\\nThis equation thus is a proposition that an arbitrary variable Xi in a Bayesian network is conditionally independent of its ancestors, given its parents. The somewhat more general proposition depicted in Fig. 7.17 on page 169 can be stated compactly as\\n\\nTheorem 7.6 A node in a Bayesian network is conditionally independent from all non-successor nodes, given its parents.\\n\\nNow we are able to greatly simplify the chain rule ((7.1) on page 132):\\n\\nYn\\n\\nYn\\n\\nPðX1; . . .; XnÞ ¼\\n\\nPðXijX1. . .; Xi(cid:5)1Þ ¼\\n\\nPðXijParentsðXiÞÞ:\\n\\nð7:25Þ\\n\\ni¼1\\n\\ni¼1\\n\\nUsing this rule we could, for example, write (7.22) on page 162 directly\\n\\nPðJ; Bur; AlÞ ¼ PðJjAlÞ PðAljBurÞ PðBurÞ:\\n\\nWe now know the most\\n\\nimportant concepts and foundations of Bayesian\\n\\nnetworks. Let us summarize them [Jen01]:\\n\\n19If for example three nodes X1, X2, X3 form a cycle, then there are the edges (X1, X2), (X2, X3) and (X3, X1) where X1 has X3 as a successor.\\n\\n7.4 Reasoning with Bayesian Networks\\n\\nFig. 7.17 Example of conditional independence in a Bayesian network. If the parent nodes E1 and E2 are given, then all non-successor nodes B1, … , B8 are independent of A\\n\\nDeﬁnition 7.7 A Bayesian network is deﬁned by: (cid:129) A set of variables and a set of directed edges between these variables. (cid:129) Each variable has ﬁnitely many possible values. (cid:129) The variables together with the edges form a directed acyclic graph (DAG). A DAG is a graph without cycles, that is, without paths of the form (A, … , A).\\n\\n(cid:129) For every variable A the CPT (that is, the table of conditional probabilities\\n\\nP(A|Parents(A))) is given.\\n\\nTwo variables A and B are called conditionally independent given C if PðA; BjCÞ ¼ PðAjCÞ (cid:4) PðBjCÞ or, equivalently, Besides the foundational rules of computation for probabilities, the following rules are also true: Bayes’ Theorem: PðAjBÞ ¼ PðBjAÞ(cid:4)PðAÞ Marginalization: PðBÞ ¼ PðA; BÞ þ Pð:A; BÞ ¼ PðBjAÞ (cid:4) PðAÞ þ PðBj:AÞ (cid:4)\\n\\nif PðAjB; CÞ ¼ PðAjCÞ.\\n\\nPðBÞ\\n\\nPð:AÞ\\n\\nConditioning: P(A|B) = ∑c P (A|B, C = c) P (C = c|B) A variable in a Bayesian network is conditionally independent of all non-successor variables given its parent variables. If X1, … , Xn−1 are no successors of Xn, we have P(Xn|X1, … , Xn−1) = P(Xn|Parents(Xn)). This con- dition must be honored during the construction of a network. During construction of a Bayesian network the variables should be ordered according to causality. First the causes, then the hidden variables, and the diagnosis variables last. Chain rule: PðX1; . . .; XnÞ ¼\\n\\nQ\\n\\nn i¼1\\n\\nPðXijParentsðXiÞÞ\\n\\n169\\n\\n170\\n\\n7 Reasoning with Uncertainty\\n\\nIn [Pea88] and [Jen01] the term d-separation is introduced for Bayesian net- works, from which a theorem similar to Theorem 7.6 on page 168 follows. We will refrain from introducing this term and thereby reach a somewhat simpler, though not a theoretically as clean representation.\\n\\n7.5 Summary\\n\\nIn a way that reﬂects the prolonged, sustained trend toward probabilistic systems, we have introduced probabilistic logic for reasoning with uncertain knowledge. After introducing the language—propositional logic augmented with probabilities or probability intervals—we chose the natural, if unusual approach via the method of maximum entropy as an entry point and showed how we can model non-monotonic reasoning with this method. Bayesian networks were then intro- duced as a special case of the MaxEnt method.\\n\\nWhy are Bayesian networks a special case of MaxEnt? When building a Bayesian network, assumptions about independence are made which are unneces- sary for the MaxEnt method. Furthermore, when building a Bayesian network, all CPTs must be completely ﬁlled in so that a complete probability distribution can be constructed. Otherwise reasoning is restricted or impossible. With MaxEnt, on the other hand, the developer can formulate all the knowledge he has at his disposal in the form of probabilities. MaxEnt then completes the model and generates the distribution. Even if (for example when interviewing an expert) only vague propositions are available, this can be suitably modeled. A proposition such as “I am pretty sure that A is true.” can for example be modeled using P(A) 2 [0.6, 1] as a probability interval. When building a Bayesian network, a concrete value must be given for the probability, if necessary by guessing. This means, however, that the expert or the developer put ad hoc information into the system. One further advantage of MaxEnt is the possibility of formulating (almost) arbitrary proposi- tions. For Bayesian networks the CPTs must be ﬁlled.\\n\\nThe freedom that the developer has when modeling with MaxEnt can be a disadvantage (especially for a beginner) because, to the Bayesian approach, it is not necessarily clear what knowledge should be modeled. When modeling with Bayesian networks the approach is quite clear: according to causal dependencies, from the causes to the effects, one edge after the other is entered into the network by testing conditional independence.20 At the end all CPTs are ﬁlled with values.\\n\\nin contrast\\n\\nHowever, the following interesting combinations of the two methods are pos- sible: we begin by building a network according to the Bayesian methodology, enter all the edges accordingly and then ﬁll the CPTs with values. Should certain values for the CPTs be unavailable, then they can be replaced with intervals or by other probabilistic logic formulas. Naturally such a network—or better: a rule\\n\\n20This is also not always quite so simple.\\n\\n7.5 Summary\\n\\nset—no longer has the special semantics of a Bayesian network. It must then be processed and completed by a MaxEnt system.\\n\\nThe ability to use MaxEnt with arbitrary rule sets has a downside, though. Similarly to the situation in logic, such rule sets can be inconsistent. For example, the two rules P(A) = 0.7 and P(A) = 0.8 are inconsistent. While the MaxEnt system PIT for example can recognize the inconsistency, if cannot give a hint about how to remove the problem.\\n\\nWe introduced the medical expert system LEXMED, a classic application for rea- soning with uncertain knowledge, and showed how it can be modeled and imple- mented using MaxEnt and Bayesian networks, and how these tools can replace the well-established, but too weak linear scoring systems used in medicine.21\\n\\nIn the LEXMED example we showed that it is possible to build an expert system for reasoning under uncertainty that is capable of discovering (learning) knowledge from the data in a database. We will give more insight into the methods of learning of Bayesian networks in Chap. 8, after the necessary foundations for machine learning have been laid.\\n\\nToday Bayesian reasoning is a large, independent ﬁeld, which we can only brieﬂy describe here. We have completely left out the handling of continuous variables. For the case of normally distributed random variables there are proce- dures and systems. For arbitrary distributions, however, the computational com- plexity is a big problem. In addition to the directed networks that are heavily based on causality, there are also undirected networks. Connected with this is a discussion about the meaning and usefulness of causality in Bayesian networks. The interested reader is directed to excellent textbooks such as [Pea88, Jen01, Whi96, DHS01], as well as the proceedings of the annual conference of the Association for Uncertainty in Artiﬁcial Intelligence (AUAI) (www.auai.org).\\n\\n7.6 Exercises\\n\\nExercise 7.1 Prove the proposition from Theorem 7.1 on page 129.\\n\\nExercise 7.2 The gardening hobbyist Max wants to statistically analyze his yearly harvest of peas. For every pea pod he picks he measures its length xi in centimeters and its weight yi in grams. He divides the peas into two classes, the good and the bad (empty pods). The measured data (xi, yi) are\\n\\ngood peas:\\n\\nx 1 2 2 3 3 4 4 5 6 y 2 3 4 4 5 5 6 6 6\\n\\nbad peas:\\n\\nx 4 6 6 7 y 2 2 3 3\\n\\n21In Sect. 8.7 and in Exercise 8.17 on page 242 we will show that the scores are equivalent to the special case naive Bayes, to the assumption that all symptoms are conditionally that independent given the diagnosis.\\n\\nis,\\n\\n171\\n\\n172\\n\\n7 Reasoning with Uncertainty\\n\\n(a) From the data, compute the probabilities P(y > 3|Class = good) and P(y ≤ 3| Class = good). Then use Bayes’ formula to determine P(Class = good|y > 3) and P(Class = good|y ≤ 3).\\n\\n(b) Which of the probabilities computed in subproblem (a) contradicts the state-\\n\\nment “All good peas are heavier than 3 grams”?\\n\\nExercise 7.3 You are supposed to predict the afternoon weather using a few simple weather values from the morning of this day. The classical probability calculation for this requires a complete model, which is given in the following table.\\n\\nSky Clear Clear Clear Clear Cloudy Cloudy Cloudy\\n\\nBar Rising Rising Falling Dry Falling Rising Rising Falling Dry\\n\\nPrec Dry Raining\\n\\nRaining Dry Raining\\n\\nP (Sky, Bar, Prec) 0.40 0.07 0.08 0.10 0 .09 0.11 0.03\\n\\nSky:\\n\\nBar:\\n\\nPrec:\\n\\nThe sky is clear or cloudy in the morning Barometer rising or falling in the morning Raining or dry in the afternoon\\n\\n(a) How many events are in the distribution for these three variables? (b) Compute P (Prec = dry|Sky = clear, Bar = rising). (c) Compute P (Prec = rain|Sky = cloudy). (d) What would you do if the last row were missing from the table?\\n\\n❄ Exercise 7.4 In a television quiz show, the contestant must choose between three closed doors. Behind one door the prize awaits: a car. Behind both of the other doors are goats. The contestant chooses a door, e.g. number one. The host, who knows where the car is, opens another door, e.g. number three, and a goat appears. The contestant is now given the opportunity to choose between the two remaining doors (one and two). What is the better choice from his point of view? To stay with the door he originally chose or to switch to the other closed door?\\n\\nExercise 7.5 Using the Lagrange multiplier method, show that, without explicit constraints, the uniform distribution p1 = p2 = … = pn = 1/n represents maximum entropy. Do not forget the implicitly ever-present constraint p1 + p2 + (cid:4)(cid:4)(cid:4) + pn = 1. How can we show this same result using indifference?\\n\\nExercise 7.6 Use the PIT system (http://www.pit-systems.de) or SPIRIT (http:// www.xspirit.de) to calculate the MaxEnt solution for P (B) under the constraint P (A) = α and P (B|A) = β. Which disadvantage of PIT, compared with calculation by hand, do you notice here?\\n\\nExercise 7.7 Given the constraints P (A) = α and P (A ∨ B) = β, manually calculate P (B) using the MaxEnt method. Use PIT to check your solution.\\n\\n7.6 Exercises\\n\\n❄ Exercise 7.8 Given the constraints from (7.10), (7.11), (7.12): p1 þ p2 = α, p1 þ p3 = γ, p1 þ p2 þ p3 þ p4 = 1. Show that p1 = αγ, p2 = α(1 − γ), p3 = γ(1 − α), p4 = (1 − α)(1 − γ) represents the entropy maximum under these constraints.\\n\\n❄ Exercise 7.9 A probabilistic algorithm calculates the likelihood p that an inbound email is spam. To classify the emails in classes delete and read, a cost matrix is then applied to the result. (a) Give a cost matrix (2 × 2 matrix) for the spam ﬁlter. Assume here that it costs the user 10 cents to delete an email, while the loss of an email costs 10 dollars (compare this to Example 1.1 on page 17 and Exercise 1.7 on page 21).\\n\\n(b) Show that, for the case of a 2 × 2 matrix, the application of the cost matrix is equivalent to the application of a threshold on the spam probability and determine the threshold.\\n\\nExercise 7.10 Given a Bayesian network with the three binary variables A, B, C and P(A) = 0.2, P(B) = 0.9, as well as the CPT shown below:\\n\\n(a) Compute P (A|B). (b) Compute P (C|A).\\n\\nA B f t t t t f f f\\n\\nP (C)\\n\\n0.1 0.2 0.9 0.4\\n\\nExercise 7.11 For the alarm example (Example 7.10 on page 159), calculate the following conditional probabilities: (a) Calculate the a priori probabilities P (Al), P (J), P (M). (b) Calculate P MjBur\\n\\nð\\n\\nÞ using the product rule, marginalization, the chain rule,\\n\\nand conditional independence.\\n\\n(c) Use Bayes’ formula to calculate P BurjM (d) Compute P AljJ; M (e) Show that the variables J and M are not independent. (f) Check all of your results with JavaBayes and with PIT (see [Ert11] for\\n\\nð Þ and P BurjJ; M ð\\n\\nÞ\\n\\nð\\n\\nÞ.\\n\\ndemo programs).\\n\\n(g) Design a Bayesian network for the alarm example, but with the altered variable ordering M, Al, Ear, Bur, J. According to the semantics of Bayesian networks, only the necessary edges must be drawn in. (Hint: the variable order given here does NOT represent causality. Thus it will be difﬁcult to intuitively determine conditional independences.) In the original Bayesian network of the alarm example, the earthquake nodes is removed. Which CPTs does this change? (Why these in particular?)\\n\\n(h)\\n\\n(i) Calculate the CPT of the alarm node in the new network.\\n\\n173\\n\\n174\\n\\n7 Reasoning with Uncertainty\\n\\nExercise 7.12 A diagnostic system is to be made for a dynamo-powered bicycle light using a Bayesian network. The variables in the following table are given.\\n\\nAbbr. Li Str Flw R V B K\\n\\nMeaning\\n\\nLight is on Street condition Dynamo ﬂywheel worn out Dynamo sliding Dynamo shows voltage Light bulb o.k. Cable o.k.\\n\\nValues\\n\\nt / f dry, wet, snow_covered t / f t / f t / f t / f t / f\\n\\nThe following variables are pairwise independent: Str, Flw, B, K. Furthermore: (R, B), (R, K), (V, B), (V, K) are independent and the following equation holds:\\n\\nPðLijV; RÞ ¼ PðLijVÞ PðVjR; StrÞ ¼ PðVjRÞ PðVjR; FlwÞ ¼ PðVjRÞ\\n\\n(a) Draw all of the edges into the graph (taking\\n\\ncausality into account).\\n\\n(b) Enter all missing CPTs into the graph (table of conditional probabilities). Freely insert plausible values for the probabilities.\\n\\n(c) Show that the network does not contain an edge\\n\\n(Str, Li).\\n\\nV t t t t f f f f\\n\\nB K P (Li) t t f f t t f f\\n\\nt f t f t f t f\\n\\n0.99 0.01 0.01 0.001 0.3 0.005 0.005 0\\n\\n(d) Compute P (V|Str = snow_covered).\\n\\nMachine Learning and Data Mining\\n\\nIf we deﬁne AI as is done in Elaine Rich’s book [Ric83]:\\n\\nArtiﬁcial Intelligence is the study of how to make computers do things at which, at the moment, people are better.\\n\\nand if we consider that the computer’s learning ability is especially inferior to that of humans, then it follows that research into learning mechanisms, and the devel- opment of machine learning algorithms is one of the most important branches of AI.\\n\\nThere is also demand for machine learning from the viewpoint of the software developer who programs, for example, the behavior of an autonomous robot. The structure of the intelligent behavior can become so complex that it is very difﬁcult or even impossible to program close to optimally, even with modern high-level languages such as PROLOG and Python.1 Machine learning algorithms are even used today to program robots in a way similar to how humans learn (see Chap. 10 or [BCDS08, RGH+06]), often in a hybrid mixture of programmed and learned behavior.\\n\\nThe task of this chapter is to describe the most important machine learning algorithms and their applications. The topic will be introduced in this section, followed by important fundamental learning algorithms in the next sections. Theory and terminology will be built up in parallel to this. The chapter will close with a summary and overview of the various algorithms and their applications. We will restrict ourselves in this chapter to supervised and unsupervised learning algorithms. As an important class of learning algorithms, neural networks will be dealt with in Chap. 9. Due to its special place and important role for autonomous robots, reinforcement learning will also have its own dedicated chapter (Chap. 10).\\n\\n1Python is a modern scripting language with very readable syntax, powerful data types, and extensive standard libraries, which can be used to this end.\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_8\\n\\n175\\n\\n8\\n\\n176\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.1 Supervised learning …\\n\\nWhat Is Learning? Learning vocabulary of a foreign language, or technical terms, or even memorizing a poem can be difﬁcult for many people. For computers, however, these tasks are quite simple because they are little more than saving text in a ﬁle. Thus memorization is uninteresting for AI. In contrast, the acquisition of mathematical skills is usually not done by memorization. For addition of natural numbers this is not at all possible, because for each of the terms in the sum x + y there are inﬁnitely many values. For each combination of the two values x and y, the triple (x, y, x + y) would have to be stored, which is impossible. For decimal numbers, this is downright impossible. This poses the question: how do we learn mathematics? The answer reads: The teacher explains the process and the students practice it on examples until they no longer make mistakes on new examples. After 50 examples the student (hopefully) understands addition. That is, after only 50 examples he can apply what was learned to inﬁnitely many new examples, which to that point were not seen. This process is known as generalization. We begin with a simple example.\\n\\nExample 8.1 A fruit farmer wants to automatically divide harvested apples into the merchandise classes A and B. The sorting device is equipped with sensors to measure two features, size and color, and then decide which of the two classes the apple belongs to. This is a typical classiﬁcation task. Systems which are capable of dividing feature vectors into a ﬁnite number of classes are called classiﬁers.\\n\\nTo conﬁgure the machine, apples are hand-picked by a specialist, that is, they are classiﬁed. Then the two measurements are entered together with their class\\n\\n8 Machine Learning and Data Mining\\n\\nTable 8.1 Training data for the apple sorting agent\\n\\nSize [cm] Color Merchandise class\\n\\n8 0.1 B\\n\\n8 0.3 A\\n\\n6 0.9 A\\n\\n3 0.8 B\\n\\nFig. 8.2 BayWa company apple sorting equipment in Kressbronn and some apples classiﬁed into merchandise classes A and B in feature space (Photo: BayWa)\\n\\nFig. 8.3 The curve drawn in into the diagram divides the classes and can then be applied to arbitrary new apples\\n\\nlabel in a table (Table 8.1. The size is given in the form of diameter in centimeters and the color by a numeric value between 0 (for green) and 1 (for red). A visu- alization of the data is listed as points in a scatterplot diagram in the right of Fig. 8.2.\\n\\nThe task in machine learning consists of generating a function from the col- lected, classiﬁed data which calculates the class value (A or B) for a new apple from the two features size and color. In Fig. 8.3 such a function is shown by the dividing line drawn through the diagram. All apples with a feature vector to the bottom left of the line are put into class B, and all others into class A.\\n\\nIn this example it is still very simple to ﬁnd such a dividing line for the two classes. It is clearly a more difﬁcult, and above all much less visualizable task, when the objects to be classiﬁed are described by not just two, but many features. In practice 30 or more features are usually used. For n features, the task consists of ﬁnding an n − 1 dimensional hyperplane within the n-dimensional feature space\\n\\n177\\n\\n… … …\\n\\n178\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.4 Functional structure of a learning agent for apple sorting (left) and in general (right)\\n\\nwhich divides the classes as well as possible. A “good” division means that the percentage of falsely classiﬁed objects is as small as possible.\\n\\nA classiﬁer maps a feature vector to a class value. Here it has a ﬁxed, usually small, number of alternatives. The desired mapping is also called target function. If the target function does not map onto a ﬁnite domain, then it is not a classiﬁcation, but rather an approximation problem. Determining the market value of a stock from given features is such an approximation problem. In the following sections we will introduce several learning agents for both types of mappings.\\n\\nThe Learning Agent We can formally describe a learning agent as a function which maps a feature vector to a discrete class value or in general to a real number. This function is not programmed, rather it comes into existence or changes itself during the learning phase, inﬂuenced by the training data. In Fig. 8.4 such an agent is presented in the apple sorting example. During learning, the agent is fed with the already classiﬁed data from Table 8.1 on page 177. Thereafter the agent constitutes as good a mapping as possible from the feature vector to the function value (e.g. merchandise class).\\n\\nWe can now attempt to approach a deﬁnition of the term “machine learning”.\\n\\nTom Mitchell [Mit97] gives this deﬁnition:\\n\\nMachine Learning is the study of computer algorithms that improve automati-\\n\\ncally through experience.\\n\\nDrawing on this, we give\\n\\nDeﬁnition 8.1 An agent is a learning agent if it improves its performance (measured by a suitable criterion) on new, unknown data over time (after it has seen many training examples).\\n\\nIt is important to test the generalization capability of the learning algorithm on unknown data, the test data. Otherwise every system that just saved the training data would appear to perform optimally just by calling up the saved data. A learning agent is characterized by the following terms: Task: the task of the learning algorithm is to learn a mapping. This could for example be the mapping from an apple’s size and color to its merchandise class,\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.5 Data Mining\\n\\nbut also the mapping from a patient’s 15 symptoms to the decision of whether or not to remove his appendix.\\n\\nVariable agent: (more precisely a class of agents): here we have to decide which learning algorithm will be worked with. If this has been chosen, thus the class of all learnable functions is determined.\\n\\nTraining data: (experience): the training data (sample) contain the knowledge which the learning algorithm is supposed to extract and learn. With the choice of training data one must ensure that it is a representative sample for the task to be learned.\\n\\nTest data: important for evaluating whether the trained agent can generalize well\\n\\nfrom the training data to new data.\\n\\nPerformance measure: for the apple sorting device, the number of correctly classiﬁed apples. We need it to test the quality of an agent. Knowing the per- formance measure is usually much easier than knowing the agent’s function. For example, it is easy to measure the performance (time) of a 10,000 meter runner. However, this does not at all imply that the referee who measures the time can run as fast. The referee only knows how to measure the performance, but not the “function” of the agent whose performance he is measuring.\\n\\nWhat Is Data Mining? The task of a learning machine to extract knowledge from training data. Often the developer or the user wants the learning machine to make the extracted knowledge readable for humans as well. It is still better if the\\n\\n179\\n\\n180\\n\\n8 Machine Learning and Data Mining\\n\\ndeveloper can even alter the knowledge. The process of induction of decision trees in Sect. 8.4 is an example of this type of method.\\n\\nSimilar challenges come from electronic business and knowledge management. A classic problem presents itself here: from the actions of visitors to his web portal, the owner of an Internet business would like to create a relationship between the characteristics of a customer and the class of products which are interesting to that customer. Then a seller will be able to place customer-speciﬁc advertisements. This is demonstrated in a telling way at www.amazon.com, where the customer is rec- ommended products which are similar to those seen in the previous visit. In many areas of advertisement and marketing, as well as in customer relationship man- agement (CRM), data mining techniques are coming into use. Whenever large amounts of data are available, one can attempt to use these data for the analysis of customer preferences in order to show customer-speciﬁc advertisements. The emerging ﬁeld of preference learning is dedicated to this purpose.\\n\\nThe process of acquiring knowledge from data, as well as its representation and application, is called data mining. The methods used are usually taken from statistics or machine learning and should be applicable to very large amounts of data at reasonable cost.\\n\\nIn the context of acquiring information, for example on the Internet or in an intranet, text mining plays an increasingly important role. Typical tasks include ﬁnding similar text in a search engine or the classiﬁcation of texts, which for example is applied in spam ﬁlters for email. In Sect. 8.7.1 we will introduce the widespread naive Bayes algorithm for the classiﬁcation of text. A relatively new challenge for data mining is the extraction of structural, static, and dynamic information from graph structures such as social networks, trafﬁc networks, or Internet trafﬁc.\\n\\nBecause the two described tasks of machine learning and data mining are for- mally very similar, the basic methods used in both areas are for the most part identical. Therefore in the description of the learning algorithms, no distinction will be made between machine learning and data mining.\\n\\nBecause of the huge commercial impact of data mining techniques, there are now many sophisticated optimizations and a whole line of powerful data mining systems, which offer a large palette of convenient tools for the extraction of knowledge from data. Such a system is introduced in Sect. 8.10.\\n\\n8.1 Data Analysis\\n\\nStatistics provides a number of ways to describe data with simple parameters. From these we choose a few which are especially important for the analysis of training data and test these on a subset of the LEXMED data from Sect. 7.3. In this example dataset, the symptoms x1,…, x15 of N = 473 patients, concisely described in\\n\\n8.1 Data Analysis\\n\\n181\\n\\nTable 8.2 Description of variables x1, … , x16. A slightly different formalization was used in Table 7.2 on page 147\\n\\nVar. num.\\n\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\\n\\nDescription\\n\\nAge Sex (1 = male, 2 = female) Pain quadrant 1 Pain quadrant 2 Pain quadrant 3 Pain quadrant 4 Local muscular guarding Generalized muscular guarding Rebound tenderness Pain on tapping Pain during rectal examination Axial temperature Rectal temperature Leukocytes Diabetes mellitus Appendicitis\\n\\nValues\\n\\nContinuous 1, 2 0, 1 0, 1 0, 1 0, 1 0, 1 0, 1 0, 1 0, 1 0, 1 Continuous Continuous Continuous 0, 1 0, 1\\n\\nTable 8.2 on page 181, as well as the class label—that is, the diagnosis (appen- dicitis positive/negative)—are listed. Patient number one, for example, is described by the vector\\n\\nx1 ¼ ð26; 1; 0; 0; 1; 0; 1; 0; 1; 1; 0; 37:9; 38:8; 23100; 0; 1Þ\\n\\nand patient number two by\\n\\nx2 ¼ ð17; 2; 0; 0; 1; 0; 1; 0; 1; 1; 0; 36:9; 37:4; 8100; 0; 0Þ\\n\\nPatient number two has the leukocyte value x2\\n\\nFor each variable xi, its average (cid:1)xi is deﬁned as\\n\\n14 ¼ 8100.\\n\\n(cid:1)xi :¼\\n\\n1 N\\n\\nXN\\n\\np¼1\\n\\nx p i\\n\\nand the standard deviation si as a measure of its average deviation from the average value as\\n\\nsi :¼\\n\\nv u u t\\n\\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\\n\\n1 N (cid:2) 1\\n\\nXN\\n\\np¼1\\n\\nðxp\\n\\ni (cid:2) (cid:1)xiÞ2\\n\\n:\\n\\nThe question of whether two variables xi and xj are statistically dependent (correlated) is important for the analysis of multidimensional data. For example, the covariance\\n\\n182\\n\\n8 Machine Learning and Data Mining\\n\\nTable 8.3 Correlation matrix for the 16 appendicitis variables measured in 473 cases\\n\\n1.\\n\\n−0.009\\n\\n0.14\\n\\n0.037\\n\\n−0.096\\n\\n0.12\\n\\n0.018\\n\\n0.051\\n\\n−0.034 −0.041\\n\\n0.034\\n\\n0.037\\n\\n0.05\\n\\n−0.037\\n\\n0.37\\n\\n0.012\\n\\n−0.009\\n\\n1.\\n\\n−0.0074 −0.019\\n\\n−0.06\\n\\n0.063 −0.17\\n\\n0.0084 −0.17\\n\\n−0.14\\n\\n−0.13\\n\\n−0.017 −0.034 −0.14\\n\\n0.045\\n\\n−0.2\\n\\n0.14\\n\\n−0.0074\\n\\n1.\\n\\n0.55\\n\\n−0.091\\n\\n0.24\\n\\n0.13\\n\\n0.24\\n\\n0.045\\n\\n0.18\\n\\n0.028\\n\\n0.02\\n\\n0.045\\n\\n0.03\\n\\n0.11\\n\\n0.045\\n\\n0.037 −0.019\\n\\n0.55\\n\\n1.\\n\\n−0.24\\n\\n0.33\\n\\n0.051\\n\\n0.25\\n\\n0.074\\n\\n0.19\\n\\n0.087\\n\\n0.11\\n\\n0.12\\n\\n0.11\\n\\n0.14\\n\\n−0.0091\\n\\n−0.096 −0.06\\n\\n−0.091\\n\\n−0.24\\n\\n1.\\n\\n0.059\\n\\n0.14\\n\\n0.034\\n\\n0.14\\n\\n0.049\\n\\n0.057\\n\\n0.064\\n\\n0.058\\n\\n0.11\\n\\n0.017\\n\\n0.14\\n\\n0.12\\n\\n0.063\\n\\n0.24\\n\\n0.33\\n\\n0.059\\n\\n1.\\n\\n0.071\\n\\n0.19\\n\\n0.086\\n\\n0.15\\n\\n0.048\\n\\n0.11\\n\\n0.12\\n\\n0.063\\n\\n0.21\\n\\n0.053\\n\\n0.018 −0.17\\n\\n0.13\\n\\n0.051\\n\\n0.14\\n\\n0.071\\n\\n1.\\n\\n0.16\\n\\n0.4\\n\\n0.28\\n\\n0.2\\n\\n0.24\\n\\n0.36\\n\\n0.29\\n\\n−0.0001\\n\\n0.33\\n\\n0.051\\n\\n0.0084\\n\\n0.24\\n\\n0.25\\n\\n0.034\\n\\n0.19\\n\\n0.16\\n\\n1.\\n\\n0.17\\n\\n0.23\\n\\n0.24\\n\\n0.19\\n\\n0.24\\n\\n0.27\\n\\n0.083\\n\\n0.084\\n\\n−0.034 −0.17\\n\\n0.045\\n\\n0.074\\n\\n0.14\\n\\n0.086\\n\\n0.4\\n\\n0.17\\n\\n1.\\n\\n0.53\\n\\n0.25\\n\\n0.19\\n\\n0.27\\n\\n0.27\\n\\n0.026\\n\\n0.38\\n\\n−0.041 −0.14\\n\\n0.18\\n\\n0.19\\n\\n0.049\\n\\n0.15\\n\\n0.28\\n\\n0.23\\n\\n0.53\\n\\n1.\\n\\n0.24\\n\\n0.15\\n\\n0.19\\n\\n0.23\\n\\n0.02\\n\\n0.32\\n\\n0.034 −0.13\\n\\n0.028\\n\\n0.087\\n\\n0.057\\n\\n0.048\\n\\n0.2\\n\\n0.24\\n\\n0.25\\n\\n0.24\\n\\n1.\\n\\n0.17\\n\\n0.17\\n\\n0.22\\n\\n0.098\\n\\n0.17\\n\\n0.037 −0.017\\n\\n0.02\\n\\n0.11\\n\\n0.064\\n\\n0.11\\n\\n0.24\\n\\n0.19\\n\\n0.19\\n\\n0.15\\n\\n0.17\\n\\n1.\\n\\n0.72\\n\\n0.26\\n\\n0.035\\n\\n0.15\\n\\n0.05\\n\\n−0.034\\n\\n0.045\\n\\n0.12\\n\\n0.058\\n\\n0.12\\n\\n0.36\\n\\n0.24\\n\\n0.27\\n\\n0.19\\n\\n0.17\\n\\n0.72\\n\\n1.\\n\\n0.38\\n\\n0.044\\n\\n0.21\\n\\n−0.037 −0.14\\n\\n0.03\\n\\n0.11\\n\\n0.11\\n\\n0.063\\n\\n0.29\\n\\n0.27\\n\\n0.27\\n\\n0.23\\n\\n0.22\\n\\n0.26\\n\\n0.38\\n\\n1.\\n\\n0.051\\n\\n0.44\\n\\n0.37\\n\\n0.045\\n\\n0.11\\n\\n0.14\\n\\n0.017\\n\\n0.21\\n\\n−0.0001\\n\\n0.083\\n\\n0.026\\n\\n0.02\\n\\n0.098\\n\\n0.035\\n\\n0.044\\n\\n0.051\\n\\n1.\\n\\n−0.0055\\n\\n0.012 −0.2\\n\\n0.045\\n\\n−0.0091\\n\\n0.14\\n\\n0.053\\n\\n0.33\\n\\n0.084\\n\\n0.38\\n\\n0.32\\n\\n0.17\\n\\n0.15\\n\\n0.21\\n\\n0.44\\n\\n−0.0055\\n\\n1.\\n\\nri j ¼\\n\\n1 N (cid:2) 1\\n\\nXN\\n\\np¼1\\n\\nðx p\\n\\ni (cid:2) (cid:1)xiÞðx p\\n\\nj (cid:2) (cid:1)xjÞ\\n\\ngives information about this. In this sum, the summand returns a positive entry for the pth data vector exactly when the deviations of the ith and jth components from the average both have the same sign. If they have different signs, then the entry is negative. Therefore the covariance r12,13 of the two different fever values should be quite clearly positive.\\n\\nHowever, the covariance also depends on the absolute value of the variables, which makes comparison of the values difﬁcult. To be able to compare the degree of dependence in the case of multiple variables, we therefore deﬁne the correlation coefﬁcient\\n\\nKi j ¼\\n\\nrij si (cid:3) sj\\n\\nfor two values xi and xj, which is nothing but a normalized covariance. The matrix K of all correlation coefﬁcients contains values between −1 and 1, is symmetric, and all of its diagonal elements have the value 1. The correlation matrix for all 16 variables is given in Table 8.3.\\n\\nThis matrix becomes somewhat more readable when we represent it as a density plot. Instead of the numerical values, the matrix elements in Fig. 8.6 on page 183 are ﬁlled with gray values. In the right diagram, the absolute values are shown. Thus we can very quickly see which variables display a weak or strong dependence. We can see, for example, that the variables 7, 9, 10 and 14 show the strongest correlation with the class variable appendicitis and therefore are more important for the diagnosis than the other variable. We also see, however, that the variables 9 and 10 are strongly correlated. This could mean that one of these two values is potentially sufﬁcient for the diagnosis.\\n\\n8.2 The Perceptron, a Linear Classifier\\n\\nFig. 8.6 The correlation matrix as a frequency graph. In the left diagram, dark stands for negative and light for positive. In the right image the absolute values were listed. Here black means Kij (cid:4) 0 (uncorrelated) and white |Kij | (cid:4) 1 (strongly correlated)\\n\\nFig. 8.7 A linearly separable two dimensional data set. The equation for the dividing straight line is a1x1 þ a2x2 ¼ 1\\n\\n8.2 The Perceptron, a Linear Classifier\\n\\nIn the apple sorting classiﬁcation example, a curved dividing line is drawn between the two classes in Fig. 8.3 on page 177. A simpler case is shown in Fig. 8.7. Here the two-dimensional training examples can be separated by a straight line. We call such a set of training data linearly separable. In n dimensions a hyperplane is needed for the separation. This represents a linear subspace of dimension n − 1. Because every (n − 1)-dimensional hyperplane in ℝn can be described by an\\n\\nequation\\n\\nXn\\n\\naixi ¼ h\\n\\ni¼1\\n\\nit makes sense to deﬁne linear separability as follows.\\n\\n183\\n\\n184\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.8 The boolean function AND is linearly separable, but XOR is not (\\n\\n^¼ true,\\n\\n^¼ false)\\n\\nDeﬁnition 8.2 Two sets M1 (cid:5) ℝn and M2 (cid:5) ℝn are called linearly separable if real numbers a1, …, an, h exist with\\n\\nXn\\n\\naixi [ h\\n\\nfor all x 2 M1\\n\\nand\\n\\nXn\\n\\naixi (cid:6) h\\n\\nfor all x 2 M2:\\n\\ni¼1\\n\\ni¼1\\n\\nThe value h is denoted the threshold.\\n\\nIn Fig. 8.8 we see that the AND function is linearly separable, but the XOR function is not. For AND, for example, the line −x1 + 3/2 separates true and false interpretations of the formula x1 ^ x2. In contrast, the XOR function does not have a straight line of separation. Clearly the XOR function has a more complex structure than the AND function in this regard.\\n\\nWith the perceptron, we present a very simple learning algorithm which can\\n\\nseparate linearly separable sets.\\n\\nDeﬁnition 8.3 Let w = (w1, … , wn) 2 ℝn be a weight vector and x 2 ℝn an input vector. A perceptron represents a function P: ℝn !{0, 1} which cor- responds to the following rule:\\n\\nPðxÞ ¼\\n\\n(cid:3)\\n\\n1 0\\n\\nif w x ¼ else.\\n\\nPn\\n\\ni¼1\\n\\nwixi [ 0;\\n\\nThe perceptron [Ros58, MP69] is a very simple classiﬁcation algorithm. It is equivalent to a two-layer neural network with activation by a threshold function, shown in Fig. 8.9 on page 185. As shown in Chap. 9, each node in the network represents a neuron, and every edge a synapse. For now, however, we will only view\\n\\n8.2 The Perceptron, a Linear Classifier\\n\\nFig. 8.9 Graphical representation of a perceptron as a two-layer neural network\\n\\nthe perceptron as a learning agent, that is, as a mathematical function which maps a feature vector to a function value. Here the input variables xi are denoted features. wixi [ 0, all points x above the hyperplane Pn wixi ¼ 0 are classiﬁed as positive ðPðxÞ ¼ 1Þ, and all others as negative PðxÞ ¼ 0. The separating hyperplane goes through the origin because h ¼ 0. We will use a little trick to show that the absence of an arbitrary threshold represents no restriction of power. First, however, we want to introduce a simple learning algo- rithm for the perceptron.\\n\\nPn\\n\\nAs we can see in the formula\\n\\ni¼1\\n\\ni¼1\\n\\n8.2.1 The Learning Rule\\n\\nWith the notation Mþ and M(cid:2) for the sets of positive and negative training patterns respectively, the perceptron learning rule reads [MP69]\\n\\nPERCEPTRONLEARNING[M+, M−] w = arbitrary vector of real numbers Repeat\\n\\nFor all x ∈ M+\\n\\nIf w x ≤ 0 Then w = w + x\\n\\nFor all x ∈ M−\\n\\nIf w x > 0 Then w = w − x\\n\\nUntil all x ∈ M+ ∪ M− are correctly classiﬁed\\n\\nThe perceptron should output the value 1 for all x 2 M+. By Deﬁnition 8.3 on page 184 this is true when w x > 0. If this is not the case then x is added to the weight vector w, whereby the weight vector is changed in exactly the right direction. We see this when we apply the perceptron to the changed vector w þ x because\\n\\nðw þ xÞ (cid:3) x ¼ w x þ x2:\\n\\nIf this step is repeated often enough, then at some point the value w x will become positive, as it should be. Analogously, we see that, for negative training data, the perceptron calculates an ever smaller value\\n\\n185\\n\\n186\\n\\n8 Machine Learning and Data Mining\\n\\nðw (cid:2) xÞ (cid:3) x ¼ w x (cid:2) x2\\n\\nwhich at some point becomes negative.2\\n\\nExample 8.2 A perceptron is to be trained on the sets M+ ¼ {(0, 1.8), (2, 0.6)} and M− ¼ {(−1.2, 1.4), (0.4, −1)}. w ¼ (1, 1) was used as an initial weight vector. The training data and the line deﬁned by the weight vector w x ¼ x1 þ x2 ¼ 0 are shown in Fig. 8.10 on page 187 in the ﬁrst picture in the top row. In addition, the weight vector is drawn as a dashed line. Because w x ¼ 0, this is orthogonal to the line. In the ﬁrst iteration through the loop of the learning algorithm, the only falsely\\n\\nclassiﬁed training example is (−1.2, 1.4) because\\n\\nð(cid:2)1:2; 1:4Þ (cid:3)\\n\\n(cid:4) (cid:5) 1 1\\n\\n¼ 0:2 [ 0:\\n\\nThis results in w ¼ (1, 1) − (−1.2, 1.4) ¼ (2.2, −0.4), as drawn in the second image in the top row in Fig. 8.10 on page 187. The other images show how, after a total of ﬁve changes, the dividing line lies between the two classes. The perceptron thus classiﬁes all data correctly. We clearly see in the example that every incorrectly classiﬁed data point from M+ “pulls” the weight vector w in its direction and every incorrectly classiﬁed point from M− “pushes” the weight vector in the opposite direction.\\n\\nIt has been shown [MP69] that the perceptron always converges for linearly\\n\\nseparable data. We have\\n\\nTheorem 8.1 Let classes M+ and M− be linearly separable by a hyperplane w x ¼ 0. Then PERCEPTRONLEARNING converges for every initialization of the vector w. The perceptron P with the weight vector so calculated divides the classes M+ and M−, that is:\\n\\nPðxÞ ¼ 1 , x 2 M þ\\n\\nand\\n\\nPðxÞ ¼ 0 , x 2 M(cid:2):\\n\\nAs we can clearly see in Example 8.2, perceptrons as deﬁned above cannot divide arbitrary linearly separable sets, rather only those which are divisible by a line through the origin, or in ℝn by a hyperplane through the origin, because the constant term h is missing from the equation\\n\\nPn\\n\\nwixi ¼ 0.\\n\\ni¼1\\n\\n2Caution! This is not a proof of convergence for the perceptron learning rule. It only shows that the perceptron converges when the training dataset consists of a single example.\\n\\n8.2 The Perceptron, a Linear Classifier\\n\\nFig. 8.10 Application of the perceptron learning rule to two positive ((cid:129)) and two negative ( ) data points. The solid line shows the current dividing line w x ¼ 0. The orthogonal dashed line is the weight vector w and the second dashed line the change vector Dw ¼ x or Dw ¼ (cid:2)x to be added, which is calculated from the currently active data point surrounded in light green\\n\\nWith the following trick we can generate the constant term. We hold the last component xn of the input vector x constant and set it to the value 1. Now the weight wn ¼: −h works like a threshold because\\n\\nXn\\n\\nwixi ¼\\n\\nXn(cid:2)1\\n\\nwixi (cid:2) h [ 0 ,\\n\\nXn(cid:2)1\\n\\nwixi [ h:\\n\\ni¼1\\n\\ni¼1\\n\\ni¼1\\n\\nSuch a constant value xn ¼ 1 in the input is called a bias unit. Because the associated weight causes a constant shift of the hyperplane, the term “bias” ﬁts well. In the application of the perceptron learning algorithm, a bit with the constant value 1 is appended to the training data vector. We observe that the weight wn, or the threshold h, is learned during the learning process.\\n\\nNow it has been shown that a perceptron Ph: ℝn−1! {0, 1}\\n\\nPhðx1; . . .; xn(cid:2)1Þ ¼\\n\\n(cid:3)\\n\\n1 0\\n\\nPn(cid:2)1 i¼1\\n\\nif else\\n\\nwixi [ h;\\n\\nwith an arbitrary threshold can be simulated by a perceptron P: ℝn ! {0, 1} with the threshold 0. If we compare (8.1) with the deﬁnition of linearly separable, then we see that both statements are equivalent. In summary, we have shown that:\\n\\n187\\n\\nð8:1Þ\\n\\n188\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.11 The six patterns used for training. The whole right pattern is one of the 22 test patterns for the ﬁrst pattern with a sequence of four inverted bits\\n\\nFig. 8.12 Relative correctness of the perceptron as a function of the number of inverted bits in the test data\\n\\nTheorem 8.2 A function f: ℝn ! {0, 1} can by represented by a perceptron if and only if the two sets of positive and negative input vectors are linearly separable.\\n\\nExample 8.3 We now train a perceptron with a threshold on six simple, graphical binary patterns, represented in Fig. 8.11, with 5 (cid:7) 5 pixels each.\\n\\nThe training data can be learned by PERCEPTRONLEARNING in four iterations over all patterns. Patterns with a variable number of inverted bits introduced as noise are used to test the system’s generalization capability. The inverted bits in the test pattern are in each case in sequence one after the other. In Fig. 8.12 the percentage of correctly classiﬁed patterns is plotted as a function of the number of false bits. After about ﬁve consecutive inverted bits, the correctness falls off sharply, which is not surprising given the simplicity of the model. In the next section we will present an algorithm that performs much better in this case.\\n\\n8.2.2 Optimization and Outlook\\n\\nAs one of the simplest neural-network-based learning algorithms, the two-layer perceptron can only divide linearly separable classes. In Sect. 9.5 we will see that multi-layered networks are signiﬁcantly more powerful. Despite its simple\\n\\n8.2 The Perceptron, a Linear Classifier\\n\\nstructure, the perceptron in the form presented converges very slowly. It can be accelerated by normalization of the weight-altering vector. The formulas w ¼ w (cid:8) x are replaced by w ¼ w (cid:8) x=jxj. Thereby every data point has the same weight during learning, independent of its value.\\n\\nThe speed of convergence heavily depends on the initialization of the vector w. Ideally it would not need to be changed at all and the algorithm would converge after one iteration. We can get closer to this goal by using the heuristic initialization X\\n\\nX\\n\\nw0 ¼\\n\\nx (cid:2)\\n\\nx;\\n\\nx2M þ\\n\\nx2M(cid:2)\\n\\nwhich we will investigate more closely in Exercise 8.5 on page 239.\\n\\nIf we compare the perceptron formula with the scoring method presented in Sect. 7.3.1, we immediately see their equivalence. Furthermore, the perceptron, as the simplest neural network model, is equivalent to naive Bayes, the simplest type of Bayesian network (see Exercise 8.17 on page 242). Thus evidently several very different classiﬁcation algorithms have a common origin.\\n\\nIn Chap. 9 we will become familiar with a generalization of the perceptron in the form of the back-propagation algorithm, which can divide non linearly separable sets through the use of multiple layers, and which possesses a better learning rule.\\n\\n8.3 The Nearest Neighbor Method\\n\\nFor a perceptron, knowledge available in the training data is extracted and saved in a compressed form in the weights wi. Thereby information about the data is lost. This is exactly what is desired, however, if the system is supposed to generalize from the training data to new data. Generalization in this case is a time-intensive process with the goal of ﬁnding a compact representation of data in the form of a function which classiﬁes new data as good as possible.\\n\\nMemorization of all data by simply saving them is quite different. Here the learning is extremely simple. However, as previously mentioned, the saved knowledge is not so easily applicable to new, unknown examples. Such an approach is very unﬁt for learning how to ski, for example. A beginner can never become a good skier just by watching videos of good skiers. Evidently, when learning movements of this type are automatically carried out, something similar happens as in the case of the perceptron. After sufﬁciently long practice, the knowledge stored in training examples is transformed into an internal representation in the brain.\\n\\nHowever, there are successful examples of memorization in which generaliza- tion is also possible. During the diagnosis of a difﬁcult case, a doctor could try to remember similar cases from the past. If his memory is sound, then he might hit upon this case, look it up in his ﬁles and ﬁnally come a similar diagnosis. For this approach the doctor must have a good feeling for similarity, in order to remember the most similar case. If he has found this, then he must ask himself whether it is similar enough to justify the same diagnosis.\\n\\n189\\n\\n190\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.13 In this example with negative and positive training examples, the nearest neighbor method groups the new point marked in black into the negative class\\n\\nWhat does similarity mean in the formal context we are constructing? We rep- resent the training samples as usual in a multidimensional feature space and deﬁne: The smaller their distance in the feature space, the more two examples are similar. We now apply this deﬁnition to the simple two-dimensional example from Fig. 8.13. Here the next neighbor to the black point is a negative example. Thus it is assigned to the negative class.\\n\\nThe distance d(x, y) between two points x 2 ℝn and y 2 ℝn can for example be\\n\\nmeasured by the Euclidean distance\\n\\ns\\n\\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Xn\\n\\ndðx; yÞ ¼ jx (cid:2) yj ¼\\n\\nðxi (cid:2) yiÞ2\\n\\n:\\n\\ni¼1\\n\\nBecause there are many other distance metrics besides this one, it makes sense to think about alternatives for a concrete application. In many applications, certain features are more important than others. Therefore it is often sensible to scale the features differently by weights wi. The formula then reads\\n\\ns\\n\\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Xn\\n\\ndwðx; yÞ ¼ jx (cid:2) yj ¼\\n\\nwiðxi (cid:2) yiÞ2\\n\\n:\\n\\ni¼1\\n\\nThe following simple nearest neighbor classiﬁcation program searches the training data for the nearest neighbor t to the new example s and then classiﬁes s exactly like t.3\\n\\nNEARESTNEIGHBOR[M+, M−, s]\\n\\nt = argminx∈M+∪M− If t ∈ M+ Then Return („+”)\\n\\n{d(s, x)}\\n\\nElse Return(„–”)\\n\\n3The functionals argmin and argmax determine, similarly to min and max, the minimum or maximum of a set or function. However, rather than returning the value of the maximum or minimum, they give the position, that is, the argument in which the extremum appears.\\n\\n8.3 The Nearest Neighbor Method\\n\\nFig. 8.14 A set of points together with their Voronoi-Diagram (left) and the dividing line generated for the two classes M+ and M−\\n\\nFig. 8.15 The nearest neighbor method assigns the new point marked in black to the wrong (positive) class because the nearest neighbor is most likely classiﬁed wrong\\n\\nIn contrast to the perceptron, the nearest neighbor method does not generate a line that divides the training data points. However, an imaginary line separating the two classes certainly exists. We can generate this by ﬁrst generating the so-called Voronoi diagram. In the Voronoi diagram, each data point is surrounded by a convex polygon, which thus deﬁnes a neighborhood around it. The Voronoi dia- gram has the property that for an arbitrary new point, the nearest neighbor among the data points is the data point, which lies in the same neighborhood. If the Voronoi diagram for a set of training data is determined, then it is simple to ﬁnd the nearest neighbor for a new point which is to be classiﬁed. The class membership will then be taken from the nearest neighbor.\\n\\nIn Fig. 8.14 we see clearly that the nearest neighbor method is signiﬁcantly more powerful than the perceptron. It is capable of correctly realizing arbitrarily complex dividing lines (in general: hyperplanes). However, there is a danger here. A single erroneous point can in certain circumstances lead to very bad classiﬁcation results. Such a case occurs in Fig. 8.15 during the classiﬁcation of the black point. The nearest neighbor method may classify this wrong. If the is an outlier of the black point positive class, then it will be classiﬁed positive rather than negative as would be intended here. An erroneous ﬁtting to random errors (noise) is called overﬁtting.\\n\\nis immediately next\\n\\nto a positive point\\n\\nthat\\n\\n191\\n\\n192\\n\\n8 Machine Learning and Data Mining\\n\\nK-NEARESTNEIGHBOR(M+, M−, s)\\n\\nV = {k nearest neighbors in M+ ∪ M−} |M+ ∩ V | > |M− ∩ V | Then If ElseIf |M+ ∩ V | < |M− ∩ V | Then Else\\n\\nReturn(„+”) Return(„–”) Return(Random(„+”, „–”))\\n\\nFig. 8.16 The K-NEARESTNEIGHBOR ALGORITHM\\n\\nFig. 8.17 Relative correctness of nearest neighbor classiﬁcation as a function of the number of inverted bits. The structure of the curve with its minimum at 13 and its maximum at 19 is related to the special structure of the training data. For comparison the values of the perceptron from Example 8.3 on page 188 are shown in gray\\n\\nTo prevent false classiﬁcations due to single outliers, it is recommended to smooth out the division surface somewhat. This can be accomplished by, for example, with the K-NEARESTNEIGHBOR algorithm in Fig. 8.16, which makes a majority decision among the k nearest neighbors.\\n\\nExample 8.4 We now apply NEARESTNEIGHBOR to Example 8.3 on page 188. Because we are dealing with binary data, we use the Hamming distance as the distance metric.4 As a test example, we again use modiﬁed training examples with n consecutive inverted bits each. In Fig. 8.17 the percentage of correctly classiﬁed test examples is shown as a function of the number of inverted bits b. For up to eight inverted bits, all patterns are correctly identiﬁed. Past that point, the number of errors quickly increases. This is unsurprising because training pattern number 2 from Fig. 8.11 on page 188 from class Mþ has a hamming distance of 9 to the two training examples, numbers 4 and 5 from the other class. This means that the test pattern is very likely close to the patterns of the other class. Quite clearly we see that nearest neighbor classiﬁcation is superior to the perceptron in this application for up to eight false bits.\\n\\n4The Hamming distance between two bit vectors is the number of different bits of the two vectors.\\n\\n8.3 The Nearest Neighbor Method\\n\\nFig. 8.18 The learning agent, which is supposed to avoid light (left), represented as a classiﬁer (middle), and as an approximation (right)\\n\\n8.3.1 Two Classes, Many Classes, Approximation\\n\\nNearest neighbor classiﬁcation can also be applied to more than two classes. Just like the case of two classes, the class of the feature vector to be classiﬁed is simply set as the class of the nearest neighbor. For the k nearest neighbor method, the class is to be determined as the class with the most members among the k nearest neighbors.\\n\\nIf the number of classes is large, then it usually no longer makes sense to use classiﬁcation algorithms because the size of the necessary training data grows quickly with the number of classes. Furthermore, in certain circumstances important information is lost during classiﬁcation of many classes. This will become clear in the following example.\\n\\nExample 8.5 An autonomous robot with simple sensors similar to the Braitenberg vehicles presented in Fig. 1.1 on page 2 is supposed to learn to move away from light. This means it should learn as optimally as possible to map its sensor values onto a steering signal which controls the driving direction. The robot is equipped with two simple light sensors on its front side. From the two sensor signals (with sl for the left and sr for the right sensor), the relationship x ¼ sr / sl is calculated. To control the electric motors of the two wheels from this value x, the difference v ¼ Ur − Ul of the two voltages Ur and Ul of the left and right motors, respectively. The learning agent’s task is now to avoid a light signal. It must therefore learn a mapping f which calculates the “correct” value v ¼ f ðxÞ.5\\n\\nFor this we carry out an experiment in which, for a few measured values x, we ﬁnd as optimal a value v as we can. These values are plotted as data points in Fig. 8.18 and shall serve as training data for the learning agent. During nearest neighbor classiﬁcation each point in the feature space (that is, on the x-axis) is classiﬁed exactly like its nearest neighbor among the training data. The function for steering the motors is then a step function with large jumps (Fig. 8.18 middle). If we want ﬁner steps, then we must provide correspondingly more training data. On\\n\\n5To keep the example simple and readable, one-dimensional.\\n\\nthe feature vector x was deliberately kept\\n\\n193\\n\\n194\\n\\n8 Machine Learning and Data Mining\\n\\nthe other hand, we can obtain a continuous function if we approximate a smooth function to ﬁt the ﬁve points (Fig. 8.18 on page 193 right). Requiring the function f to be continuous leads to very good results, even with no additional data points. For the approximation of functions on data points there are many mathematical methods, such as polynomial interpolation, spline interpolation, or the method of least squares. The application of these methods becomes problematic in higher dimensions. The special difﬁculty in AI is that model-free approximation methods are needed. That is, a good approximation of the data must be made without knowledge about special properties of the data or the application. Very good results have been achieved here with neural networks and other nonlinear function approximators, which are presented in Chap. 9.\\n\\nThe k nearest neighbor method can be applied in a simple way to the approximation problem. In the algorithm K-NEARESTNEIGHBOR, after the set V = {x1, x2, … , xk} is determined, the k nearest neighbors average function value\\n\\nXk\\n\\n1 k\\n\\n^f ðxÞ ¼\\n\\nf ðxiÞ\\n\\ni¼1 is calculated and taken as an approximation ^f for the query vector x. The larger k becomes, the smoother the function ^f is.\\n\\nð8:2Þ\\n\\n8.3.2 Distance Is Relevant\\n\\nIn practical application of discrete as well as continuous variants of the k nearest neighbor method, problems often occur. As k becomes large, there typically exist more neighbors with a large distance than those with a small distance. Thereby the calculation of ^f is dominated by neighbors that are far away. To prevent this, the k neighbors are weighted such that the more distant neighbors have lesser inﬂuence on the result. During the majority decision in the algorithm K-NEARESTNEIGHBOR, the “votes” are weighted with the weight\\n\\nwi ¼\\n\\n1 1 þ adðx; xiÞ2\\n\\n;\\n\\nð8:3Þ\\n\\nwhich decreases with the square of the distance. The constant a determines the speed of decrease of the weights. Equation (8.2) is now replaced by\\n\\n^f ðxÞ ¼\\n\\nPk\\n\\nwi f ðxiÞ wi\\n\\ni¼1 Pk\\n\\ni¼1\\n\\n:\\n\\nFor uniformly distributed concentration of points in the feature space, this ensures that the inﬂuence of points asymptotically approaches zero as distance increases. Thereby it becomes possible to use many or even all training data to classify or approximate a given input vector.\\n\\n8.3 The Nearest Neighbor Method\\n\\nFig. 8.19 Comparison of the k-nearest neighbor method (upper row) with k ¼ 1 (left), k ¼ 2 (middle) and k ¼ 6 (right), to its distance weighted variant (lower row) with a ¼ 20 (left), a ¼ 4 (middle) and a ¼ 1 (right) on a one-dimensional dataset\\n\\nTo get a feeling for these methods, in Fig. 8.19 the k-nearest neighbor method (in the upper row) is compared with its distance weighted optimization. Due to the averaging, both methods can generalize, or in other words, cancel out noise, if the number of neighbors for k-nearest neighbor or the parameter a is set appropriately. The diagrams show nicely that the distance weighted method gives a much smoother approximation than k-nearest neighbor. With respect to approximation quality, this very simple method can compete well with sophisticated approxima- tion algorithms such as nonlinear neural networks, support vector machines, and Gaussian processes.\\n\\nThere are many alternatives to the weight function (also called kernel) given in (8.3) on page 194. For example a Gaussian or similar bell-shaped function can be used. For most applications, the results are not very sensible on the selection of the kernel. However, the width parameter a which for all these functions has to be set manually has great inﬂuence on the results, as shown in Fig. 8.19. To avoid such an inconvenient manual adaptation, optimization methods have been developed for automatically setting this parameter [SA94, SE10].\\n\\n8.3.3 Computation Times\\n\\nAs previously mentioned, training is accomplished in all variants of the nearest neighbor method by simply saving all training vectors together with their labels (class values), or the function value f(x). Thus there is no other learning algorithm that learns as quickly. However, answering a query for classiﬁcation or approxi- mation of a vector x can become very expensive. Just ﬁnding the k nearest neighbors for n training data requires a cost which grows linearly with n. For classiﬁcation or approximation, there is additionally a cost which is linear in k. The total computation time thus grows as H(n + k). For large amounts of training data, this can lead to problems.\\n\\n195\\n\\n196\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.20 To determine avalanche hazard, a function is approximated from training data. Here for comparison are a local model (solid line), and a global model (dashed line)\\n\\n8.3.4 Summary and Outlook\\n\\nBecause nothing happens in the learning phase of the presented nearest neighbor methods, such algorithms are also denoted lazy learning, in contrast to eager learning, in which the learning phase can be expensive, but application to new examples is very efﬁcient. The perceptron and all other neural networks, decision tree learning, and the learning of Bayesian networks are eager learning methods. Since the lazy learning methods need access to the memory with all training data for approximating a new input, they are also called memory-based learning.\\n\\nTo compare these two classes of learning processes, we will use as an example the task of determining the current avalanche hazard from the amount of newly fallen snow in a certain area of Switzerland.6 In Fig. 8.20 values determined by experts are entered, which we want to use as training data. During the application of a eager learning algorithm which undertakes a linear approximation of the data, the dashed line shown in the ﬁgure is calculated. Due to the restriction to a straight line, the error is relatively large with a maximum of about 1.5 hazard levels. During lazy learning, nothing is calculated before a query for the current hazard level arrives. Then the answer is calculated from several nearest neighbors, that is, locally. It could result in the curve shown in the ﬁgure, which is put together from line segments and shows much smaller errors. The advantage of the lazy method is its locality. The approximation is taken locally from the current new snow level and not globally. Thus for fundamentally equal classes of functions (for example linear functions), the lazy algorithms are better.\\n\\nNearest neighbor methods are well suited for all problem situations in which a good local approximation is needed, but which do not place a high requirement on the speed of the system. The avalanche predictor mentioned here, which runs once per day, is such an application. Nearest neighbor methods are not suitable when a description of the knowledge extracted from the data must be understandable by\\n\\n6The three day total of snowfall is in fact an important feature for determining the hazard level. In practice, however, additional attributes are used [Bra01]. The example used here is simpliﬁed.\\n\\n8.3 The Nearest Neighbor Method\\n\\nFeature Defective part: Bicycle model: Year: Power source: Bulb condition: Light cable condition:\\n\\nQuery Rear light Marin Pine Mountain 1993 Battery ok ?\\n\\nCase from case base Front light VSF T400 2001 Dynamo ok ok\\n\\nSolution\\n\\nDiagnosis: Repair:\\n\\n? ?\\n\\nFront electrical contact missing Establish front electrical contact\\n\\nFig. 8.21 Simple diagnosis example for a query and the corresponding case from the case base\\n\\nhumans, which today is the case for many data mining applications (see Sect. 8.4). In recent years these memory-based learning methods are becoming popular, and various improved variants (for example locally weighted linear regression) have been applied [Cle79].\\n\\nTo be able to use the described methods, the training data must be available in the form of vectors of integers or real numbers. They are thus unsuitable for applications in which the data are represented symbolically, for example as ﬁrst order logic formulas. We will now brieﬂy discuss this.\\n\\n8.3.5 Case-Based Reasoning\\n\\nIn case-based reasoning (CBR), the nearest neighbor method is extended to sym- bolic problem descriptions and their solutions. CBR is used in the diagnosis of technical problems in customer service or for telephone hotlines. The example shown in Fig. 8.21 about the diagnosis of a bicycle light going out illustrates this type of situation.\\n\\nA solution is searched for the query of a customer with a defective rear bicycle light. In the right column, a case similar to the query in the middle column is given. This stems from the case base, which corresponds to training data in the nearest neighbor method. If we simply took the most similar case, as we do in the nearest neighbor method, then we would end up trying to repair the front light when the rear light is broken. We thus need a reverse transformation of the solution to the discovered similar problem back to the query. The most important steps in the solution to a CBR case are carried out in Fig. 8.22 on page 198. The transformation in this example is simple: rear light is mapped to front light.\\n\\nAs beautiful and simple as this methods seems in theory, in practice the con- struction of CBR diagnostic systems is a very difﬁcult task. The three main difﬁ- culties are: Modeling The domains of the application must be modeled in a formal context. Here logical monotony, which we know from Chap. 4, presents difﬁculties. Can the developer predict and map all possible special cases and problem variants?\\n\\n197\\n\\n198\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.22 If for a case x a similar case y is found, transformation must be determined and its inverse applied to the discovered case y\\n\\nthen to obtain a solution for x,\\n\\nSimilarity Finding a suitable similarity metric for symbolic, non-numerical features. Transformation Even if a similar case is found, is not yet clear how the\\n\\nit transformation mapping and its inverse should look. Indeed there are practical CBR systems for diagnostic applications in use today. However, due to the reasons mentioned, these remain far behind human experts in performance and ﬂexibility. An interesting alternative to CBR are the Bayesian networks presented in Chap. 7. Often the symbolic problem representation can also be mapped quite well to discrete or continuous numerical features. Then the mentioned inductive learning methods such as decision trees or neural networks can be used successfully.\\n\\n8.4 Decision Tree Learning\\n\\nDecision tree learning is an extraordinarily important algorithm for AI because it is very powerful, but also simple and efﬁcient for extracting knowledge from data. Compared to the two already introduced learning algorithms, it has an important advantage. The extracted knowledge is not only available and usable as a black box function, but rather it can be easily understood, interpreted, and controlled by humans in the form of a readable decision tree. This also makes decision tree learning an important tool for data mining.\\n\\nWe will discuss function and application of decision tree learning using the C4.5 algorithm. C4.5 was introduced in 1993 by the Australian Ross Quinlan and is an improvement of its predecessor ID3 (Iterative Dichotomiser 3, 1986). It is freely available for noncommercial use [Qui93]. A further development, which works even more efﬁciently and can take into account the costs of decisions, is C5.0 [Qui93].\\n\\nThe CART (Classiﬁcation and Regression Trees, 1984) system developed by Leo Breiman [BFOS84] works similarly to C4.5. It has a convenient graphical user interface, but is very expensive.\\n\\nTwenty years earlier, in 1964, the CHAID (Chi-square Automatic Interaction Detectors) system, which can automatically generate decision trees, was introduced by J. Sonquist and J. Morgan. It has the noteworthy characteristic that it stops the growth of the tree before it becomes too large, but today it has no more relevance.\\n\\nthe\\n\\n8.4 Decision Tree Learning\\n\\nTable 8.4 Variables for the skiing classiﬁcation problem\\n\\nVariable Ski (goal variable) Should I drive to the nearest ski resort with enough snow? Sun (feature) Is there sunshine today? Snow_Dist (feature) (cid:6) 100, >100 Distance to the nearest ski resort with good snow conditions (over/under 100 km) Is it the weekend today?\\n\\nValue\\n\\nDescription\\n\\nyes, no yes, no\\n\\nWeekend (feature)\\n\\nyes, no\\n\\nAlso interesting is the data mining tool KNIME (Konstanz Information Miner), which has a friendly user interface and, using the WEKA Java library, also makes induction of decision trees possible. In Sect. 8.10 we will introduce KNIME.\\n\\nNow we ﬁrst show in a simple example how a decision tree can be constructed from training data, in order to then analyze the algorithm and apply it to the more complex LEXMED example for medical diagnosis.\\n\\n8.4.1 A Simple Example\\n\\nA devoted skier who lives near the high sierra, a beautiful mountain range in California, wants a decision tree to help him decide whether it is worthwhile to drive his car to a ski resort in the mountains. We thus have a two-class problem ski yes/no based on the variables listed in Table 8.4.\\n\\nFigure 8.23 on page 200 shows a decision tree for this problem. A decision tree is a tree whose inner nodes represent features (attributes). Each edge stands for an attribute value. At each leaf node a class value is given.\\n\\nThe data used for the construction of the decision tree are shown in Table 8.5 on page 200. Each row in the table contains the data for one day and as such represents a sample. Upon closer examination we see that row 6 and row 7 contradict each other. Thus no deterministic classiﬁcation algorithm can correctly classify all of the data. The number of falsely classiﬁed data must therefore be (cid:9) 1. The tree in Fig. 8.23 on page 200 thus classiﬁes the data optimally.\\n\\nHow is such a tree created from the data? To answer this question we will at ﬁrst restrict ourselves to discrete attributes with ﬁnitely many values. Because the number of attributes is also ﬁnite and each attribute can occur at most once per path, there are ﬁnitely many different decision trees. A simple, obvious algorithm for the construction of a tree would simply generate all trees, then for each tree calculate the number of erroneous classiﬁcations of the data, and at the end choose the tree with the minimum number of errors. Thus we would even have an optimal algorithm (in the sense of errors for the training data) for decision tree learning.\\n\\nThe obvious disadvantage of this algorithm is its unacceptably high computation time, as soon as the number of attributes becomes somewhat larger. We will now develop a heuristic algorithm which, starting from the root, recursively builds\\n\\n199\\n\\n200\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.23 Decision tree for the skiing classiﬁcation problem. In the lists to the right of the nodes, the numbers of the corresponding training data are given. Notice that of the leaf nodes sunny = yes only two of the three examples are classiﬁed correctly\\n\\nTable 8.5 Data set for the skiing classiﬁcation problem\\n\\nDay\\n\\n1 2 3 4 5 6 7 8 9 10 11\\n\\nSnow_Dist\\n\\n(cid:6)100 (cid:6)100 (cid:6)100 (cid:6)100 >100 >100 >100 >100 >100 >100 >100\\n\\nWeekend\\n\\nyes yes yes no yes yes yes yes no no no\\n\\nSun\\n\\nyes yes no yes yes yes yes no yes yes no\\n\\nSkiing\\n\\nyes yes yes yes yes yes no no no no no\\n\\na decision tree. First the attribute with the highest information gain (Snow_ Dist) is chosen for the root node from the set of all attributes. For each attribute value ((cid:6)100, >100) there is a branch in the tree. Now for every branch this process is repeated recursively. During generation of the nodes, the attribute with the highest information gain among the attributes which have not yet been used is always chosen, in the spirit of a greedy strategy.\\n\\n8.4.2 Entropy as a Metric for Information Content\\n\\nThe described top-down algorithm for the construction of a decision tree, at each step selects the attribute with the highest information gain. We now introduce the\\n\\n8.4 Decision Tree Learning\\n\\nentropy as the metric for the information content of a set of training data D. If we only look at the binary variable skiing in the above example, then D can be described as\\n\\nD ¼ (yes, yes, yes, yes, yes, yes, no, no, no, no, no)\\n\\nwith estimated probabilities\\n\\np1 ¼ PðyesÞ ¼ 6=11\\n\\nand\\n\\np2 ¼ PðnoÞ ¼ 5=11:\\n\\nHere we evidently have a probability distribution p ¼ (6/11, 5/11). In general, for an n class problem this reads\\n\\np ¼ ðp1; . . .; pnÞ\\n\\nwith\\n\\nXn\\n\\npi ¼ 1:\\n\\ni¼1\\n\\nTo introduce the information content of a distribution we observe two extreme cases. First let\\n\\np ¼ ð1; 0; 0; . . .; 0Þ:\\n\\nThat is, the ﬁrst one of the n events will certainly occur and all others will not. The uncertainty about the outcome of the events is thus minimal. In contrast, for the uniform distribution\\n\\n(cid:4) 1 n\\n\\n(cid:5)\\n\\n; 1 n\\n\\n; . . .; 1 n\\n\\np ¼\\n\\nthe uncertainty is maximal because no event can be distinguished from the others. Here Claude Shannon asked himself how many bits would be needed to encode such an event. In the certain case of (8.4) zero bits are needed because we know that the case i ¼ 1 always occurs. In the uniformly distributed case of (8.5) there are n equally probable possibilities. For binary encodings, log 2n bits are needed here. Because all 1 individual probabilities are pi ¼ 1/n, log2 pi bits are needed for this encoding. In the general case p ¼ (p1, … , pn), if the probabilities of the elementary events deviate from the uniform distribution, then the expectation value H for the number pi with of bits is calculated. To this end we will weight all values log2 their probabilities and obtain Xn\\n\\n1 pi ¼ (cid:2) log2\\n\\nXn\\n\\nH ¼\\n\\npið(cid:2) log2\\n\\npiÞ ¼ (cid:2)\\n\\npi log2\\n\\npi:\\n\\ni¼1\\n\\ni¼1\\n\\nThe more bits we need to encode an event, clearly the higher the uncertainty about the outcome. Therefore we deﬁne:\\n\\n201\\n\\nð8:4Þ\\n\\nð8:5Þ\\n\\n202\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.24 The entropy function for the case of two classes. We see the maximum at p ¼ 1/2 and the symmetry with respect to swapping p and 1 − p\\n\\nDeﬁnition 8.4 The Entropy H as a metric for the uncertainty of a probability distribution is deﬁned by7\\n\\nHð pÞ ¼ Hðp1; . . .; pnÞ :¼ (cid:2)\\n\\nXn\\n\\npi log2\\n\\npi:\\n\\ni¼1\\n\\nA detailed derivation of this formula is found in [SW76]. If we substitute the certain event p ¼ (1, 0, 0, … , 0), then 0 log 2 0, an undeﬁned expression results. We solve this problem by the deﬁnition 0 log2 0 := 0 (see Exercise 8.10 on page 240). Now we can calculate H(1, 0, … , 0) = 0. We will show that the entropy in the Pn pi ¼ 1 takes on its maximum value i¼1 nÞ. In the case of an event with two possible\\n\\nhypercube [0, 1]n under the constraint n ; . . .; 1 with the uniform distribution ð1 outcomes, which correspond to two classes, the result is\\n\\nHð pÞ ¼ Hðp1; p2Þ ¼ Hðp1; 1 (cid:2) p1Þ ¼ (cid:2)ðp1 log2\\n\\np1 þ ð1 (cid:2) p1Þ log2ð1 (cid:2) p1ÞÞ:\\n\\nThis expression is shown as a function of p1 in Fig. 8.24 with its maximum at p1 = 1/2.\\n\\nBecause each classiﬁed dataset D is assigned a probability distribution p by estimating the class probabilities, we can extend the concept of entropy to data by the deﬁnition\\n\\n7In (7.9) on page 138 the natural logarithm rather than log 2 is used in the deﬁnition of entropy. Because here, and also in the case of the MaxEnt method, entropies are only being compared, this difference does not play a role. (see Exercise 8.12 on page 240).\\n\\n8.4 Decision Tree Learning\\n\\nHðDÞ ¼ Hð pÞ:\\n\\nNow, since the information content I(D) of the dataset D is meant to be the opposite of uncertainty. Thus we deﬁne:\\n\\nDeﬁnition 8.5 The information content of a dataset is deﬁned as\\n\\nIðDÞ :¼ 1 (cid:2) HðDÞ:\\n\\n8.4.3 Information Gain\\n\\nIf we apply the entropy formula to the example, the result is\\n\\nHð6=11; 5=11Þ ¼ 0:994\\n\\nDuring construction of a decision tree, the dataset is further subdivided by each new attribute. The more an attribute raises the information content of the distribution by dividing the data, the better that attribute is. We deﬁne accordingly:\\n\\nDeﬁnition 8.6 The information gain G(D, A) through the use of the attribute A is determined by the difference of the average information content of the dataset D ¼ D1 [ D2 [ ⋅ ⋅ ⋅ [ Dn divided by the n-value attribute A and the information content I(D) of the undivided dataset, which yields\\n\\nGðD; AÞ ¼\\n\\nXn\\n\\ni¼1\\n\\njDij jDj\\n\\nIðDiÞ (cid:2) IðDÞ:\\n\\nWith (8.6) we obtain from this\\n\\nGðD; AÞ ¼\\n\\nXn\\n\\ni¼1\\n\\njDij jDj Xn\\n\\n¼ 1 (cid:2)\\n\\ni¼1\\n\\n¼ HðDÞ (cid:2)\\n\\nIðDiÞ (cid:2) IðDÞ ¼\\n\\nXn\\n\\ni¼1\\n\\njDij jDj\\n\\njDij jDj Xn\\n\\ni¼1\\n\\nHðDiÞ (cid:2) 1 þ HðDÞ\\n\\njDij jDj\\n\\nHðDiÞ:\\n\\nð1 (cid:2) HðDiÞÞ (cid:2) ð1 (cid:2) HðDÞÞ\\n\\n203\\n\\nð8:6Þ\\n\\nð8:7Þ\\n\\n204\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.25 The calculated gain for the various attributes reﬂects whether the division of the data by the respective attribute results in a better class division. The more the distributions generated by the attribute deviate from the uniform distribution, the higher the information gain\\n\\nApplied to our example for the attribute Snow_Dist, this yields\\n\\n(cid:4)\\n\\n(cid:5)\\n\\nGðD; Snow DistÞ ¼ HðDÞ (cid:2)\\n\\n¼ 0:994 (cid:2)\\n\\n(cid:4)\\n\\n4 11 4 11\\n\\nHðD (cid:6) 100Þ þ\\n\\n7 11\\n\\nHðDÞ[100 (cid:5)\\n\\n(cid:3) 0 þ\\n\\n7 11\\n\\n(cid:3) 0:863\\n\\n¼ 0:445:\\n\\nAnalogously we obtain\\n\\nGðD; WeekendÞ ¼ 0:150\\n\\nand\\n\\nGðD; SunÞ ¼ 0:049:\\n\\nThe attribute Snow_Dist now becomes the root node of the decision tree. The situation of the selection of this attribute is once again clariﬁed in Fig. 8.25.\\n\\nThe two attribute values (cid:6)100 and >100 generate two edges in the tree, which correspond to the subsets D(cid:6)100 and D>100. For the subset D(cid:6)100 the classiﬁcation is clearly yes. thus the tree terminates here. In the other branch D>100 there is no clear result. Thus the algorithm repeats recursively. From the two attributes still avail- able, Sun and Weekend, the better one must be chosen. We calculate\\n\\nGðD[100; WeekendÞ ¼ 0:292\\n\\nand\\n\\nGðD[100; SunÞ ¼ 0:170:\\n\\nThe node thus gets the attribute Weekend assigned. For Weekend = no the tree terminates with the decision Ski = no. A calculation of the gain here returns the value 0. For Weekend = yes, Sun results in a gain of 0.171. Then the construction of the tree terminates because no further attributes are available, although example\\n\\n8.4 Decision Tree Learning\\n\\nnumber 7 is falsely classiﬁed. The ﬁnished tree is already familiar from Fig. 8.23 on page 200.\\n\\n8.4.4 Application of C4.5\\n\\nThe decision tree that we just generated can also be generated by C4.5. The training data are saved in a data ﬁle ski.data in the following format:\\n\\n<=100, yes, yes, <=100, yes, yes, <=100, yes, no, <=100, no, >100, >100, >100, >100, >100, >100, >100,\\n\\nyes, yes, yes, yes, yes, yes, yes, yes, no, no, no, no,\\n\\nyes, yes, no,\\n\\nyes yes yes yes yes yes no no no no no\\n\\nThe information about attributes and classes is stored in the ﬁle ski.names\\n\\n(lines beginning with “|” are comments):\\n\\n|Classes: no: do not ski, yes: go skiing | no,yes. | |Attributes | Snow_Dist: Weekend: Sun:\\n\\n<=100,>100. no,yes. no,yes.\\n\\nC4.5 is then called from the Unix command line and generates the decision tree shown below, which is formatted using indentations. The option -f is for the name of the input ﬁle, and the option -m speciﬁes the minimum number of training data points required for generating a new branch in the tree. Because the number of training data points in this example is extremely small, -m 1 is sensible here. For larger datasets, a value of at least -m 10 should be used.\\n\\n205\\n\\n206\\n\\n8 Machine Learning and Data Mining\\n\\nunixprompt> c4.5 -f ski -m 1\\n\\nC4.5 [release 8] decision tree generator\\n\\nWed Aug 23 10:44:49 2010\\n\\n----------------------------------------\\n\\nOptions:\\n\\nFile stem <ski> Sensible test requires 2 branches with >=1 cases\\n\\nRead 11 cases (3 attributes) from ski.data\\n\\nDecision Tree:\\n\\nSnow_Dist = <=100: ja (4.0) Snow_Dist = >100: | | | |\\n\\nWeekend = no: no (3.0) Weekend = yes: | Sun = no: no (1.0) | Sun = yes: yes (3.0/1.0)\\n\\nSimplified Decision Tree:\\n\\nSnow_Dist = <=100: yes (4.0/1.2) Snow_Dist = >100: no (7.0/3.4)\\n\\nEvaluation on training data (11 items):\\n\\nBefore Pruning ---------------- Size\\n\\nErrors\\n\\nAfter Pruning --------------------------- Size\\n\\nErrors\\n\\nEstimate\\n\\n7\\n\\n1(9.1%)\\n\\n3\\n\\n2(18.2%)\\n\\n(41.7%) <<\\n\\nAdditionally, a simpliﬁed tree with only one attribute is given. This tree, which was created by pruning (see Sect. 8.4.7), will be important for an increasing amount of training data. In this little example it does not yet makes much sense. The error rate for both trees on the training data is also given. The numbers in parentheses after the decisions give the size of the underlying dataset and the number of errors. For example, the line Sun = yes: yes (3.0/1.0) in the top tree indicates that for this leaf node Sun = yes, three training examples exist, one of which is falsely classiﬁed. The user can thus read from this whether the decision is statistically grounded and/or certain.\\n\\nIn Fig. 8.26 on page 207 we can now give the schema of the learning algorithm\\n\\nfor generating a decision tree.\\n\\nWe are now familiar with the foundations of the automatic generation of deci- sion trees. For the practical application, however, important extensions are needed. We will introduce these using the already familiar LEXMED application.\\n\\n8.4 Decision Tree Learning\\n\\nGENERATEDECISIONTREE(Data,Node) Amax = Attribute with maximum information gain If G(Amax) = 0\\n\\nThen Node becomes leaf node with most frequent class in Data Else assign the attribute Amax to Node\\n\\nFor each value a1, . . . , an of Amax , generate\\n\\na successor node: K1, . . . , Kn\\n\\nDivide Data into D1, . . . , Dn with Di = {x ∈ Data|Amax(x) = ai} For all i ∈ {1, . . . , n}\\n\\nIf all x ∈ Di belong to the same class Ci\\n\\nThen generate leaf node Ki of class Ci Else GENERATEDECISIONTREE(Di, Ki )\\n\\nFig. 8.26 Algorithm for the construction of a decision tree\\n\\n8.4.5 Learning of Appendicitis Diagnosis\\n\\nIn the research project LEXMED, an expert system for the diagnosis of appendicitis was developed on top of a database of patient data [ES99, SE00]. The system, which works with the method of maximum entropy, is described in Sect. 7.3.\\n\\nWe now use the LEXMED database to generate a decision tree for diagnosing appendicitis with C4.5. The symptoms used as attributes are deﬁned in the ﬁle app.names:\\n\\n|Definition of the classes and attributes | |Classes 0=appendicitis negative | 1=appendicitis positive 0,1. | |Attributes | Age: continuous. Sex_(1=m___2=w): 1,2. Pain_Quadrant1_(0=no__1=yes): 0,1. Pain_Quadrant2_(0=no__1=yes): 0,1. Pain_Quadrant3_(0=no__1=yes): 0,1. Pain_Quadrant4_(0=no__1=yes): 0,1. Local_guarding_(0=no__1=yes): 0,1. Generalized_guarding_(0=no__1=yes): 0,1. Rebound_tenderness_(0=no__1=yes): 0,1. Pain_on_tapping_(0=no__1=yes): 0,1.\\n\\n207\\n\\n208\\n\\n8 Machine Learning and Data Mining\\n\\nPain_during_rectal_examination_(0=no__1=yes): 0,1. Temp_axial: continuous. Temp_rectal: continuous. Leukocytes: continuous. Diabetes_mellitus_(0=no__1=yes): 0,1\\n\\nWe see that, besides many binary attributes such as the various pain symptoms, continuous symptoms such as age and fever temperature also occur. In the fol- lowing training data ﬁle, app.data, in each line a case is described. In the ﬁrst line is a 19-year-old male patient with pain in the third quadrant (lower right, where the appendix is), the two fever values 36.2 and 37.8 degree Celsius a leukocyte value of 13400 and a positive diagnosis, that is, an inﬂamed appendix.\\n\\n19,1,0,0,1,0,1,0,1,1,0,362,378,13400,0,1 13,1,0,0,1,0,1,0,1,1,1,383,385,18100,0,1 32,2,0,0,1,0,1,0,1,1,0,364,374,11800,0,1 18,2,0,0,1,1,0,0,0,0,0,362,370,09300,0,0 73,2,1,0,1,1,1,0,1,1,1,376,380,13600,1,1 30,1,1,1,1,1,0,1,1,1,1,377,387,21100,0,1 56,1,1,1,1,1,0,1,1,1,0,390,?,14100,0,1 36,1,0,0,1,0,1,0,1,1,0,372,382,11300,0,1 36,2,0,0,1,0,0,0,1,1,1,370,379,15300,0,1 33,1,0,0,1,0,1,0,1,1,0,367,376,17400,0,1 19,1,0,0,1,0,0,0,1,1,0,361,375,17600,0,1 12,1,0,0,1,0,1,0,1,1,0,364,370,12900,0,0 ...\\n\\nWithout going into detail about the database, it is important to mention that only patients who were suspected of having appendicitis upon arrival at the hospital and were then operated upon are included in the database. We see in the seventh row that C4.5 can also deal with missing values. The data contain 9764 cases.\\n\\nunixprompt> c4.5 -f app -u -m 100\\n\\nC4.5 [release 8] decision tree generator\\n\\nWed Aug 23 13:13:15 2006\\n\\n----------------------------------------\\n\\nRead 9764 cases (15 attributes) from app.data\\n\\nDecision Tree:\\n\\n8.4 Decision Tree Learning\\n\\nLeukocytes <= 11030 : | Rebound_tenderness = 0: | | Temp_rectal > 381 : 1 (135.9/54.2) | | Temp_rectal <= 381 : | | | Local_guarding = 0: 0 (1453.3/358.9) | | | Local_guarding = 1: | | | | Sex_(1=m___2=w) = 1: 1 (160.1/74.9) | | | | Sex_(1=m___2=w) = 2: 0 (286.3/97.6) | Rebound_tenderness = 1: | | Leukocytes <= 8600 : | | | Temp_rectal > 378 : 1 (176.0/59.4) | | | Temp_rectal <= 378 : | | | | Sex_(1=m___2=w) = 1: | | | | | Local_guarding = 0: 0 (110.7/51.7) | | | | | Local_guarding = 1: 1 (160.6/68.5) | | | | Sex_(1=m___2=w) = 2: | | | | | Age <= 14 : 1 (131.1/63.1) | | | | | Age > 14 : 0 (398.3/137.6) | | Leukocytes > 8600 : | | | Sex_(1=m___2=w) = 1: 1 (429.9/91.0) | | | Sex_(1=m___2=w) = 2: | | | | Local_guarding = 1: 1 (311.2/103.0) | | | | Local_guarding = 0: | | | | | Temp_rectal <= 375 : 1 (125.4/55.8) | | | | | Temp_rectal > 375 : 0 (118.3/56.1) Leukocytes > 11030 : | Rebound_tenderness = 1: 1 (4300.0/519.9) | Rebound_tenderness = 0: | | Leukocytes > 14040 : 1 (826.6/163.8) | | Leukocytes <= 14040 : | | | Pain_on_tapping = 1: 1 (260.6/83.7) | | | Pain_on_tapping = 0: | | | | Local_guarding = 1: 1 (117.5/44.4) | | | | Local_guarding = 0: | | | | | Temp_axial <= 368 : 0 (131.9/57.4) | | | | | Temp_axial > 368 : 1 (130.5/57.8)\\n\\nSimplified Decision Tree:\\n\\nLeukocytes > 11030 : 1 (5767.0/964.1) Leukocytes <= 11030 : | Rebound_tenderness = 0: | | Temp_rectal > 381 : 1 (135.9/58.7) | | Temp_rectal <= 381 : | | | Local_guarding = 0: 0 (1453.3/370.9) | | | Local_guarding = 1: | | | | Sex_(1=m___2=w) = 1: 1 (160.1/79.7) | | | | Sex_(1=m___2=w) = 2: 0 (286.3/103.7) | Rebound_tenderness = 1: | | Leukocytes > 8600 : 1 (984.7/322.6) | | Leukocytes <= 8600 :\\n\\n209\\n\\n210\\n\\n8 Machine Learning and Data Mining\\n\\n| | | Temp_rectal > 378 : 1 (176.0/64.3) | | | Temp_rectal <= 378 : | | | | Sex_(1=m___2=w) = 1: | | | | | Local_guarding = 0: 0 (110.7/55.8) | | | | | Local_guarding = 1: 1 (160.6/73.4) | | | | Sex_(1=m___2=w) = 2: | | | | | Age <= 14 : 1 (131.1/67.6) | | | | | Age > 14 : 0 (398.3/144.7)\\n\\nEvaluation on training data (9764 items):\\n\\nBefore Pruning ---------------- Errors Size\\n\\nAfter Pruning --------------------------- Estimate Errors Size\\n\\n37 2197(22.5%)\\n\\n21 2223(22.8%)\\n\\n(23.6%)\\n\\n<<\\n\\nEvaluation on test data (4882 items):\\n\\nBefore Pruning ---------------- Errors Size\\n\\nAfter Pruning --------------------------- Estimate Errors Size\\n\\n37 1148(23.5%)\\n\\n21 1153(23.6%)\\n\\n(23.6%)\\n\\n<<\\n\\n(a) (b) ---- ---- 758 885 268 2971\\n\\n<-classified as\\n\\n(a): class 0 (b): class 1\\n\\n8.4.6 Continuous Attributes\\n\\nIn the trees generated for the appendicitis diagnosis there is a node Leukocytes > 11030 which clearly comes from the continuous attribute Leukocytes by setting a threshold at the value 11030. C4.5 thus has made a binary attribute Leukocytes > 11030 from the continuous attribute Leukocytes. The threshold HD; A for an attribute A is determined by the following algorithm: for all values v which occur in the training data D, the binary attribute A > v is generated and its information gain is calculated. The threshold HD; A is then set to the value v with the maximum infor- mation gain, that is:\\n\\nHD; A ¼ argmaxvfGðD; A [ vÞg:\\n\\nFor an attribute such as the leukocyte value or the patient’s age, a decision based on a binary discretization is presumably too imprecise. Nevertheless there is no need to discretize ﬁner because each continuous attribute is tested on each newly generated node and can thus occur repeatedly in one tree with a different threshold HD;A. Thus we ultimately obtain a very good discretization whose ﬁneness ﬁts the problem.\\n\\n8.4 Decision Tree Learning\\n\\n8.4.7 Pruning—Cutting the Tree\\n\\nSince the time of Aristotle it has been stipulated that of two scientiﬁc theories which explain the same situation equally well, the simpler one is preferred. This law of economy, also now known as Occam’s razor, is of great importance for machine learning and data mining.\\n\\nA decision tree is a theory for describing the training data. A different theory for describing the data is the data themselves. If the tree classiﬁes all data without any errors, but is much more compact and thus more easily understood by humans, then it is preferable according to Occam’s razor. The same is true for two decision trees of different sizes. Thus the goal of every algorithm for generating a decision tree must be to generate the smallest possible decision tree for a given error rate. Among all trees with a ﬁxed error rate, the smallest tree should always be selected.\\n\\nUp until now we have not deﬁned the term error rate precisely. As already mentioned several times, it is important that the learned tree not just memorize the training data, rather that it generalizes well. To test the ability of a tree to generalize, we divide the available data into a set of training data and a set of v. The test data are hidden from the learning algorithm and only used for testing. If a large dataset is available, such as the appendicitis data, then we can for example use two-thirds of the data for learning and the remaining third for testing.\\n\\nAside from better comprehensibility, Occam’s razor has another important jus- tiﬁcation: generalization ability. The more complex the model (here a decision tree), the more details are represented, but to the same extent the less is the model transferable to new data. This relationship is illustrated in Fig. 8.27. Decision trees of various sizes were trained against the appendicitis data. In the graph, classiﬁ- cation errors on both the training data and on the test data are given. The error rate on the training data decreases monotonically with the size of the tree. Up to a tree size of 55 nodes, the error rate on test data also decreases. If the tree grows further, however, then the error rate starts to increase again! This effect, which we have already seen in the nearest neighbor method, is called overﬁtting.\\n\\nFig. 8.27 Learning curve of C4.5 on the appendicitis data. We clearly see the overﬁtting of trees with more than 55 nodes\\n\\n211\\n\\n212\\n\\n8 Machine Learning and Data Mining\\n\\nWe will give this concept, which is important for nearly all learning processes,\\n\\na general deﬁnition taken from [Mit97]:\\n\\nDeﬁnition 8.7 Let a speciﬁc learning algorithm, that is, a learning agent, be given. We call an agent A overﬁt to the training data if there is another agent A′ whose error on the training data is larger than that of A, but whose error on the whole distribution of data is smaller than the error of A.\\n\\nHow can we now ﬁnd this point of minimum error on the test data? The most obvious algorithm is called cross-validation. During construction of the tree, the error on the test data is measured in parallel. As soon as the error rises signiﬁcantly, the tree with the minimum error is saved. This algorithm is used by the CART system mentioned earlier.\\n\\nC4.5 works somewhat differently. First, using the algorithm GENERATEDECI- SIONTREE from Fig. 8.26 on page 207, it generates a tree which is usually overﬁt. Then, using pruning, it attempts to cut away nodes of the tree until the error on the test data, estimated by the error on the training data, begins to rise.8 Like the con- struction of the tree, this is also a greedy algorithm. This means that once a node is pruned, it cannot be re-inserted, even if this later turns out to be better.\\n\\n8.4.8 Missing Values\\n\\nFrequently individual attribute values are missing from the training data. In the LEXMED dataset, the following entry occurs:\\n\\n56; 1; 1; 1; 1; 1; 0; 1; 1; 1; 0; 390; ?; 14100; 0; 1;\\n\\nin which one of the fever values is missing. Such data can nevertheless be used during construction of the decision tree. We can assign the attribute the most frequent value from the whole dataset or the most frequent of all data points from the same class. It is even better to substitute the probability distribution of all attribute values for the missing attribute value and to split the training example into branches according to this distribution. This is incidentally a reason for the occurrence of non-integer values in the expressions in parentheses next to the leaf nodes of the C4.5 tree.\\n\\nMissing values can occur not only during learning, but also during classiﬁcation.\\n\\nThese are handled in the same way as during learning.\\n\\n8It would be better to use the error on the test data directly. At least when the amount of training data is sufﬁcient to justify a separate testing set.\\n\\n8.4 Decision Tree Learning\\n\\n8.4.9 Summary\\n\\nLearning of decision trees is a favorite approach to classiﬁcation tasks. The reasons for this are its simple application and speed. On a dataset of about 10 000 LEXMED data points with 15 attributes each, C4.5 requires about 0.3 s for learning. This is very fast compared to other learning algorithms.\\n\\nFor the user it is also important, however, that the decision tree as a learned model can be understood and potentially changed. It is also not difﬁcult to auto- matically transform a decision tree into a series of if-then-else statements and thus efﬁciently build it into an existing program.\\n\\nBecause a greedy algorithm is used for construction of the tree as well as during pruning, the trees are in general suboptimal. The discovered decision tree does usually have a relatively small error rate. However, there is potentially a better tree, because the heuristic greedy search of C4.5 prefers small trees and attributes with high information gain at the top of the tree. For attributes with many values, the presented formula for the information gain shows weaknesses. Alternatives to this are given in [Mit97].\\n\\n8.5 Cross-Validation and Overfitting\\n\\nAs discussed in Sect. 8.4.7, many learning algorithms have the problem of over- ﬁtting. Powerful learning algorithms, such as for example decision tree learning, can adapt the complexity of the learned model to the complexity of the training data. This leads to overﬁtting if there is noise in the data.\\n\\nWith cross-validation one attempts to optimize the model complexity such that it minimizes the classiﬁcation or approximation error on an unknown test data set. This requires the model complexity to be controllable by a parameter c. For decision trees this is, for example, the size of the tree. For the k nearest neighbor method from Sect. 8.3, the parameter is k, the number of nearest neighbors, while for neural networks it is the number of hidden neurons (see Chap. 9).\\n\\nWe vary a parameter c while training the algorithm on a training data set and choose the value of c that minimizes the error on an independent testing data set. k-ary cross-validation works according to the following schema:\\n\\nCrossValidation(X , k) Partition data into k equally sized blocks X = X 1 ∪ . . . ∪ X k For all γ ∈ {γmin, . . . , γmax} For all i ∈ {1, . . . , k}\\n\\nTrain a model of complexity γ on X \\\\X i Compute the error E(γ, X i) on the test set X i i=1 E(γ, X i)\\n\\n(cid:2)k\\n\\nCompute the mean error E(γ) = 1 k\\n\\nChoose the value γopt = argminγ E(γ) with smallest mean error Train the ﬁnal model with complexity γopt on the whole data set X\\n\\n213\\n\\n214\\n\\n8 Machine Learning and Data Mining\\n\\nThe whole data set X is divided into k equally sized blocks. Then the algorithm is trained k times on k (cid:2) 1 blocks and tested on the remaining block. The k computed errors are averaged and the value copt with the smallest mean error is chosen to train the ﬁnal model on the whole data set X.\\n\\nIf the training set X is large, it may be divided, for example, into k ¼ 3 or k ¼ 10 blocks. The resulting increase in computational complexity is usually acceptable. For small training sets, it is preferable to train on all n feature vectors if possible. Then one can choose k ¼ n, which yields what is called leave-one-out cross-validation.\\n\\nimportant and most used automatic optimiza- the overﬁtting problem. tion method for model We can gain additional insight into this problem from short look at the so called bias variance tradeoff. forces the non-optimal approximation of the data in a certain direction (bias). On the other hand, if an overly complex model is used, it will tend to overﬁt any data set. Thus, for a new data sample from the same distribution, it may learn a very therefore varies greatly for a change in the data different model. The model (variance).\\n\\nCross-validation is the most\\n\\ncomplexity.\\n\\nIt\\n\\nsolves\\n\\nIf an overly simple model\\n\\nis used,\\n\\nthis\\n\\nThis relationship can be illustrated by the example of function approximation using polynomials. If one chooses the degree of the polymonial as the complexity parameter c, degree 1 approximates a line, which is not a good approximation for non-trivial data. If, on the other hand, the polynomial degree equals n (cid:2) 1 for n points, it tends to hit every point and reduce the error on the training data to zero. This can be seen in Fig. 8.28 left on page 215, in which eight data points have been generated using\\n\\nyðxÞ ¼ sinðxÞ þ gð0; 0:2Þ;\\n\\nwhere g(0, 0.2) generates normally distributed noise with zero mean and a standard deviation of 0.2.\\n\\nIn the left image, in addition to the data points, the underlying sine function (black) is inscribed, as well as a straight line (red) approximated using the method of least squares (see Sect. 9.4 oder [Ert15]). In addition, a seventh degree poly- nomial (green) is interpolated through the points, hitting each point, as well as a fourth degree polynomial (blue), which comes closest to the sine curve. In the right image, the same approximations were carried out again on eight new data points. One can see very nicely that despite differing data, both straight lines deviate greatly from the data (large bias), but have a similar function graph (very small variance). Both seventh degree polynomials, on the other hand, ﬁt the data perfectly (zero bias), but they have very different function graphs (very large variance). This is an obvious overﬁtting effect. The fourth degree polynomial presents a good compromise. Cross-validation would presumably yield degree four as the optimal solution.\\n\\n8.6 Learning of Bayesian Networks\\n\\n1.5\\n\\n1.5\\n\\n1\\n\\n1\\n\\n0.5\\n\\n0.5\\n\\n0\\n\\n0\\n\\n0.5\\n\\n0.5\\n\\n1\\n\\n1\\n\\n1.5\\n\\n1.5\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\nFig. 8.28 Two different data sets approximated by polynomials of degree 1, 4 and 7. The bias variance tradeoff is clearly visible\\n\\n8.6 Learning of Bayesian Networks\\n\\nIn Sect. 7.4, it was shown how to build a Bayesian network manually. Now we will introduce algorithms for the induction of Bayesian networks. Similar to the learning process described previously, a Bayesian network is automatically gen- erated from a ﬁle containing training data. This process is typically decomposed into two parts. 1. Learning the network structure: For given variables, the network topology is generated from the training data. This ﬁrst step is by far the more difﬁcult one and will be given closer attention later.\\n\\n2. Learning the conditional probabilities: For known network topologies, the CPTs must be ﬁlled with values. If enough training data is available, all necessary conditional probabilities can be estimated by counting the frequencies in the data. This step can be automated relatively easily. We will now explain how Bayesian networks learn using a simple algorithm\\n\\nfrom [Jen01].\\n\\n8.6.1 Learning the Network Structure\\n\\nthe causal During the development of a Bayesian network (see Sect. 7.4.6), dependency of the variables must be taken into account in order to obtain a simple network of good quality. The human developer relies on background knowledge, which is unavailable to the machine. Therefore, this procedure cannot be easily automated.\\n\\nFinding an optimal structure for a Bayesian network can be formulated as a classic search problem. Let a set of variables V1, … , Vn and a ﬁle with training data\\n\\n6\\n\\n215\\n\\n7\\n\\n216\\n\\n8 Machine Learning and Data Mining\\n\\nbe given. We are looking for a set of directed edges without cycles between the nodes V1, … , Vn, that is, a directed acyclic graph (DAG) which reproduces the underlying data as well as possible.\\n\\nFirst we observe the search space. The number of different DAGs grows more than exponentially with the number of nodes. For ﬁve nodes there are 29281 and for nine nodes about 1015 different DAGs [MDBM00]. Thus an uninformed combinatorial search (see Sect. 6.2) in the space of all graphs with a given set of variables is hopeless if the number of variables grows. Therefore heuristic algo- rithms must be used. This poses the question of an evaluation function for Bayesian networks. It is possible to measure the classiﬁcation error of a network during application to a set of test data, as is done, for example, in C4.5 (see Sect. 8.4). For this, however, the probabilities calculated by the Bayesian network must be mapped to a decision.\\n\\nA direct measurement of the quality of a network can be taken over the prob- ability distribution. We assume that, before the construction of the network from the data, we could determine (estimate) the distribution. Then we begin the search in the space of all DAGs, estimate the value of the CPTs for each DAG (that is, for each Bayesian network) using the data, and from that we calculate the distribution and compare it to the distribution known from the data. For the comparison of distributions we will obviously need a distance metric.\\n\\nLet us consider the weather prediction example from Exercise 7.3 on page 172\\n\\nwith the three variables Sky, Bar, Prec, and the distribution\\n\\nPðSky; Bar; PrecÞ ¼ ð0:40; 0:07; 0:08; 0:10; 0:09; 0:11; 0:03; 0:12Þ:\\n\\nIn Fig. 8.29 on page 217 two Bayesian networks are presented, which we will now compare with respect to their quality. Each of these networks makes an assumption of independence, which is validated in that we determine the distribution of the network and then compare this with the original distribution (see Exercise 8.16 on page 241).\\n\\nBecause, for constant predetermined variables, the distribution is clearly repre- sented by a vector of constant length, we can calculate the Euclidian norm of the difference of the two vectors as a distance between distributions. We deﬁne\\n\\ndqðx; yÞ ¼\\n\\nX\\n\\nðxi (cid:2) yiÞ2\\n\\ni\\n\\nas the sum of the squares of the distances of the vector components and calculate the distance dq (Pa, P) ¼ 0.0029 of the distribution Pa of network 1 from the original distribution. For network 2 we calculate dq (Pb, P) ¼ 0.014. Clearly network 1 is a better approximation of the distribution. Often, instead of the square distance, the so-called Kullback–Leibler distance\\n\\nX\\n\\ndkðx; yÞ ¼\\n\\nyiðlog2\\n\\nyi (cid:2) log2\\n\\nxiÞ;\\n\\ni\\n\\n8.6 Learning of Bayesian Networks\\n\\nFig. 8.29 Two Bayesian networks Exercise 7.3 on page 172\\n\\nfor modeling the weather prediction example from\\n\\nan information theory metric, is used. With it we calculate dk (Pa, P) ¼ 0.017 and dk (Pb, P) ¼ 0.09 and come to the same conclusion as before. It is to be expected that networks with many edges approximate the distribution better than those with few edges. If all edges in the network are constructed, then it becomes very con- fusing and creates the risk of overﬁtting, as is the case in many other learning algorithms. To avoid overﬁtting, we give small networks a larger weight using a heuristic evaluation function\\n\\nf ðNÞ ¼ SizeðNÞ þ w (cid:3) dkðPN; PÞ:\\n\\nHere Size(N) is the number of entries in the CPTs and PN is the distribution of network N. w is a weight factor, which must be manually ﬁt.\\n\\nThe learning algorithm for Bayesian networks thus calculates the heuristic evaluation f(N) for many different networks and then chooses the network with the smallest value. As previously mentioned, the difﬁculty consists of the reduction of the search space for the network topology we are searching for. As a simple algorithm it is possible, starting from a (for example causal) ordering of the vari- ables V1, … , Vn, to include in the graph only those edges for which i < j. We start with the maximal model which fulﬁlls this condition. This network is shown in Fig. 8.30 for ﬁve ordered variables.\\n\\nNow, for example in the spirit of a greedy search (compare Sect. 6.3.1), one\\n\\nedge after another is removed until the value f no longer decreases.\\n\\nThis algorithm is not practical for larger networks in this form. The large search space, the manual tuning of the weight w, and the necessary comparison with a goal distribution P are reasons for this, because these can simply become too large, or the available dataset could be too small.\\n\\nIn fact, research into learning of Bayesian networks is still in full swing, and there is a large number of suggested algorithms, for example the EM algorithm (see Sect. 8.9.2), Markov chain Monte Carlo methods, and Gibbs sampling [DHS01, Jor99, Jen01, HTF09]. Besides batch learning, which has been presented here, in which the network is generated once from the whole dataset, there are also incre- mental algorithms, in which each individual new case is used to improve the\\n\\nFig. 8.30 The maximal network with ﬁve variables and edges (Vi, Vj) which fulﬁll the condition i < j\\n\\n217\\n\\n218\\n\\n8 Machine Learning and Data Mining\\n\\nnetwork. Implementations of these algorithms also exist, such as Hugin (www. hugin.com) and Bayesware (www.bayesware.com).\\n\\n8.7 The Naive Bayes Classifier\\n\\nIn Fig. 7.14 on page 166 the diagnosis of appendicitis was modeled as a Bayesian network. Because directed edges start at a diagnosis node and none end there, Bayes’ formula must be used to answer a diagnosis query. For the symptoms S1, … , Sn and the k-value diagnosis D with the values b1, … , bk we calculate the probability\\n\\nPðDjS1; . . .; SnÞ ¼\\n\\nPðS1; . . .; SnjDÞ (cid:3) PðDÞ PðS1; . . .; SnÞ\\n\\n;\\n\\nfor the diagnosis given the patient’s symptoms. In the worst case, that is, if there were no independent variables, all combinations of all symptoms and D would need to be determined for all 20 643 840 probabilities of the distribution P(S1, … , Sn, D). This would require an enormous database. In the case of LEXMED’s Bayesian net- work, the number of necessary values (in the CPTs) is reduced to 521. The network can be further simpliﬁed, however, in that we assume all symptom variables are conditionally independent given D, that is:\\n\\nPðS1; . . .; SnjDÞ ¼ PðS1jDÞ (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) PðSnjDÞ:\\n\\nThe Bayesian network for appendicitis is then simpliﬁed to the star shown in Fig. 8.31 on page 219. Thus we obtain the formula\\n\\nPðDjS1; . . .; SnÞ ¼\\n\\nQn\\n\\nPðDÞ\\n\\nPðSijDÞ\\n\\ni¼1 PðS1; . . .; SnÞ\\n\\n:\\n\\nð8:8Þ\\n\\nThe computed probabilities are transformed into a decision by a simple naive Bayes classiﬁer, which chooses the maximum P(D ¼ di |S1, … , Sn) from all values di in D. That is, it determines\\n\\ndNaive(cid:2)Bayes ¼ argmaxi2f1;...;kg\\n\\nP ðD ¼ dijS1; . . .; SnÞ:\\n\\nBecause the denominator in (8.8) is constant, it can be omitted during maximiza- tion, which results in the naive Bayes formula\\n\\ndNaive(cid:2)Bayes ¼ argmaxi2f1;...;kg\\n\\nPðD ¼ diÞ\\n\\nYn\\n\\nPðSjjDÞ:\\n\\nj¼1\\n\\n8.7 The Naive Bayes Classifier\\n\\nFig. 8.31 Bayesian network for the LEXMED application with the assumption that all symptoms are conditionally independent given the diagnosis\\n\\nBecause several nodes now have less ancestors, the number of values necessary to describe the LEXMED distribution in the CPTs decreases, according to (7.24) on page 165, to\\n\\n6 (cid:3) 4 þ 5 (cid:3) 4 þ 2 (cid:3) 4 þ 9 (cid:3) 4 þ 3 (cid:3) 4 þ 10 (cid:3) ð1 (cid:3) 4Þ þ 3 ¼ 143:\\n\\nFor a medical diagnostic system like LEXMED this simpliﬁcation would not be acceptable. But for tasks with many independent variables, naive Bayes is partly or even very well suited, as we will see in the text classiﬁcation example.\\n\\nBy the way, naive Bayes classiﬁcation is equally expressive as the linear score system described in Sect. 7.3.1 (see Exercise 8.17 on page 242). That is, all scores share the underlying assumption that all symptoms are conditionally independent given the diagnosis. Nevertheless, scores are still used in medicine today. Despite the fact that it was generated from a better, representative database, the Ohmann score compared with LEXMED in Fig. 7.10 on page 156 has a worse quality of diagnosis. Its limited expressiveness is certainly a reason for this. For example, as with naive Bayes, it is not possible to model dependencies between symptoms using scores.\\n\\n219\\n\\n220\\n\\n8 Machine Learning and Data Mining\\n\\nEstimation of Probabilities If we observe the naive Bayes formula in (8.8) on page 218, we see that the whole expression becomes zero as soon as one of the factors P(Si |D) on the right side becomes zero. Theoretically there is nothing wrong here. In practice, however, this can lead to very uncomfortable effects if the P(Si |D) are small, because these are estimated by counting frequencies and substituting them into\\n\\nPðSi ¼ x j D ¼ yÞ ¼\\n\\njSi ¼ x ^ D ¼ yj jD ¼ yj\\n\\n:\\n\\nAssume that for the variables Si : P(Si ¼ x | D ¼ y) ¼ 0.01 and that there are 40 training cases with D ¼ y. Then with high probability there is no training case with Si ¼ x and D ¼ y, and we estimate P(Si ¼ x | D ¼ y) ¼ 0. For a different value D ¼ z, assume that the relationships are similarly situated, but the estimate results in values greater than zero for all P(Si ¼ x | D ¼ z). Thus the value D ¼ z is always preferred, which does not reﬂect the actual probability distribution. Therefore, when estimating probabilities, the formula\\n\\nPðAjBÞ (cid:4)\\n\\njA ^ Bj jBj\\n\\n¼\\n\\nnAB nB\\n\\nis replaced by\\n\\nPðAjBÞ (cid:4)\\n\\nnAB þ mp nB þ m\\n\\n;\\n\\nwhere p ¼ P(A) is the a priori probability for A, and m is a constant which can be freely chosen and is known as the “equivalent data size”. The larger m becomes, the larger the weight of the a priori probability compared to the value determined from the measured frequency.\\n\\n8.7.1 Text Classification with Naive Bayes\\n\\nNaive Bayes is very successful and proliﬁc today in text classiﬁcation. Its primary, and at the same time very important, application is the automatic ﬁltering of email into desired and undesired, or spam emails. In spam ﬁlters such as SpamAssassin [Sch04], among other methods a naive Bayes classiﬁer is used that learns to sep- arate desired emails from spam. SpamAssassin is a hybrid system which performs an initial ﬁltering using black and white lists. Black lists are lists of blocked email addresses from spam senders whose emails are always deleted, and white lists are those with senders whose emails are always delivered. After this preﬁltering, the remaining emails are classiﬁed by the naive Bayes classiﬁer according to their actual content, in other words, according to the text. The detected class value is then evaluated by a score, together with other attributes from the header of the email such as the sender’s domain, the MIME type, etc., and then ﬁnally ﬁltered.\\n\\n8.7 The Naive Bayes Classifier\\n\\nHere the learning capability of the naive Bayes ﬁlter is quite important. For this the user must at ﬁrst manually classify a large number of emails as desired or spam. Then the ﬁlter is trained. To stay up to date, the ﬁlter must be regularly retrained. For this the user should correctly classify all emails which were falsely classiﬁed by the ﬁlter, that is, put them in the appropriate folders. The ﬁlter is then continually retrained with these emails.\\n\\nBeside spam ﬁltering, there are many other applications for automatic text classiﬁcation. Important applications include ﬁltering of undesired entries in Internet discussion forums, and tracking websites with criminal content such as militant or terrorist activities, child pornography or racism. It can also be used to customize search engines to ﬁt the user’s preferences in order to better classify the search results. In the industrial and scientiﬁc setting, company-wide search in databases or in the literature is in the foreground of research. Through its learning ability, a ﬁlter can adapt to the habits and wishes of each individual user.\\n\\nWe will introduce the application of naive Bayes to text analysis on a short\\n\\nexample text by Alan Turing from [Tur50]:\\n\\n“We may hope that machines will eventually compete with men in all purely intellectual ﬁelds. But which are the best ones to start with? Even this is a difﬁcult decision. Many people think that a very abstract activity, like the playing of chess, would be best. It can also be maintained that it is best to provide the machine with the best sense organs that money can buy, and then teach it to understand and speak English. This process could follow the normal teaching of a child. Things would be pointed out and named, etc. Again I do not know what the right answer is, but I think both approaches should be tried.”\\n\\nSuppose that texts such as the one given should be divided into two classes: “I” for interesting and “¬I for uninteresting. Suppose also that a database exists of texts which are already classiﬁed. Which attributes should be used? In a classical approach to the construction of a Bayesian network, we deﬁne a set of attributes such as the length of the text, the average sentence length, the relative frequency of speciﬁc punctuation marks, the frequency of several important words such as “I”, “machines”, etc. During classiﬁcation using naive Bayes, in contrast, a surprisingly primitive algorithm is selected. For each of the n word positions in the text, an attribute si is deﬁned. All words which occur in the text are allowed as possible values for all positions si. Now for the classes I and ¬I the values\\n\\nPðIjs1; . . .; snÞ ¼ c (cid:3) PðIÞ\\n\\nYn\\n\\nPðsijIÞ\\n\\ni¼1\\n\\nand P(¬I|s1, … , sn) must be calculated and then the class with the maximum value selected. In the above example with a total of 113 words, this yields\\n\\nPðIjs1; . . .; snÞ\\n\\n¼ c (cid:3) PðIÞ (cid:3) Pðs1 ¼ “We”jIÞ (cid:3) Pðs2 ¼ “may”jIÞ. . .. . Pðs113 ¼ “tried”jIÞ\\n\\n221\\n\\nð8:9Þ\\n\\n222\\n\\n8 Machine Learning and Data Mining\\n\\nand\\n\\nPð:Ijs1; . . .; snÞ\\n\\n¼ c (cid:3) ðP:Þ (cid:3) Pðs1 ¼ “We”j:IÞ\\n\\n(cid:3) Pðs2 ¼ “may”j:IÞ (cid:3) (cid:3) (cid:3) (cid:3) (cid:3)Pð s113 ¼ “tried”j:IÞ:\\n\\nThe learning here is quite simple. The conditional probabilities P(si | I), P(si | ¬I) and the a priori probabilities P(I), P(¬I) must simply be calculated. We now addi- tionally assume that the P(si | I) are not dependent on position in the text. This means, for example, that\\n\\nPðs61 ¼ “and”jIÞ ¼ Pðs69 ¼ “and”jIÞ ¼ Pðs86 ¼ “and”jIÞ:\\n\\nWe could thus use the expression P(and|I), with the new binary variable and, as the probability of the occurrence of “and” at an arbitrary position.\\n\\nThe implementation can be accelerated somewhat if we ﬁnd the frequency ni of\\n\\nevery word wi which occurs in the text and use the formula\\n\\nPðIjs1; . . .; snÞ ¼ c (cid:3) PðIÞ\\n\\nYl\\n\\nPðwijIÞ\\n\\nni\\n\\nð8:10Þ\\n\\ni¼1\\n\\nwhich is equivalent to (8.9) on page 221. Please note that the index i in the product only goes to the number l of different words which occur in the text.\\n\\nDespite its simplicity, naive Bayes delivers excellent results for text classiﬁca- tion. Spam ﬁlters which work with naive Bayes achieve error rates of well under one percent. The systems DSPAM and CRM114 can even be so well trained that they only incorrectly classify one in 7000 or 8000 emails respectively. This cor- responds to a correctness of nearly 99.99%.\\n\\n8.8 One-Class Learning\\n\\nClassiﬁcation tasks in supervised learning require all training data to be given class labels. However, there are applications for which only one class of labels is avail- able. A classic example is detecting errors in complex technical systems. The task is to recognize whether a device is defective or not. This sounds like an ordinary two-class problem. In the training data, the state of the device (good or defective) must be provided manually. In practice, the classiﬁer training happens during deployment of the device or later on demand while it is being used in production. Data capture in the error-free condition is not a problem. Collection of data from the defective system however is problematic due to the following reasons:\\n\\n(cid:129) Measurement on production equipment\\n\\nthat has intentionally been made leads to\\n\\ndefective is associated with high costs because the measurement expensive downtime.\\n\\n8.8 One-Class Learning\\n\\n(cid:129) Measurement using defective equipment with errors that actually occur in practice is often impossible because, during the deployment of the equipment, the later occurring potential errors may be unknown.\\n\\n(cid:129) No engineer knows in advance exactly which kind of errors will occur in a new piece of equipment. If now, during training, measurements with a few types of errors are taken, this may lead to poor classiﬁcation results. If the errors used for training are not representative for the device, i.e. if some types of errors are missing in the training data, then the learned class-separating hypersurface in the feature space will often lead to false-positive classiﬁcations.\\n\\nIn such cases, it is not possible to train a two-class classiﬁer in the standard way. Instead, one can use one-class learning, which gets by with data from one class during training.\\n\\nAssume that there is no negative data are available. Positive data, i.e. data from error-free operation, however, are available in sufﬁcient amount. Thus a learning algorithm is needed which can, based on the error-free data, capture all error-free operational states of the device and classify all others as negative.\\n\\nFor this purpose, there are a number of different algorithms known as one-class learning algorithms [Tax01], such as nearest neighbor data description (NNDD), support vector data description (SVDD, see also Sect. 9.6), etc. In statistics, these and related algorithms fall under the category of outlier detection.\\n\\nOne of the best known algorithms today is the local outlier factor (LOF, [BKNS00]), which calculates an outlier score for a point that is to be classiﬁed based on its density estimate. For large data sets with millions of points, and for high-dimensional data points with thousands of dimensions, the algorithms dis- cussed so far are not suitable. The EXPoSE algorithm presented in [SEP16] has a failure rate about as low as that of LOF, but it is suitable for large high-dimensional data sets due to its constant computation time. With the simple example of NNDD we will now brieﬂy introduce the one-class learning principle.\\n\\n8.8.1 Nearest Neighbor Data Description\\n\\nSimilar to the nearest neighbor method from Sect. 8.3, the nearest neighbor data description algorithm belongs to the category of lazy learning algorithms. During learning, the only things that happen are normalization and storage of the feature vectors. Thus the qualiﬁer “lazy” learning. Normalization of each individual feature is necessary in order to give each feature the same weight. Without normalization, a feature in the range ½0; 10(cid:2)4(cid:10) would be lost in the noise of another feature in the range ½(cid:2)10000; þ 10000(cid:10). Thus all features are linearly scaled onto the interval [0, 1].9 The actual nearest neighbor algorithm ﬁrst comes into play during classiﬁcation.\\n\\n9Feature scaling is necessary or advantageous for many machine learning algorithms.\\n\\n223\\n\\n224\\n\\n8 Machine Learning and Data Mining\\n\\nLet X ¼ ðx1; . . .; xnÞ be a training set consiting of n feature vectors. A new point\\n\\nq, which is yet to be classiﬁed, is accepted if\\n\\nDðq; NNðqÞÞ (cid:6) c (cid:1)D;\\n\\nð8:11Þ\\n\\nthat is, if the distance to a nearest neighbor is no larger than c (cid:1)D. Here Dðx; yÞ is a distance metric, for example, as used here, the Euclidean distance. The function\\n\\nNNðqÞ ¼ argmin\\n\\nfDðq; xÞg\\n\\nx2X\\n\\nreturns a nearest neighbor of q and\\n\\n(cid:1)D ¼\\n\\n1 n\\n\\nXn\\n\\ni¼1\\n\\nDðxi; NNðxiÞÞ\\n\\nis the mean distance of the nearest neighbors to all data points in X. To calculate (cid:1)D the distance of each point to its nearest neighbor is calculated. (cid:1)D is then the arithmetic mean of these calculated distances. One could use c ¼ 1 in Eq. 8.11, but would then obtain suboptimal classiﬁcation results. It is better to determine the parameter c using cross-validation (Sect. 8.5) such that the NNDD classiﬁer’s error rate is as small as possible.\\n\\nThe NNDD algorithm used here is a modiﬁcation of the NNDDT algorithm used in [Tax01], in which a point q is accepted if the distance to its nearest neighbor in the training data is no larger than the distance of the discovered nearest neighbor to its own nearest neighbor in the training data. Formally this reads\\n\\nDðq; NNðqÞÞ (cid:6) DðNNðqÞ; NNðNNðqÞÞÞ:\\n\\nð8:12Þ\\n\\nThis formula has several disadvantages compared to Inequality 8.5. First, the use of NNDDT (Inequality 8.12) results in intuitively ugly lines of class separation, as shown in Fig. 8.32 right on page 225 in a simple two-dimensional example. In the center of Fig. 8.32 on page 225 we can see that the bottom left data point deﬁnes a large area of inﬂuence, despite or directly because of the fact that it is far from all other data points. NNDD with Eq. 8.5 on page 201 on the other hand yields the circular class partitions of a constant radius shown in Fig. 8.32 left on page 225. This intuitive conjecture is conﬁrmed by practical experiments.\\n\\n8.9 Clustering\\n\\nIf we search in a search engine for the term “mars”, we will get results like “the planet mars” and “Chocolate, confectionery and beverage conglomerate” which are semantically quite different. In the set of discovered documents there are two\\n\\n8.9 Clustering\\n\\nNNDD with γ = 0.5\\n\\nNNDDT with γ = 0.5\\n\\nNNDDT with γ = 1.0\\n\\nFig. 8.32 NNDD and NNDDT applied to a set of 16 selected two-dimensional data points (black points). In each case, class membership has been determined for 10,000 randomly selected points. The points marked in red were classiﬁed as positive, i.e. assigned the training set’s class, while the blue points are classiﬁed as negative\\n\\nFig. 8.33 Simple two-dimensional example with four clearly separated clusters\\n\\nnoticeably different clusters. Google, for example, still lists the results in an unstructured way. It would be better if the search engine separated the clusters and presented them to the user accordingly because the user is usually interested in only one of the clusters.\\n\\nThe distinction of clustering in contrast to supervised learning is that the training data are unlabeled. Thus the pre-structuring of the data by the supervisor is missing. Rather, ﬁnding structures is the whole point of clustering. In the space of training data, accumulations of data such as those in Fig. 8.33 are to be found. In a cluster, the distance of neighboring points is typically smaller than the distance between points of different clusters. Therefore the choice of a suitable distance metric for points, is of fundamental importance. As before, we assume in the following that every data object is described by a vector of numerical attributes.\\n\\nthat\\n\\nis, for objects to be grouped and for clusters,\\n\\n8.9.1 Distance Metrics\\n\\nAccordingly for each application, the various distance metrics are deﬁned for the distance d between two vectors x and y in ℝn. The most common is the Euclidean distance\\n\\n225\\n\\n226\\n\\n8 Machine Learning and Data Mining\\n\\ns\\n\\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Xn\\n\\ndeðx; yÞ ¼\\n\\nðxi (cid:2) yiÞ2\\n\\n:\\n\\ni¼1\\n\\nSomewhat simpler is the sum of squared distances\\n\\ndqðx; yÞ ¼\\n\\nXn\\n\\nðxi (cid:2) yiÞ2;\\n\\ni¼1\\n\\nwhich, for algorithms in which only distances are compared, is equivalent to the Euclidean distance (Exercise 8.20 on page 242). Also used are the aforementioned Manhattan distance\\n\\ndmðx; yÞ ¼\\n\\nXn\\n\\njxi (cid:2) yij\\n\\ni¼1\\n\\nas well as the distance of the maximum component\\n\\nd1ðx; yÞ ¼ max i¼1;...;n\\n\\njxi (cid:2) yij\\n\\nwhich is based on the maximum norm. During text classiﬁcation, the normalized projection of the two vectors on each other, that is, the normalized scalar product\\n\\nx y jxj jyj\\n\\nis frequently calculated, where | x | is the Euclidian norm of x. Because this formula is a metric for the similarity of the two vectors, as a distance metric the inverse\\n\\ndsðx; yÞ ¼\\n\\njxj jyj x y\\n\\ncan be used, or “>” and “<” can be swapped for all comparisons. In the search for a text, the attributes x1, … , xn are calculated similarly to naive Bayes as components of the vector x as follows. For a dictionary with 50,000 words, the value xi equals the frequency of the ith dictionary word in the text. Since normally almost all components are zero in such a vector, during the calculation of the scalar product, nearly all terms of the summation are zero. By exploiting this kind of information, the implementation can be sped up signiﬁcantly (Exercise 8.21 on page 243).\\n\\n8.9.2 k-Means and the EM Algorithm\\n\\nthe k-means Whenever the number of clusters is already known in advance, algorithm can be used. As its name suggests, k clusters are deﬁned by their average\\n\\n8.9 Clustering\\n\\nvalue. First the k cluster midpoints l1, …, lk are initialized to the coordinates of k randomly or manually selected data points. (Note: Selection of arbitrary points (which are not data points) as cluster centers may lead to empty clusters.) Then the following two steps are repeatedly carried out: (cid:129) Classiﬁcation of all data to their nearest cluster midpoint (cid:129) Recomputation of the cluster midpoint. The following scheme results as an algorithm:\\n\\nK-MEANS(x1, . . . , xn, k) µ initialize cluster centers 1 Repeat\\n\\n= , . . . , x i 1\\n\\nµk\\n\\nx= i\\n\\nk (e.g. randomly)\\n\\nclassify x1, . . . , xn to each’s nearest µi , . . . , µk recalculate µ Until no change in µ 1 , . . . , µk) Return(µ 1\\n\\n1\\n\\n, . . . , µk\\n\\nThe calculation of the cluster midpoint l for points x1, … , xl is done by\\n\\nl ¼\\n\\n1 l\\n\\nXl\\n\\ni¼1\\n\\nxi:\\n\\nThe execution on an example is shown in Fig. 8.34 for the case of two classes. We see how after three iterations, the class centers, which were ﬁrst randomly chosen, stabilize. While this algorithm does not guarantee convergence, it usually converges very quickly. This means that the number of iteration steps is typically much smaller than the number of data points. Its complexity is O(ndkt), where n is\\n\\nFig. 8.34 k-means with two classes (k = 2) applied to 30 data points. Far left is the dataset with the initial centers, and to the right is the cluster after each iteration. After three iterations convergence is reached\\n\\n227\\n\\n228\\n\\n8 Machine Learning and Data Mining\\n\\nthe total number of points, d the dimensionality of the feature space, and t the number of iteration steps.\\n\\nIn many cases, the necessity of giving the number of classes in advance poses an inconvenient limitation. Therefore we will next introduce an algorithm which is more ﬂexible.\\n\\nBefore that, however, we will mention the EM algorithm, which is a continuous variant of k-means, for it does not make a ﬁrm assignment of the data to classes, rather, for each point it returns the probability of it belonging to the various classes. Here we must assume that the type of probability distribution is known. Often the normal distribution is used. The task of the EM algorithm is to determine the parameters (mean li and covariance matrix Ri of the k multi-dimensional normal distributions) for each cluster. Similarly to k-means, the two following steps are repeatedly executed: Expectation: For each data point the probability P(Cj | xi) that it belongs to each cluster is calculated.\\n\\nMaximization: Using the newly calculated probabilities, the parameters of the\\n\\ndistribution are recalculated.\\n\\nThereby a softer clustering is achieved, which in many cases leads to better results. This alternation between expectation and maximization gives the algorithm its name. In addition to clustering, for example, the EM algorithm is used to learn Bayesian networks [DHS01].\\n\\n8.9.3 Hierarchical Clustering\\n\\nIn hierarchical clustering we begin with n clusters consisting of one point each. Then the nearest neighbor clusters are combined until all points have been com- bined into a single cluster, or until a termination criterion has been reached. We obtain the scheme\\n\\nHIERARCHICALCLUSTERING(x1, . . . , xn, k) initialize C1 = {x1}, . . . , Cn = {xn} Repeat\\n\\nFind two clusters Ci and Cj with the smallest distance Combine Ci and Cj\\n\\nUntil Termination condition reached Return(tree with clusters)\\n\\nThe termination condition could be chosen as, for example, a desired number of clusters or a maximum distance between clusters. In Fig. 8.35 on page 229 this algorithm is represented schematically as a binary tree, in which from bottom to top\\n\\n8.9 Clustering\\n\\nFig. 8.35 In hierarchical clustering, the two clusters with the smallest distance are combined in each step\\n\\nFig. 8.36 The nearest neighbor algorithm applied to the data from Fig. 8.34 on page 227 at different levels with 12, 6, 3, 1 clusters\\n\\nin each step, that is, at each level, two subtrees are connected. At the top level all points are uniﬁed into one large cluster.\\n\\nIt is so far unclear how the distances between the clusters are calculated. Indeed, in the previous section we deﬁned various distance metrics for points, but these cannot be used on clusters. A convenient and often used metric is the distance between the two closest points in the two clusters Ci and Cj:\\n\\ndminðCi; CjÞ ¼ min\\n\\nx2Ci; y2Cj\\n\\ndðx; yÞ:\\n\\nThus we obtain the nearest neighbor algorithm, whose application is shown in Fig. 8.36.10 We see that this algorithm generates a minimum spanning tree.11 The example furthermore shows that the two described algorithms generate quite different clusters. This tells us that for graphs with clusters which are not clearly separated, the result depends heavily on the algorithm or the chosen distance metric.\\n\\nFor an efﬁcient implementation of this algorithm, we ﬁrst create an adjacency matrix in which the distances between all points is saved, which requires O(n2) time\\n\\n10The nearest neighbor algorithm is not to be confused with the nearest neighbor method for classiﬁcation from Sect. 8.3. 11A minimum spanning tree is an acyclic, undirected graph with the minimum sum of edge lengths.\\n\\n229\\n\\n230\\n\\n8 Machine Learning and Data Mining\\n\\nand memory. If the number of clusters does not have an upper limit, the loop will iterate n − 1 times and the asymptotic computation time becomes O(n3).\\n\\nTo calculate the distance between two clusters, we can also use the distance\\n\\nbetween the two farthest points\\n\\ndmaxðCi; CjÞ ¼ max\\n\\nx2Ci; y2Cj\\n\\ndðx; yÞ\\n\\nand obtain the farthest neighbor algorithm. Alternatively, the distance of the cluster’s midpoint dl (Ci, Cj) = d(li, lj) is used. Besides the clustering algorithm presented here, there are many others, for which we direct the reader to [DHS01] for further study.\\n\\n8.9.4 How is the Number of Clusters Determined?\\n\\nIn all of the clustering algorithms discussed so far, the user must specify the number of clusters. In many cases, users have no idea what a sensible number of clusters would be, rather they simply want a “good” partitioning, i.e. separation of their n data points into clusters. Because there is, unfortunately, no absolute and generally accepted standard for the quality of a partition, we present a approved heuristic method for evaluating a clustering using the silhouette width criterion presented in [Rou87]. Before we discuss this criterion, let us ﬁrst show how it can be applied. Assume that we had a program that could combinatorially enumerate all possible partitions for a set of n data points. Then we could simply apply the silhouette width criterion to each partition and use the maximum to determine the best partition. But because the number of partitions grows quickly with the number of data points, this method is not practical. We could, however, simply apply one of the presented clustering algorithms (for example k-means), let it run for k from 1 to n, and then use the silhouette width criterion to determine the best k and its respective partition.\\n\\nWhat we are still missing is such a criterion that measures the quality of a partition. The idea, very roughly, is as follows: the mean distance between two arbitrary points within the same cluster should be smaller than the distance between two arbitrary points that are in different neighboring clusters. The ratio of the mean distance between points in neighboring clusters and the mean distance between points within the cluster should be maximized.\\n\\nLet the data points ðx1; . . .; xnÞ be given, as well as a clustering function\\n\\nc : xi 7! cðxiÞ;\\n\\nwhich assigns each point to a cluster. Let (cid:1)dði; ‘Þ be the mean distance from xi to all points (6¼ xi) in cluster ‘. Then aðiÞ ¼ (cid:1)dði; cðxiÞÞ is the mean distance from xi to all other points in its own cluster. If xi is the only point in the cluster, we set aðiÞ ¼ 0.\\n\\nbðiÞ ¼ min j6¼cðxiÞ\\n\\nf (cid:1)dði; jÞg\\n\\nis the smallest mean distance from point xi to a cluster to which xi does not belong.\\n\\n8.9 Clustering\\n\\nWe now deﬁne the function 8 <\\n\\nsðiÞ ¼\\n\\n:\\n\\n0 bðiÞ (cid:2) aðiÞ maxfaðiÞ; bðiÞg\\n\\nif aðiÞ ¼ 0\\n\\notherwise\\n\\nthe point xi ﬁts into a cluster. Then we have\\n\\nwhich measures how well (cid:2)1 (cid:6) sðiÞ (cid:6) 1 and 8 <\\n\\nif xi is in the middle of a clearly delineated cluster. if xi lies on the border of two clusters.\\n\\n1 0 (cid:2)1 if xi is in the “wrong” cluster.\\n\\nsðiÞ ¼\\n\\n:\\n\\nReﬂecting the idea formulated above, we now seek a partition that maximizes the mean\\n\\nS ¼\\n\\n1 n\\n\\nXn\\n\\ni¼1\\n\\nsðiÞ\\n\\nof s(i) over all points, called silhouette width criterion. For k-means clustering, this can be done with the OMRk algorithm presented in [BJCdC14]:\\n\\nOMRk(x 1, . . . , x n, p, kmax) S∗ = −∞ For k = 2 To kmax For i=1 To p\\n\\nGenerate a random partition with k clusters (initialization) Obtain a partition P with k-means Determine S for P If S > S∗ Then\\n\\nS∗ = S; k∗ = k; P ∗ = P\\n\\nReturn (k∗, P ∗)\\n\\nThis algorithm repeatedly applies k-means for different values of k. Because the result of k-means depends heavily on its initialization, for every k, p different random initializations are tried in the inner loop, and then the function returns the optimal k(cid:11) and the corresponding best partition P(cid:11). The OMRk algorithm can also be used with other clustering algorithms such as the EM algorithm and hierarchical clustering.\\n\\nExample 4 The top left diagram in Fig. 8.37 on page 232 shows a set of two-dimensional data points with four obvious clusters. The OMRk algorithm was run on this data with p ¼ 30 and kmax ¼ 9. In the following eight diagrams, the ﬁgure shows the best partition together with its quality S for each k. The algorithm ﬁnds the maximum value S ¼ 0:786 at k ¼ 5. This does not reﬂect the natural (to\\n\\n231\\n\\n232\\n\\n8 Machine Learning and Data Mining\\n\\nFig. 8.37 Results of the OMRk algorithm for k ¼ 2 to 9. The best value of S ¼ 0:786 was found with k ¼ 5\\n\\nthe human eye) grouping of the points into four clusters. In the partition found for k ¼ 4, several points which should belong to the blue cluster are assigned to the red cluster. This is because k-means minimizes the distance to the cluster center point, and the points are closer to the center of the red cluster. The higher density of points in the red cluster is not taken into account.\\n\\nThe EM algorithm, which can approximate the difference in point density by using a normal distribution, performs signiﬁcantly better. As shown in Fig. 8.38 on page 233, the EM algorithm ﬁnds almost exactly the same aforementioned natural distribution for k ¼ 4.\\n\\nFinally we should reiterate that all of the methods we have described are only heuristic greedy search algorithms which do not explore the entire space of all partitions. The silhouette width criterion described is only a heuristic estimation of a partition’s “quality”. There can be no absolute measure of quality for partitions because, as we have shown in even such a simple two-dimensional example, dif- ferent people can in certain cases prefer very different groupings. We should also mention that there are many other interesting clustering algorithms such as the density-based DBSCAN algorithm.\\n\\n8.10 Data Mining in Practice\\n\\nFig. 8.38 A partition generated by the EM algorithm with k ¼ 4\\n\\n8.10 Data Mining in Practice\\n\\nAll the learning algorithms presented so far can be used as tools for data mining. For the user it is, however, sometimes quite troublesome to get used to new soft- ware tools for each application and furthermore to put the data to be analyzed into the appropriate format for each particular case.\\n\\nA number of data mining systems address these problems. Most of these systems offer a convenient graphical user interface with diverse tools for visualization of the data, for preprocessing such as manipulation of missing values, and for analysis. For analysis, the learning algorithms presented here are used, among others.\\n\\nThe comprehensive open-source Java library WEKA deserves a special mention. It offers a large number of algorithms and simpliﬁes the development of new algorithms. The freely available system KNIME, which we will brieﬂy introduce in the following section, offers a convenient user interface and all the types of tools mentioned above. KNIME also uses WEKA modules. Furthermore it offers a simple way of controlling the data ﬂow of the chosen visualization, preprocessing, and analysis tools with a graphical editor. A large number of other systems meanwhile offer quite similar functionality, such as the open-source project RapidMiner (www. rapidminer.com), the system Clementine (www.spss.com/clementine) sold by SPSS, and the KXEN analytic framework (www.kxen.com).\\n\\n8.10.1 The Data Mining Tool KNIME\\n\\nUsing the LEXMED data, we will now show how to extract knowledge from data using KNIME (Konstanz Information Miner, www.knime.org). First we generate a deci- sion tree as shown in Fig. 8.39 on page 234. After creating a new project, a workﬂow is built graphically. To do this, the appropriate tools are simply taken out of the node repository with the mouse and dragged into the main workﬂow window.\\n\\n233\\n\\n234\\n\\n8 Machine Learning and Data Mining\\n\\nThe training and test data from the C4.5 ﬁle can be read in with the two ﬁle reader nodes without any trouble. These nodes can, however, also be quite easily conﬁgured for other ﬁle formats. The sideways trafﬁc light under the node shows its status (not ready, conﬁgured, executed). Then node J48 is selected from the WEKA library [WF01], which contains a Java implementation of C4.5. The conﬁguration for this is quite simple. Now a predictor node is chosen, which applies the generated tree to the test data. It inserts a new column into the test data table “Prediction” with the classiﬁcation generated by the tree. From there the scorer node calculates the confusion matrix shown in the ﬁgure, which gives the number of correctly classiﬁed cases for both classes in the diagonal, and additionally the number of false positive and false negative data points.\\n\\nOnce the ﬂow is completely built and all nodes conﬁgured, then an arbitrary node can be executed. It automatically ensures that predecessor nodes are executed, if necessary. The J48 node generates the view of the decision tree, shown in the right of the ﬁgure. This tree is identical with the one generated by C4.5 in Sect. 8.4.5, although here the node TRekt<=378 is shown collapsed.\\n\\nFor comparison, a project for learning a multilayer perceptron (see Sect. 9.5) is shown in Fig. 8.40 on page 235. This works similarly to the previously introduced\\n\\nFig. 8.39 The KNIME user interface with two additional views, which show the decision tree and the confusion matrix\\n\\n8.10 Data Mining in Practice\\n\\nFig. 8.40 The KNIME user interface with the workﬂow window, the learning curve, and the confusion matrix\\n\\nlinear perceptron, but it can also divide non linearly separable classes. Here the ﬂow is somewhat more complex. An extra node is needed for error handling missing values when preprocessing each of the two ﬁles. We set it so that those lines will be deleted. Because neural networks cannot deal with arbitrary values, the values of all variables are scaled linearly into the interval [0, 1] using the “normalizer” node.\\n\\nAfter applying the RProp learner, an improvement on backpropagation (see Sect. 9.5), we can analyze the progression in time of the approximation error in the learning curve shown. In the confusion matrix, the scorer outputs the analysis of the test data. The “CSV Writer” node at the bottom right serves to export the result ﬁles, which can then be used externally to generate the ROC curve shown in Fig. 7.10 on page 156, for which there is unfortunately no KNIME tool (yet).\\n\\n235\\n\\n236\\n\\n8 Machine Learning and Data Mining\\n\\nIn summary, we can say the following about KNIME (and similar tools): for projects with data analysis requirements which are not too exotic, it is worthwhile to work with such a powerful workbench for analysis of data. The user already saves a lot of time with the preprocessing stage. There is a large selection of easily usable “mining tools”, such as nearest neighbor classiﬁers, simple Bayesian network learning algorithms, as well as the k-means clustering algorithm (Sect. 8.9.2). Evaluation of the results, for example cross validation, can be easily carried out. It remains to be mentioned, that besides those shown, there are many other tools for visualization of the data. Furthermore, the developers of KNIME have made an extension to KNIME available, with which the user can program his own tools in Java or Python.\\n\\nHowever, it should also be mentioned that the user of this type of data mining system should bring along solid prior knowledge of machine learning and the use of data mining techniques. The software alone cannot analyze data, but in the hand of a specialist it becomes a powerful tool for the extraction of knowledge from data. For the beginner in the fascinating ﬁeld of machine learning, such a system offers an ideal and simple opportunity to test one’s knowledge practically and to compare the various algorithms. The reader may verify this in Exer- cise 8.22 on page 243.\\n\\n8.11 Summary\\n\\nWe have thoroughly covered several algorithms from the established ﬁeld of supervised learning, including decision tree learning, Bayesian networks, and the nearest neighbor method. These algorithms are stable and efﬁciently usable in various applications and thus belong to the standard repertoire in AI and data mining. The same is true for the clustering algorithms, which work without a “supervisor” and can be found, for example, in search engine applications. Rein- forcement learning as another ﬁeld of machine learning uses no supervisor either. In contrast to supervised learning, where the learner receives the correct actions or answers as the labels in the training data, in reinforcement learning only now and then positive or negative feedback is received from the environment. In Chap. 10 we will show how this works. Not quite as hard is the task in semi-supervised learning, a young sub-area of machine learning, where only very few out of a large number of training data are labeled.\\n\\nSupervised learning is now a well established area with lots of successful applications. For supervised learning of data with continuous labels any function approximation algorithm can be employed. Thus there are many algorithms from various areas of mathematics and computer science. In Chap. 9 we will introduce various types of neural networks, least squares algorithms and support vector machines, which are all function approximators. Nowadays, Gaussian processes are very popular because they are very universal and easy to apply and provide the user with an estimate of the uncertainty of the output values [RW06].\\n\\n8.11 Summary\\n\\nThe following taxonomy gives an overview of the most important learning\\n\\nalgorithms and their classiﬁcation. Supervised learning (cid:129) Lazy learning\\n\\n– k nearest neighbor method (classiﬁcation + approximation) – Locally weighted regression (approximation) – Case-based learning (classiﬁcation + approximation)\\n\\n(cid:129) Eager learning\\n\\n– Decision trees induction (classiﬁcation) – Learning of Bayesian networks (classiﬁcation + approximation) – Neural networks (classiﬁcation + approximation) – Gaussian processes (classiﬁcation + approximation) – Support vector machines – Wavelets, splines, radial basis functions, …\\n\\nUnsupervised learning (clustering) (cid:129) Nearest neighbor algorithm (cid:129) Farthest neighbor algorithm (cid:129) k-means (cid:129) Neural networks Reinforcement learning (cid:129) Value iteration (cid:129) Q learning (cid:129) TD learning (cid:129) Policy gradient methods (cid:129) Neural networks What has been said about supervised learning is only true, however, when working with a ﬁxed set of known attributes. An interesting, still open ﬁeld under intensive research is automatic feature selection. In Sect. 8.4, for learning with decision trees, we presented an algorithm for the calculation of the information gain of attributes that sorts the attributes according to their relevance and uses only those which improve the quality of the classiﬁcation. With this type of method it is possible to automatically select the relevant attributes from a potentially large base set. This base set, however, must be manually selected.\\n\\nStill open and also not clearly deﬁned is the question of how the machine can ﬁnd new attributes. Let us imagine a robot which is supposed to pick apples. For this he must learn to distinguish between ripe and unripe apples and other objects. Traditionally we would determine certain attributes such as the color and form of pixel regions and then train a learning algorithm using man- ually classiﬁed images. It is also possible that for example a neural network could be trained directly with all pixels of the image as input, which for high resolution is linked with severe computation time problems, however. Approa- features would be ches which automatically make suggestions for desired here.\\n\\nrelevant\\n\\n237\\n\\n238\\n\\n8 Machine Learning and Data Mining\\n\\nClustering provides one approach to feature selection. Before training the apple recognition machine, we let clustering run on the data. For (supervised) learning of the classes apple and non apple, the input is no longer all of the pixels, rather only the classes found during clustering, potentially together with other attributes. Clustering at any rate can be used for automatic, creative “dis- covery” of features. It is, however, uncertain whether the discovered features are relevant. The relatively young but successful deep learning algorithms, which combine large, complex neural networks with unsupervised pre-processing, rep- resent a breakthrough in feature generation, and therefore a milestone in AI. We will cover this topic in Sect. 9.7.\\n\\nThe following problem is yet more difﬁcult: assume that the video camera used for apple recognition only transmits black and white images. The task can no longer be solved well. It would be nice if the machine would be creative on its own account, for example by suggesting that the camera should be replaced with a color camera. This would be asking for too much today.\\n\\nIn addition to specialized works about all subﬁelds of machine learning, there are very good textbooks such as [Mit97, Bis06, DHS01, HTF09, Alp04]. Above all, I recommend the excellent book by Peter Flach [Fla12], which paves the way to a deep understanding of the concepts and algorithms of machine learning with very good explanations and examples. For current research results, a look into the freely the available Journal of Machine Learning Research (http://jmlr.csail.mit.edu), Machine Learning Journal, as well as the proceedings of the International Con- ference on Machine Learning (ICML) is recommended. For every developer of learning algorithms, the Machine Learning Repository [DNM98] of the University of California at Irvine (UCI) is interesting, with its large collection of training and test data for learning algorithms and data mining tools. MLOSS, which stands for machine learning open source software, is an excellent directory of links to freely available software (www.mloss.org).\\n\\n8.12 Exercises\\n\\n8.12.1 Introduction\\n\\nExercise 8.1\\n\\n(a) Specify the task of an agent which should predict the weather for the next day given measured values for temperature, air pressure, and humidity. The weather should be categorized into one of the three classes: sunny, cloudy, and rainy.\\n\\n(b) Describe the structure of a ﬁle with the training data for this agent. Exercise 8.2 Show that the correlation matrix is symmetric and that all diagonal elements are equal to 1.\\n\\n8.12 Exercises\\n\\n8.12.2 The Perceptron\\n\\nExercise 8.3 Apply the perceptron learning rule to the sets\\n\\nM þ ¼ fð0; 1:8Þ; ð2; 0:6Þg and M(cid:2) ¼ fð(cid:2)1:2; 1:4Þ; ð0:4; (cid:2)1Þg\\n\\nfrom Example 8.2 on page 186 and give the result of the values of the weight vector.\\n\\nExercise 8.4 Given the table to the right with the training data:\\n\\n(a) Using a graph, show that the data are linearly\\n\\nseparable.\\n\\n(b) Manually determine the weights w1 and w2, as well as the threshold H of a perceptron (with threshold) which correctly classiﬁes the data. (c) Program the perceptron learning rule and apply your program to the table. Compare the discovered weights with the manually calculated ones.\\n\\nNum.\\n\\n1 2 3 4 5 6 7 8\\n\\nx1 6 7 8 9 8 8 9 9\\n\\nx2 1 3 2 0 4 6 2 5\\n\\nExercise 8.5\\n\\n(a) Give a visual interpretation of the heuristic initialization\\n\\nX\\n\\nX\\n\\nw0 ¼\\n\\nxi (cid:2)\\n\\nxi;\\n\\nxi2M þ\\n\\nxi2M(cid:2)\\n\\nof the weight vector described in Sect. 8.2.2.\\n\\n(b) Give an example of a linearly separable dataset for which this heuristic does\\n\\nnot produce a dividing line.\\n\\n8.12.3 Nearest Neighbor Method\\n\\nExercise 8.6\\n\\n(a) Show the Voronoi diagram for neighboring\\n\\nsets of points.\\n\\n(b) Then draw in the class division lines.\\n\\nExercise 8.7 Let the table with training data from Exercise 8.4 be given. In the following, use the Manhattan distance d(a, b), deﬁned as d(a, b) ¼ |a1 − b1| +\\n\\n239\\n\\nClass\\n\\n0 0 0 0 1 1 1 1\\n\\n240\\n\\n8 Machine Learning and Data Mining\\n\\n|a2 − b2|, to determine the distance d between two data points a ¼ (a1, a2) and b ¼ (b1, b2). (a) Classify the vector v = (8, 3.5) with the nearest neighbor method. (b) Classify the vector v = (8, 3.5) with the k nearest neighbor method for\\n\\nk = 2, 3, 5.\\n\\n❄Exercise 8.8\\n\\n(a) Show that in a two-dimensional feature space it is reasonable, as claimed in (8.3) on page 194, to weight the k nearest neighbors by the inverse of the squared distance.\\n\\n(b) Why would a weighting using w0\\n\\ni ¼\\n\\n1\\n\\n1 þ adðx; xiÞ make less sense?\\n\\nExercise 8.9 (a) Write a program implementing the k-Nearest-Neighbor-Method for classiﬁcation. (b) Apply this program for k = 1 (i.e. one nearest neighbor) to the Lexmed-data from http://www.hs-weingarten.de/*ertel/kibuch/uebungen/. Use for training the ﬁle app1.data and for testing the ﬁle app1.test. Do not forget to nor- malize all features before training.\\n\\n(c) Apply leave-one-out cross-validation on the whole data set app1.data [ to determine the optimal number of nearest neighbors k and\\n\\napp1.test determine the classiﬁcation error.\\n\\n(d) Repeat the cross-validation with not normalized data. (e) Compare your results with those given in Fig. 9.14 on page 266 for the least\\n\\nsquares method and RProp.\\n\\n8.12.4 Decision Trees\\n\\nthe deﬁnition 0 log 2 0 := 0 is\\n\\nExercise 8.10 Show that in other words, that the function f(x) = x log2 x thereby becomes continuous in the origin.\\n\\nreasonable,\\n\\nExercise 8.11 Determine the entropy for the following distributions. (cid:4)\\n\\n(cid:5)\\n\\n(cid:4)\\n\\n(cid:5)\\n\\nðdÞ\\n\\n(a) (cid:4)\\n\\n1 2\\n\\n(1, 0, 0, 0, 0)\\n\\n; 1 4\\n\\n; 1 8\\n\\n; 1 16\\n\\n; 1 16\\n\\n(cid:5)\\n\\nðbÞ\\n\\nðeÞ\\n\\n(cid:4)\\n\\n1 5\\n\\n1 2 ; 1 5\\n\\n; 1 2 ; 1 5\\n\\n; 0; 0; 0\\n\\n; 1 5\\n\\n; 1 5\\n\\n(cid:5)\\n\\nðfÞ\\n\\nðcÞ (cid:4)\\n\\n1 2\\n\\n; 1 ; 1 1 4 2 4 ; 1 ; 1 ; 1 16 8 4\\n\\n; 0; 0\\n\\n; 1 32\\n\\n; . . .\\n\\nExercise 8.12 (a) Show that the two different deﬁnitions of entropy from (7.9) on page 138 and Deﬁnition 8.4 on page 202 only differ by a constant factor, that is, that\\n\\n(cid:5)\\n\\n8.12 Exercises\\n\\nXn\\n\\npi log2\\n\\npi ¼ c\\n\\nXn\\n\\npi ln pi\\n\\ni¼1\\n\\ni¼1\\n\\nand give the constant c.\\n\\n(b) Show that for the MaxEnt method and for decision trees it makes no difference\\n\\nwhich of the two formulas we use.\\n\\nExercise 8.13 Develop a decision tree for the dataset D from Exercise 8.4 on page 239. (a) Treat both attributes as discrete. (b) Now treat attribute x2 as continuous and x1 as discrete. (c) Have C4.5 generate a tree with both variants. Use -m 1 -t 10 as parameters\\n\\nin order to get different suggestions.\\n\\nExercise 8.14 Given the following decision tree and tables for both training and test data:\\n\\nTraining data\\n\\nt\\n\\nt\\n\\nt\\n\\nt\\n\\nt\\n\\nA B\\n\\nt t t f f f\\n\\nt f t f f t\\n\\nC\\n\\nf f t t f t\\n\\nClass t t f f f t\\n\\nTest data\\n\\nA B\\n\\nt t f f\\n\\nt f t f\\n\\nC\\n\\nf f f t\\n\\n(a) Give the correctness of the tree for the training and test data. (b) Give a propositional logic formula equivalent to the tree. (c) Carry out pruning on the tree, draw the resulting tree, and give its correctness\\n\\nfor the training and test data.\\n\\n❄Exercise 8.15\\n\\n(a) When determining the current attribute, the algorithm for generating decision trees (Fig. 8.26 on page 207) does not eliminate the attributes which have already been used further up in the tree. Despite this, a discrete attribute occurs in a path at most once. Why?\\n\\n(b) Why can continuous attributes occur multiple times?\\n\\n8.12.5 Learning of Bayesian Networks\\n\\nExercise 8.16 Use the distribution given in Exercise 7.3 on page 172 and deter- mine the CPTs for the three Bayesian networks:\\n\\nClass t t f f\\n\\n241\\n\\n242\\n\\n8 Machine Learning and Data Mining\\n\\n(a)\\n\\n(b)\\n\\n(c)\\n\\n(d) Determine the distribution for the two networks from (a) and (b) and compare\\n\\nthese with the original distribution. Which network is “better”?\\n\\n(e) Now determine the distribution for network (c). What does occur to you? Justify! ❄❄ Exercise 8.17 Show that for binary variables S1, … , Sn and binary class variable\\n\\nK, a linear score of the form (cid:3)\\n\\ndecision ¼\\n\\nif w1S1 þ (cid:3) (cid:3) (cid:3) þw nSn [ H\\n\\npositive negative else\\n\\nis equally expressive in relation to the perceptron and to the naive Bayes classiﬁer, which both decide according to the formula\\n\\ndecision ¼\\n\\n(cid:3)\\n\\nif PðKjS1; . . .; SnÞ [ 1=2;\\n\\npositive negative else\\n\\nExercise 8.18 In the implementation of text classiﬁcation with naive Bayes, exponent underﬂow can happen quickly because the factors P(wi |K) (which appear in (8.10) on page 222) are typically all very small, which can lead to extremely small results. How can we mitigate this problem?\\n\\n➳❄ Exercise 8.19 Write a program for naive Bayes text analysis. Then train and test it on text benchmarks using a tool of your choice. Counting the frequency of words in the text can be done easily in Linux with the command\\n\\ncat <datei> | tr -d “[:punct:]” | tr -s “[:space:]” “\\\\n” | sort | uniq -ci\\n\\nObtain the Twenty Newsgroups data by Tom Mitchell in the UCI machine learning benchmark collection (Machine Learning Repository) [DNM98]. There you will also ﬁnd a reference to a naive Bayes program for text classiﬁcation by Mitchell.\\n\\n8.12.6 Clustering\\n\\nExercise 8.20 Show that for algorithms which only compare distances, applying a strictly monotonically increasing function f to the distance makes no difference. In other words you must show that the distance d1(x, y) and the distance d2(x, y): = f (d1(x,y)) lead to the same result with respect to the ordering relation.\\n\\n8.12 Exercises\\n\\nExercise 8.21 Determine the distances ds (scalar product) of the following texts to each other. x1: We will introduce the application of naive Bayes to text analysis on a short\\n\\nexample text by Alan Turing from [Tur50].\\n\\nx2: We may hope that machines will eventually compete with men in all purely\\n\\nintellectual ﬁelds. But which are the best ones to start with?\\n\\nx3: Again I do not know what the right answer is, but I think both approaches\\n\\nshould be tried.\\n\\n8.12.7 Data Mining\\n\\nExercise 8.22 Use KNIME (www.knime.de) and (a) Load the example ﬁle with the Iris data from the KNIME directory and experiment with the various data representations, especially with the scatter- plot diagrams.\\n\\n(b) First train a decision tree for the three classes, and then train an RProp\\n\\nnetwork.\\n\\n(c) Load the appendicitis data on this book’s website. Compare the classiﬁcation quality of the k nearest neighbor method to that of an RProp network. Optimize k as well as the number of hidden neurons of the RProp network. (d) Obtain a dataset of your choice from the UCI data collection for data mining at http://kdd.ics.uci.edu or for machine learning at http://mlearn.ics.uci.edu/ MLRepository.html and experiment with it.\\n\\n243\\n\\nNeural Networks\\n\\nNeural networks are networks of nerve cells in the brains of humans and animals. The human brain has about 100 billion nerve cells. We humans owe our intelligence and our ability to learn various motor and intellectual capabilities to the brain’s complex relays and adaptivity. For many centuries biologists, psychologists, and doctors have tried to understand how the brain functions. Around 1900 came the revolutionary realization that these tiny physical building blocks of the brain, the nerve cells and their connections, are responsible for awareness, associations, thoughts, consciousness, and the ability to learn.\\n\\nThe ﬁrst big step toward neural networks in AI was made 1943 by McCulloch and Pitts in an article entitled “A logical calculus of the ideas immanent in nervous activity” [AR88]. They were the ﬁrst to present a mathematical model of the neuron as the basic switching element of the brain. This article laid the foundation for the construction of artiﬁcial neural networks and thus for this very important branch of AI.\\n\\nWe could consider the ﬁeld of modeling and simulation of neural networks to be the bionics branch within AI.1 Nearly all areas of AI attempt to recreate cognitive processes, such as in logic or in probabilistic reasoning. However, the tools used for modeling—namely mathematics, programming languages, and digital computers— have very little in common with the human brain. With artiﬁcial neural networks, the approach is different. Starting from knowledge about the function of natural neural networks, we attempt to model, simulate, and even reconstruct them in hardware. Every researcher in this area faces the fascinating and exciting challenge of comparing results with the performance of humans.\\n\\nIn this chapter we will attempt to outline the historical progression by deﬁning a model of the neuron and its interconnectivity, starting from the most important biological important and fundamental models: the Hopﬁeld model, two simple associative memory models, and the— exceedingly important in practice—backpropagation algorithm.\\n\\ninsights. Then we will present several\\n\\n1Bionics is concerned with unlocking the “discoveries of living nature” and its innovative conversion into technology [Wik13].\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_9\\n\\n245\\n\\n9\\n\\n246\\n\\n9 Neural Networks\\n\\nFig. 9.1 Two stages of the modeling of a neural network. Above a biological model and below a formal model with neurons and directed connections between them\\n\\n9.1 From Biology to Simulation\\n\\nEach of the roughly 100 billion neurons in a human brain has, as shown in a simpliﬁed representation in Fig. 9.1, the following structure and function. Besides the cell body, the neuron has an axon, which can make local connections to other neurons over the dendrites. The axon can, however, grow up to a meter long in the form of a nerve ﬁber through the body.\\n\\nThe cell body of the neuron can store small electrical charges, similarly to a capacitor or battery. This storage is loaded by incoming electrical impulses from other neurons. The more electric impulse comes in, the higher the voltage. If the voltage exceeds a certain threshold, the neuron will ﬁre. This means that it unloads its store, in that it sends a spike over the axon and the synapses. The electrical current divides and reaches many other neurons over the synapses, in which the same process takes place.\\n\\nNow the question of the structure of the neural network arises. Each of the roughly 1011 neurons in the brain is connected to roughly 1000 to 10 000 other neurons, which yields a total of over 1014 connections. If we further consider that this gigantic number of extremely thin connections is made up of soft, three-dimensional tissue and that experiments on human brains are not easy to carry out, then it becomes clear why we do not have a detailed circuit diagram of the\\n\\n9.1 From Biology to Simulation\\n\\nbrain. Presumably we will never be capable of completely understanding the circuit diagram of our brain, based solely on its immense size.\\n\\nFrom today’s perspective, it is no longer worth even trying to make a complete circuit diagram of the brain, because the structure of the brain is adaptive. It changes itself on the ﬂy and adapts according to the individual’s activities and environmental inﬂuences. The central role here is played by the synapses, which create the connection between neurons. At the connection point between two neurons, it is as if two cables meet. However, the two leads are not perfectly conductively connective, rather there is a small gap, which the electrons cannot directly jump over. This gap is ﬁlled with chemical substances, so-called neuro- transmitters. These can be ionized by an applied voltage and then transport a charge over the gap. The conductivity of this gap depends on many parameters, for example the concentration and the chemical composition of the neurotransmitter. It is enlightening that the function of the brain reacts very sensitively to changes of for example through the inﬂuence of alcohol or this synaptic connection, other drugs.\\n\\nHow does learning work in such a neural network? The surprising thing here is that it is not the actual active units, namely the neurons, which are adaptive, rather it is the connections between them, that is, the synapses. Speciﬁcally, this can change their conductivity. We know that a synapse is made stronger by however much more electrical current it must carry. Stronger here means that the synapse has a higher conductivity. Synapses which are used often obtain an increasingly higher weight. For synapses which are used infrequently or are not active at all, the conductivity continues to decrease. This can even lead to them dying off.\\n\\nAll neurons in the brain work asynchronously and in parallel, but, compared to a computer, at very low speed. The time for a neural impulse takes about a millisecond, exactly the same as the time for the ions to be transported over the synaptic gap. The clock frequency of the neuron then is under one kilohertz and is thus lower than that of modern computers by a factor of 106. This disadvantage, however, is more than compensated for in many complex cognitive tasks, such as image recognition, by the very high degree of parallel processing in the network of nerve cells.\\n\\nThe connection to the outside world comes about through sensor neurons, for example on the retina in the eyes, or through nerve cells with very long axons which reach from the brain to the muscles and thus can carry out actions such as the movement of a leg.\\n\\nis still unclear how the principles discussed make intelligent behavior possible. Just like many researchers in neuroscience, we will attempt to explain using simulations of a simple mathematical model how cognitive tasks, for example pattern recognition, become possible.\\n\\nHowever,\\n\\nit\\n\\n9.1.1 The Mathematical Model\\n\\nFirst we replace the continuous time axis with a discrete time scale. The neuron i carries out the following calculation in a time step. The “loading” of the activation\\n\\n247\\n\\n248\\n\\n9 Neural Networks\\n\\npotential is accomplished simply by summation of the weighted output values x1, … , xn of all incoming connections over the formula\\n\\nXn\\n\\nwijxj:\\n\\nj¼1\\n\\nThis weighted sum is calculated by most neural models. Then an activation function f is applied to it and the result\\n\\nXn\\n\\n!\\n\\nxi ¼ f\\n\\nwijxj\\n\\nj¼1\\n\\nis passed on to the neighboring neurons as output over the synaptic weights. In Fig. 9.2 this kind of modeled neuron is shown. For the activation function there are a number of possibilities. The simplest is the identity: f ðxÞ ¼ x. The neuron thus calculates only the weighted sum of the input values and passes this on. However, this frequently leads to convergence problems with the neural dynamics because the function f ðxÞ ¼ x is unbounded and the function values can grow beyond all limits over time.\\n\\nVery well restricted,\\n\\nin contrast,\\n\\nis the threshold function (Heaviside step\\n\\nfunction)\\n\\nHH xð Þ ¼\\n\\n(\\n\\n0 1\\n\\nif x\\\\H; else.\\n\\nThe whole neuron then computes its output as\\n\\nxi ¼\\n\\n(\\n\\n0\\n\\n1\\n\\nX\\n\\nif\\n\\nelse.\\n\\nn j¼1\\n\\nwijxj\\\\H;\\n\\nThis formula is identical to (8.1) on page 187, in other words, to a perceptron with the threshold H (Fig. 9.3 on page 249). The input neurons 1, … , n here have only the function of variables which pass on their externally set values x1, … , xn unchanged. The step function is quite sensible for binary neurons because the activation of a neuron can only take on the values zero or one anyway. In contrast, for continuous\\n\\nFig. 9.2 The structure of a formal neuron, which applies the activation function f to the weighted sum of all inputs\\n\\n9.1 From Biology to Simulation\\n\\nFig. 9.3 The neuron with a step function works like a perceptron with a threshold\\n\\nFig. 9.4 The sigmoid function for various values of the parameter T. We can see that in the limit T ! 0 the step function results\\n\\nneurons with activations between 0 and 1, the step function creates a discontinuity. However, this can be smoothed out by a sigmoid function, such as\\n\\nf ðxÞ ¼\\n\\n1 1 þ e(cid:2)x(cid:2)H\\n\\nT\\n\\nwith the graph in Fig. 9.4. Near the critical area around the threshold H, this function behaves close to linearly and it has an asymptotic limit. The smoothing can be varied by the parameter T.\\n\\nModeling learning is central to the theory of neural networks. As previously mentioned, one possibility of learning consists of strengthening a synapse according to how many electrical impulses it must transmit. This principle was postulated by D. Hebb in 1949 and is known as the Hebb rule:\\n\\nIf there is a connection wij between neuron j and neuron i and repeated signals are sent from neuron j to neuron i, which results in both neurons being simultaneously active, then the weight wij is reinforced. A possible formula for the weight change Dwij is\\n\\nDwij ¼ gxixj\\n\\nwith the constant η (learning rate), which determines the size of the individual learning steps.\\n\\n249\\n\\n250\\n\\n9 Neural Networks\\n\\nThere are many modiﬁcations of this rule, which then result in different types of networks or learning algorithms. In the following sections, we will become familiar with a few of these.\\n\\n9.2 Hopfield Networks\\n\\nLooking at the Hebb rule, we see that for neurons with values between zero and one, the weights can only grow with time. It is not possible for a neuron to weaken or even die according to this rule. This can be modeled, for example, by a decay constant which weakens an unused weight by a constant factor per time step, such as 0.99. This problem is solved quite differently by the model presented by Hopﬁeld in 1982 [Hop82]. It uses binary neurons, but with the two values −1 for inactive and 1 for active. Using the Hebb rule we obtain a positive contribution to the weight whenever two neurons are simultaneously active. If, however, only one of the two neurons is active, Dwij is negative.\\n\\nHopﬁeld networks, which are a beautiful and visualizable example of auto-associative memory, are based on this idea. Patterns can be stored in auto-associative memory. To call up a saved pattern, it is sufﬁcient to provide a similar pattern. The store then ﬁnds the most similar saved pattern. A classic application of this is handwriting recognition.\\n\\nIn the learning phase of a Hopﬁeld network, N binary coded patterns, saved in the vectors q1, … , qN, are supposed to be learned. Each component q j i 2 f(cid:2)1; 1g of such a vector q j represents a pixel of a pattern. For vectors consisting of n pixels, a neural network with n neurons is used, one for each pixel position. The neurons are fully connected with the restriction that the weight matrix is symmetric and all diagonal elements wij are zero. That is, there is no connection between a neuron and itself.\\n\\nThe fully connected network includes complex feedback loops, so-called\\n\\nrecurrences, in the network (Fig. 9.5).\\n\\nN patterns can be learned by simply calculating all weights with the formula\\n\\nwij ¼\\n\\n1\\n\\nN\\n\\nXN\\n\\nk¼1\\n\\ni qk qk j\\n\\n:\\n\\nð9:1Þ\\n\\nThis formula points out an interesting relationship to the Hebb rule. Each pattern in which the pixels i and j have the same value makes a positive contribution to the weight\\n\\nFig. 9.5 Recurrent connections between two neurons in a Hopﬁeld network\\n\\n9.2 Hopfield Networks\\n\\nwij. Each other pattern makes a negative contribution. Since each pixel corresponds to a neuron, here the weights between neurons which simultaneously have the same value are being reinforced. Please note this small difference to the Hebb rule.\\n\\nOnce all the patterns have been stored, the network can be used for pattern recognition. We give the network a new pattern x and update the activations of all neurons in an asynchronous process according to the rule\\n\\nxi ¼\\n\\n(\\n\\n(cid:2)1 if\\n\\nP\\n\\n1\\n\\nelse\\n\\nn j¼1 j6¼i\\n\\nwijxj \\\\ 0;\\n\\nuntil the network becomes stable, that is, until no more activations change. As a program schema this reads as follows:\\n\\nHOPFIELDASSOCIATOR(q) Initialize all neurons: x = q Repeat\\n\\ni = Random(1, n) Update neuron i according to (9.2)\\n\\nUntil x converges Return (x)\\n\\n9.2.1 Application to a Pattern Recognition Example\\n\\nWe apply the described algorithm to a simple pattern recognition example. It should recognize digits in a 10 (cid:3) 10 pixel ﬁeld. The Hopﬁeld network thus has 100 neurons with a total of\\n\\n100 (cid:4) 99 2\\n\\n¼ 4950\\n\\nweights. First the patterns of the digits 1, 2, 3, 4 in Fig. 9.6 above on page 252 are trained. That is, the weights are calculated by (9.1) on page 250. Then we put in the pattern with added noise and let the Hopﬁeld dynamics run until convergence. In rows 2 to 4 in the ﬁgure, ﬁve snapshots of the network’s development are shown during recognition. At 10% noise all four learned patterns are very reliably rec- ognized. Above about 20% noise the algorithm frequently converges to other learned patterns or even to patterns which were not learned. Several such pattern are shown in Fig. 9.6 on page 252 below.\\n\\nNow we save the digits 0 to 9 (Fig. 9.7 on page 253 top) in the same network and test the network again with patterns that have a random amount of about 10% inverted pixels. In the ﬁgure we clearly see that the Hopﬁeld iteration often does not converge to the most similar learned state even for only 10% noise. Evidently the\\n\\n251\\n\\nð9:2Þ\\n\\n252\\n\\n9 Neural Networks\\n\\nFig. 9.6 Dynamics of a Hopﬁeld network. In rows 2, 3 and 4 we can easily see how the network converges and the learned pattern is recognized after about 300 to 400 iterations. In the last row several stable states are shown which are reached by the network when the input pattern deviates too much from all learned patterns\\n\\nnetwork can securely save and recognize four patterns, but for ten patterns its memory capacity is exceeded. To understand this better, we will take a quick look into the theory of this network.\\n\\n9.2.2 Analysis\\n\\nIn 1982, John Hopﬁeld showed in [Hop82] that this model is formally equivalent to a physical model of magnetism. Small elementary magnets, so-called spins,\\n\\n9.2 Hopfield Networks\\n\\nFig. 9.7 For ten learned states the network shows chaotic behavior. Even with little noise the network converges to the wrong patterns or to artifacts\\n\\nFig. 9.8 Comparison between the neural and physical interpretation of the Hopﬁeld model\\n\\nmutually inﬂuence each other over their magnetic ﬁelds (see Fig. 9.8). If we observe two such spins i and j, they interact over a constant wij and the total energy of the system is then\\n\\nE ¼ (cid:2)\\n\\n1 2\\n\\nX\\n\\ni;j\\n\\nwijxixj:\\n\\nBy the way, wii ¼ 0 in physics too, because particles have no self-interaction. Because physical interactions are symmetric, wij ¼ wji.\\n\\nA physical system in equilibrium takes on a (stable) state of minimal energy and thus minimizes E(x, y). If such a system is brought into an arbitrary state, then it moves toward a state of minimal energy. The Hopﬁeld dynamics deﬁned in (9.2) on page 251 correspond exactly to this principle because it updates the state in each iteration such that, of the two states −1 and 1, the one with smaller total energy is taken on. The contribution of the neuron i to total energy is\\n\\n(cid:2)\\n\\n1 2 xi\\n\\nXn\\n\\nwijxj:\\n\\nj6¼i\\n\\n253\\n\\n254\\n\\n9 Neural Networks\\n\\nIf now\\n\\nXn\\n\\nwijxj \\\\ 0;\\n\\nj6¼i\\n\\nthen xi ¼ (cid:2)1 results in a negative contribution to the total energy, and xi ¼ 1 results in a positive contribution. For xi ¼ (cid:2)1, the network takes on a state of lower energy than it does for xi ¼ 1. Analogously, we can assert that in the case of\\n\\nXn\\n\\nwijxj (cid:5) 0;\\n\\nj6¼i\\n\\nit must be true that xi ¼ 1.\\n\\nIf each individual iteration of the neural dynamics results in a reduction of the energy function, then the total energy of the system decreases monotonically with time. Because there are only ﬁnitely many states, the network moves in time to a state of minimal energy. Now we have the exciting question: what do these minima of the energy function mean?\\n\\nAs we saw in the pattern recognition experiment, in the case of few learned patterns the system converges to one of the learned patterns. The learned patterns represent minima of the energy function in the state space. If however too many patterns are learned, then the system converges to minima which do not correspond to learned patterns. Here we have a transition from an ordered dynamics into a chaotic one.\\n\\nHopﬁeld and other physicists have investigated exactly this process and have shown that there is in fact a phase transition at a critical number of learned patterns. If the number of learned patterns exceeds this value, then the system changes from the ordered phase into the chaotic.\\n\\nIn magnetic physics there is such a transition from the ferromagnetic mode, in which all elementary magnets try to orient themselves parallel, to a so-called spin glass, in which the spins interact chaotically. A more visualizable example of such a physical phase transition is the melting of an ice crystal. The crystal is in a high state of order because the H2O molecules are strictly ordered. In liquid water, by contrast, the structure of the molecules is dissolved and their positions are more random.\\n\\nIn a neural network there is then a phase transition from ordered learning and recognition of patterns to chaotic learning in the case of too many patterns, which can no longer be recognized for certain. Here we deﬁnitely see parallels to effects which we occasionally experienced ourselves.\\n\\nWe can understand this phase transition [RMS92] if we bring all neurons into a the learned weights from (9.1) on wijqj, which is relevant for updating neuron i. This\\n\\npattern state, for example q1, and insert page 250 into the term results in\\n\\nP\\n\\nn j¼1; j6¼i\\n\\n9.2 Hopfield Networks\\n\\nXn\\n\\nj¼1 j6¼i\\n\\nwijq1\\n\\nj ¼\\n\\n1\\n\\nn\\n\\nXn\\n\\nj¼1 j6¼i\\n\\nXN\\n\\nk¼1\\n\\nj q1\\n\\ni qk qk\\n\\nj ¼\\n\\n1\\n\\nn\\n\\nXn\\n\\nj¼1 j6¼i\\n\\nq1 j\\n\\n(cid:2)\\n\\nq1 j\\n\\n(cid:3)2\\n\\nþ\\n\\nXN\\n\\nk¼2\\n\\nj q1\\n\\ni qk qk\\n\\nj\\n\\n!\\n\\n¼ q1\\n\\ni þ\\n\\n1\\n\\nn\\n\\nXn\\n\\nj¼1 j6¼i\\n\\nXN\\n\\nk¼2\\n\\nj q1\\n\\ni qk qk\\n\\nj\\n\\n:\\n\\nHere we see the ith component of the input pattern plus a sum with (n − 1)(N −1) terms. If these summands are all statistically independent, then we can describe the sum by a normally distributed random variable with standard deviation\\n\\n1\\n\\nn\\n\\np\\n\\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ðn (cid:2) 1ÞðN (cid:2) 1Þ\\n\\n(cid:6)\\n\\nr\\n\\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ N (cid:2) 1 n\\n\\n:\\n\\nStatistical independence can be achieved, for example, with uncorrelated random patterns. The sum then generates noise which is not disruptive as long as N (cid:7) n, which means that the number of learned patterns stays much smaller than the number of neurons. If, however, N (cid:6) n, then the inﬂuence of the noise becomes as large as the pattern and the network reacts chaotically. A more exact calculation of the phase transition gives N ¼ 0:146 n as the critical point. Applied to our example, this means that for 100 neurons, up to 14 patterns can be saved. Since the patterns in the example however are strongly correlated, the critical value is much lower, evidently between 0.04 and 0.1. Even a value of 0.146 is much lower than the storage capacity of a traditional memory list (Exercise 9.3 on page 286).\\n\\nHopﬁeld networks in the presented form only work well when patterns with roughly 50% 1-bits are learned. If the bits are very asymmetrically distributed, then the neurons must be equipped with a threshold [Roj96]. In physics this is analogous to the application of an outer magnetic ﬁeld, which also brings about an asymmetry of the spin 1/2 and the spin −1/2 states.\\n\\n9.2.3 Summary and Outlook\\n\\nThrough its biological plausibility, the well understood mathematical model, and above all through the impressive simulations in pattern recognition, the Hopﬁeld model contributed to a wave of excitement about neural networks and to the rise of neuroinformatics as an important branch of AI.2 Subsequently many further net- work models were developed. On one hand, networks without back-couplings were investigated because their dynamics is signiﬁcantly easier to understand than recurrent Hopﬁeld networks. On the other hand, attempts were made to improve the storage capacity of the networks, which we will go into in the next section.\\n\\n2Even the author was taken up by this wave, which carried him from physics into AI in 1987.\\n\\n255\\n\\n256\\n\\n9 Neural Networks\\n\\nA special problem of many neural models was already evident in the Hopﬁeld model. Even if there is a guarantee of convergence, it is not certain whether the network will converge to a learned state or get stuck at a local minimum. The Boltzmann machine, with continuous activation values and a probabilistic update rule for its network dynamics, was developed as an attempt to solve this problem. Using a “temperature” parameter, we can vary the amount of random state changes and thus attempt to escape local minima, with the goal of ﬁnding a stable global minimum. This algorithm is called “simulated annealing”. Annealing is a process of heat treating metals with the goal of making the metal stronger and more “stable”.\\n\\nThe Hopﬁeld model carries out a search for a minimum of the energy function in the space of activation values. It thereby ﬁnds the pattern saved in the weights, and which is thus represented in the energy function. The Hopﬁeld dynamics can also be applied to other energy functions, as long as the weight matrix is symmetric and the diagonal elements are zero. This was successfully demonstrated by Hopﬁeld and Tank on the traveling salesman problem [HT85, Zel94]. The task here is, given n cities and their distance matrix, to ﬁnd the shortest round trip that visits each city exactly once.\\n\\n9.3 Neural Associative Memory\\n\\nA traditional list memory can in the simplest case be a text ﬁle in which strings of digits are saved line by line. If the ﬁle is sorted by line, then the search for an element can be done very quickly in logarithmic time, even for very large ﬁles.\\n\\nList memory can also be used to create mappings, however. For example, a telephone book is a mapping from the set of all entered names to the set of all telephone numbers. This mapping is implemented as a simple table, typically saved in a database.\\n\\nAccess control to a building using facial recognition is a similar task. Here we could also use a database in which a photo of every person is saved together with the person’s name and possibly other data. The camera at the entrance then takes a picture of the person and searches the database for an identical photo. If the photo is found, then the person is identiﬁed and gets access to the building. However, a building with such a control system would not get many visitors because the probability that the current photo matches the saved photo exactly is very small.\\n\\nIn this case it is not enough to just save the photo in a table. Rather, what we want is associative memory, which is capable of not only assigning the right name to the photo, but also to any of a potentially inﬁnite set of “similar” photos. A function for ﬁnding similarity should be generated from a ﬁnite set of training data, namely the saved photos labeled with the names. A simple approach for this is the nearest neighbor method introduced in Sect. 8.3. During learning, all of the photos are simply saved.\\n\\n9.3 Neural Associative Memory\\n\\nTo apply this function, the photo most similar to the current one must be found in the database. For a database with many high-resolution photos, this process, depending on the distance metric used, can require very long computation times and thus cannot be implemented in this simple form. Therefore, instead of such a lazy algorithm, we will prefer one which transfers the data into a function which then creates a very fast association when it is applied.\\n\\nFinding a suitable distance metric presents a further problem. We would like a person to be recognized even if the person’s face appears in another place on the photo (translation), or if it is smaller, larger, or even rotated. The viewing angle and lighting might also vary.\\n\\nThis is where neural networks show their strengths. Without requiring the developer to think about a suitable similarity metric, they still deliver good results. We will introduce two of the simplest associative memory models and begin with a model by Teuvo Kohonen, one of the pioneers in this area.\\n\\nThe Hopﬁeld model presented in the previous chapter would be too difﬁcult to use for two reasons. First, it is only an auto-associative memory, that is, an approximately identical mapping which maps similar objects to the learned original. Second, the complex recurrent dynamics is often difﬁcult to manage in practice. Therefore we will now look at simple two-layer feedforward networks.\\n\\n9.3.1 Correlation Matrix Memory\\n\\nIn [Koh72] Kohonen introduced an associative memory model based on elementary linear algebra. This maps query vectors x 2 ℝn to result vectors y 2 ℝm. We are looking for a matrix W which correctly maps out of a set of training data\\n\\nT ¼ fðq1; t1\\n\\nÞ; . . .; ðqN; t NÞg\\n\\nwith N query-response pairs all query vectors to their responses.3 That is, for p ¼ 1, … , N it must be the case that\\n\\nt p ¼ W (cid:4) q p;\\n\\nor\\n\\nt p i ¼\\n\\nXn\\n\\nwijq p j\\n\\n:\\n\\nj¼1\\n\\n3For a clear differentiation between training data and other values of a neuron, in the following discussion we will refer to the query vector as q and the desired response as t (target).\\n\\n257\\n\\nð9:3Þ\\n\\nð9:4Þ\\n\\n258\\n\\n9 Neural Networks\\n\\nFig. 9.9 Representation of the Kohonen associative memory as a two-layer neural network\\n\\nTo calculate the matrix elements wij, the rule\\n\\nwij ¼\\n\\nXN\\n\\nj t p q p\\n\\ni\\n\\nð9:5Þ\\n\\np¼1\\n\\nis used. These two linear equations can be simply understood as a neural network if we deﬁne, as in Fig. 9.9, a two-layer network with q as the input layer and t as the output layer. The neurons of the output layer have a linear activation function, and (9.5) is used as the learning rule, which corresponds exactly to the Hebb rule.\\n\\nBefore we show that the network recognizes the training data, we need the\\n\\nfollowing deﬁnition:\\n\\nDeﬁnition 9.1 Two vectors x and y are called orthonormal if\\n\\n(\\n\\nxT (cid:4) y ¼\\n\\n1\\n\\n0\\n\\nif x ¼ y; else.\\n\\nThus\\n\\nTheorem 9.1 If all N query vectors qp in the training data are orthonormal, then every vector qp is mapped to the target vector tp by multiplication with the matrix W from (9.5).\\n\\nProof: We substitute (9.5) into (9.4) on page 257 and obtain\\n\\nðW (cid:4) qpÞi ¼\\n\\nXn\\n\\nwijq p\\n\\nj ¼\\n\\nXn\\n\\nXN\\n\\ni qp j tr qr\\n\\nj ¼\\n\\nXn\\n\\n(cid:5)\\n\\nj tp j qp qp i þ\\n\\nXN\\n\\nj qp j tr qr i\\n\\n(cid:6)\\n\\nj¼1\\n\\nj¼1\\n\\nr¼1\\n\\nj¼1\\n\\nr¼1 r6¼p\\n\\n¼tp i\\n\\nXn\\n\\nj qp qp j j¼1 |ﬄﬄﬄﬄ{zﬄﬄﬄﬄ} ¼1\\n\\nþ\\n\\nXN\\n\\nr¼1 r6¼p\\n\\ntr i\\n\\nXn\\n\\nj qp qr j j¼1 |ﬄﬄﬄﬄ{zﬄﬄﬄﬄ} ¼0\\n\\n¼ tp i\\n\\nh\\n\\n9.3 Neural Associative Memory\\n\\nif the query vectors are orthonormal, all patterns will be correctly mapped to the respective targets. However, orthonormality is too strong a restriction. In Sect. 9.4 we will present an approach that overcomes this limitation.\\n\\nThus,\\n\\nSince linear mappings are continuous and injective, we know that the mapping from query vectors to target vectors preserves similarity. Similar queries are thus mapped to similar targets due to the continuity. At the same time we know, however, that different queries are mapped to different targets. If the network was trained to map faces to names and if the name Henry is assigned to a face, then we are sure that for the input of a similar face, an output similar to “Henry” will be produced, but “Henry” itself is guaranteed not to be calculated. If the output can be interpreted as a string, then, for example, it could be “Genry” or “Gfnry”. To arrive at the most similar learned case, Kohonen uses a binary coding for the output neuron. The calculated result of a query is rounded if its value is not zero or one. Even then we have no guarantee that we will hit the target vector. Alternatively, we could add a subsequent mapping of the calculated answer to the learned target vector with the smallest distance.\\n\\n9.3.2 The Binary Hebb Rule\\n\\nIn the context of associative memory, the so-called binary Hebb rule was suggested. It requires that the pattern is binary-encoded. This means that for all patterns qp 2 {0, 1}n and tp 2 {0, 1}m. Furthermore, the summation from (9.5) on page 258 is replaced by a simple logical OR and we obtain the binary Hebb rule\\n\\nwij ¼\\n\\n_N\\n\\np¼1\\n\\nj t p q p\\n\\ni\\n\\n:\\n\\nThe weight matrix is thus also binary, and a matrix element wij is equal to one if and only if at least one of the entries q1 j t1 is not zero. All other matrix i elements are zero. We are tempted to believe that a lot of information is lost here during learning because, when a matrix element takes on the value 1 once, it cannot be changed by additional patterns. Figure 9.10 on page 260 shows how the matrix is ﬁlled with ones for an example with n ¼ 10; m ¼ 6 after learning three pairs. To retrieve the saved patterns we simply multiply a query vector q by the matrix and look at the result Wq. We test this on the example and get Fig. 9.11 on page 260. We see that in the target vector on the right side there is the value 3 in the place where the learned target vector had a one. The correct results would be obtained by setting a threshold value of 3. In the general case we choose the number of ones in the query vector as the threshold. Each output neuron thus works like a perceptron, albeit with a variable threshold.\\n\\n; . . .; qN\\n\\nj tN i\\n\\nAs long as the weight matrix is sparse, this algorithm performs well. However, if many different patterns are saved, the matrix becomes more and more dense. In the\\n\\n259\\n\\nð9:10Þ\\n\\n260\\n\\n9 Neural Networks\\n\\nFig. 9.10 The matrix W after saving three pairs (q1, t1), (q2, t2), (q3, t3). Empty ﬁelds correspond to the value 0\\n\\nFig. 9.11 Calculation of the product Wq1, Wq2, Wq3\\n\\nextreme case it contains only ones. Then, after setting the threshold, all answers would consist of just ones and would no longer contain any information.\\n\\nThis case rarely occurs as long as the number of bits saved in the matrix does not become too large. The matrix has a size of m n elements. A pair to be saved has m + n bits. One can show [Pal80] that the number of memorizable patterns Nmax is determined by the following condition:\\n\\na ¼\\n\\n¼\\n\\nnumber of storable bits number of binary matrix elements\\n\\nð\\n\\nm þ n mn\\n\\nÞNmax\\n\\n(cid:8) ln 2 (cid:6) 0:69:\\n\\nð9:11Þ\\n\\nFor a list memory we have a ¼ 1. Associative memory with the binary Hebb rule has a maximum memory efﬁciency of a ¼ 0:69 compared to a ¼ 0:72 for Kohonen associative memory and a ¼ 0:292 for Hopﬁeld networks [Pal80, Pal91]. The memory capacity of the binary Hebb rule is thus surprisingly high in comparison to the Kohonen model with continuous neurons.\\n\\nIt is obvious that such memory becomes “full” less quickly when the query and target vectors are sparsely populated with ones. This is not the only reason why the encoding of input and output for associative memories—as also for other neural networks—is very important for good performance. We will now demonstrate this on an application of this memory with suitably chosen encodings of input and output.\\n\\n9.3 Neural Associative Memory\\n\\n9.3.3 A Spelling Correction Program\\n\\nAs an application of the described associative memory with the binary Hebb rule, we choose a program that corrects erroneous inputs and maps them to saved words from a dictionary. Clearly an auto-associative memory would be needed here. However, because we encode the query and target vectors differently, this is not the case. For the query vectors q we choose a pair encoding. For an alphabet with 26 characters there are 26 ⋅ 26 ¼ 676 ordered pairs of letters. With 676 bits, the query vector has one bit for each of the possible pairs\\n\\naa; ab; . . .; az; ba; . . .; bz; . . .; za; . . .; zz:\\n\\nIf a pair of letters occurs in the word, then a one will be entered in the appropriate place. For the word “hans”, for instance, the slots for “ha”, “an”, and “ns” are ﬁlled with ones. For the target vector t, 26 bits are reserved for each position in the word up to a maximum length (for example ten characters). For the ith letter in the alphabet in position j in the word then the bit number (j − 1) ⋅ 26 + i is set. For the word “hans”, bits 8, 27, 66, and 97 are set. For a maximum of 10 letters per word, the target vector thus has a length of 260 bits.\\n\\nThe weight matrix W thus has a size of 676 ⋅ 260 bits ¼ 199420 bits, which by\\n\\n(9.11) on page 260 can store at most\\n\\nNmax (cid:8) 0:69 m n m þ n\\n\\n¼ 0:69\\n\\n676 (cid:4) 260 676 þ 260 (cid:6) 130\\n\\nwords. With 72 ﬁrst names, we save about half that many and test the system. The stored names and the output of the program for several example inputs are given in Fig. 9.12 on page 262. The threshold is always initialized to the number of bits in the encoded query. Here this is the number of letter pairs, thus the word length minus one. Then it is stepwise reduced to two. We could further automate the choice of the threshold by comparing with the dictionary for each attempted threshold and output the word found when the comparison succeeds.\\n\\nThe reaction to the ambiguous inputs “andr” and “johanne” is interesting. In both cases, the network creates a mix of two saved words that ﬁt. We see here an important strength of neural networks. They are capable of making associations to similar objects without an explicit similarity metric. However, similarly to heuristic search and human decision making, there is no guarantee for a “correct” solution.\\n\\nSince the training data must be available in the form of input-output pairs for all neural models which have been introduced so far, we are dealing with supervised learning, which is also the case for the networks introduced in the following sections.\\n\\n261\\n\\n262\\n\\n9 Neural Networks\\n\\nStored words:\\n\\nagathe, agnes, alexander, andreas, andree, anna, annemarie, astrid, august, bernhard, bjorn, cathrin, christian, christoph, corinna, corrado, dieter, elisabeth, elvira, erdmut, ernst, evelyn, fabrizio, frank, franz, geoffrey, georg, gerhard, hannelore, harry, herbert, ingilt, irmgard, jan, johannes, johnny, juergen, karin, klaus, ludwig, luise, manfred, maria, mark, markus, marleen, martin, matthias, norbert, otto, patricia, peter, phillip, quit, reinhold, renate, robert, robin, sabine, sebastian, stefan, stephan, sylvie, ulrich, ulrike, ute, uwe, werner, wolfgang, xavier\\n\\nAssociations of the program:\\n\\ninput pattern: harry\\n\\ninput pattern: andrees\\n\\nthreshold: 4, answer: harry threshold: 3, answer: harry threshold: 2, answer: horryrrde\\n\\n------------------------------- input pattern: ute\\n\\nthreshold: 6, answer: a threshold: 5, answer: andree threshold: 4, answer: andrees threshold: 3, answer: mnnrens threshold: 2, answer: morxsnssr\\n\\nthreshold: 2, answer: ute\\n\\n------------------------------- input pattern: johanne\\n\\n------------------------------- input pattern: gerhar\\n\\nthreshold: 5, answer: gerhard threshold: 4, answer: gerrarn threshold: 3, answer: jerrhrrd threshold: 2, answer: jurtyrrde\\n\\nthreshold: 6, answer: johnnnes threshold: 5, answer: johnnnes threshold: 4, answer: jornnnrse threshold: 3, answer: sorrnyrse threshold: 2, answer: wtrrsyrse\\n\\n------------------------------- input pattern: johnnnes\\n\\n------------------------------- input pattern: egrhard\\n\\nthreshold: 6, answer: threshold: 5, answer: threshold: 4, answer: gerhard threshold: 3, answer: gernhrrd threshold: 2, answer: irrryrrde\\n\\nthreshold: 6, answer: joh threshold: 5, answer: johnnnes threshold: 4, answer: johnnyes threshold: 3, answer: jonnnyes threshold: 2, answer: jornsyrse\\n\\n------------------------------- input pattern: johnnyes\\n\\n------------------------------- input pattern: andr\\n\\nthreshold: 3, answer: andrees threshold: 2, answer: anexenser\\n\\nthreshold: 7, answer: threshold: 6, answer: joh threshold: 5, answer: johnny threshold: 4, answer: johnnyes threshold: 3, answer: johnnyes threshold: 2, answer: jonnnyes\\n\\nFig. 9.12 Application of the spelling correction program to various learned or erroneous inputs. The correct inputs are found with the maximum (that is, the ﬁrst attempted) threshold. For erroneous inputs the threshold must be lowered for a correct association\\n\\n9.4 Linear Networks with Minimal Errors\\n\\n9.4 Linear Networks with Minimal Errors\\n\\nThe Hebb rule used in the neural models presented so far works with associations between neighboring neurons. In associative memory, this is exploited in order to learn a mapping from query vectors to targets. This works very well in many cases, espe- cially when the query vectors are linearly independent. If this condition is not fulﬁlled, for example when too much training data is available, the question arises: how do we ﬁnd the optimal weight matrix? Optimal means that it minimizes the average error. We humans are capable of learning from mistakes. The Hebb rule does not offer this possibility. The backpropagation algorithm, described in the following, uses an elegant solution known from function approximation to change the weights such that the error on the training data is minimized.\\n\\nLet N pairs of training vectors\\n\\nT ¼ fðq1; t1Þ; . . .; ðqN; tNÞg\\n\\nbe given with qp 2 [0, 1]n t p 2 [0, 1]m. We are looking for a function f : [0, 1]n ! [0, 1]m which minimizes the squared error\\n\\nXN\\n\\nð f ðq pÞ (cid:2) t pÞ\\n\\n2\\n\\np¼1\\n\\non the data. Let us ﬁrst assume that the data contains no contradictions. That is, there is no query vector in the training data which should be mapped to two different targets. In this case it is not difﬁcult to ﬁnd a function that minimizes the squared error. In fact, there exist inﬁnitely many functions which make the error zero. We deﬁne the function\\n\\nf ðqÞ ¼ 0;\\n\\nif q 62 fq1; . . .; qNg\\n\\nand\\n\\nf ðqpÞ ¼ t p 8 p 2 f1; . . .; Ng:\\n\\nThis is a function which even makes the error on the training data zero. What more could we want? Why are we not happy with this function?\\n\\nThe answer is: because we want to build an intelligent system! Intelligent means, among other things, that the learned function can generalize well from the training data to new, unknown data from the same representative data set. In other words it means: we do not want overﬁtting of the data by memorization. What is it then that we really want?\\n\\nWe want a function that is smooth and “evens out” the space between the points. Continuity and the ability to take multiple derivatives would be sensible require- ments. Because even with these conditions there are still inﬁnitely many functions which make the error zero, we must restrict this class of functions even further.\\n\\n263\\n\\n264\\n\\n9 Neural Networks\\n\\n9.4.1 Least Squares Method\\n\\nThe simplest choice is a linear mapping. We begin with a two-layer network (Fig. 9.13) in which the single neuron y of the second layer calculates its activation using\\n\\nXn\\n\\n!\\n\\ny ¼ f\\n\\nwixi\\n\\ni¼1\\n\\nwith f ðxÞ ¼ x. The fact that we are only looking at output neurons here does not pose a real restriction because a two-layer network with two or more output neurons can always be separated into independent networks with identical input neurons for each of the original output neurons. The weights of the subnetworks are all inde- pendent. Using a sigmoid function instead of the linear activation does not give any advantage here because the sigmoid function is strictly monotonically increasing and does not change the order relation between various output values.\\n\\nDesired is a vector w which minimizes the squared error\\n\\nEðwÞ ¼\\n\\nXN\\n\\n2 ðw q p (cid:2) t pÞ\\n\\n¼\\n\\nXN\\n\\nXn\\n\\nwiqi\\n\\np (cid:2) t p\\n\\n!2\\n\\n:\\n\\np¼1\\n\\np¼1\\n\\ni¼1\\n\\nAs a necessary condition for a minimum of this error function all partial derivatives must be zero. Thus we require that for j ¼ 1; . . .; n : !\\n\\n@E @wj\\n\\n¼ 2\\n\\nXN\\n\\np¼1\\n\\nXn\\n\\ni¼1\\n\\nwiq p\\n\\ni (cid:2) t p\\n\\nq p j ¼ 0:\\n\\nMultiplying this out yields\\n\\nXN\\n\\np¼1\\n\\nXn\\n\\ni¼1\\n\\nwiq p\\n\\ni q p\\n\\nj (cid:2) t pq p\\n\\nj\\n\\n!\\n\\n¼ 0;\\n\\nFig. 9.13 A two-layer network with an output neuron\\n\\n9.4 Linear Networks with Minimal Errors\\n\\nand exchanging the sums results in the linear system of equations\\n\\nXn\\n\\nwi\\n\\nXN\\n\\ni q p q p\\n\\nj ¼\\n\\nXN\\n\\nt pq p j\\n\\n;\\n\\ni¼1\\n\\np¼1\\n\\np¼1\\n\\nwhich with\\n\\nXN\\n\\nXN\\n\\nAij ¼\\n\\ni q p q p\\n\\nj\\n\\nand\\n\\nbj ¼\\n\\nt pq p j\\n\\np¼1\\n\\np¼1\\n\\ncan be written as the matrix equation\\n\\nAw ¼ b:\\n\\nThese so-called normal equations always have at least one solution and, when A is invertible, exactly one. Furthermore, the matrix A is positive-deﬁnite, which has the implication that the discovered solution in the unique case is a global minimum. This algorithm is known as least squares method.\\n\\nThe calculation time for setting up the matrix A grows with H(N ⋅ n2) and the time for solving the system of equations as O(n3). This method can be extended very simply to incorporate multiple output neurons because, as already mentioned, for two-layer feedforward networks the output neurons are independent of each other.\\n\\n9.4.2 Application to the Appendicitis Data\\n\\nAs an application we now determine a linear score for the appendicitis diagnosis. From the LEXMED project data, which is familiar from Sect. 8.4.5, we use the least squares method to determine a linear mapping from symptoms to the continuous class variables AppScore with values in the interval [0, 1] and obtain the linear combination\\n\\nAppScore ¼ 0:00085 Age (cid:2) 0:125 Sex þ 0:025 P1Q þ 0:035 P2Q (cid:2) 0:021 P3Q\\n\\n(cid:2) 0:025 P4Q þ 0:12 TensLoc þ 0:031 TensGlo þ 0:13 Losl þ 0:081 Conv þ 0:0034 RectS þ 0:0027 TAxi þ 0:0031 TRec þ 0:000021 Leuko (cid:2) 0:11 Diab (cid:2) 1:83:\\n\\nThis function returns continuous variables for AppScore, although the actual binary class variable App only takes on the values 0 and 1. Thus we have to decide on a threshold value, as with the perceptron. The classiﬁcation error of the score as a function of the threshold is listed in Fig. 9.14 on page 266 for the training data and the test data. We clearly see that both curves are nearly the same and have their minimum at H ¼ 0:5. In the small difference of the two curves we see that overﬁtting is not a problem for this method because the model generalizes from the test data very well.\\n\\n265\\n\\nð9:12Þ\\n\\nð9:13Þ\\n\\n266\\n\\n9 Neural Networks\\n\\nFig. 9.14 Least squares error for training and test data\\n\\nAlso in the ﬁgure is the result for the nonlinear, three-layer RProp network (Sect. 9.5) with a somewhat lower error for threshold values between 0.2 and 0.9. For practical application of the derived score and the correct determination of the threshold H it is important to not only look at the error, but also to differentiate by type of error (namely false positive and false negative), as is done in the LEXMED application in Fig. 7.10 on page 156. In the ROC curve shown there, the score calculated here is also shown. We see that the simple linear model is clearly inferior to the LEXMED system. Evidently, linear approximations are not powerful enough for many complex applications.\\n\\n9.4.3 The Delta Rule\\n\\nLeast squares is, like the perceptron and decision tree learning, a so-called batch learning algorithm, as opposed to incremental learning. In batch learning, all training data must be learned in one run. If new training data is added, it cannot simply be learned in addition to what is already there. The whole learning process must be repeated with the enlarged set. This problem is solved by incremental learning algorithms, which can adapt the learned model to each additional new example. In the algorithms we will look at in the following discussion, we will additively update the weights for each new training example by the rule\\n\\nwj ¼ wj þ Dwj:\\n\\nTo derive an incremental variant of the least squares method, we reconsider the above calculated n partial derivatives of the error function\\n\\n@E @wj\\n\\n¼ 2\\n\\nXN\\n\\np¼1\\n\\nXn\\n\\ni¼1\\n\\nwiq p\\n\\ni (cid:2) t p\\n\\n!\\n\\nq p j\\n\\n:\\n\\n9.4 Linear Networks with Minimal Errors\\n\\nThe gradient\\n\\nrE ¼\\n\\n(cid:5)\\n\\n@E @w1\\n\\n; . . .;\\n\\n@E @wn\\n\\n(cid:6)\\n\\nas a vector of all partial derivatives of the error function points in the direction of the strongest rise of the error function in the n-dimensional space of the weights. While searching for a minimum, we will therefore follow the direction of the negative gradient. As a formula for changing the weights we obtain\\n\\nDwj ¼ (cid:2)\\n\\ng 2\\n\\n@E @wj\\n\\n¼ (cid:2)g\\n\\nXN\\n\\np¼1\\n\\nXn\\n\\ni¼1\\n\\nwiq p\\n\\ni (cid:2) t p\\n\\n!\\n\\nq p j\\n\\n;\\n\\nwhere the learning rate η is a freely selectable positive constant. A larger η speeds up convergence but at the same time raises the risk of oscillation around minima or ﬂat valleys. Therefore, the optimal choice of η is not a simple task (see Fig. 9.15). A large η, for example η ¼ 1, is often used to start with, and then slowly shrunk.\\n\\nBy replacing the activation\\n\\nyp ¼\\n\\nXn\\n\\nwiq p i\\n\\ni¼1\\n\\nof the output neuron for applied training example qp, the formula is simpliﬁed and we obtain the delta rule\\n\\nDwj ¼ g\\n\\nXN\\n\\nðtp (cid:2) ypÞqp j\\n\\n:\\n\\np¼1\\n\\nThus for every training example the difference between the target tp and the actual output yp of the network is calculated for the given input qp. After summing over all patterns, the weights are then changed proportionally to the sum. This algorithm is shown in Fig. 9.16 on page 268.\\n\\nFig. 9.15 Gradient descent for a large η (left) and a very small η (right) into a valley descending ﬂatly to the right. For a large η there are oscillations around the valley. For a η which is too small, in contrast, convergence in the ﬂat valley happens very slowly\\n\\n267\\n\\n268\\n\\n9 Neural Networks\\n\\nDELTALEARNING(TrainingExamples, η) Initialize all weights wj randomly Repeat\\n\\nw 0\\n\\nFor all (qp, t p) TrainingExamples\\n\\nCalculate network output yp wpqp\\n\\nw η(t p\\n\\nyp)qp\\n\\nw\\n\\nw w\\n\\nw\\n\\nUntil w converges\\n\\nFig. 9.16 Learning a two-layer linear network with the delta rule. Notice that the weight changes always occur after all of the training data are applied\\n\\nDELTALEARNINGINCREMENTAL(TrainingExamples, η) Initialize all weights wj randomly Repeat\\n\\nFor all (qp, t p) ∈ TrainingExamples\\n\\nCalculate network output yp = wpqp w = w + η(t p − yp)qp\\n\\nUntil w converges\\n\\nFig. 9.17 Incremental variant of the delta rule\\n\\nWe see that the algorithm is still not really incremental because the weight changes only occur after all training examples have been applied once. We can this deﬁciency by directly changing the weights (incremental gradient correct descent) after every training example (Fig. 9.17), which, strictly speaking, is no longer a correct implementation of the delta rule.\\n\\n9.4.4 Comparison to the Perceptron\\n\\nthe least squares The learning rule for perceptrons introduced in Sect. 8.2.1, method, and the delta rule can be used to generate linear functions from data. For perceptrons however, in contrast to the other methods, a classiﬁer for linearly separable classes is learned through the threshold decision. The other two methods, however, generate a linear approximation to the data. As shown in Sect. 9.4.2, a classiﬁer can be generated from the linear mapping, if desired, by application of a threshold function.\\n\\n9.4 Linear Networks with Minimal Errors\\n\\nThe perceptron and the delta rule are iterative algorithms for which the time until convergence depends heavily on the data. In the case of linearly separable data, an upper limit on the number of iteration steps can be found for the perceptron. For the delta rule, in contrast, there is only a guarantee of asymptotic convergence without limit [HKP91].\\n\\nFor least squares, learning consists of setting up and solving a linear system of equations for the weight vector. There is thus a hard limit on the computation time. Because of this, the least squares method is always preferable when incremental learning is not needed.\\n\\n9.5 The Backpropagation Algorithm\\n\\nWith the backpropagation algorithm, we now introduce the most-used neural model. The reason for its widespread use its universal versatility for arbitrary approximation tasks. The algorithm originates directly from the incremental delta rule. In contrast to the delta rule, it applies a nonlinear sigmoid function on the weighted sum of the inputs as its activation function. Furthermore, a back- propagation network can have more than two layers of neurons. The algorithm legendary PDP became collection [RM86].\\n\\nknown\\n\\nthrough\\n\\nthe\\n\\narticle\\n\\n[RHR86]\\n\\nin\\n\\nthe\\n\\nIn Fig. 9.18 on page 270 a typical backpropagation network with an input layer, a hidden layer, and an output layer is shown. Since the current output value xp j of the output layer neuron is compared with the target output value tp j , these are drawn parallel to each other. Other than the input neurons, all neurons calculate their current value xj by the rule\\n\\nXn\\n\\n!\\n\\nxj ¼ f\\n\\nwjixi\\n\\ni¼1\\n\\nwhere n is the number of neurons in the previous layer. We use the sigmoid function\\n\\nf ðxÞ ¼\\n\\n1 1 þ e(cid:2)x\\n\\n:\\n\\nAnalogous to the incremental delta rule, the weights are changed proportional to the negative gradient of the quadratic error function summed over the output neurons\\n\\nEpðwÞ ¼\\n\\n1 2\\n\\nX\\n\\nðt p\\n\\nk (cid:2) x p kÞ\\n\\n2\\n\\nk2output\\n\\n269\\n\\nð9:14Þ\\n\\n270\\n\\n9 Neural Networks\\n\\nFig. 9.18 A three-layer backpropagation network with n1 neurons in the ﬁrst, n2 neurons in the second, and n3 neurons in the third layer\\n\\nfor the training pattern p:\\n\\nDpwji ¼ (cid:2)g\\n\\n@Ep @wji\\n\\n:\\n\\nFor deriving the learning rule, the above expression is substituted for Ep. Within the expression, xk is replaced by (9.14) on page 269. Within the equation, the outputs xi of the neurons of the next, deeper layer occur recursively, etc. By multiple appli- cations of the chain rule (see [RHR86] or [Zel94]) we obtain the backpropagation learning rule\\n\\nDpwji ¼ gd p\\n\\nj x p\\n\\ni\\n\\n;\\n\\nwith\\n\\nd p j ¼\\n\\n8 <\\n\\n:\\n\\nj ð1 (cid:2) x p x p j ð1 (cid:2) x p x p j Þ\\n\\nj (cid:2) x p j Þðt p j Þ P k d p wkj\\n\\nk\\n\\nif j is an output neuron,\\n\\nif j is a hidden neuron,\\n\\nwhich is also denoted the generalized delta rule. For all neurons, the formula for changing weight wji from neuron i to neuron j (see Fig. 9.19 on page 271) contains, like the Hebb rule, a term gxp j Þ creates the symmetry, which is missing from the Hebb rule, between the activations 0 and 1 of neuron j. For the output neurons, the factor ðtp j (cid:2) xp j Þ takes care of a weight change pro- portional to the error. For the hidden neurons, the value dp j of neuron j is calculated recursively from all changes dp\\n\\ni xp\\n\\nj . The new factor ð1 (cid:2) xp\\n\\nk of the neurons of the next higher level.\\n\\n9.5 The Backpropagation Algorithm\\n\\nFig. 9.19 Designation of the neurons and weights for the application of the backpropagation rule\\n\\nThe entire execution of the learning process is shown in Fig. 9.20. After cal- culating the output of the network (forward propagation) for a training example, the approximation error is calculated. This is then used during backward propagation to alter the weights backward from layer to layer. The whole process is then applied to all training examples and repeated until the weights no longer change or a time limit is reached.\\n\\nIf we build a network with at least one hidden layer, nonlinear mappings can be learned. Without hidden layers, the output neurons are no more powerful than a linear neuron, despite the sigmoid function. The reason for this is that the sigmoid function is strictly monotonic. The same is true for multi-layer networks which only use a linear function as an activation function, for example the identity function. This is because chained executions of linear mappings is linear in aggregate.\\n\\nJust like with the perceptron, the class of the functions which can be represented\\n\\nare also enlarged if we use a variable sigmoid function\\n\\nBACKPROPAGATION(TrainingExamples, η) Initialize all weights wj to random values Repeat\\n\\nFor all (qp, t p) ∈ TrainingExamples\\n\\n1. Apply the query vector qp to the input layer 2. Forward propagation: For all layers from the ﬁrst hidden layer upward\\n\\nFor each neuron of the layer\\n\\nn Calculate activation xj = f ( i=1 3. Calculation of the square error Ep(w) 4. Backward propagation:\\n\\nwj ixi)\\n\\nFor all levels of weights from the last downward\\n\\nFor each weight wj i wj i = wj i + ηδp\\n\\nj xp\\n\\ni\\n\\nUntil w converges or time limit is reached\\n\\nFig. 9.20 The backpropagation algorithm\\n\\n271\\n\\n272\\n\\n9 Neural Networks\\n\\nf ðxÞ ¼\\n\\n1 1 þ e(cid:2)ðx(cid:2)HÞ\\n\\n:\\n\\nwith threshold H. This is implemented analogously to the way shown in Sect. 8.2, in which a neuron whose activation always has the value one and which is con- nected to neurons in the next highest level is inserted into the input layer and into each hidden layer. The weights of these connections are learned normally and represent the threshold H of the successor neurons.\\n\\n9.5.1 NETtalk: A Network Learns to Speak\\n\\nSejnowski and Rosenberg demonstrated very impressively in 1986 what back- propagation is capable of performing [SR86]. They built a system that is able to understandably read English text aloud from a text ﬁle. The architecture of the network shown in Fig. 9.21 consists of an input layer with 7 (cid:3) 29 ¼ 203 neurons in which the current letter and three previous letters, as well as three subsequent, letters are encoded. For each of these seven letters, 29 neurons are reserved for the characters “a...z ,.”. The input is mapped onto the 26 output neurons over 80 hidden neurons, each of which stands for a speciﬁc phoneme. For example, the “a” in “father” is pronounced deep, accented, and central. The network was trained with 1,000 words, which were applied randomly one after another letter by letter. For each letter, the target output was manually given for its intonation. To translate the intonation attributes into actual sounds, part of the speech synthesis system\\n\\nFig. 9.21 The NETtalk network maps a text to its pronunciation attributes\\n\\n9.5 The Backpropagation Algorithm\\n\\nFig. 9.22 The NETtalk network in JNNS. In the left window from left to right we see the 7 ⋅ 29 input neurons, 80 hidden, and 26 output neurons. Due to their large number, the weights are omitted from the network. Top right is a learning curve showing the development of the squared error over time\\n\\nDECtalk was used. Through complete interconnectivity, the network contains a total of 203 (cid:3) 80 þ 80 (cid:3) 26 ¼ 18320 weights.\\n\\nThe system was trained using a simulator on a VAX 780 with about 50 cycles over all words. Thus, at about 5 characters per word on average, about 5 (cid:4) 50 (cid:4) 1000 ¼ 250 000 iterations of the backpropagation algorithm were needed. At a rate of about one character per second, this means roughly 69 hours of computation time. The developers observed many properties of the system which are quite similar to human learning. At ﬁrst the system can only speak unclearly or use simple words. With time it continued to improve and ﬁnally reached 95% cor- rectness of pronounced letters.\\n\\nFor visual experiments with neural networks, the Java Neural Network Simu- lator JNNS is recommended [Zel94]. The NETtalk network, loaded and trained with JNNS, is shown in Fig. 9.22.\\n\\n9.5.2 Learning of Heuristics for Theorem Provers\\n\\nIn Chap. 6, algorithms for heuristic search, such as the A⋆-algorithm and the IDA⋆-algorithm were discussed. To reach a signiﬁcant reduction of the search space, a good heuristic is needed for the implementation of these algorithms. In\\n\\n273\\n\\n274\\n\\n9 Neural Networks\\n\\nFig. 9.23 Search tree with a successful path whose nodes are evaluated as positive, and the negatively rated unsuccessful branches\\n\\nSect. 4.1 the problem of the exploding search space during theorem provers’ search for a proof was demonstrated. This problem is caused by the large number of possible inference steps in each step.\\n\\nWe now attempt to build heuristic proof controlling modules which evaluate the various alternatives for the next step and then choose the alternative with the best rating. In the case of resolution, the rating of the available clauses could be done by a function which, based on certain attributes of clauses such as the number of literals, the complexity of the terms, etc., calculates a value for each pair of resolvable clauses. In [ESS89, SE90] such a heuristic was learned for the theorem prover SETHEO using backpropagation. While learning the heuristic, the attributes of the current clause were saved as training data for the positive class for every proof found at every branch during the search. At all branches which did not lead to a proof, the attributes of the current clause were saved as training data for the negative class. Such a tree, with a successful path and the corresponding node ratings, is sketched in Fig. 9.23.\\n\\nA backpropagation network was trained on this data and then used to evaluate clauses in the prover. For each clause, 16 numeric attributes were calculated, normalized, and encoded in an input neuron. The network was trained with 25 hidden neurons and one output neuron for the class (positive/negative).\\n\\nIt was shown that the number of attempted inferences can be reduced by many orders of magnitude for difﬁcult problems, which ultimately reduces the compu- tation time from hours to seconds. Thereby it became possible to prove theorems which, without heuristics, were out of reach.\\n\\n9.5.3 Problems and Improvements\\n\\nBackpropagation is now 25 years old and has proved itself in various applications, for example in pattern recognition and in robotics. However, its application is sometimes problematic. Especially when the network has many thousands of weights and there is a lot of training data to learn, two problems come up:\\n\\nThe network often converges to local minima of the error function. Furthermore, backpropagation often converges very slowly. This means that many iterations over\\n\\n9.5 The Backpropagation Algorithm\\n\\nFig. 9.24 Abrupt direction changes are smoothed out by the use of the momentum term. An iteration without the momentum term (left) in comparison to iteration with the momentum term (right)\\n\\ntraining patterns are needed. Many improvements have been suggested to all alleviate these problems. As mentioned in Sect. 9.4.3, oscillations can be avoided by slowly reducing the learning rate η as shown in Fig. 9.15 on page 267.\\n\\nAnother method for reducing oscillations is the use of a momentum term while updating the weights, which ensures that the direction of gradient descent does not change too dramatically from one step to the next. Here for the current weight change Dpwji (t) at time t another part of the change Dpwji (t − 1) from the previous step is added. The learning rule then changes to\\n\\nDpwjiðtÞ ¼ gdp\\n\\nj xp\\n\\ni þ cDpwjiðt (cid:2) 1Þ\\n\\nwith a parameter c between zero and one. This is depicted in a two-dimensional example in Fig. 9.24.\\n\\nAnother idea is to minimize the linear error function instead of the square error\\n\\nfunction, which reduces the problem of slow convergence into ﬂat valleys.\\n\\nGradient descent in backpropagation is ultimately based on a linear approxi- mation of the error function. Quickprop, an algorithm which uses a quadratic approximation to the error function and thus achieves faster convergence, was created by Scott Fahlmann.\\n\\nThrough smart uniﬁcation of the improvements mentioned and other heuristic tricks, Martin Riedmiller achieved a further optimization with the RProp algo- rithm [RB93]. RProp has replaced backpropagation and is the new state of the art feedforward neural network approximation algorithm. In Sect. 8.9 we applied RProp to the classiﬁcation of the appendicitis data and achieved an error which is approximately the same size as that of a learned decision tree.\\n\\n9.6 Support Vector Machines\\n\\nFeedforward neural networks with only one layer of weights are linear. Linearity leads to simple networks and fast learning with guaranteed convergence. Further- more, the danger of overﬁtting is small for linear models. For many applications, however, the linear models are not strong enough, for example because the relevant classes are not linearly separable. Here multi-layered networks such as backprop- agation come into use, with the consequence that local minima, convergence problems, and overﬁtting can occur.\\n\\n275\\n\\n276\\n\\n9 Neural Networks\\n\\nFig. 9.25 Two classes with the maximally dividing line. The circled points are the support vectors\\n\\nA promising approach, which brings together the advantages of linear and nonlinear models, follows the theory of support vector machines (SVM), which we will roughly outline using a two class problem.4\\n\\nIn the case of two linearly separable classes, it is easy to ﬁnd a dividing hyper plane, for example with the perceptron learning rule. However, there are usually inﬁnitely many such planes, as in the two-dimensional example in Fig. 9.25. We are looking for a plane which has the largest minimum distance to both classes. This plane is usually uniquely deﬁned by a few points in the border area. These points, the so-called support vectors, all have the same distance to the dividing line. To ﬁnd the support vectors, there is an efﬁcient optimizing algorithm. It is interesting that the optimal dividing hyperplane is determined by a few parameters, namely by the support vectors. Thus the danger of overﬁtting is small.\\n\\nSupport vector machines apply this algorithm to non linearly separable problems in a two-step process: In the ﬁrst step, a nonlinear transformation is applied to the data, with the property that the transformed data is linearly separable. In the second step the support vectors are then determined in the transformed space.\\n\\nThe ﬁrst step is highly interesting, but not quite simple. In fact, it is always possible to make the classes linearly separable by transforming the vector space, as long as the data contains no contradictions.5 Such a separation can be reached for example by introducing a new (n + 1)th dimension and the deﬁnition (\\n\\nxn þ 1 ¼\\n\\n1\\n\\n0\\n\\nif x 2 class 1; if x 2 class 0:\\n\\nHowever, this formula does not help much because it is not applicable to new points of an unknown class which are to be classiﬁed. We thus need a general transformation which is as independent as possible from the current data. It can be shown that there are such generic transformations even for arbitrarily shaped class division boundaries in the original vector space. In the transformed space, the data are then linearly separable. However, the number of dimensions of the new vector space grows exponentially with the number of dimensions of the original vector\\n\\n4Support vector machines are not neural networks. Due to their historical development and mathematical relationship to linear networks, however, they are discussed here. 5A data point is contradictory if it belongs to both classes.\\n\\n9.6 Support Vector Machines\\n\\nspace. However, the large number of new dimensions is not so problematic because, when using support vectors, the dividing plane, as mentioned above, is determined by only a few parameters.\\n\\nThe central nonlinear transformation of the vector space is called the kernel, because of which support vector machines are also known as kernel methods. The original SVM theory developed for classiﬁcation tasks has been extended and can now be used on regression problems also.\\n\\nThe mathematics used here is very interesting, but too extensive for an initial introduction. To delve deeper into this promising young branch of machine learning, we refer the reader to [SS02, Alp04] and [Bur98].\\n\\n9.7 Deep Learning\\n\\nIn Chaps. 8 and 9 we saw that there are many good learning algorithms today which are capable of learning non-trivial, sometimes complex classiﬁcations or approxi- mations for all sorts of applications, such as diagnosis and prognosis based on sensor inputs. We have also seen that, up to now, generation of features has not been successful. Instead it is the job of a data scientist to ﬁnd a sensible small set of features, which can then be used as input for the learning algorithm.\\n\\nWhy do we not simply use all available sensor data, i.e. a direct image of the world, as input? For example, for object recognition in a photo we could use all ten million pixels as an input vector, which has a length of 30 million (in the case of RGB or HSV images, which each have three color values). What is the problem with this approach? The answer is known as the “curse of the dimensionality”. This means, among other things, that training time grows very fast, often exponentially, with the dimension of the input data. To keep computation times within bounds, we must therefore ﬁrst reduce the input data to short feature vectors. As described above, this mapping is usually created manually. However, for many applications, such as object classiﬁcation in images it is difﬁcult if not impossible to manually ﬁnd the formula for features. One old method for automatic reduction of dimensions is principal component analysis (PCA) [Ert15], which determines the directions of highest variance (i.e. the principal component) in the vector space of the training data and projects the data into the subspace of the principal components using a linear transformation. Because of the missing nonlearity of the compression map- ping PCA is not as powerful as the new methods described below.\\n\\nSince about 1995 work has been done on deep learning, a highly promising class of algorithms to solve this problem, and there are now impressive successes to report. Deep learning includes methods such as convolutional neural networks (CNNs), or deep belief networks and variations thereof. The architectures of the multi-layered neural networks with up to twenty or more layers are in part very complex and cannot be explained in detail here. [LBH15] is a good review article, and a very detailed introduction can be found in [GBC16], as well as in deeplearning.stanford. edu. Now let us try to understand the most important principles.\\n\\n277\\n\\n278\\n\\n9 Neural Networks\\n\\nPattern recognition is simple in low-dimensional spaces or in case of classiﬁ- cation when the classes are linearly separable. For classes that are not linearly separable in high dimensional spaces, however, problems arise because here learning poses a nonlinear optimization problem. In principle there are solutions using gradient descent algorithms such as backpropagation. However, convergence problems and unacceptably high computation times arise for the classical algo- rithms, especially when networks with many hidden layers are used. Thus other methods have been sought.\\n\\n9.7.1 Nature as Example\\n\\nAll successful approaches in deep learning to date work with many layers of neurons. The network is split into two parts, similar to the feedforward network shown schematically in Fig. 9.26. After a preprocessing layer there are several layers which are pre-trained by unsupervised learning (UL). Each layer in this UL network represents features of the input pattern. The lower the layer, the simpler the features. In object recognition in photos, the features of the lower layers typically represent edges or lines in different orientations.6 Complex features such as the\\n\\n.....\\n\\noutput layer\\n\\nSL\\n\\nbackpropagation− network\\n\\n.....\\n\\nn−th feature layer\\n\\n. . . . .\\n\\n.....\\n\\n2nd feature layer\\n\\nUL\\n\\n.....\\n\\n1st feature layer\\n\\n.....\\n\\ninput layer\\n\\npreprocessing\\n\\n.....\\n\\nraw data\\n\\nFig. 9.26 Simpliﬁed architecture of a deep learning network. It is composed of a preprocessor, several layers (in this case two) for unsupervised feature detection and then one classical neural network, which, in this example, is trained by backpropagation\\n\\n6Visual deeplearning.stanford.edu.\\n\\nrepresentations of such edge features,\\n\\nincluding explanation, can be found at\\n\\n9.7 Deep Learning\\n\\npresence of a face can form on higher layers. This architecture shows certain similarities to the structure of the brains of humans and animals. Starting from the sense organs, for example the eyes, the brain is built up in many layers, and the higher the layer, the more abstract the information to be found there. However, there is still far too little known about how neural networks work in nature that leads to a signiﬁcant beneﬁt in deep learning [GBC16].\\n\\nAttached to the UL network is a classical supervised learning (SL) network, which can be trained with backpropagation or RProp. Thus the learning process works as follows:\\n\\n1. Unsupervised training of all feature layer weights. 2. Supervised training of the SL network with gradient descent.\\n\\nThe unsupervised learning of the weights to the feature layers remains to be the features will be extracted. Feature explained. By learning these weights, extraction should have the property that input data is mapped into a lower dimensional space, if possible without (too much) loss of information. One could therefore see feature extraction as a form of compression. We now outline one of the many possible algorithms for feature learning.\\n\\n9.7.2 Stacked Denoising Autoencoder\\n\\nThe feature layer’s weights are determined by a fundamentally simple and obvi- ous algorithm called stacked denoising autoencoder [VLL+10], as shown in Fig. 9.27.\\n\\nTo train the ﬁrst hidden layer, an autoencoder is trained with a supervised learning method, for example RProp. Its job is to learn the identity mapping of all input vectors x onto itself. The feature layer acts as the hidden layer. To make sure that features form in this layer, it is important to avoid overﬁtting. The classical approach would be to use cross-validation over the number of hidden neurons in the feature layer. That would lead to a small number of hidden neurons and thus few features. It has been shown that such a compressed encoding of the features is not optimal. So-called sparse coding leads to better results.\\n\\n.....\\n\\ncopy of input layer\\n\\ndecoder\\n\\n.....\\n\\nfeature layer\\n\\nencoder\\n\\n.....\\n\\ninput layer\\n\\nFig. 9.27 Autoencoder for learning the weights of a feature layer. A classical backpropagation network can be used here to learn the identity mapping from the current input layer onto itself. Features form in the hidden layer during learning\\n\\n279\\n\\n280\\n\\n9 Neural Networks\\n\\nIn order to ﬁnd many different features via such a sparse coding, denoising, rather than cross-validation, will be used. During each learning cycle, the values of some of the input neurons are randomly changed, for example with Gaussian noise or by setting the activation to 0 or 1. The distance\\n\\njjy (cid:2) xjj\\n\\nof the calculated output vector y from the original input vector x is used as the error function for the weight change. Thus the encoder is trained to become robust against noise. During learning, a variable parameter weights the noisiﬁed neurons more strongly than the others, so as to optimize the desired effect of the noise on learning. After the autoencoder is trained, the actually unneeded decoder layer, i.e. the second layer of weights, is removed. The ﬁrst layer is frozen and used to calculate the features in the UL network in Fig. 9.26 on page 278. Then, with this ﬁxed ﬁrst layer of weights, the second feature layer is trained with the autoencoder algorithm, frozen, and so on until the last feature layer. Thus the unsupervised part of the learning is ﬁnished.\\n\\nNow the SL portion of the network is trained with a classical supervised learning algorithm. For each training example, the output of of the UL network, i.e., the last feature layer is used as input, and a particular data label is the target output. During backpropagation, the weights of the feature layers can be trained again for the purpose of ﬁne-tuning, or they can be left unchanged.\\n\\nWe have not yet mentioned the preprocessing step, which differs depending on the application. Often all of the input variables are normalized. For photos, the pixel image is often transformed into a lower dimensional space, similar to PCA, using a process called whitening, with the goal of making the generated features less redundant. Here one could also directly use PCA.\\n\\n9.7.3 Other Methods\\n\\nIn addition to the stacked denoising autoencoder, the previously mentioned con- volutional neural networks play an important role. To reduce complexity and save time, the feature layers are not fully connected, rather each feature neuron retains input from only a few neurons in the layer below. The layers also alternate between convolution and pooling layers. In each convolution layer, a trainable linear ﬁlter is used on the input neurons, and in the pooling layer, an average, maximum or a more complex function is calculated from the input neurons.\\n\\nAlso quite popular are deep belief networks, which use restricted Boltzmann\\n\\nmachines for learning [HOT06].\\n\\nDiscovery of features with UL networks can be completely replaced by clus- tering, in which, for every discovered cluster, a binary feature determines whether a point belongs to that particular cluster. One can also use kernel PCA, a nonlinear generalization of PCA, to learn features. As mentioned earlier, the fully connected SL network can also be replaced by other learning algorithms, for instance by a support vector machine.\\n\\n9.7 Deep Learning\\n\\n9.7.4 Systems and Implementations\\n\\nEven the best current implementations of deep learning systems are very compu- tationally intensive. The cause of the long computation times is the size of the input layer and the high number of layers in the network. This effect is ampliﬁed further if, the training data are elements of a high-dimensional vector space. To represent the trained classes well, a great many data vectors are needed, which makes the computation time even longer.\\n\\nin the case of a large input\\n\\nlayer,\\n\\nThis means that a training run can take from minutes up to even days. In addition, the system parameters of the complex networks have to be conﬁgured, which, in turn, has a big impact on the quality of the results. As we know from Chap. 8, optimal metaparameters for a learning algorithm can be found with cross-validation, that is by trying out all combinations of values. Due to the complexity of deep learning algorithms, there are many parameters, and the set of parameter combinations grows exponentially with the number of parameters. Therefore, the naive application of cross-validation is not practical. Instead, algo- rithms are used which look for a point in the space of the system’s metaparameters that minimizes errors on the validation data [BBBK11]. Examples of such meta- parameters are the number of layers in the UL network as well as the SL network, the number of neurons in the individual layers, the learning rates, and the degree of interconnectedness for CNN networks. The algorithms for optimization of meta- parameters use, for example, random search or gradient descent in the space of metaparameters [MDA15].\\n\\nThus, for high-dimensional data sets, simple PCs are overwhelmed. Today one works with multiprocessor machines and the learning algorithms are run highly parallel on modern graphics cards, which ultimately leads to training times of hours to weeks for training with hyper parameter optimization.\\n\\nThirty-eight different freely available software systems are listed at http:// deeplearning.net/software_links.7 Theano, with special support for graphics cards, and the systems Pylearn2 und Keras, which are based on Theano, are especially interesting. Each of these are programmed in the Python programming language. Tensorﬂow from Google Deep Brain also uses Python. The only additional requirement for a successful project is a suitably fast machine, which can be rented from cloud service providers as needed.\\n\\n9.7.5 Applications of Deep Learning\\n\\nEspecially in object classiﬁcation in photos, deep learning has shown impressive progress. In [Ben16] there is a collection of results for various data sets. For example, in the recognition of hand-written digits, the classiﬁcation accuracy of 99.8% has been achieved on the MNIST data set presented in [LBBH98] (Fig. 9.28 on page 282 links). On the SVHN data set (Fig. 9.28 on page 282 center) generated\\n\\n7Accessed April, 2016.\\n\\n281\\n\\n282\\n\\n9 Neural Networks\\n\\nFig. 9.28 Examples of MNIST data (left), SVHN data (center) and an example photo for the image description system\\n\\nas part of the Google Street View Project with photos of home address numbers [NWC+11], the accuracy is at about 98.3%. It is even possible to describe photos in complete sentences [VTBE15]. For the photo shown in the right of Fig. 9.28, this system produces the sentence “A person riding a motorcycle on a dirt road.” It uses a convolutional neural network for object recognition and a recurrent neural network for text generation.\\n\\nIn addition to many other applications, there are impressive variants of deep\\n\\nlearning for creatively generating works of art.\\n\\n9.8 Creativity\\n\\nWith deep learning, programs have emerged that can, for example, compose jazz melodies [BBSK10]. A recurrent neural network is used in [Kar15] to generate texts, syntactically correct XML code, even LATEX source with mathematical notation and short computer programs. Hence recurrent neural networks can learn grammar. A program trained on Shakespeare’s body of work can, from scratch, generate new texts such as:\\n\\nKING LEAR: O, if you were a feeble sight, the courtesy of your law, Your sight and several breath, will wear the gods With his heads, and my hands are wonder’d at the deeds, So drop upon your lordship’s head, and your opinion Shall be against your honour.\\n\\nWikipedia describes creativity as a phenomenon in which something new and somehow valuable is formed. It is easy to appreciate this deﬁnition in light of the above text. Let us imagine a program that randomly generates text letter by letter. The result might begin something like\\n\\nNitDjgMQsQfI 6zz1B:6xZkgp1xe.EqyD7z(C\\n\\nWhile the text is new, it is not very valuable and therefore not a creative artifact. On the other hand, a program that queries and outputs text from a database does not\\n\\n9.8 Creativity\\n\\ncount as creative either because it produces nothing new. A creative program, for example, should write a novel that is both interesting and new at the same time. One could easily check the text’s novely, for example by using software for testing the originality of student thesis papers. Interestingness is more difﬁcult. It would be good if a program could generate new texts that a person would enjoy based on the ratings which that person has given to previously read literature.\\n\\nSomewhat more formally, we could deﬁne an adaptive creative program as one that takes some objects from a given distribution of training data, then approximates the distribution and randomly creates a new object from this distribution.\\n\\nFig. 9.29 Artistic pictures of the view of Tübingen in the top left generated with the network from [GEB15]. For each pair of images, the famous painting whose style was used to transform the photo can be seen in the bottom left (Source [GEB15])\\n\\n283\\n\\n284\\n\\n9 Neural Networks\\n\\nOne very nice example of such a creative effort is the CNN in [GEB15], which takes two input images and generates a new one that assumes the style of the ﬁrst image and the content of the second. The results of this program are shown impressively in Fig. 9.29 on page 283. The photo in the top left is transformed into the respective styles of each painting by the old masters.\\n\\nThe program uses the VGG network [SZ15], which is a convolutional network that was pre-trained on the imagenet large scale visual recognition challenge with hundreds of object categories and millions of images [RDS+15]. From this pre-trained object classiﬁcation network only the feature layers, but not the fully connected classiﬁcation layers, are used here. Rather, the authors use only sixteen convolutional layers and ﬁve pooling layers.\\n\\nTo gain a better understanding of the procedure we ﬁrst solve two simpler problems. The photograph is applied and propagated forward through the network, generating the features of the photo image. Now a pixel image with white noise is applied and propagated forward through the network, generating the features of the white noise image. Using gradient descent search, the pixels of the white noise image are now varied until the distance between the features of the image features to those of the photograph is minimized. Ultimately this reproduces the photo’s content.\\n\\nThen the photograph is replaced by the old work of art which is applied to another network with the same structure and, similarly to the photo, the features are computed. Again a pixel image with white noise is applied to the network. Using gradient descent search, it is changed until the distance between the correlations of the generated features and those of the painting are minimized. Ultimately this reproduces the painting’s style.\\n\\nNow, to transfer the syle of the painting onto the photo, both methods are combined. We apply gradient descent search on a pixel image with white noise but now we minimize a linear combination of both of the above distance functions. Thus an image in the style of the painting with the content of the photo is created.\\n\\n9.9 Applications of Neural Networks\\n\\nIn addition to the applications given as examples thus far, there are countless applications for neural networks in all areas of industry, especially for deep learning. Pattern recognition in all of its forms is a very important area, whether analysis of photos to recognize people or faces, recognition of ﬁsh swarms in sonar readings, recognition and classiﬁcation of military vehicles in radar scans, or any number of other applications. Neural networks can also be trained to recognize spoken language and hand written text.\\n\\nNeural networks are not only used for recognizing objects and scenes. They can be trained to control self driving cars or robots based on sensor data, as well as for heuristically controlling search in backgammon and chess computers. An inter- esting use of networks for reinforcement learning is described in Sect. 10.8\\n\\n9.9 Applications of Neural Networks\\n\\nFor quite some time, neural networks, in addition to statistical methods, have been used successfully to forecast stock prices and to judge the creditworthiness of bank customers. Speed trading of international ﬁnancial transactions would be impossible without the help of smart and fast neural networks that autonomously decide about buying or selling.\\n\\nOther machine learning algorithms can as well be used for many of these applications. Due to the great commercial success of data mining, decision tree learning and support vector machines, there are neural algorithms for many applications as well as others that are not biologically motivated at all. The ﬁeld of neural networks is a subarea of machine learning.\\n\\n9.10 Summary and Outlook\\n\\nWith the perceptron, the delta rule, backpropagation, and, built upon them, deep learning, we have introduced the most important class of feedforward networks and shown their relationship to scores and naive Bayes, and also to the method of least squares.\\n\\nNearest to their biological role models are the fascinating Hopﬁeld networks. However, due to their complex dynamics, they are difﬁcult to handle in practice. In all neural models, stored information is distributed over many weights. Therefore, the death of a few neurons has little noticeable effect on the function of a brain. The network is robust against small disturbances due to the distributed storage of the data. Related to this is the ability to recognize noisy patterns, which has been shown in several examples.\\n\\nThe distributed representation of knowledge has the disadvantage, however, that it is difﬁcult for knowledge engineers to localize knowledge. It is practically impossible to analyze and understand the many weights in a trained fully connected neural net- work. In contrast, it is relatively easy to understand the learned knowledge in a decision tree, and even to represent it as a logical formula. Predicate logic, which allows formalization of relationships, is especially expressive and elegant. For example, a predicate grandmother(katrin, klaus) is easy to understand. This type of relationship can also be learned with neural networks. However, it is not possible to locate the “grandmother neuron” in the network. Thus, even today there are problems when connecting neural networks with symbol processing systems. A very important step in this direction is the automatic feature generation of deep learning networks.\\n\\nOf the great number of interesting neural models, many have not been discussed. For example, self-organizing maps, introduced by Kohonen, are very interesting. A self-organizing map performs biologically inspired mapping from a sensory neural layer onto a second layer of neurons with the property that this mapping is adaptive and preserves similarities.\\n\\nOne problem with the networks presented here is incremental learning. If, for example, a fully trained backpropagation network is trained further with new pat- terns, many and eventually all of the old patterns will begin to be quickly forgotten.\\n\\n285\\n\\n286\\n\\n9 Neural Networks\\n\\nTo solve this problem, Carpenter and Grossberg developed adaptive resonance theory (ART), which has lead to a set of neural models.\\n\\nAs further reading, we recommend the textbooks [Bis05, Zel94, RMS92, Roj96, GBC16]. The collected volumes [AR88, APR90] are recommended for those interested in studying the history of this exciting area from the older, original works.\\n\\n9.11 Exercises\\n\\n9.11.1 From Biology to Simulation\\n\\nExercise 9.1 Show the point symmetry of the sigmoid function to ðh; 1 2Þ.\\n\\n9.11.2 Hopfield Networks\\n\\nExercise 9.2 Use the Hopﬁeld network applet at http://www.cbu.edu/*pong/ai/ hopﬁeld/hopﬁeldapplet.html and test the memory capacity for correlated and uncorrelated patterns (with randomly set bits) with a 10 (cid:3) 10 grid size. Use a pattern with 10% noise for testing. Exercise 9.3 Compare the theoretical limit N ¼ 0:146 n for the maximum number of patterns which can be stored, given in Sect. 9.2.2, with the capacity of classical binary storage of the same size.\\n\\n9.11.3 Linear Networks with Minimal Errors\\n\\n➳ Exercise 9.4\\n\\n(a) Write a program for learning a linear mapping with the least mean square method. This is quite simple: one must only set up the normal equations by (9.12) on page 265 and then solve the system of equations.\\n\\n(a) Apply this program to the appendicitis data on this book’s website and determine a linear score. Give the error as a function of the threshold, as in Fig. 9.14 on page 266.\\n\\n(c) Now determine the ROC curve for this score.\\n\\n9.11.4 Backpropagation\\n\\nExercise 9.5 Using a backpropagation program (for example in JNNS or KNIME), create a network with eight input neurons, eight output neurons, and three hidden neurons. Then train the network with eight training data pairs qp, qp with the\\n\\n9.11 Exercises\\n\\nproperty that, in the pth vector qp, the pth bit is one and all other bits are zero. The network thus has the simple task of learning an identical mapping. Such a network is called an 8-3-8 encoder. After the learning process, observe the encoding of the three hidden neurons for the input of all eight learned input vectors. What do you notice? Now repeat the experiment, reduce the number of hidden neurons to two and then one. Exercise 9.6 In order to show that backpropagation networks can divide non lin- early separable sets, train the XOR function. The training data for this is: ((0, 0), 0), ((0, 1), 1), ((1, 0), 1), ((1, 1), 0).\\n\\n(a) Create a network with two neurons each in the input layer and hidden layer,\\n\\nand one neuron in the output layer, and train this network.\\n\\n(b) Now delete the hidden neurons and connect the input layer directly to the\\n\\noutput neurons. What do you observe?\\n\\nExercise 9.7 (a) Show that any multi-layer backpropagation network with a linear activation function is equally powerful as a two-layer one. For this it is enough to show that successive executions of linear mappings is a linear mapping.\\n\\n(b) Show that a two-layer backpropagation network with any strictly monotonic activation function is not more powerful for classiﬁcation tasks than one without an activation function or with a linear activation function.\\n\\n9.11.5 Support Vector Machines\\n\\n❄ Exercise 9.8 Two non linearly separable two-dimensional sets of training data M+ and M− are given. All points in M+ are within the unit circle x2 2 ¼ 1 and all points in M− are outside. Give a coordinate transform f : ℝ2 ! ℝ2 which makes the data linearly separable. Give the equation of the dividing line and sketch the two spaces and the data points.\\n\\n1 þ x2\\n\\n287\\n\\nReinforcement Learning\\n\\n10.1 Introduction\\n\\nAll of the learning algorithms described so far—except the clustering algorithms— belong to the class of supervised learning. In supervised learning, the agent is supposed to learn a mapping from the input variables to the output variables. Here it is important that for each individual training example, all values for both input variables and output variables are provided. In other words, we need a teacher or a database in which the mapping to be learned is approximately deﬁned for a sufﬁ- cient number of input values. The sole task of the machine learning algorithm is to ﬁlter out the noise from the data and ﬁnd a function which approximates the mapping well, even between the given data points.\\n\\nIn reinforcement learning the situation is different and more difﬁcult because no training data is available. We begin with a simple illustrative example from robotics, which then is used as an application for the various algorithms.\\n\\nReinforcement learning is very valuable in the ﬁeld of robotics, where the tasks to be performed are frequently complex enough to defy encoding as programs and no training data is available. The robot’s task consists of ﬁnding out, through trial and error (or success), which actions are good in a certain situation and which are not. In many cases we humans learn in a very similar way. For example, when a child learns to walk, this usually happens without instruction, rather simply through reinforcement. Successful attempts at walking are rewarded by forward progress, and unsuccessful attempts are penalized by often painful falls. Positive and negative reinforcement are also important factors in successful learning in school and in many sports (see Fig. 10.1 on page 290).\\n\\nA greatly simpliﬁed movement task is learned in the following example.\\n\\nExample 10.1 A robot whose mechanism consists only of a rectangular block and an arm with two joints gy and gx is shown in Fig. 10.2 on page 290 (see [KMK97]). The robot’s only possible actions are the movement of gy up or down and of gx right or left. Furthermore, we only allow movements of ﬁxed discrete units (for example,\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_10\\n\\n10\\n\\n289\\n\\n290\\n\\n10 Reinforcement Learning\\n\\nFig. 10.1 “Maybe next time I should start turning a bit sooner or go slower?”—Learning from negative reinforcement\\n\\nFig. 10.2 By moving both of its joints, the crawling robot in the left of the image can move forward and backward. The walking robot on the right side must correspondingly move the frame up and down or left and right. The feedback for the robot’s movement is positive for movement to the right and negative for movement to the left\\n\\nof 10-degree increments). The agent’s task consists of learning a policy which allows it to move as quickly as possible to the right. The walking robot in Fig. 10.2 works analogously within the same two-dimensional state space.\\n\\nA successful action sequence is shown in Table 10.1 on page 291. The action at time t ¼ 2 results in the loaded arm moving the body one unit length to the right. Nice animations of this example can be found through [KMK97] and [Tok06].\\n\\nBefore we go into the learning algorithm, we must suitably model the task mathematically. We describe the state of the robot by the two variables gx and gy for the position of the joints, each with ﬁnitely many discrete values. The state of the robot is thus encoded as a vector (gx, gy). The number of possible joint positions is nx, or respectively ny. We use the horizontal position of the robot’s body, which can take on real values, to evaluate the robot’s actions. Movements to the right are\\n\\n10.1 Introduction\\n\\nTable 10.1 A cycle of a periodic series of movements with systematic forward movement\\n\\nCrawling robot\\n\\nRunning robot\\n\\nTime t\\n\\ngy\\n\\nState\\n\\ngx\\n\\nReward x\\n\\n0\\n\\nUp\\n\\nLeft\\n\\n0\\n\\n1\\n\\nUp\\n\\nRight\\n\\n0\\n\\n2\\n\\nDown\\n\\nRight\\n\\n0\\n\\n3\\n\\nDown\\n\\nLeft\\n\\n1\\n\\nFig. 10.3 The state space of the example robot in the case of two possible positions for each of the joints (left) and in the case of four horizontal and vertical positions for each (middle). In the right image an optimal policy is given\\n\\nrewarded with positive changes to x, and movements to the left are punished with negative changes.\\n\\nIn Fig. 10.3 the state space for two variants of this is shown in simpliﬁed form.1 In the left example, both joints have two positions, and in the middle example they each have four positions. An optimal policy is given in Fig. 10.3 right.\\n\\n10.2 The Task\\n\\nAs shown in Fig. 10.4 on page 292, we distinguish between the agent and its environment. At time t the world, which includes the agent and its environment, is described by a state st 2 S. The set S is an abstraction of the actual possible states\\n\\n1The arm movement space consisting of arcs is rendered as a right-angled grid.\\n\\n291\\n\\nAction at\\n\\nRight\\n\\nDown\\n\\nLeft\\n\\nUp\\n\\n292\\n\\n10 Reinforcement Learning\\n\\nFig. 10.4 The agent and its interaction with the environment\\n\\nof the world because, on one hand, the world cannot be exactly described, and on the other hand the agent often only has incomplete information about the actual state because of measurement errors. The agent then carries out an action at 2 A at time t. This action changes the world and thus results in the state st + 1 at time t þ 1. The state transition function d deﬁned by the environment determines the new state st + 1 ¼ d(st, at). It cannot be inﬂuenced by the agent.\\n\\nAfter executing action at, the agent obtains immediate reward rt ¼ r(st, at) (see Fig. 10.4). The immediate reward rt ¼ r(st, at) is always dependent on the current state and the executed action. r(st, at) ¼ 0 means that the agent receives no immediate feedback for the action at. During learning, rt > 0 should result in positive and rt < 0 in negative reinforcement of the evaluation of the action at in state st. In reinforcement learning especially, applications are being studied in which no immediate reward happens for a long time. A chess player for example learns to improve his game from won or lost matches, even if he gets no immediate reward for all individual moves. Here we can see the difﬁculty of assigning the reward at the end of a sequence of actions to all the actions in the sequence that led to this point (credit assignment problem).\\n\\nIn the crawling robot’s case the state consists of the position of the two joints,\\n\\nthat is, s ¼ (gx, gy). The reward is given by the distance x traveled.\\n\\nA policy p : S ! A is a mapping from states to actions. The goal of reinforcement learning is that the agent learns an optimal policy based on its experiences. A policy is optimal if it maximizes reward in the long run, that is, over many steps. But what does “maximize reward” mean exactly? We deﬁne the value, or the discounted reward\\n\\nV p\\n\\nðstÞ ¼ rt þ crt þ 1 þ c2rt þ 2 þ (cid:2) (cid:2) (cid:2) ¼\\n\\nX1\\n\\ncirt þ i\\n\\nð10:1Þ\\n\\ni¼0\\n\\nof a policy p when we start in starting state st. Here 0 (cid:3) c < 1 is a constant, which ensures that future feedback is discounted more the farther in the future that it happens. The immediate reward rt is weighted the strongest. This reward function is the most predominantly used. An alternative which is sometimes interesting is the average reward\\n\\nV p\\n\\nðstÞ ¼ lim h!1\\n\\n1 h\\n\\nXh\\n\\ni¼0\\n\\nrt þ i:\\n\\nð10:2Þ\\n\\n10.2 The Task\\n\\nA policy p⋆is called optimal, if for all states s\\n\\nV pH\\n\\nðsÞ (cid:4) V p\\n\\nðsÞ:\\n\\nThat is, it is at least as good as all other policies according to the deﬁned value will be denoted V⋆. function. For better readability, the optimum value function V pH The agents discussed here, or their policies, only use information about the current state st to determine the next state, and not the prior history. This is justiﬁed if the reward of an action only depends on the current state and current actions. Such processes are called Markov decision processes (MDP). In many applications, especially in robotics, the actual state of the agent is not exactly known, which makes planning actions even more difﬁcult. The reason for this may be a noisy sensor signal. We call such a process a partially observable Markov decision process (POMDP).\\n\\n10.3 Uninformed Combinatorial Search\\n\\nThe simplest possibility of ﬁnding a successful policy is the combinatorial enu- meration of all policies, as described in Chap. 6. However, even in the simple Example 10.1 on page 289 there are a very many policies, which causes combi- natorial search to be associated with extremely high computational cost. In Fig. 10.5 the number of possible actions is given for every state. From that, the number of possible policies is calculated as the product of the given values, as shown in Table 10.2 on page 294.\\n\\nFor arbitrary values of nx and ny there are always four corner nodes with two possible actions, 2(nx − 2) + 2(ny − 2) edge nodes with three actions, and (nx − 2) (ny − 2) inner nodes with four actions. Thus there are\\n\\n2432ðnx(cid:5)2Þ þ 2ðny(cid:5)2Þ4ðnx(cid:5)2Þðny(cid:5)2Þ\\n\\nFig. 10.5 The state space for the example with the values 2, 3, 4, 5 for nx and ny. The number of possible actions is given for each state in the respective circles\\n\\n293\\n\\nð10:3Þ\\n\\n294\\n\\n10 Reinforcement Learning\\n\\nTable 10.2 Number of policies for differently sized state spaces in the example\\n\\nnx, ny 2 3 4 5\\n\\nNumber of states\\n\\n4 9 16 25\\n\\nNumber of policies 24 = 16 24344 = 5184 243844 (cid:6) 2.7 (cid:7) 107 2431249 (cid:6) 2.2 (cid:7) 1012\\n\\ndifferent policies for ﬁxed nx and ny. The number of policies thus grows expo- nentially with the number of states. This is true in general if there is more than one possible action per state. For practical applications this algorithm is therefore useless. Even heuristic search, described in Chap. 6, cannot be used here. Since the direct reward for almost all actions is zero, it cannot be used as a heuristic evalu- ation function.\\n\\nThe computational cost rises even higher when we consider that (in addition to enumerating all policies), the value V p (s) must be calculated for every generated policy p and every starting state s. The inﬁnite sum in V p (s) must be cut off for use in a practical calculation; however, due to the exponential reduction of the ci factors in (10.1) on page 292, this does not present a problem.\\n\\nIn Example 10.1 on page 289 the difference xt+1 − xt can be used as an immediate reward for an action at, which means that every movement of the robot’s body to the right is rewarded with 1 and every movement of the robot’s body to the left is penalized with −1. In Fig. 10.6, two policies are shown. Here the immediate reward is zero everywhere other than in the bottom row of the state space. The left policy p1 is better in the long term because, for long action sequences, the average progress per action is 3/8 ¼ 0.375 for p1 and 2/6 (cid:6) 0.333 for p2. If we use (10.1) on page 292 for V p (s), the result is the following table with starting state s0 at the top left and various c values:\\n\\nc V p1 ðs0Þ V p2 ðs0Þ\\n\\n0.9\\n\\n2.52 2.39\\n\\n0.8375\\n\\n1.156 1.156\\n\\n0.8\\n\\n0.77 0.80\\n\\nHere we see that policy p1 is superior to policy p2 when gamma ¼ 0.9, and the reverse is true when gamma ¼ 0.8. For c (cid:6) 0.8375 both policies are equally good. We can clearly see that a larger c results in a larger time horizon for the evaluation of policies.\\n\\nFig. 10.6 Two different policies for the example\\n\\n10.4 Value Iteration and Dynamic Programming\\n\\n10.4 Value Iteration and Dynamic Programming\\n\\nIn the naive approach of enumerating all policies, much redundant work is per- formed, because many policies are for the most part identical. They may only differ slightly. Nevertheless every policy is completely newly generated and evaluated. This suggests saving intermediate results for parts of policies and reusing them. This approach to solving optimization problems was introduced as dynamic pro- gramming by Richard Bellman already in 1957 [Bel57]. Bellman recognized that for an optimal policy it is the case that:\\n\\nIndependent of the starting state st and the ﬁrst action at, all subsequent decisions pro- ceeding from every possible successor state st+1 must be optimal.\\n\\nBased on the so-called Bellman principle, it becomes possible to ﬁnd a globally optimal policy through local optimization of individual actions. We will derive this principle for MDPs together with a suitable iteration algorithm.\\n\\nDesired is an optimal policy p⋆ which fulﬁlls (10.3) on page 293 and (10.1) on\\n\\npage 292. We rewrite the two equations and obtain\\n\\nV H ðstÞ ¼\\n\\nmax at;at þ 1;at þ 2;...\\n\\nðrðst; atÞ þ crðst þ 1; at þ 1Þ þ c2rðst þ 2; at þ 2Þ þ (cid:2) (cid:2) (cid:2)Þ:\\n\\nSince the immediate reward r(st, at) only depends on st and at, but not on the successor states and actions, the maximization can be distributed, which ultimately results in the following recursive characterization of V⋆:\\n\\nV HðstÞ ¼ max\\n\\nat ¼ max at\\n\\n½r st; at ð\\n\\n½\\n\\nr st; at ð\\n\\nÞ þ c max\\n\\nat þ 1;at þ 2;... (cid:8): Þ þ cV H ðst þ 1Þ\\n\\nr st þ 1; at þ 1 ð ð\\n\\nÞ þ c r st þ 2; at þ 2\\n\\nð\\n\\nÞ þ (cid:2) (cid:2) (cid:2)\\n\\nÞ(cid:8)\\n\\nEquation (10.5) results from the substitution t ! t + 1 in (10.4). Written somewhat simpler:\\n\\nV H ðsÞ ¼ max\\n\\na\\n\\n½rðs; aÞ þ cV Hðdðs; aÞÞ(cid:8):\\n\\nThis equation implies, as does (10.1) on page 292, that, to calculate V⋆(s), the immediate reward is added to the reward of all successor states, discounted by the factor c. If V⋆(d(s, a)) is known, then V⋆(s) clearly results by simple local opti- mization over all possible actions a in state s. This corresponds to the Bellman principle, because of which (10.6) is also called the Bellman equation.\\n\\n295\\n\\nð10:4Þ\\n\\nð10:5Þ\\n\\nð10:6Þ\\n\\n296\\n\\n10 Reinforcement Learning\\n\\nThe optimal policy p⋆(s) carries out an action in state s which results in the\\n\\nmaximum value V⋆. Thus,\\n\\npH ðsÞ ¼ argmax\\n\\n½rðs; aÞ þ cV H ðdðs; aÞÞ(cid:8):\\n\\nð10:7Þ\\n\\na\\n\\nFrom the recursion equation (10.6) on page 295 an iteration rule for approximat- ing V⋆: follows in a straightforward manner:\\n\\n^VðsÞ ¼ max\\n\\na\\n\\n½rðs; aÞ þ c ^Vðdðs; aÞÞ(cid:8):\\n\\nð10:8Þ\\n\\nTo begin the approximate values ^VðsÞ for all states are initialized, for example with the value zero. Now ^VðsÞ is repeatedly updated for each state by recursively falling back on the value ^Vðdðs; aÞÞ of the best successor state. This process of calculating V⋆ is called value iteration and is shown schematically in Fig. 10.7. It can be shown that value iteration converges to V⋆ [SB98]. An excellent analysis of dynamic programming algorithms can be found in [Sze10], where, based on con- traction properties of the particular algorithms (for example value iteration), con- vergence can be proven using Banach’s ﬁxed-point theorem.\\n\\nIn Fig. 10.8 on page 297 this algorithm is applied to Example 10.1 on page 289 with c ¼ 0.9. In each iteration the states are processed row-wise from bottom left to top right. Shown are several beginning iterations and in the second image in the bottom row the stable limit values for V⋆.\\n\\nWe clearly see the progression of the learning in this sequence. The agent repeatedly explores all states, carries out value iteration for each state and saves the policy in the form of a tabulated function V⋆, which then can be further compiled into an efﬁciently usable table p⋆.\\n\\nIncidentally, to ﬁnd an optimal policy from V⋆ it would be wrong to choose the action in state st which results in the state with the maximum V⋆ value. Corre- sponding to (10.7), the immediate reward r(st, at) must also be added because we\\n\\nFig. 10.7 The algorithm for value iteration\\n\\nVALUEITERATION() For all s ∈ S ˆV (s) = 0\\n\\nRepeat\\n\\nFor all s ∈ S\\n\\nˆV (s) = maxa[r(s, a) + γ ˆV (δ(s, a))]\\n\\nUntil ˆV (s) does not change\\n\\n10.4 Value Iteration and Dynamic Programming\\n\\nFig. 10.8 Value iteration in the example with 3 (cid:7) 3 states. The last two images show two optimal policies. The numbers next to the arrows give the immediate reward r(s, a) of each action\\n\\nare searching for V⋆(st) and not V⋆(st+1). Applied to state s ¼ (2, 3) in Fig. 10.8, this means\\n\\npH 2; 3ð\\n\\nÞ ¼ argmax f\\n\\na2 left;right;up\\n\\n¼ argmax left;right;up\\n\\nf\\n\\ng\\n\\n½\\n\\nr s; að\\n\\nÞ þ cV H d s; að ð\\n\\nÞ\\n\\nÞ\\n\\n(cid:8)\\n\\ng 1 þ 0:9 (cid:2) 2:66; (cid:5)1 þ 0:9 (cid:2) 4:05; 0 þ 0:9 (cid:2) 3:28 f\\n\\ng\\n\\n¼ argmax left;right;up\\n\\nf\\n\\ng\\n\\n3:39; 2:65; 2:95 f\\n\\ng ¼ left.\\n\\nIn (10.7) on page 296 we see that the agent in state st must know the immediate reward rt and the successor state st+1 ¼ d(st, at) to choose the optimal action at. It must also have a model of the functions r and d. Since this is not the case for many practical applications, algorithms are needed which can also work without knowledge of r and d. Section 10.6 is dedicated to such an algorithm.\\n\\n297\\n\\n298\\n\\n10 Reinforcement Learning\\n\\n10.5 A Learning Walking Robot and Its Simulation\\n\\nA graphical user interface for simple experiments with reinforcement learning is shown in Fig. 10.9 [TEF09]. The user can observe reinforcement learning for differently sized two-dimensional state spaces. For better generalization, back- propagation networks are used to save the state (see Sect. 10.8). The feedback editor shown at the bottom right, with which the user can manually supply feedback about the environment, is especially interesting for experiments. Not shown is the menu for setting up the parameters for value iteration and backpropagation learning.\\n\\nFig. 10.9 Four different windows of the walking robot simulator\\n\\n10.5 A Learning Walking Robot and Its Simulation\\n\\nreal crawling robots with the same two-dimensional discrete state space were developed speciﬁcally for teach- ing [TEF09].2 The two robots are shown in Fig. 10.10. Each moves with a servo actuator. The servos are controlled by a microcontroller or through a wireless interface directly from a PC. Using simulation software, the feedback matrix of the robot can be visualized on the PC. With this saved feedback, a policy can be trained on the PC (which computes faster), then loaded again into the robot and executed. However, the robot can also learn autonomously. For a state space of size 5 (cid:7) 5 this takes about 30 seconds.\\n\\nBesides the simulation,\\n\\ntwo small,\\n\\nIt is interesting to observe the difference between the simulation and the “real” robot. In contrast to the simulation, the crawler learns policies in which it never lifts its arm from the ground, but nonetheless moves forward very efﬁciently. The reason for this is that, depending on the surface of the underground, the tip of the “un- derarm” can grip the ground during backward movement, but slides through during forward movement. This effect is very sensibly perceived through the distance measuring sensors and evaluated accordingly during learning.\\n\\nThe robot’s adaptivity results in surprising effects. For example, we can observe how the crawler, despite a defective servo which slips at a certain angle, nonetheless learns to walk (more like hobbling). It is even capable of adapting to changed situations by changing policies. A thoroughly desirable effect is the ability, given differently smooth surfaces (for example, different rough carpets) to learn an optimal policy for each. It also turns out that the real robot is indeed very adaptable given a small state space of size 5 (cid:7) 5.\\n\\nThe reader may (lacking a real robot) model various surfaces or servo defects by varying feedback values and then observing the resulting policies (Exercise 10.3 on page 311).\\n\\nFig. 10.10 Two versions of the crawling robot\\n\\n2Further www.hs-weingarten.de/*ertel/kibuch.\\n\\ninformation and related sources about crawling robots are available through\\n\\n299\\n\\n300\\n\\n10 Reinforcement Learning\\n\\n10.6 Q-Learning\\n\\nA policy based on evaluation of possible successor states is clearly not useable if the agent does not have a model of the world, that is, when it does not know which state a possible action leads to. In most realistic applications the agent cannot resort to such a model of the world. For example, a robot which is supposed to grasp complex objects cannot predict whether the object will be securely held in its grip after a gripping action, or whether it will remain in place.\\n\\nIf there is no model of the world, an evaluation of an action at carried out in state st is needed even if it is still unknown where this action leads to. Thus we now work with an evaluation function Q(st, at) for states with their associated actions. With this function, the choice of the optimal action is made by the rule\\n\\npH ðsÞ ¼ argmax\\n\\nQðs; aÞ:\\n\\nð10:9Þ\\n\\na\\n\\nTo deﬁne the evaluation function we again use stepwise discounting of the eval- uation for state-action pairs which occur further into the future, just as in (10.1) on page 292. We thus want to maximize rt + crt+1+ c2rt+2+ ⋅ ⋅ ⋅. Therefore, to evaluate action at in state st we deﬁne in analogy to (10.4) on page 295:\\n\\nQðst; atÞ ¼ max\\n\\nat þ 1;at þ 2;...ðrðst; atÞ þ crðst þ 1; at þ 1Þ þ c2rðst þ 2; at þ 2Þ þ (cid:2) (cid:2) (cid:2)Þ: ð10:10Þ\\n\\nAnalogously to the approach for value iteration, we bring this equation into a simple recursive form by\\n\\nQ st; at ð\\n\\nÞ ¼ max\\n\\nat þ 1;at þ 2;...\\n\\n(cid:2) r st; at ð\\n\\nÞ þ cr st þ 1; at þ 1\\n\\nð\\n\\nÞ þ c2r st þ 2; at þ 2 ð\\n\\nÞ þ (cid:2) (cid:2) (cid:2)\\n\\n(cid:3)\\n\\n¼ r st; at ð\\n\\nÞ þ c max\\n\\nat þ 1;at þ 2;...\\n\\nð\\n\\nr st þ 1; at þ 1 ð\\n\\nÞ þ cr st þ 2; at þ 2\\n\\nð\\n\\nÞ þ (cid:2) (cid:2) (cid:2)\\n\\nÞ\\n\\n¼ r st; at ð\\n\\nÞ þ c max at þ 1 Þ þ c max at þ 1 Þ þ c max at þ 1 Þ þ c max\\n\\n¼ r st; at ð\\n\\n¼ r st; at ð\\n\\n¼ r s; að\\n\\na0\\n\\nðr st þ 1; at þ 1\\n\\nð\\n\\nÞ þ c max at þ 2\\n\\nQ st þ 1; at þ 1\\n\\nð\\n\\nÞ\\n\\nQ d st; at ð\\n\\nð\\n\\nÞ; at þ 1\\n\\nÞ\\n\\nQ d s; að\\n\\nð\\n\\nÞ; a0\\n\\nÞ:\\n\\nr st þ 2; at þ 2 ð ð\\n\\nÞ þ (cid:2) (cid:2) (cid:2)\\n\\nÞÞ\\n\\nð10:11Þ\\n\\nWhat then is the advantage compared to value iteration? The old equation is only slightly rewritten, but this turns out to be exactly the right approach to a new algo- rithm. Instead of saving V⋆, now the function Q is saved, and the agent can choose its actions from the functions d and r without a model of the world. We still do not have a process, however, which can learn Q directly, that is, without knowledge of V H . From the recursive formulation of Q(s, a), an iteration algorithm for determining Q(s, a) can be derived in a straightforward manner. We initialize a table ^Qðs; aÞ for all states arbitrarily, for example with zeroes, and iteratively carry out\\n\\n10.6 Q-Learning\\n\\nQ-LEARNING() For all s ∈ S, a ∈ A\\n\\nˆQ(s, a) = 0 (or randomly)\\n\\nRepeat\\n\\nSelect (e.g. randomly) a state s Repeat\\n\\nSelect an action a and carry it out Obtain reward r and new state s ˆQ(s, a) := r(s, a) + γ maxa s := s\\n\\nˆQ(s , a )\\n\\nUntil s is an ending state Or time limit reached\\n\\nUntil ˆQ converges\\n\\nFig. 10.11 The algorithm for Q-learning\\n\\n^Qðs; aÞ ¼ rðs; aÞ þ c max\\n\\na0\\n\\n^Qðdðs; aÞ; a0Þ:\\n\\nIt remains to note that we do not know the functions r and d. We solve this problem quite pragmatically by letting the agent in its environment in state s carry out action a. The successor state is then clearly d(s, a) and the agent receives its reward from the environment. The algorithm shown in Fig. 10.11 implements this algo- rithm for Q-learning.\\n\\nThe application of the algorithm to Example 10.1 on page 289 with c ¼ 0.9 and nx ¼ 3, ny ¼ 2 (that is, in a 2 (cid:7) 3 grid) is shown in Fig. 10.12 on page 302 as an example. In the ﬁrst picture, all Q values are initialized to zero. In the second picture, after the ﬁrst action sequence, the four r values which are not equal to zero become visible as Q values. In the last picture, the learned optimal policy is given. The following theorem, whose proof is found in [Mit97], shows that this algorithm converges not just in the example, but in general.\\n\\nTheorem 10.1 Let a deterministic MDP with limited immediate reward r(s, a) be given. Equation (10.12) with 0 (cid:3) c < 1 is used for learning. Let ^Qnðs; aÞ be the value for ^Qðs; aÞ after n updates. If each state-action pair is visited inﬁnitely often, then ^Qnðs; aÞ converges to Q(s, a) for all valuess and a for n ! ∞.\\n\\nProof Since each state-action transition occurs inﬁnitely often, we look at suc- cessive time intervals with the property that, in every interval, all state-action transitions occur at least once. We now show that the maximum error for all entries in the ^Q table is reduced by at the factor c in each of these intervals. Let\\n\\nleast\\n\\n301\\n\\nð10:12Þ\\n\\n302\\n\\n10 Reinforcement Learning\\n\\nFig. 10.12 Q-learning applied to the example with nx ¼ 3, ny ¼ 2. The gray arrows mark the actions carried out in each picture. The updated Q values are given. In the last picture, the current policy, which is also optimal, is shown\\n\\nDn ¼ max s;a\\n\\nj ^Qnðs; aÞ (cid:5) Qðs; aÞj\\n\\nbe the maximum error in the table ^Qn and s′ ¼ d(s, a). For each table entry ^Qnðs; aÞ we calculate its contribution to the error after an interval as\\n\\n(cid:4) (cid:4) ^Qn þ 1 s; að\\n\\nÞ (cid:5) Q s; að\\n\\nÞ\\n\\n(cid:4) (cid:4)\\n\\n^Qn s0; a0 ð ^Qn s0; a0 ð j ^Qn s0; a0 ð j ^Qn s00; a0 ð\\n\\nQ s0; a0 ð\\n\\n¼ jðr þ c max\\n\\nÞÞ (cid:5) ðr þ c max\\n\\na0\\n\\na0\\n\\n¼ cjmax\\n\\nQ s0; a0 ð\\n\\nÞ (cid:5) max\\n\\nÞj\\n\\na0 (cid:3) c max\\n\\na0 Þ (cid:5) Q s0; a0 ð\\n\\nÞj\\n\\na0\\n\\nÞj ¼ c Dn:\\n\\n(cid:3) c max s00a0\\n\\nÞ (cid:5) Q s00; a0 ð\\n\\nÞÞj\\n\\nThe ﬁrst inequality is true because, for arbitrary functions f and g,\\n\\nlj max x\\n\\nf ðxÞ (cid:5) max\\n\\nx\\n\\ngðxÞj (cid:3) max\\n\\nx\\n\\nj f ðxÞ (cid:5) gðxÞj\\n\\nand the second inequality is true because, by additional variation of the state s00, the resulting maximum cannot become smaller. Thus it has been shown that Dn+1 (cid:3) cDn. Since the error in each interval is reduced by a factor of at least c, after k intervals it is at most ck D0, and, as a result, D0 is bounded. Since each state is visited inﬁnitely many times, there are inﬁnitely many intervals and Dn converges □ to zero.\\n\\n10.6 Q-Learning\\n\\nAccording to Theorem 10.1 on page 301 Q-learning converges independently of the actions chosen during learning. This means that for convergence it does not matter which actions the agent chooses, as long as each is executed inﬁnitely often. The speed of convergence, however, certainly depends on which paths the agent takes during learning (see Sect. 10.7).\\n\\n10.6.1 Q-Learning in a Nondeterministic Environment\\n\\nIn many robotics applications, the agent’s environment is nondeterministic. This means that the reaction of the environment to the action a in state s at two different points in time can result in different successor states and rewards. Such a nondeter- ministic Markov process is modeled by a probabilistic transition function d(s, a) and probabilistic immediate reward r(s, a). To deﬁne the Q function, each time the expected value must be calculated over all possible successor states. Equa- tion (10.11) on page 300 is thus generalized to\\n\\nQðst; atÞ ¼ Eðrðs; aÞÞ þ c\\n\\nX\\n\\ns0\\n\\nPðs0js; aÞ max\\n\\na0\\n\\nQðs0; a0Þ;\\n\\nwhere Pðs0js; aÞ is the probability of moving from state s to the successor state s0 with action a. Unfortunately there is no guarantee of convergence for Q-learning in the nondeterministic case if we proceed as before according to (10.12) on page 301. This is because, in successive runs through the outer loop of the algorithm in Fig. 10.11 on page 301, the reward and successor state can be completely different for the same state s and same action a. This may result in an alternating sequence which jumps back and forth between several values. To avoid this kind of strongly jumping Q values, we add the old weighted Q value to the right side of (10.12) on page 301. This stabilizes the iteration. The learning rule then reads\\n\\n^Qnðs; aÞ ¼ ð1 (cid:5) anÞ ^Qn(cid:5)1ðs; aÞ þ an\\n\\nh rðs; aÞ þ c max\\n\\na0\\n\\n^Qn(cid:5)1ðdðs; aÞ; a0Þ\\n\\ni\\n\\nwith a time-varying weighting factor\\n\\nan ¼\\n\\n1 1 þ bnðs; aÞ\\n\\n:\\n\\nThe value bn (s, a) indicates how often the action a was executed in state s at the nth iteration. For small values of bn (that is, at the beginning of learning) the stabilizing term ^Qn(cid:5)1ðs; aÞ does not come into play, for we want the learning process to make quick progress. Later, however, bn gets bigger and thereby prevents excessively large jumps in the sequence of ^Q values. When integrating (10.14) into Q-learning, the values bn (s, a) must be saved for all state-action pairs. This can be accom- plished by extending the table of ^Q values.\\n\\n303\\n\\nð10:13Þ\\n\\nð10:14Þ\\n\\n304\\n\\n10 Reinforcement Learning\\n\\nFor a better understanding of (10.14) on page 303, we simplify this by assuming\\n\\nan ¼ a is a constant and transforming it as follows:\\n\\n^Qn s; að\\n\\nÞ ^Qn(cid:5)1 s; að\\n\\n^Qn(cid:5)1 d s; að\\n\\nÞ ¼ 1 (cid:5) a ð ¼ ^Qn(cid:5)1 s; að\\n\\nÞ þ a½r s; að\\n\\nÞ þ c max ^Qn(cid:5)1\\n\\nd s; að\\n\\nÞ; a0 Þ(cid:8) Þ (cid:5) ^Qn(cid:5)1 s; að\\n\\nð\\n\\na0\\n\\nÞ þ a½r s; að\\n\\nÞ(cid:8): |ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ} TD(cid:5)error\\n\\nÞ þ c max\\n\\nÞ; a0\\n\\nð\\n\\na0\\n\\nThe new Q value ^Qnðs; aÞ can clearly be represented as the old ^Qn(cid:5)1ðs; aÞ plus a times a correction term which is the same as the Q value’s change in this step. The correction term is called the TD-error, or temporal difference error, and the above equation for changing the Q value is a special case of TD-Learning, an important class of learning algorithms [SB98]. For a ¼ 1 we obtain the Q-learning described above. For a ¼ 0 the ^Q values are completely unchanged. Thus no learning takes place.\\n\\n10.7 Exploration and Exploitation\\n\\nFor Q-learning so far, only a coarse algorithm schema has been given. Especially lacking is a description of the choice of the starting state each time and the actions to be carried out in the inner loop of Fig. 10.11 on page 301. For the selection of the next action there are two possibilities. Among the possible actions, one can be chosen randomly. In the long term this results in a uniform exploration of all possible actions or policies, but with very slow convergence. An alternative to this is the exploitation of previously learned ^Q values. Here the agent always chooses the action with the highest ^Q value. This results in relatively fast convergence of a speciﬁc trajectory. Other paths, however, remain unvisited all the way to the end. In the extreme case then we can obtain non-optimal policies. In Theorem 10.1 on page 301 it is therefore required that every state-action pair is visited inﬁnitely many times. It is recommended to use a combination of exploration and exploita- tion with a high exploration portion at the beginning and reduce it more and more over time.\\n\\nThe choice of the starting state also inﬂuences the speed of learning. In the ﬁrst three pictures in Fig. 10.12 on page 302 we can clearly see that, for the ﬁrst iterations, only the Q values in the immediate vicinity of state-action pairs are changed by immediate reward. Starting farther away from this kind of point results in much unnecessary work. This suggests transferring prior knowledge about state-action pairs with immediate reward into starting states nearby these points. In the course of learning then more distant starting states can be selected.\\n\\n10.8 Approximation, Generalization and Convergence\\n\\n10.8 Approximation, Generalization and Convergence\\n\\nAs Q-learning has been described so far, a table with all Q values is explicitly saved in a table. This is only possible when working with a ﬁnite state space with ﬁnitely many actions. If the state space is inﬁnite, however, for example in the case of continuous variables, then it is neither possible to save all Q values nor to visit all state-action pairs during learning.\\n\\nNonetheless there is a simple way of using Q-learning and value iteration on continuous variables. The Q(s, a) table is replaced by a neural network, for example a backpropagation network with the input variables s, a and the Q value as the target output. For every update of a Q value, the neural network is presented a training example with (s, a) as input and Q(s, a) as target output. At the end we have a ﬁnite representation of the function Q(s, a). Since we only ever have ﬁnitely many training examples, but the function Q(s, a) is deﬁned for inﬁnitely many inputs, we thus automatically obtain a generalization if the network size is chosen appropriately (see Chap. 9). Instead of a neural network, we can also use another supervised learning algorithm or a function approximator such as a support vector machine or a Gaussian process.\\n\\nHowever, the step from ﬁnitely many training examples to a continuous function can become very expensive in certain situations. Q-learning with function approximation might not converge because Theorem 10.1 on page 301 is only true if each state-action pair is visited inﬁnitely often.\\n\\nHowever, convergence problems can also come up in the case of ﬁnitely many state-action pairs when Q-learning is used on a POMDP. Q-learning can be applied—in both described variants—to deterministic and nondeterministic Markov processes (MDPs). For a POMDP it can happen that the agent, due to noisy sensors for example, perceives many different states as one. Often many states in the real world are purposefully mapped to one so-called observation. The resulting obser- vation space is then much smaller than the state space, whereby learning becomes faster and overﬁtting can be avoided (see Sect. 8.4.7).\\n\\nHowever, by bundling together multiple states, the agent can no longer differ- entiate between the actual states, and an action may lead it into many different successor states, depending on which state it is really in. This can lead to con- vergence problems for value iteration or for Q-learning. In the literature (e.g., in [SB98]) many different approaches to a solution are suggested.\\n\\nAlso very promising are so-called policy improvement methods and their derived policy gradient methods, in which Q values are not changed, but rather the policy is changed directly. In this scheme a policy is searched for in the space of all policies, which maximizes the cumulative discounted reward ((10.1) on page 292). One possibility of achieving this is by following the gradient of the cumulative reward to a maximum. The policy found in this way then clearly optimizes the cumulative reward. In [PS08] it is shown that this algorithm can greatly speed up learning in applications with large state spaces, such as those which occur for humanoid robots.\\n\\n305\\n\\n306\\n\\n10 Reinforcement Learning\\n\\n10.9 Applications\\n\\nThe practical utility of reinforcement learning has meanwhile been shown many times over. From a large number of examples of this, we will brieﬂy present a small selection.\\n\\nTD-learning, together with a backpropagation network with 40 to 80 hidden neurons was used very successfully in TD-gammon, a backgammon-playing pro- gram [Tes95]. The only immediate reward for the program is the result at the end of the game. An optimized version of the program with a two-move lookahead was trained against itself in 1.5 million games. It went on to defeat world-class players and plays as well as the three best human players.\\n\\nThere are many applications in robotics. For example, in the RoboCup Soccer Simulation League, the best robot soccer teams now successfully use reinforcement learning [SSK05, Robb]. Balancing a pole, which is relatively easy for a human, has been solved successfully many times with reinforcement learning.\\n\\nAn impressive demonstration of the learning ability of robots was given by Russ Tedrake at IROS 2008 in his presentation about a model airplane which learns to land at an exact point, just like a bird landing on a branch [Ted08]. Because air currents become very turbulent during such highly dynamic landing approach, the associated differential equation, the Navier–Stokes equation, is unsolvable. Landing therefore cannot be controlled in the classical mathematical way. Tedrake’s com- ment about this:\\n\\n“Birds don’t solve Navier–Stokes!”\\n\\nBirds can clearly learn to ﬂy and land even without the Navier–Stokes equation. Tedrake showed that this is now also possible for airplanes.\\n\\nToday it is also possible to learn to control a real car in only 20 minutes using Q-learning and function approximation [RMD07]. This example shows that real industrial applications in which few measurements must be mapped to actions can be learned very well in short time.\\n\\nReal robots still have difﬁculty learning in high-dimensional state-action spaces because, compared to a simulation, real robots get feedback from the environment relatively slowly. Due to time limitations, the many millions of necessary training cycles are therefore not realizable. Here, besides fast learning algorithms, methods are needed which allow at least parts of the learning to happen ofﬂine, that is, without feedback from the environment.\\n\\n10.10 AlphaGo, the Breakthrough in Go\\n\\nAlthough alpha-beta pruning has been used successfully by chess computers, it cannot achieve the same success in Go programs due to the game’s large branching factor of roughly 250, as was described in Sect. 6.6.2. It has been known for some in Go the next move should be chosen using pattern recognition time that\\n\\n10.10 AlphaGo, the Breakthrough in Go\\n\\nalgorithms on the current board position. Yet all prior attempts had not been very successful. This changed when Google DeepMind presented AlphaGo in [SHM þ16]. This program uses Monte Carlo tree search (MCTS) to generate training data, deep learning to evaluate board positions and reinforcement learning to improve its strategy by playing against itself. Roughly sketched, the algorithm works as follows:\\n\\nThe learning process’s most important goal is to learn a policy p which, for a position s, calculates the probability p(a|s) of winning for every possible move a. If this probability function is found, then it will be used in the game such that when in a position s, the best of all possible moves\\n\\na(cid:9) ¼ argmax\\n\\np ajs ð\\n\\nÞ\\n\\na\\n\\nwill be chosen. Because the program should play better than its human role model in the end, it is a multi-stage process to learn a policy p that is as good as possible. First, using saved champion-level games, two different move selection functions are learned (stage 1). The stronger of the two will then be further improved using reinforcement learning (stage 2) and transformed into a position evaluation function (stage 3). Finally, the program plays by combining the resulting position evaluation function with the two move selection policies from stage 1 in a complex MCTS search process.\\n\\nStage 1: Deep Learning Based on Saved Games The KGS Go Server contains many complete Go matches by champion players with a total of thirty million board positions s and the players’ respective moves a(s). All of these moves are used to train a convolutional neural network (CNN).\\n\\nThe CNN takes a board position s as input and, after training, should return the probability prðajsÞ for all legal moves a. The de facto realization of prðajsÞ is a 19 (cid:7) 19 matrix, which represents the board. The values for each of the 361 points on the board stand for the probability of winning for the corresponding move. An example for the application of such a policy function can be seen in Fig. 10.13 on page 308. For the board state s left, prðajsÞ is represented in a density graph on the right. On this turn, the player may set a white stone on one of the points. For nearly all of the available points, the probability of winning is zero, but for a few inter- esting points, the probability is represented as a grayscale value. Apparently the most desirable place to put the stone is in the top middle in row 2, column 11.\\n\\nDuring training, the 19 (cid:7) 19 board state for each of the thirty million champion moves in the database is given as input to the network, and the target output is the 19 (cid:7) 19 matrix with all values set to null and the square selected by the expert set to one.\\n\\nOn a holdout set of test data it has not yet seen, the complex thirteen-layer CNN network used by AlphaGo chooses the correct move up to 57% of the time, which demonstrates an impressive gain over the best performance to date of about 44% by other Go programs.\\n\\n307\\n\\n308\\n\\n10 Reinforcement Learning\\n\\nFig. 10.13 Density graph (right) of a move probability prðajsÞ for the player with white stones in the board state shown on the left. For each white square, prðajsÞ ¼ 0. The darker the square, the higher the value for a move by white onto the corresponding grid point\\n\\nIn addition to the policy prðajsÞ, a much simpler rollout policy ppðajsÞ is trained on the same training data with a simpler CNN. It chooses moves correctly only 24% of the time, but does calculations about 1000 times faster.\\n\\nStage 2: Improving the Learned Policy with Reinforcement Learning Next, the move selection function is improved using reinforcement learning. This second step is necessary in order for the program to play better than its human role models.\\n\\nThe policy prðajsÞ learned from the database is used by AlphaGo to play against itself. After every game, the current policy pqðajsÞ is improved using stochastic gradient descent. To avoid overﬁtting, AlphaGo does not always play against the current version, rather against a randomly selected earlier one. Stage 3: Learning a Position’s Value Next, the current policy pqðajsÞ is used to train a board state evaluation function V(s) using value iteration (see Sect. 10.4). The Final AlphaGo Game Policy and its Performance AlphaGo’s actual playing algorithm uses a complex MCTS algorithm in which the tree is expanded a bit from the current position. From the leaf nodes thus generated, the game is played to the end using a fast but simple rollout policy ppðajsÞ. The position is then evaluated using the simulated game’s outcome together with the value function V(s). Despite having a very good policy, the computational cost is immense because of the complexity of this MCTS algorithm. AlphaGo’s highest playing ability, that was used to defeated Go grandmaster Lee Sedol of Korea 4:1, was reached on a parallel computer with 1202 CPUs and 176 GPUs.\\n\\nIn summary, AlphaGo represents a great milestone in the history of AI. This achievement was made possible by using deep learning and by the astounding engineering effort of a large team of experts in the various areas of machine learning.\\n\\n10.11 Curse of Dimensionality\\n\\n10.11 Curse of Dimensionality\\n\\nDespite success in recent years, reinforcement learning remains an active area of learning algorithms known research in AI, not today are still impractical for high-dimensional state and action spaces due to their gigantic computation time. This problem is known as the “curse of dimensionality”.\\n\\nleast because even the best\\n\\nIn the search for solutions to this problem, scientists observe animals and humans during learning. Here we notice that learning in nature takes place on many levels of abstraction. A baby ﬁrst learns simple motor and language skills on the lowest level. When these are well learned, they are saved and can later be called up any time and used. Translated into the language of computer science, this means that every learned ability is encapsulated in a module and then, on a higher level, represents an action. By using such complex actions on a higher level, the action space becomes greatly reduced and thus learning is accelerated. In a similar way, states can be abstracted and thus the state space can be shrunk. This learning on multiple levels is called hierarchical learning [BM03].\\n\\nAnother approach to modularization of learning is distributed learning, or multi-agent learning [PL05]. When learning a humanoid robot’s motor skills, up to 50 different motors must be simultaneously controlled, which results in 50-dimensional state space and also a 50-dimensional action space. To reduce this gigantic complexity, central control is replaced by distributed control. For example, each individual motor could get an individual control which steers it directly, if possible independently of the other motors. In nature, we ﬁnd this kind of control in insects. For example, the many legs of a millipede are not steered by a central brain, rather each pair of legs has its own tiny “brain”.\\n\\nSimilar to uninformed combinatorial search, reinforcement learning has the task of ﬁnding the best of a huge number of policies. The learning task becomes sig- niﬁcantly easier if the agent has a more or less good policy before learning begins. Then the high-dimensional learning tasks can be solved sooner. But how do we ﬁnd such an initial policy? Here there are two main possibilities.\\n\\nThe ﬁrst possibility is classical programming. The programmer provides the agent with a policy comprising a program which he considers good. Then a switchover occurs, for example to Q-learning. The agent chooses, at least at the beginning of learning, its actions according to the programmed policy and thus is led into “interesting” areas of the state-action space. This can lead to dramatic reductions in the search space of reinforcement learning.\\n\\nIf traditional programming becomes too complex, we can begin training the robot or agent by having a human proscribe the right actions. In the simplest case, this is done by manual remote-control of the robot. The robot then saves the proscribed action for each state and generalizes using a supervised learning algo- rithm such as backpropagation or decision tree learning. This so-called demon- stration learning [BCDS08, SE10] thus also provides an initial policy for the subsequent reinforcement learning.\\n\\n309\\n\\n310\\n\\n10 Reinforcement Learning\\n\\n10.12 Summary and Outlook\\n\\nToday we have access to well-functioning and established learning algorithms for training our machines. The task for the human trainer or developer, however, is still demanding for complex applications. There are namely many possibilities for how to structure the training of a robot and it will not be successful without experi- mentation. This experimentation can be very tedious in practice because each new learning project must be designed and programmed. Tools are needed here which, besides the various learning algorithms, also offer the trainer the ability to combine these with traditional programming and demonstration learning. One of the ﬁrst of this kind of tool is the Teaching-Box [ESCT09], which in addition to an extensive program library also offers templates for the conﬁguration of learning projects and for communication between the robot and the environment. For example, the human teacher can give the robot further feedback from the keyboard or through a speech interface in addition to feedback from the environment.\\n\\nReinforcement learning is a fascinating and active area of research that will be increasingly used in the future. More and more robot control systems, but also other programs, will learn through feedback from the environment. Today there exist a multitude of variations of the presented algorithms and also completely different algorithms. The scaling problem remains unsolved. For small action and state spaces with few degrees of freedom, impressive results can be achieved. If the number of degrees of freedom in the state space grows to 18, for example for a simple humanoid robot, then learning becomes very expensive.\\n\\nFor further foundational lectures, we recommend the compact introduction into reinforcement learning in Tom Mitchell’s book [Mit97]. The standard work by Sutton and Barto [SB98] is thorough and comprehensive, as is the survey article by Kaelbling, Littman and Moore [KLM96].\\n\\n10.13 Exercises\\n\\nExercise 10.1 (a) Calculate the number of different policies for n states and n actions. Thus\\n\\ntransitions from each state to each state are possible.\\n\\n(b) How does the number of policies change in subproblem (a) if empty actions,\\n\\ni.e., actions from one state to itself, are not allowed.\\n\\n(c) Using arrow diagrams like those in Fig. 10.3 on page 291, give all policies for\\n\\ntwo states.\\n\\n(d) Using arrow diagrams, give all policies without empty actions for three states.\\n\\nExercise 10.2 Use value iteration manually on Example 10.1 on page 289 with nx ¼ ny ¼ 2.\\n\\n10.13 Exercises\\n\\nExercise 10.3 Carry out various experiments using a value iteration simulator. (a) (b) Reproduce the results from Exercise 10.2 on page 310 by ﬁrst putting in the feedback with the feedback editor and then carrying out value iteration. (c) Model surfaces of differing smoothness and observe how the policy changes. (d) With a similar feedback matrix, enlarge the state space incrementally up to about 100 (cid:7) 100 and ﬁt the discount factor c such that a sensible policy results.\\n\\nInstall the value iteration simulator from [Tok06].\\n\\n❄ Exercise 10.4 Show that for the example calculation in Fig. 10.8 on page 297 the\\n\\nexact value is V⋆(3, 3) ¼ 1.9/(1 − 0.96) (cid:6) 4.05499.\\n\\nExercise 10.5 Carry out Q-learning on the 3 (cid:7) 3 grid on the right. The state in the middle right is an absorbing goal state.\\n\\nExercise 10.6 A robot arm with n joints (dimensions) and ‘ discrete states per joint is given. Actions from each state to each state are possible (if the robot does nothing, this is evaluated as an (empty) action). (a) Give a formula for the number of states and the number of actions in each state\\n\\nand for the number of policies for the robot.\\n\\n(b) Create a table with the number of strategies for n ¼ 1, 2, 3, 4, 8 and ‘ ¼ 1, 2, 3,\\n\\n4, 10.\\n\\n(c) To reduce the number of possible strategies, assume that\\n\\nthe number of possible actions per joint is always equal to 2 and that the robot can only move one joint at a time. Give a new formula for the number of strategies and create the associated table.\\n\\n(d) With the calculated result, justify that an agent which operates autonomously and adaptively with n ¼ 8 and l ¼ 10 can certainly be called intelligent.\\n\\n311\\n\\nSolutions for the Exercises\\n\\n11.1 Introduction\\n\\nExercise 1.3 Many well-known inference processes, learning processes, etc. are NP-complete or even undecidable. What does this mean for AI?\\n\\nExercise 1.4 If a problem is NP-complete or can be described as “hard”, that means that there are instances in which the problem cannot be solved in an acceptable amount of time. This is the so-called worst case. In some applications we have to live with the fact that in the worst case an efﬁcient solution is impos- sible. This means that even in the future there will be practically relevant problems which in certain special cases are unsolvable.\\n\\nAI will therefore neither ﬁnd a universal formula, nor build a super machine with which all problems become solvable. It gives itself rather the task of building systems with a higher probability of ﬁnding a solution, or with a higher probability of ﬁnding fast, optimal solutions. We humans in everyday life deal with subop- timal solutions quite well. The reason is, quite simply, the excessive cost of ﬁnding the optimal solution. For example, it only takes me seven minutes to ﬁnd my way from point A to point B with a map in an unfamiliar city. The shortest path would have taken only six minutes. Finding the shortest path, however, would have taken perhaps an hour. The proof of the optimality of the path might be even costlier.\\n\\nExercise 1.5\\n\\n(a) The output depends not only on the input, but also on the contents of the memory. For an input x, depending on the contents of the memory, the output could be y1 or y2. It is thus not unique and therefore not a function. If one considers the contents of the memory as a further input, then the output is unique (because the agent is deterministic) and the agent represents a function.\\n\\n(b)\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4_11\\n\\n11\\n\\n313\\n\\n314\\n\\n11 Solutions for the Exercises\\n\\nExercise 1.6\\n\\n(a) Velocity vxðtÞ ¼\\n\\n@x @t\\n\\n¼ lim Dt!0\\n\\nxðtÞ (cid:2) xðt (cid:2) DtÞ Dt\\n\\n(cid:3)\\n\\nxðtÞ (cid:2) xðt (cid:2) DtÞ Dt\\n\\n. vy\\n\\nis calculated\\n\\nanalogously.\\n\\n@2x (b) Acceleration axðtÞ ¼ @t2 ¼ #\\n\\n@ @t\\n\\nvxðtÞ ¼ lim Dt!0\\n\\nvxðtÞ (cid:2) vxðt (cid:2) DtÞ Dt\\n\\n¼ lim Dt!0\\n\\n\" xðtÞ (cid:2) xðt (cid:2) DtÞ ðDtÞ2\\n\\n(cid:2)\\n\\nxðt (cid:2) DtÞ (cid:2) xðt (cid:2) 2DtÞ ðDtÞ2\\n\\nxðtÞ (cid:2) 2xðt (cid:2) DtÞ þ xðt (cid:2) 2DtÞ ðDtÞ2 gously. One also needs the position at the three times t − 2Dt, t − Dt, t.\\n\\n. ay\\n\\nis calculated analo-\\n\\n¼\\n\\nExercise 1.7 (a) Costs for agent 1 = 11 ⋅ 100 cents + 1 ⋅ 1 cent = 1,101 cents.\\n\\nCosts for 2 = 0 ⋅ 100 cents + 38 ⋅ 1 cent = 38 cents. Therefore agent 2 saves 1,101 cents − 38 cents = 1,063 cents. (b) Proﬁt for agent 1 = 189 ⋅ 100 cents + 799 ⋅ 1 cent = 19,699 cents. Proﬁt for agent 2 = 200 ⋅ 100 cents + 762 ⋅ 1 cent = 20,762 cents. Agent 2 therefore has 20,762 cents − 19,699 cents = 1,063 cents higher proﬁt. If one assesses the lost proﬁts due to errors, the utility-based agent makes the same decisions as a cost-based agent.\\n\\n11.2 Propositional Logic\\n\\nExercise 2.1 With the signature R = {r1, r2, …, rn} and the grammar variables 〈formula〉, the syntax of propositional logic can be deﬁned as follows:\\n\\nh formulai :: ¼ r1 r2j\\n\\njwj f j (cid:4) (cid:4) (cid:4) rnj j h formulai :h formulai ð h formulai _ h formulai h formulai , h formulai\\n\\nj j\\n\\nj\\n\\nÞjh formulai ^ h formulai jh formulai ) h formulai\\n\\nExercise 2.2 Proof by the truth table method. Exercise 2.3 (a) (¬A _ B) ^ (¬B _ A) Exercise 2.4 (a) satisﬁable Exercise 2.6\\n\\n(b) (¬A _ B) ^ (¬B _ A)\\n\\n(c) unsatisﬁable\\n\\n(b) true\\n\\n(c) t\\n\\nIn Exercise 2.3(c) it was already shown that A ^ (A ) B) ) B is a tautology. The deduction theorem thus ensures the correctness of the inference rule. (b) We show by the truth table method that (A _ B) ^ (¬B _ C) ) (A _ C) is a\\n\\n(a)\\n\\ntautology.\\n\\nExercise 2.7 Application of the resolution rule to the clause (f _ B) and (¬B _ f) yields the resolvent (f _ f) (cid:5) (f). Now we apply the resolution rule to the clauses B and ¬B and obtain the empty clause as the resolvent. Because (f _ B) (cid:5) B and\\n\\n11.2 Propositional Logic\\n\\n(¬B _ f) (cid:5) ¬B, (f) (cid:5) (). It is important in practice that, whenever the empty clause is derived, it is due to a contradiction. Exercise 2.8 If KB contains a contradiction, then there are two clauses A and ¬A, which allow the empty clause to be derived. The contradiction in KB is clearly still in KB ^ ¬Q. Therefore it also allows the empty clause to be derived.\\n\\nExercise 2.9 (a) (A _ B) ^ (¬A _ ¬B)\\n\\n(b) (A _ B) ^ (B _ C) ^ (A _ C)\\n\\nExercise 2.10 Formalization: Accomplice: A, Car: C, Key: K\\n\\nWB (cid:5) ðA ) CÞ ^ ½ð:A ^ :KÞ _ ðA ^ KÞ(cid:6) ^ K\\n\\nTransformation into CNF: (¬A ^ ¬K) _ (A ^ K) (cid:5) (¬K _ A) ^ (¬A _ K) Try to prove C and add ¬C to the set of clauses. The CNF clause set is\\n\\nð:A _ CÞ1 ^ ð:K _ AÞ2 ^ ð:A _ KÞ3 ^ ðKÞ4 ^ ð:CÞ5\\n\\n:\\n\\nResolution proof : Resð2; 4Þ : Resð1; 6Þ : Resð7; 5Þ :\\n\\nðAÞ6 ðCÞ7 ðÞ8\\n\\nThus C has been shown.\\n\\nExercise 2.11 (a) KB (cid:5) (A _ B) ^ (¬B _ C), Q (cid:5) (A _ C)\\n\\nKB ^ ¬Q (cid:5) (A _ B)1 ^ (¬B _ C)2 ^ (¬A)3 ^ (¬C)4\\n\\nResolution proof: Resð1; 3Þ : Resð2; 4Þ : Resð5; 6Þ : (b) ¬(¬B ^ (B _ ¬A) ) ¬A) (cid:5) (¬B)1 ^ (B _ ¬A)2 ^ (A)3\\n\\nðBÞ5 ð:BÞ6 ðÞ\\n\\nResolution proof: Resð1; 2Þ : Resð3; 4Þ :\\n\\nð:AÞ4 ðÞ\\n\\nExercise 2.12 By application of the equivalences from Theorem 2.1 on page 26, we can immediately prove the claims.\\n\\nExercise 2.13\\n\\nResð8; 9Þ : ð Resð3; 10Þ : F ^ E ) f ð Resð6; 11Þ : A ^ B ^ C ^ E ) f ð Resð1; 12Þ : B ^ C ^ E ) f ð Resð2; 13Þ : C ^ E ) f ð Resð3; 14Þ : E ) f ð Resð5; 15Þ : ðÞ\\n\\nC ^ F ^ E ) f\\n\\nÞ10\\n\\nÞ11\\n\\nÞ13\\n\\nÞ14\\n\\nÞ15\\n\\nÞ12\\n\\n315\\n\\n316\\n\\n11 Solutions for the Exercises\\n\\n11.3 First-Order Predicate Logic\\n\\nExercise 3.1 (a) 8x male(x) , ¬female(x) (b) 8x 8y 9z father(x, y) , male(x) ^ child(y, x, z) (c) 8x 8y siblings(x, y) , [(9z father(z, x) ^ father(z, y)) _ (9z mother(z, x) ^\\n\\nmother(z, y))]\\n\\n(d) 8x 8y 8z parents(x, y, z) , father(x, z) ^ mother(y, z) (e) 8x 8y uncle(x, y) , 9z 9u child(y, z, u) ^ siblings(z, x) ^ male(x) (f) 8x 8y ancestorðx; yÞ , 9z childðy; x; zÞ _ 9u 9v childðu; x; vÞ ^ ancestorðu; yÞÞ\\n\\nExercise 3.2\\n\\n(a) 8x 9y 9z father(y, x) ^ mother(z, x) (b) 9x 9y child(y, x, z) (c) 8x bird(x) ) ﬂies(x) (d) 9x 9y 9z animal(x) ^ animal(y) ^ eats(x, y) ^ eats(y, z) ^ grain(z) (e) 8x animal(x) ) (9y (eats(x, y) ^ (plant(y) _ (animal(y) ^ 9z plant(z) ^ eats\\n\\n(y, z) ^ much_smaller(y, x)))\\n\\nExercise 3.3 8x 8y 9z x = father(y) , male(x) ^ child(y, x, z)\\n\\nExercise 3.4 ∀x ∀y x < y ∨ y < x ∨ x = y, ∀x ∀y x < y ⇒ ¬y < x, ∀x ∀y ∀z x < y ∧ y < z ⇒ x < z\\n\\nExercise 3.5\\n\\n(a) MGU: x / f(z), u / f(y), term: p(f(z), f(y)) (b) not uniﬁable (c) MGU: x / cos y, z / 4 − 7 ⋅ cos y, term: cos y = 4 − 7 ⋅ cos y (d) not uniﬁable (e) MGU: u=f ðgðw; wÞ; gðgðw; wÞ; gðw; wÞÞ; gðgðgðw; wÞ; gðw; wÞÞ; gðgðw; wÞ; gðw; wÞÞÞÞ x=gðw; wÞ; y=gðgðw; wÞ; gðw; wÞÞ z=gðgðgðw; wÞ; gðw; wÞÞ; gðgðw; wÞ; gðw; wÞÞÞ term: qðf ðgðw; wÞ; gðgðw; wÞ; gðw; wÞÞ; gðgðgðw; wÞ; gðw; wÞÞ; gðgðw; wÞ; gðw; wÞÞÞÞ; f ðgðw; wÞ; gðgðw; wÞ; gðw; wÞÞ; gðgðgðw; wÞ; gðw; wÞÞ; gðgðw; wÞ; gðw; wÞÞÞÞÞ\\n\\nExercise 3.7 (a) Let the unsatisﬁable formula p(x) ^ ¬p(x) ^ r(x) be given. We choose the\\n\\n(b)\\n\\nclause r(x) as the SOS, so no contradiction can be derived. If the SOS is already unsatisﬁable, then no contradiction can be derived. If not, then resolution steps between clauses from SOS and (KB ^ ¬Q)\\\\SOS are necessary.\\n\\n11.3 First-Order Predicate Logic\\n\\nIf there is no complement to a literal L in a clause K, then the literal L will remain in every clause that is derived using resolution from clause K. Thus the empty clause cannot be derived from K or its resolvent, nor any future resolvent. Exercise 3.8 ¬Q ^ KB (cid:5) (e = n)1 ^ (n ⋅ x = n)2 ^ (e ⋅ x = x)3 ^ (¬a = b)4\\n\\n(c)\\n\\nProof : Demð1; 2Þ :\\n\\nðe (cid:4) x ¼ eÞ5\\n\\nTra,Symð3; 5Þ : ðx ¼ eÞ6 Demð4; 6Þ : Demð7; 6Þ :\\n\\nð:e ¼ bÞ7 ð:e ¼ eÞ8 ðÞ\\n\\nHere “Dem” stands for demodulation. Clause number 6 was derived by application of transitivity and symmetry of the equality in clauses 3 and 5. The empty clause is obtained by applying the reﬂexive propery of equality to e, which yields ðe ¼ eÞ, and, via resolution with clause 8, results in the empty clause. Exercise 3.9 The LOP input ﬁles are: (a) a;b<-. <-a,b. b;c<-. <-b,c. c;a<-. <-c,a.\\n\\n(b) <- shaves(barb,X),\\n\\n(c) e = n.\\n\\nshaves(X,X). shaves(barb,X); shaves(X,X) <-.\\n\\n<- a = b. m(m(X,Y),Z)=m(X,m(Y,Z)). m(e,X) = X. m(n,X) = n.\\n\\n11.4 Limitations of Logic\\n\\nExercise 4.1 (a) Correctly: We take a complete proof calculus for PL1. With it we ﬁnd a proof for every true formula in ﬁnite time. For all unsatisﬁable formulas I proceed as follows: I apply the calculus to ¬/ and show that ¬/ is true. Thus / is false. Thus we can prove every true formula from PL1 and refute every false formula. Unfortunately, this process is unsuitable for satisﬁable formulas.\\n\\nExercise 4.2\\n\\n(a) He shaves himself if and only if he does not shave. (contradiction) (b) The set of all sets which do not contain themselves. It contains itself if and\\n\\nonly if it does not contain itself.\\n\\n11.5 PROLOG\\n\\nExercise 5.1 PROLOG signals a stack overﬂow. The reason is PROLOG’s depth-ﬁrst search, which always chooses the ﬁrst uniﬁable predicate in the input ﬁle. With recursive predicates such as equality, this causes a non-terminating recursion.\\n\\n317\\n\\n318\\n\\n11 Solutions for the Exercises\\n\\nExercise 5.2 write_path( [H1,H2|T] ) :- write_path([H2|T]), write_move(H2,H1). write_path( [_X] ). write_move( state(X,W,Z,K), state(Y,W,Z,K) ) :-\\n\\nwrite(’Farmer from ’), write(X), write(’ to ’), write(Y), nl.\\n\\nwrite_move( state(X,X,Z,K), state(Y,Y,Z,K) ) :-\\n\\nwrite(’Farmer and wolf from ’), write(X), write(’ to ’), write(Y),nl.\\n\\nwrite_move( state(X,W,X,K), state(Y,W,Y,K) ) :-\\n\\nwrite(’Farmer and goat from ’),write(X),write(’ to ’), write(Y), nl.\\n\\nwrite_move( state(X,W,Z,X), state(Y,W,Z,Y) ) :-\\n\\nwrite(’Farmer and cabbage from ’),write(X),write(’ to ’), write(Y), nl.\\n\\nExercise 5.3\\n\\n(a)\\n\\nthe solutions found. The uniﬁcation in the fact\\n\\nIt plan(Goal,Goal,Path,Path). takes care of this.\\n\\nis needed to output\\n\\n(b) The conditions for entry into the clauses of the predicate safe overlap each other. The backtracking caused by the fail leads to the execution of the second or third alternative of safe, where the same solution will clearly be found again. A cut at the end of the ﬁrst two safe clauses solves the problem. Alternatively, all safe states can be explicitly given, as for example safe (state(left,left,left,right)).\\n\\nExercise 5.5\\n\\n?- one(10). one(0) :- write(1). one(N) :- N1 is N-1, one(N1), one(N1).\\n\\nExercise 5.6 (a) With n1 ¼ | L1|, n2 ¼ | L2| it is true that Tappendðn1; n2Þ ¼ Hðn1Þ. (b) With n ¼ | L| it is true that TnrevðnÞ ¼ Hðn2Þ. (c) With n ¼ | L| it is true that TaccrevðnÞ ¼ HðnÞ.\\n\\nExercise 5.7 For the trees with symbols on the inner nodes, we can use the terms a(b,c) and a(b(e,f,g),c(h),d), for example. Trees without symbols: tree(b,c), or tree(tree(e,f,g),tree(h),d).\\n\\nExercise 5.8 SWI-PROLOG programs: (a) ﬁb(0,1). ﬁb(1,1).\\n\\nﬁb(N,R) :- N1 is N-1, ﬁb(N1, R1), N2 is N-2, ﬁb(N2,R2), R is R1 + R2.\\n\\n(b) T(n) = H(2n). (c) :- dynamic ﬁb/2. ﬁb(0,1). ﬁb(1,1).\\n\\nﬁb(N,R) :- N1 is N-1, ﬁb(N1, R1), N2 is N-2, ﬁb(N2,R2), R is R1 + R2, asserta(ﬁb(N,R)).\\n\\n11.5 PROLOG\\n\\n(d)\\n\\nIf the facts ﬁb(0,1) to ﬁb(k,f i b(k)) were already calculated, then for the call ﬁb(n,X) it is true that (cid:2)\\n\\nTðnÞ ¼\\n\\nH 1ð Þ H n (cid:2) k ð\\n\\nÞ\\n\\nif n (cid:7) k; if n [ k:\\n\\n(e) Because after the ﬁrst n calls to the ﬁb predicate, all further calls have direct\\n\\naccess to facts.\\n\\nExercise 5.9\\n\\n(a) The solution is not disclosed here. (b)\\n\\nstart :-\\n\\nfd_domain([Briton, Swede, Dane, Norwegian, German],1,5), fd_all_different([Briton, Swede, Dane, Norwegian, German]), fd_domain([Tea, Coffee, Water, Beer, Milk],1,5), fd_all_different([Tea, Coffee, Water, Beer, Milk]), fd_domain([Red, White, Green, Yellow, Blue],1,5), fd_all_different([Red, White, Green, Yellow, Blue]), fd_domain([Dog, Bird, Cat, Horse, Fish],1,5), fd_all_different([Dog, Bird, Cat, Horse, Fish]), fd_domain([Pallmall, Dunhill, Marlboro, Winfield, Rothmanns],1,5), fd_all_different([Pallmall, Dunhill, Marlboro, Winfield, Rothmanns]), fd_labeling([Briton, Swede, Dane, Norwegian, German]), Briton #= Red, Swede #= Dog, Dane #= Tea, Green #= White - 1, Green #= Coffee, Pallmall #= Bird, ,3=#kliM Yellow #= Dunhill, Norwegian #= 1, dist(Marlboro,Cat)#= 1, dist(Horse,Dunhill) #= 1, % The man with the horse lives next to the Dunhill smoker Winfield #= Beer, dist(Norwegian,Blue) #= 1, % The Norwegian lives next to the blue house German #= Rothmanns, dist(Marlboro,Water)#=1, write([Briton, Swede, Dane, Norwegian, German]), nl, write([Dog, Bird, Cat, Horse, Fish]), nl.\\n\\n% The Briton lives in the red house % The Swede doesn’t have a dog % The Dane likes to drink tea % The green house is to the left of the white house % The owner of the green house drinks coffee % The person who smokes Pall Mall has a bird klimsknirdesuohelddimehtninamehT% % The owner of the yellow house smokes Dunhill % The Norwegian lives in the first house % The Marlboro smoker lives next to the cat\\n\\n% The Winfield smoker likes to drink beer\\n\\n% The German smokes Rothmanns % The Marlboro smoker’s neighbor drinks water.\\n\\n11.6 Search, Games and Problem Solving\\n\\nExercise 6.1 (a) On the last level there are bd nodes. All previous levels together have\\n\\nNbðdmaxÞ ¼\\n\\nXd(cid:2)1\\n\\ni¼0\\n\\nbi ¼\\n\\nbd (cid:2) 1 b (cid:2) 1\\n\\n(cid:3) bd(cid:2)1\\n\\nnodes, if b becomes large. Because bd /bd−1 = b there are about b times as many nodes on the last level as on all other levels together.\\n\\n319\\n\\n320\\n\\n11 Solutions for the Exercises\\n\\n(b)\\n\\nFor a non-constant branching factor this statement is no longer true, as the following counter-example shows: A tree that branches heavily up to the level before last, followed by a level with a constant branching factor of 1, has exactly as many nodes on the last level as on the level before last.\\n\\nExercise 6.2\\n\\n(a)\\n\\nIn Fig. 6.3 on page 94 the structure of the tree at the second level with its eight nodes repeats. Thus we can calculate the average branching factor bm from b2 m ¼ 8 to bm ¼\\n\\nﬃﬃﬃ . 8\\n\\np\\n\\n(b) Here the calculation is not so simple because the root node of the tree branches more heavily than all others. We can say, however, that in the interior of the tree the branching factor is exactly 1 smaller than it would be without the cycle check. Thus bm (cid:3)\\n\\nﬃﬃﬃ 8\\n\\np\\n\\n(cid:2) 1 (cid:3) 1:8.\\n\\nExercise 6.3 (a) For the average branching factor the number of leaf nodes is ﬁxed. For the effective branching factor, in contrast, the number of nodes in the whole tree is ﬁxed.\\n\\n(b) Because the number of all nodes in the tree is usually a better measurement of the computation time for searching an entire tree than the number of leaf nodes. (c) For a large b according to (6.1) on page 96 we have n (cid:3) (cid:2)bd þ 1=(cid:2)b ¼ (cid:2)bd,\\n\\nﬃﬃﬃ ndp\\n\\nyielding (cid:2)b ¼\\n\\n.\\n\\nExercise 6.4\\n\\n(a) 3-puzzle: 4! = 24 states, 8-puzzle: 9! = 362 880 states,\\n\\n15-puzzle: 16! = 20 922 789 888 000 states.\\n\\n(b) After moving the empty square 12 times in the clockwise direction we reach the starting state again and thus create a cyclical sub-space with 12 states.\\n\\nExercise 6.6 (a) Mathematica program:\\n\\n0 BreadthFirstSearch[Node_, Goal_, Depth_] := Module[{i, NewNodes={}}, 1 2 3 4 5 6 7 8 9 10 11 ]\\n\\nFor[i=1, i<= Length[Node], i++,\\n\\nNewNodes = Join[ NewNodes, Successors[ Node[[i]] ] ]; If[MemberQ[Successors[ Node[[i]] ], Goal], Return[{\"Solution found\", Depth+1}], 0\\n\\n]\\n\\n];\\n\\nIf[Length[NewNodes] > 0,\\n\\nBreadthFirstSearch[NewNodes, Goal, Depth+1], Return[{\"Fail\", Depth}]\\n\\n]\\n\\n(b) Because the depth of the search space is not limited.\\n\\n11.6 Search, Games and Problem Solving\\n\\nExercise 6.7\\n\\n(a) For a constant cost 1 the cost of all paths at depth d are smaller than the costs of all paths at depth d + 1. Since all nodes at depth d are tested before the ﬁrst node at depth d + 1, a solution of length d is guaranteed to be found before a solution of length d + 1.\\n\\n(b) For the neighboring search tree, node number 2 and the solution node number 3 of cost 10 are generated ﬁrst. The search terminates. The nodes 4 and 5 with path costs of 2 each are not generated. The optimal solution is therefore not found.\\n\\nExercise 6.8\\n\\n(a)\\n\\nIt is appropriate to store the predecessor nodes of the current search path in a linked list. This has the advantage that during expansion only one node must be appended and during backtracking only one node must be deleted.\\n\\n(b) A look at Fig. 6.9 on page 100 shows that without the cycle check, and thus without storing the predecessor nodes, we must store Hððb (cid:2) 1Þ (cid:4) dÞ nodes. When storing all predecessors in the tree structure, Hðb (cid:4) dÞ storage space is necessary because at each level all nodes must be stored.\\n\\n(c)\\n\\nXd\\n\\nk¼1\\n\\nXd\\n\\nd db (cid:4) ¼ H b\\n\\nk (cid:4) bk ¼ b\\n\\nbk ¼ b (cid:5)\\n\\nk¼1\\n\\nb2 dbd þ 1\\n\\nd db\\n\\nbd þ 1 (cid:2) b b (cid:2) 1\\n\\n¼ HðdbdÞ\\n\\n¼\\n\\nb\\n\\nðb (cid:2) 1Þ2 ðdbd þ 1 (cid:2) ðd þ 1Þbd þ 1Þ\\n\\nExercise 6.11 (a) Let dH ðx; yÞ be the shortest distance from x to y. Then a path from x over z to\\n\\n(b)\\n\\ny cannot be even shorter. Then dH ðx; yÞ (cid:7) dH ðx; zÞ þ dHðz; yÞ. If the only direct connection between x and y is a curvy mountain pass, then it can be shorter to drive from x to y through z. Use a sketch to illustrate this.\\n\\nExercise 6.13 Depth-ﬁrst search: the new node must have a lower rating than all open nodes. Breadth-ﬁrst search: the new node must have a higher rating than all open nodes.\\n\\nExercise 6.14 Just like an admissible heuristic, the wife underestimates the dis- tance to the goal. This could result in the two of them ﬁnding the quickest way to the goal—though with great effort. This is only true, however, if the lady always underestimates the distance.\\n\\n321\\n\\n322\\n\\n11 Solutions for the Exercises\\n\\nExercise 6.16\\n\\n(a)\\n\\n11.7 Reasoning with Uncertainty\\n\\nExercise 7.1 1. PðXÞ ¼ jXj 2. P(;) = 1 − P(X) = 0. 3. For pairwise exclusive events A and B: PðA _ BÞ ¼ jA _ Bj\\n\\njXj ¼ 1.\\n\\njXj ¼ jAj þ jBj\\n\\njXj ¼\\n\\nPðAÞ þPðBÞ.\\n\\n4. For two complementary events A and ¬A: PðAÞ þ Pð:AÞ ¼ PðAÞ þ PðX (cid:2) AÞ ¼ jXj ¼ jXj\\n\\njXj ¼ 1. jXj ¼ jAj þ jBj(cid:2)jA ^ Bj 5. PðA _ BÞ ¼ jA _ Bj jXj 6. For A (cid:8) B: PðAÞ ¼ jAj jXj (cid:7) jBj jXj ¼ PðBÞ. 7. According to Deﬁnition 7.1 on page 127: A1 _ ⋅⋅⋅ _ An =X and\\n\\n¼ PðAÞ þ PðBÞ (cid:2) PðA ^ BÞ.\\n\\nP\\n\\nn i¼1 PðAiÞ ¼\\n\\nP\\n\\nP n i¼1\\n\\nn\\n\\njAij\\n\\n¼ jXj\\n\\njAij jXj ¼\\n\\njXj ¼ jA1 _ (cid:4)(cid:4)(cid:4) _ Anj\\n\\njXj ¼ 1.\\n\\ni¼1\\n\\njXj\\n\\nExercise 7.2\\n\\nðaÞ\\n\\nP y [ 3 j Class ¼ good ð\\n\\nÞ ¼ 7=9 P Class ¼ good ð\\n\\nÞ ¼ 9=13 P y [ 3\\n\\nð\\n\\nÞ ¼ 7=13;\\n\\nP Class ¼ good j y [ 3 ð\\n\\nÞ ¼\\n\\nP y [ 3 j Class ¼ good ð\\n\\nÞ (cid:4) P Class ¼ good\\n\\nð\\n\\nP y [ 3\\n\\nð\\n\\nÞ\\n\\nÞ\\n\\n¼\\n\\n7=9 (cid:4) 9=13 7=13\\n\\n¼ 1;\\n\\nP Class ¼ good j y (cid:7) 3\\n\\nð\\n\\nÞ ¼\\n\\nP y (cid:7) 3 j Class ¼ good\\n\\nð\\n\\nÞ (cid:4) P Class ¼ good\\n\\nð\\n\\nP y (cid:7) 3 Þ\\n\\nð\\n\\nÞ\\n\\n¼\\n\\n2=9 (cid:4) 9=13 6=13\\n\\n¼\\n\\n1 3\\n\\n:\\n\\n(b) P(y > 3 | Class = good) = 7/9.\\n\\n11.7 Reasoning with Uncertainty\\n\\nExercise 7.3\\n\\n(a) eight events.\\n\\n(b) PðPrec ¼ dryjSky ¼ clear; Bar ¼ risingÞ\\n\\n¼\\n\\nPðPrec ¼ dry; Sky ¼ clear; Bar ¼ risingÞ PðSky ¼ clear; Bar ¼ risingÞ\\n\\n¼\\n\\n0:4 0:47\\n\\n¼ 0:85:\\n\\n(c) Missing row in the table:\\n\\nCloudy\\n\\nFalling\\n\\nRaining\\n\\n0.12\\n\\nP Prec ¼ rainingjSky ¼ cloudy\\n\\nð\\n\\nÞ ¼\\n\\nP Prec ¼ raining; Sky ¼ cloudy ð P Sky ¼ cloudy\\n\\nð\\n\\nÞ\\n\\n¼\\n\\n0:23 0:35\\n\\n¼ 0:66:\\n\\n(d) The missing probability measure is 0.15. The indifference principle (Deﬁni- tion 7.5 on page 140) now requires that both missing rows be symmetrically allocated the value 0.075.\\n\\nExercise 7.4 The candidate ﬁrst chose door number 1. Then the host opened door number 3. We introduce the abbreviation Ai for “Car is behind door number i (before the experiment begins) and Mi (moderator) opens door number i”. Then P(A1) = P(A2) = P(A3) = 1/3 and we compute\\n\\nfor “Host\\n\\nP A1 M3 j\\n\\nð\\n\\nÞ ¼\\n\\nP M3 A1j\\n\\nð\\n\\nÞP A1ð Þ\\n\\nP M3 ð\\n\\nÞ\\n\\n¼\\n\\nP M3 A1j\\n\\nð\\n\\nÞP A1ð\\n\\nP M3 A1j ð Þ þ P M3 A2j ð\\n\\nÞP A1ð ÞP A2ð\\n\\nÞ Þ þ P M3 A3j ð\\n\\nÞP A3ð\\n\\nÞ\\n\\n¼\\n\\n1=2 (cid:4) 1=3 1=2 (cid:4) 1=3 þ 1 (cid:4) 1=3 þ 0 (cid:4) 1=3\\n\\n¼\\n\\n1=2 (cid:4) 1=3 1=2\\n\\n¼ 1=3;\\n\\nP A2 M3 j\\n\\nð\\n\\nÞ ¼\\n\\nP M3 A2j\\n\\nð\\n\\nÞP A2ð Þ\\n\\nP M3 ð\\n\\nÞ\\n\\n¼\\n\\n1 (cid:4) 1=3 1=2\\n\\n¼ 2=3:\\n\\nThus it is clear that it is better to switch to the other door.\\n\\n323\\n\\nÞ\\n\\n324\\n\\n11 Solutions for the Exercises\\n\\nP\\n\\nExercise 7.5 The Lagrange function reads L ¼ (cid:2) Setting the partial derivatives with respect to pi and pj equal to zero yields\\n\\nn\\n\\ni¼1 pi ln pi þ kð\\n\\nP n i¼1 pi (cid:2) 1Þ.\\n\\n@L @pi\\n\\n¼ (cid:2) ln pi (cid:2) 1 þ k ¼ 0\\n\\nand\\n\\n@L @pj\\n\\n¼ (cid:2) ln pj (cid:2) 1 þ k ¼ 0:\\n\\nSubtraction of these two equations results in pi = pj for all i, j 2 {1, …, n}. Thus p1= p2 = ⋅⋅⋅ = pn = 1/n.\\n\\nExercise 7.6\\n\\nPIT input data: var A{t,f}, B{t,f}; P([A=t]) = 0.5; P([B=t] | [A=t]) = 0.94; QP([B=t]);\\n\\nPIT output: Reporting State of Queries\\n\\nNr Truthvalue Probability Query 1 UNSPECIFIED 7.200e-01 QP([B=t]);\\n\\nBecause PIT operates numerically, it cannot compute symbolically with the\\n\\nparameters a and b, rather only with concrete numbers.\\n\\nExercise 7.7 If: p1 ¼ P(A, B), p2 ¼ P(A, ¬B), p3 ¼ P(¬A, B), p4 ¼ P(¬A, ¬B)\\n\\nThe constraints are : p1 þ p2 ¼ a\\n\\nð11:1Þ\\n\\np4 ¼ 1 (cid:2) b\\n\\nð11:2Þ\\n\\np1 þ p2 þ p3 þ p4 ¼ 1\\n\\nð11:3Þ\\n\\nIt follows that p3 = b − a. From (11.1) we infer, based on indifference, that p1 = p2 = a/2. Thus P(B) = p1 + p3 = a/2 + b − a = b − a/2 = P(A _ B) − 1/2P(A). The corresponding PIT program reads\\n\\nvar A{t,f}, B{t,f}; P([A=t]) = 0.6; P([B=t] OR [A=t]) = 0.94; QP([B=t]).\\n\\nExercise 7.8 The Lagrange function reads\\n\\nL ¼ (cid:2)\\n\\nX4\\n\\npi ln pi þ k1ðp1 þ p2 (cid:2) aÞ þ k2ðp1 þ p3 (cid:2) cÞ þk 3ðp1 þ p2 þ p3 þ p4 (cid:2) 1Þ:\\n\\ni¼1\\n\\nð11:4Þ\\n\\n11.7 Reasoning with Uncertainty\\n\\nTaking partial derivatives for p1, p2, p3, p4 we obtain\\n\\n@L @p1 @L @p2 @L @p3 @L @p4\\n\\n¼ (cid:2) ln p1 (cid:2) 1 þ k1 þ k2 þ k3 ¼ 0;\\n\\n¼ (cid:2) ln p2 (cid:2) 1 þ k1 þ k3 ¼ 0;\\n\\n¼ (cid:2) ln p3 (cid:2) 1 þ k2 þ k3 ¼ 0;\\n\\n¼ (cid:2) ln p4 (cid:2) 1 þ k3 ¼ 0\\n\\nand compute\\n\\nð\\n\\n11:5\\n\\nÞ(cid:2) 11:6 ð\\n\\nÞ : ln p2 (cid:2) ln p1 þ k2 ¼ 0;\\n\\nð\\n\\n11:7\\n\\nÞ(cid:2) 11:8 ð\\n\\nÞ : ln p4 (cid:2) ln p3 þ k2 ¼ 0;\\n\\nfrom which it follows that p3 / p4 = p1/p2, which we immediately substitute into (7.12) on page 143:\\n\\np2p3 p4\\n\\nþ p2 þ p3 þ p4 ¼ 1\\n\\n(cid:4)\\n\\n(cid:5)\\n\\np3 p4\\n\\n1 þ (7.10)(cid:2)(7.11): p2 ¼ p3 þ a (cid:2) c\\n\\n, p2\\n\\nþ p3 þ p4 ¼ 1\\n\\nwhich, substituted for p2, yields ðp3 þ a (cid:2) cÞð1 þ p3 p4\\n\\nÞ þ p3 þ p4 ¼ 1\\n\\n(7.10) in (7.12): a þ p3 þ p4 ¼ 1:\\n\\nThus we eliminate p4 in (11.8), which results in\\n\\nð\\n\\np3 þ a (cid:2) c\\n\\n(cid:4)\\n\\nÞ 1 þ\\n\\np3 1 (cid:2) a (cid:2) p3\\n\\n(cid:5)\\n\\nþ 1 (cid:2) a ¼ 1\\n\\n, p3 þ a (cid:2) c\\n\\nð\\n\\nÞ 1 (cid:2) a (cid:2) p3 þ p3 ð\\n\\nÞ ¼ a 1 (cid:2) a (cid:2) p3\\n\\nð\\n\\nÞ\\n\\n, p3 þ a (cid:2) c\\n\\nð\\n\\nÞ 1 (cid:2) a ð\\n\\nÞ ¼ a 1 (cid:2) a (cid:2) p3\\n\\nð\\n\\nÞ\\n\\n325\\n\\nð11:5Þ\\n\\nð11:6Þ\\n\\nð11:7Þ\\n\\nð11:8Þ\\n\\nð11:9Þ\\n\\nð11:10Þ\\n\\nð11:11Þ\\n\\nð11:12Þ\\n\\nð11:13Þ\\n\\nð11:14Þ\\n\\nð11:15Þ\\n\\n326\\n\\n11 Solutions for the Exercises\\n\\n, p3 þ a (cid:2) c (cid:2) ap3 (cid:2) a2 þ ac ¼ a (cid:2) a2 (cid:2) ap3\\n\\nð11:16Þ\\n\\n, p3 ¼ c 1 (cid:2) a\\n\\nð\\n\\nÞ:\\n\\nð11:17Þ\\n\\nWith (11.12) on page 325 it follows that p4 = (1 − a)(1 − c) and with (11.11) on page 325 we obtain p2 = a(1 − c) and p1 = ac.\\n\\nExercise 7.9 (a)\\n\\nM ¼\\n\\n(cid:4)\\n\\n0 10\\n\\n1000 0\\n\\n(cid:5) :\\n\\n(b)\\n\\nM (cid:4)\\n\\n(cid:4)\\n\\np 1 (cid:2) p\\n\\n(cid:5)\\n\\n¼\\n\\n(cid:4)\\n\\n1000 (cid:4) ð1 (cid:2) pÞ 10 (cid:4) p\\n\\n(cid:5) :\\n\\nBecause the decision between delete or read is found by establishing the minimum of the two values, it is sufﬁcient to compare the two values: 1000(1 − p) < 10p, which results in p > 0.99. In general, for a matrix M ¼ we compute the threshold k/(k + 1).\\n\\n(cid:6)\\n\\n(cid:7)\\n\\n0 k 1 0\\n\\nExercise 7.10\\n\\n(a) Because A and B are independent, P(A|B) = P(A) = 0.2. (b) By applying the conditioning rule (page 176) we obtain\\n\\nPðCjAÞ ¼ PðCjA; BÞPðBÞ þ PðCjA; :BÞPð:BÞ ¼ 0:2 (cid:4) 0:9 þ 0:1 (cid:4) 0:1 ¼ 0:19:\\n\\nExercise 7.11 (a)\\n\\nP Alð\\n\\nÞ ¼ P Al; Bur; Ear ð\\n\\nÞ þ P Al; :Bur; Ear ð\\n\\nð þ P Al; Bur; :Ear ð þ P AljBur; :Ear þ P Alj:Bur; :Ear\\n\\nð Þ þ P Al; :Bur; :Ear\\n\\nÞ\\n\\nÞ\\n\\n¼ P AljBur; Ear\\n\\nÞP Bur; Ear ð ÞP :Bur; :Ear\\n\\nÞ þ P Alj:Bur; Ear\\n\\nð\\n\\nð\\n\\nÞP Bur; Ear\\n\\nð ð\\n\\nÞ\\n\\nð\\n\\nÞ\\n\\nÞP :Bur; Ear\\n\\nð\\n\\nÞ\\n\\n¼ 0:95 (cid:4) 0:001 (cid:4) 0:002 þ 0:29 (cid:4) 0:999 (cid:4) 0:002 þ 0:94 (cid:4) 0:001 (cid:4) 0:998\\n\\nþ 0:001 (cid:4) 0:999 (cid:4) 0:998\\n\\n¼ 0:00252; ð\\n\\nP Jð Þ ¼ P J; Al\\n\\nÞ þ P J; :Al ð\\n\\nÞ ¼ P JjAl ð\\n\\nP Mð\\n\\n¼ 0:9 (cid:4) 0:0025 þ 0:05 (cid:4) 1 (cid:2) 0:0025 ð Þ þ P M; :Al Þ ¼ P MjAl ð ð ¼ 0:7 (cid:4) 0:0025 þ 0:01 (cid:4) 1 (cid:2) 0:0025 ð\\n\\nÞ ¼ P M; Al ð\\n\\nÞP Alð\\n\\nÞ þ P Jj:Al ð Þ ¼ 0:052; ÞP Alð Þ þ P Mj:Al ð Þ ¼ 0:0117:\\n\\nÞP :Al ð\\n\\nÞ\\n\\nÞP :Al ð\\n\\nÞ\\n\\n11.7 Reasoning with Uncertainty\\n\\n327\\n\\n(b)\\n\\nP AljBur\\n\\nð\\n\\nÞ ¼ P AljBur; Ear\\n\\nÞ þ P AljBur; :Ear\\n\\nÞP Ear ð ¼ 0:95 (cid:4) 0:002 þ 0:94 (cid:4) 0:998 ¼ 0:94002;\\n\\nð\\n\\nð\\n\\nÞP :Ear\\n\\nð\\n\\nÞ\\n\\nP MjBur\\n\\nð\\n\\nÞ ¼ P M; Bur ð =P Bur Þ ð ¼ P MjAl ÞP AljBur ½ ð =P Bur Þ ð ¼ P MjAl ÞP AljBur ð ¼ 0:7 (cid:4) 0:94002 þ 0:01 (cid:4) 0:05998 ¼ 0:659:\\n\\nÞ=P Bur ð\\n\\nÞ þ P M; :Al; Bur\\n\\nÞ ¼ P M; Al; Bur\\n\\nð\\n\\nÞ\\n\\nð\\n\\n(cid:6)\\n\\n½\\n\\nÞP Bur ð\\n\\nð\\n\\nÞP :AljBur\\n\\nð\\n\\nÞP Bur ð\\n\\nÞ þ P Mj:Al ð\\n\\nÞ þ P Mj:Al ð\\n\\nÞP :AljBur\\n\\nð\\n\\nð\\n\\nÞ\\n\\nÞ\\n\\n(cid:6)\\n\\n(c)\\n\\nPðBurjMÞ ¼\\n\\nPðMjBurÞPðBurÞ PðMÞ\\n\\n¼\\n\\n0:659 (cid:4) 0:001 0:0117\\n\\n¼ 0:056:\\n\\n(d)\\n\\nP (Al|J, M) =\\n\\n=\\n\\n=\\n\\nP (Al, J, M) P (J, M) 1 1 + P (¬Al,J,M) P (Al,J,M) 1 1 + 0.05·0.01·0.9975 0.9·0.7·0.0025\\n\\nP (Al, J, M) P (Al, J, M) + P (¬Al, J, M)\\n\\n=\\n\\n1 1 + P (J|¬Al)P (M|¬Al)P (¬Al)\\n\\n=\\n\\nP (J|Al)P (M|Al)P (Al)\\n\\n= 0.761,\\n\\nP (J, M|Bur) = P (J, M|Al)P (Al|Bur) + P (J, M|¬Al)P (¬Al|Bur))\\n\\n= P (J|Al)P (M|Al)P (Al|Bur)\\n\\n+ P (J|¬Al)P (M|¬Al)P (¬Al|Bur))\\n\\n= 0.9 · 0.7 · 0.94 + 0.05 · 0.01 · 0.06 = 0.5922, P (Al|¬Bur) = P (Al|¬Bur, Ear)P (Ear) + P (Al|¬Bur, ¬Ear)P (¬Ear)\\n\\n= 0.29 · 0.002 + 0.001 · 0.998 = 0.00158, P (J, M|¬Bur) = P (J, M|Al)P (Al|¬Bur) + P (J, M|¬Al)P (¬Al|¬Bur))\\n\\n= P (J|Al)P (M|Al)P (Al|¬Bur)\\n\\n+ P (J|¬Al)P (M|¬Al)P (¬Al|¬Bur))\\n\\n= 0.9 · 0.7 · 0.00158 + 0.05 · 0.01 · 0.998 = 0.00149,\\n\\nP (J, M) = P (J, M|Bur)P (Bur) + P (J, M|¬Bur)P (¬Bur)\\n\\n= 0.5922 · 0.001 + 0.00149 · 0.999 = 0.00208,\\n\\nP (Bur|J, M) =\\n\\nP (J, M|Bur)P (Bur) P (J, M)\\n\\n= 0.5922 · 0.001/0.00208 = 0.284.\\n\\n(e) P(J)P(M) = 0.052 ⋅ 0.0117 = 0.00061, but P(J, M) = 0.00208 (see above).\\n\\nTherefore P(J)P(M) 6¼ P(J, M).\\n\\n328\\n\\n11 Solutions for the Exercises\\n\\n(f) See Sect. 7.4.5 on page 163.\\n\\n(g)\\n\\nMary\\n\\nEarthquake\\n\\nBurglary\\n\\nAlarm\\n\\nJohn\\n\\n(h) Only the CPT of alarm node the of changes. All the nodes other are independent of or earthquake of independent earthquake given alarm.\\n\\n(i)\\n\\nBur\\n\\nt f\\n\\nP(Al)\\n\\n0.94 0.0016\\n\\nare solutions Other possible the to due counterproductive vari- able order. Thus we a learn: Always use variable that respects causality!\\n\\norder\\n\\nExercise 7.12 (a), (b) See Fig. 11.1. (c) The edge (Str, Li) is missing because Li and Str are conditionally independent given V, i.e., P(Li|V, Str) = P(Li|V) because the street condition has no direct inﬂuence on the light, rather only over the dynamo and the voltage it generates. (The conditional independence Li and Str given V cannot be shown here by calculation based on available data for P(Li|V, Str) and P(Li|V), rather it can only be asserted.) (d) For the given CPTs:\\n\\nP RjStr\\n\\nð\\n\\nP VjStr\\n\\nð\\n\\nÞ ¼ P RjStr; Flw\\n\\nÞ þ P RjStr; :Flw\\n\\nð ¼ 0:95 (cid:4) 0:4 þ 0:7 (cid:4) 0:6 ¼ 0:8; Þ þ P Vj:R ð\\n\\nð\\n\\nÞP Flwð\\n\\nÞ ¼ P VjRð\\n\\nð ¼ 0:04 (cid:4) 0:8 þ 0:99 (cid:4) 0:2 ¼ 0:23:\\n\\nÞP RjStr\\n\\nð\\n\\nÞP :RjStr\\n\\nÞ\\n\\nÞP :Flw\\n\\nð\\n\\nÞ\\n\\nFig. 11.1 Bayesian network for the bicycle light. The CPT for Li is given in the problem statement\\n\\n11.8 Machine Learning and Data Mining\\n\\n11.8 Machine Learning and Data Mining\\n\\nExercise 8.1 (a) The agent is a function A, which maps the vector (t1, t2, t3, d1, d2, d3, f1, f2, f3) consisting of three values each for temperature, pressure, and humidity to a class value k 2 {sunny, cloudy, rainy}.\\n\\n(b) The training data ﬁle consists of a line of the form\\n\\nti1; ti2; ti3; di1; di2; di3; fi1; fi2; fi3; ki\\n\\nfor every single training data item. The index i runs over all training data. A concrete ﬁle could for example begin:\\n\\n17,17,17,980,983,986,63,61,50,sunny 20,22,25,990,1014,1053,65,66,69,sunny 20,22,18,990,979,929,52,60,61,rainy\\n\\nExercise 8.2 We can see the symmetry directly using the deﬁnition of correlation coefﬁcients, or of covariance. Swapping i and j does not change the value. For the diagonal elements we calculate\\n\\nKii ¼\\n\\nrii si (cid:4) si\\n\\n¼\\n\\nP\\n\\nN\\n\\np¼1ðx p P\\n\\ni (cid:2) (cid:2)xiÞðx p p¼1ðx p i (cid:2) (cid:2)xiÞ2\\n\\ni (cid:2) (cid:2)xiÞ\\n\\nN\\n\\n¼ 1:\\n\\nExercise 8.3 The sequence of values for w is\\n\\nð1; 1Þ; ð2:2; (cid:2)0:4Þ; ð1:8; 0:6Þ; ð1:4; 1:6Þ; ð2:6; 0:2Þ; ð2:2; 1:2Þ\\n\\nExercise 8.4\\n\\n(a)\\n\\n(b) Drawing a straight line in the graph yields:\\n\\n1:5x1 þ x2 (cid:2) 15 [ 0:\\n\\nAfter 442 iterations, the perceptron learn- ing algorithms with the start vector w = (1, 1 ,1) returns:\\n\\nw ¼ ð11; 16; (cid:2)129Þ:\\n\\nThis corresponds to: 0.69x1 + x2 − 8.06 > 0\\n\\n329\\n\\n330\\n\\n11 Solutions for the Exercises\\n\\nExercise 8.5\\n\\nP\\n\\nx points in an “average” direction of all positive points and x points in an “average” direction of all negative points. The difference of these vectors points from the negative points toward the positive points. The dividing line is then perpendicular to this difference vector.\\n\\n(a) The vector\\n\\nP\\n\\nx2M þ\\n\\nx2M(cid:2)\\n\\n(b) The point cloud of the positive and negative points dominate during the calculation of w. The two outliers hardly play a role here. For determining the dividing line, however, they are important.\\n\\nExercise 8.6\\n\\nExercise 8.7\\n\\n(a) Nearest neighbor is (8, 4), thus class 1. (b) k = 2: class undeﬁned since one instance of class 1 and one instance of class 2.\\n\\nk = 3: decision 2:1 for class 0. k = 5: decision 2:3 for class 1.\\n\\nExercise 8.8\\n\\n(a) To be able to make general statements, we must assume that the points are evenly distributed in the feature space, which means that the number of points per area is overall approximately equal. Now we calculate the area Ad of a narrow ring of width dd and distance d around the point x:\\n\\nAd ¼ pðd þ DdÞ2 (cid:2) pd2 ¼ pðd2 þ 2dDd þ Dd2Þ (cid:2) pd2 (cid:3) 2pdDd:\\n\\nThe total weight of all points in the ring of width Dd and distance d is thus proportional to dwi = d/(1 + ad2) (cid:3) 1/(ad) for d ! ∞.\\n\\n(b) For this weighting, the total weight of all points in the ring of width Dd and i ¼ d=d ¼ 1. Thus each ring of width distance d would be proportional to dw0 Dd would have the same weight, independent of its distance to point x. This certainly does not make sense because the immediate surroundings are most important for the approximation.\\n\\n11.8 Machine Learning and Data Mining\\n\\n331\\n\\nExercise 8.10 limx!0 x log2 x ¼ limx!0 log2 x 1=x ¼ limx!0 for the second equation l’Hospital’s rule was used.\\n\\n1=ðx ln 2Þ (cid:2)1=x2 ¼ limx!0\\n\\n(cid:2)x ln 2 ¼ 0, where\\n\\nExercise 8.11 (a) 0\\n\\n(b) 1\\n\\n(c) 1.5\\n\\n(d) 1.875\\n\\n(e) 2.32\\n\\n(f) 2\\n\\nExercise 8.12\\n\\n(a) From log2x = ln x/ln 2, it follows that c = 1/ln 2 (cid:3) 1.44. (b) Since both entropy functions only differ by a constant factor, the position of the entropy maximum does not change. Extrema under constraints also maintain the same position. Thus the factor c does not cause a problem the information for the MaxEnt method. For learning of decision trees, gains of various attributes are compared. Because here only the ordering matters, but not the absolute value, the factor c does not cause a problem here either.\\n\\nExercise 8.13 (a) For the ﬁrst attribute we calculate\\n\\nG D; x1 ð\\n\\nX9\\n\\nj\\n\\nDx1¼i Dj j\\n\\nj\\n\\nÞ ¼ H Dð Þ (cid:2)\\n\\nH Dx1¼i\\n\\nÞ\\n\\nð\\n\\ni¼6\\n\\n(cid:4)\\n\\n1 8\\n\\n1 8\\n\\n¼ 1 (cid:2)\\n\\nH Dx1¼6\\n\\nH Dx1¼7\\n\\nð\\n\\nÞ þ\\n\\nÞ þ\\n\\nð\\n\\n(cid:4)\\n\\n3 8\\n\\n3 8\\n\\n(cid:4) 0:918\\n\\n(cid:4) 0:918 þ\\n\\n¼ 1 (cid:2) 0 þ 0 þ\\n\\n3 H Dx1¼8 8 (cid:5)\\n\\nð\\n\\nÞ þ\\n\\n¼ 0:311\\n\\n3 8\\n\\nH Dx1¼9\\n\\nð\\n\\nÞ\\n\\n(cid:5)\\n\\nG(D, x2) = 0.75, thus x2 is selected. For x2 = 0, 1, 3, 4, 5, 6 the decision is clear. For x2 = 2, x1 is selected. The tree then has the form\\n\\nx2 = 0: 0 (1/0) x2 = 1: 0 (1/0) x2 = 2: | x1 = 6: 0 (0/0) | x1 = 7: 0 (0/0) | x1 = 8: 0 (1/0) | x1 = 9: 1 (1/0) x2 = 3: 0 (1/0) x2 = 4: 1 (1/0) x2 = 5: 1 (1/0) x2 = 6: 1 (1/0)\\n\\n(b)\\n\\nInformation gain for the continuous attribute x2 as the root node:\\n\\nThreshold H G(D, x2 (cid:7) H)\\n\\n0\\n\\n0.138\\n\\n1\\n\\n0.311\\n\\n2\\n\\n0.189\\n\\n3\\n\\n0.549\\n\\n4\\n\\n0.311\\n\\n5\\n\\n0.138\\n\\n332\\n\\n11 Solutions for the Exercises\\n\\nSince G(D, x2 (cid:7) 3) = 0.549 > 0.311 = G(D, selected. For x2 (cid:7) 3 the classiﬁcation is not unique. We GðDx2 (cid:7) 3; GðDx2 (cid:7) 3; x1Þ ¼ 0:322, calculate x2 (cid:7) 0Þ ¼ 0:073, GðDx2 (cid:7) 3; x2 (cid:7) 1Þ ¼ 0:17, GðDx2 (cid:7) 3; x2 (cid:7) 2Þ ¼ 0:073. Thus x1 is selected. For x1 = 9 the classiﬁcation is not unique. ; Here the decision is clearly GðDðx2 (cid:7) 3;x1¼9Þ x1Þ ¼ 0, GðDðx2 (cid:7) 3;x1¼9Þ x2 (cid:7) 1 is chosen, and the tree once again has 100% correctness.\\n\\nx1),\\n\\nx2 (cid:7) 3 is\\n\\n; x2 (cid:7) 1Þ ¼ 1,\\n\\nTree: x2 <= 3: | x1 = 6: 0 (1/0) | x1 = 7: 0 (0/0) | x1 = 8: 0 (1/0) | x1 = 9: | | x2 <= 1: 0 (1/0) | | x2 > 1: 1 (1/0) x2 > 3: 1 (3/0)\\n\\nExercise 8.14\\n\\n(a) 100% for the training data, 75% for the test data, 90% total correctness. (b) (c)\\n\\n(A ^ ¬C) _ (¬A ^ B)\\n\\n66.6% correctness for the training data 100% correctness for the test data 80% total correctness\\n\\nExercise 8.15\\n\\n(a) Equation (8.7) on page 203 for calculating the information gain of an attribute\\n\\nA reads\\n\\nInfoGainðD; AÞ ¼ HðDÞ (cid:2)\\n\\nXn\\n\\ni¼1\\n\\njDij jDj\\n\\nHðDiÞ;\\n\\nwhere n is the number of the values of the currently observed attribute. If the attribute A is tested somewhere in the subtree as a successor of the value aj, then only the value A = aj will occur in the dataset Dj and each of its sub- sets D′. Thus D0 ¼ D0\\n\\nj and for all k 6¼ j, jD0\\n\\nkj ¼ 0 and we obtain\\n\\nInfoGain(D0,A) ¼ HðD0Þ (cid:2)\\n\\nXn\\n\\ni¼1\\n\\njD0 ij jD0j\\n\\nHðD0\\n\\niÞ ¼ HðD0\\n\\njÞ (cid:2)\\n\\njD0 jj jD0 jj\\n\\njÞ ¼ 0:\\n\\nHðD0\\n\\nThe repeated attribute thus has an information gain of zero, because of which it is no longer used.\\n\\n(b) From every continuous attribute A a binary attribute A > HD,A is generated. If, further down in the tree, the attribute A is discretized again with a different threshold HD′,A, then the attribute A > HD′,A is different from A > HD,A. If it then has a higher information gain than all other attributes, it will be used in the tree. However, this also means that for multiple occurrences of a continuous attribute, the thresholds must be different.\\n\\n11.8 Machine Learning and Data Mining\\n\\n333\\n\\nExercise 8.16\\n\\n(a)\\n\\nP(Sky = clear) = 0.65 P(Bar = rising) = 0.67\\n\\nSky\\n\\nClear Clear Cloudy Cloudy\\n\\nBar\\n\\nRising Falling Falling Falling\\n\\nP(Prec = dry|Sky, Bar)\\n\\n0.85 0.44 0.45 0.2\\n\\n(b) P(Sky = clear) = 0.65\\n\\nBar\\n\\nP(Prec = dry|Bar)\\n\\nSky\\n\\nP(Bar = rising|Sky)\\n\\nRising Falling\\n\\n0.73 0.33\\n\\nClear Cloudy\\n\\n0.72 0.57\\n\\n(c) The necessary CPTs for P(Prec|Sky, Bar) and P(Bar|Sky), as well as P(Sky) are\\n\\nalready known.\\n\\n(d) Pa = (0.37, 0.065, 0.095, 0.12, 0.11, 0.13, 0.023, 0.092) Pb = (0.34, 0.13, 0.06, 0.12, 0.15, 0.054, 0.05, 0.1) P = (0.4, 0.07, 0.08, 0.1, 0.09, 0.11, 0.03, 0.12) Quadratic distance: dq (Pa, P) = 0.0029, Kullback–Leibler dist.: dk (Pa, P) = 0.017, Both distance metrics show that the network (a) approximates the distribution better than network (b). This means that the assumption that Prec and Sky are conditionally independent given Bar is less likely true than the assumption that Sky and Bar are independent.\\n\\n(original distribution)\\n\\ndq (Pb, P) = 0.014 dk (Pb, P) = 0.09\\n\\n(e) Pc = (0.4, 0.07, 0.08, 0.1, 0.09, 0.11, 0.03, 0.12). This distribution is exactly equal to the original distribution P. This is not surprising because there are no missing edges in the network. This means that no independencies were assumed.\\n\\nExercise 8.17 We can immediately see that scores and perceptrons are equiva- lent by comparing their deﬁnitions. Now for the equivalence to naive Bayes: First we establish that P(K | S1, … , Sn) > 1/2 is equivalent to P(K | S1, … , Sn) > P(¬K | S1, … , Sn) because P(¬K | S1, … , Sn) > 1 − P(K | S1, … , Sn). We are in fact dealing with a binary naive Bayes classiﬁer here.\\n\\nWe apply the logarithm to the naive Bayes formula\\n\\nPðKjS1; . . .; SnÞ ¼\\n\\nPðS1jKÞ (cid:4) (cid:4) (cid:4) (cid:4) (cid:4) PðSnjKÞ (cid:4) PðKÞ PðS1; . . .; SnÞ\\n\\n;\\n\\nand obtain\\n\\nlog P KjS1; . . .; Sn\\n\\nð\\n\\nÞ ¼ log P S1jKð\\n\\nÞ þ (cid:4) (cid:4) (cid:4) þ log P SnjKð\\n\\nÞ þ log P Kð Þ\\n\\n(cid:2) log P S1; . . .; Sn ð\\n\\nÞ:\\n\\nð11:18Þ\\n\\n334\\n\\n11 Solutions for the Exercises\\n\\nTo obtain a score, we must interpret the variables S1, …, Sn as numeric variables with the values 1 and 0. We can easily see that\\n\\nlog PðSijKÞ ¼ ðlog PðSi ¼ 1jKÞ (cid:2) log PðSi ¼ 0jKÞÞSi þ log PðSi ¼ 0jKÞ:\\n\\nIt follows that\\n\\nXn\\n\\nXn\\n\\nlog P SijKð\\n\\nÞ ¼\\n\\nðlog P Si ¼ 1jK ð\\n\\nÞ (cid:2) log P Si ¼ 0jK ð\\n\\nÞÞSi\\n\\ni¼1\\n\\ni¼1\\n\\nþ\\n\\nXn\\n\\nlog P Si ¼ 0jK\\n\\nð\\n\\nÞ:\\n\\ni¼1\\n\\nNow we deﬁne wi = log P(Si = 1|K) − log P(Si = 0|K) and c ¼ 0jKÞ and simplify\\n\\nP\\n\\nn i¼1 log PðSi ¼\\n\\nXn\\n\\nlog PðSijKÞ ¼\\n\\nXn\\n\\nwiSi þ c:\\n\\ni¼1\\n\\ni¼1\\n\\nSubstituted in (11.18) on page 333 we obtain\\n\\nlog PðKjS1; . . .; SnÞ ¼\\n\\nXn\\n\\nwiSi þ c þ log PðKÞ (cid:2) log PðS1; . . .; SnÞ:\\n\\ni¼1\\n\\nFor the decision K it must be the case, according to the deﬁnition of the Bayes classiﬁer, that logP(K|S1, … ,Sn) > log(1/2). Thus it must either be the case that\\n\\nXn\\n\\nwiSi þ c þ log PðKÞ (cid:2) log PðS1; . . .; SnÞ [ logð1=2Þ\\n\\ni¼1\\n\\nor that\\n\\nXn\\n\\nwiSi [ log 1=2 (cid:2) c (cid:2) log PðKÞ þ log PðS1; . . .; SnÞ;\\n\\ni¼1\\n\\nwith which we have deﬁned a score with the threshold H = log 1/2 − c − log P(K) + log P(S1, …, Sn ). Because all of the transformations can be reversed, we can also transform any score into a Bayesian classiﬁer. With that, the equivalence has been shown. Exercise 8.18 Taking the logarithm of (8.10) on page 222 results in\\n\\nlog PðIjs1; . . . ; snÞ ¼ log c þ log PðIÞ þ\\n\\nXl\\n\\nni log PðwijIÞ:\\n\\ni¼1\\n\\n11.8 Machine Learning and Data Mining\\n\\nThereby very small negative numbers become moderate negative numbers. Since the logarithm function grows monotonically, to determine the class we maximize according to the rule\\n\\nINaive(cid:2)Bayes ¼ argmax I2fw; f g\\n\\nlog PðIjs1; . . .; snÞ:\\n\\nThe disadvantage of this method is the somewhat longer computation time in the learning phase for large texts. During classiﬁcation the time does not increase, because the values logP(I|s1, … , sn) can be saved during learning. Exercise 8.20 Let f be strictly monotonically increasing, that is, 8x, y x < y ) f(x) < f(y). If now d1(s, t) < d1(u, v), then clearly d2(s, t) = f(d1(s, t)) < f(d1(u, v)) = d2(u, v). Because the inverse of f is also strictly monotonic, the reverse is true, that is, d2(s, t) < d2(u, v) ) d1(s, t) < d1(u, v). Thus it has been shown that d2(s, t) < d2(u, v) , d1(s, t) < d1(u, v). Exercise 8.21\\n\\nx1x2 ¼ 4;\\n\\nx2x3 ¼ 2;\\n\\nx1x3 ¼ 1;\\n\\nand thus\\n\\nand thus\\n\\nand thus\\n\\ndsðx1; x2Þ ¼\\n\\ndsðx2; x3Þ ¼\\n\\ndsðx1; x3Þ ¼\\n\\np\\n\\np\\n\\np\\n\\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 23 (cid:4) 26 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 4 26 (cid:4) 20 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2 23 (cid:4) 20 1\\n\\n¼ 6:11\\n\\n¼ 11:4\\n\\n¼ 21:4\\n\\nSentences 1 and 2 are most similar w.r.t. the distance metric ds. Exercise 8.22 Help for problems with KNIME: www.knime.org/forum\\n\\n11.9 Neural Networks\\n\\nExercise 9.1 We want to show that f(H + x) + f(H − x) = 1.\\n\\n1 1 þ e(cid:2)x Þ þ f H (cid:2) x ð\\n\\nf H þ x ð\\n\\nÞ ¼\\n\\nf H þ x ð\\n\\nT\\n\\n¼\\n\\nex 1 þ ex\\n\\nT\\n\\nT\\n\\nÞ ¼ 1:\\n\\n;\\n\\nf H (cid:2) x ð\\n\\nÞ ¼\\n\\n1 1 þ ex\\n\\nT\\n\\n;\\n\\nExercise 9.3 Each pattern saved in the network has a size of n bits. The network has a total of n(n − 1)/2 weights. If we reserve 16 bits per weight and deﬁne binary storage of size 16n(n − 1)/2 as equally large, then this can clearly store N = 8n(n − 1)/ n = 4(n − 1) patterns n bits in size. For large n we obtain N = 4n as the limit. If we take the quotient a of the number of storable bits and the number of available storage cells, as in (9.11) on page 260, then we obtain the value 1 for the list memory and the value a = 0.146n2/(16n(n − 1)/2) (cid:3) 0.018 for the Hopﬁeld network. The classical storage thus has (for 16 bits per weight), a capacity roughly 55 times higher.\\n\\n335\\n\\n336\\n\\n11 Solutions for the Exercises\\n\\nExercise 9.4 (a) Mathematica program for the least square method. LeastSq[q_,a_] := Module[{Nq,Na,m,A,b,w},\\n\\nNq = Length[q]; m = Length[q[[1]]]; Na = Length[a]; If[Nq != Na, Print[\"Length[q] != Length[a]\"]; Exit, 0]; A = Table[N[Sum[q[[p,i]] q[[p,j]], {p,1,Nq}]], {i,1,m}, {j,1,m}]; b = Table[N[Sum[a[[p]] q[[p,j]], {p,1,Nq}]], {j,1,m}]; w = LinearSolve[A,b]\\n\\n] LeastSq::usage = \"LeastSq[x,y,f] computes from the query vectors q[[1]],...,\\n\\nq[[m]] a table of coefficients w[[i]] for a linear mapping f[x] =\\n\\nSum[w[[i]] x[[i]], {i,1,m}] with f[q[[p]]] = a[[p]].\"\\n\\n(b)\\n\\n(c)\\n\\nExercise 9.6 (a) Learning works without errors. without errors!\\n\\n(b) Learning does not work\\n\\nExercise 9.7\\n\\n(a) A mapping f is called linear if for all x, y, k it is the case that f(x + y) = f(x) + f(y) and f(kx) = kf(x). Now let f and g be linear mappings. Then f(g(x + y)) = f(g(x) + g(y)) = f(g(x)) + f(g(y)) and f(g(kx)) = f(kg(x)) = kf(g(x)). Thus, successive executions of linear mappings are a linear mapping.\\n\\n(b) We observe two arbitrary output neurons j and k. Each of the two represent a class. Classiﬁcation is done by forming the maximum of the two activations. wkixi be the weighted sum of values arriving at Let netj = neurons j and k. Furthermore, let netj > netk. Without an activation function, class j is output. Now if a strictly monotonic activation function f is applied, nothing changes in the result because, due to the function being strictly monotonic, f(netj) > f(netk).\\n\\nP i\\n\\nP i\\n\\nwjixi and netk =\\n\\nExercise 9.8 f1ðx1; x2Þ ¼ x2 f2ðx1; x2Þ ¼ x2 1 transformed space has the equation y1 + y2 = 1.\\n\\n;\\n\\n2. Then the dividing line in the\\n\\n11.10 Reinforcement Learning\\n\\n11.10 Reinforcement Learning\\n\\nExercise 10.1 (a) nn\\n\\n(b) (n−1)n\\n\\n(c)\\n\\n(d)\\n\\nExercise 10.2 Value iteration yields the sequence of value tables below. Depending on the order of value updates, the intermediate conﬁgurations may differ, but not the ﬁnal conﬁguration.\\n\\n0 0\\n\\n0 0\\n\\n→ 0.81 0.73\\n\\n0.9 1\\n\\n→ 1.35 1.21\\n\\n1.49 1.66\\n\\n→ · · · → 2.36 2.12\\n\\nExercise 10.3\\n\\n(c)\\n\\n(d) We see that the longer a policy becomes (i.e., the more steps, for example, that a cycle of a cyclical strategy has), the closer the value c must be to 1 because a higher value for c makes a longer memory possible. However, value iteration converges that much more slowly.\\n\\nExercise 10.4 The value V⋆(3, 3) at bottom right in the state matrix is changed as follows:\\n\\nV H ð3; 1Þ ¼ 0:9V H ð2; 1Þ ¼ 0:92V H ð2; 2Þ ¼ 0:93V H ð2; 3Þ ¼ 0:94V H ð3; 3Þ: ð11:19Þ\\n\\nThis chain of equations follows from (10.6) on page 295 because, for all given state transitions, the maximum immediate reward is r(s, a) = 0, and it is the case that\\n\\nV H ðsÞ ¼ max\\n\\na\\n\\n½rðs; aÞ þ cV Hðdðs; aÞÞ(cid:6) ¼ cV H ðdðs; aÞÞ ¼ 0:9V H ðdðs; aÞÞ:\\n\\n337\\n\\n2.62 2.91\\n\\n338\\n\\n11 Solutions for the Exercises\\n\\nFrom (10.6) on page 295 it also follows that V⋆(3, 2) ¼ 1 þ 0:9V⋆(3, 1), because r(s, a) ¼ 1 is maximal. Analogously it is true that V⋆(3, 3) ¼ 1 þ 0:9 V⋆(3, 2) and the circle closes. The two last equations together yield V⋆(3, 3) ¼ 1 þ 0:9ð1 þ 0:9 V⋆(3, 1)). From (11.19) on page 337 it follows that V⋆(3, 1) ¼ 0:94V⋆(3, 3). Substituted in V⋆(3, 3), this yields\\n\\nV H ð3; 3Þ ¼ 1 þ 0:9ð1 þ 0:95V H ð3; 3ÞÞ;\\n\\nfrom which the claim follows.\\n\\nExercise 10.5\\n\\nStable Q-values and an optimal policy:\\n\\nExercise 10.6 (a) Number of states = ‘n. Number of actions per state = ‘n. Number of poli-\\n\\ncies ¼ ‘nð\\n\\n‘n Þ\\n\\n¼ ‘n‘n.\\n\\n(b)\\n\\nn = 1 n = 2 n = 3 n = 4 n = 8\\n\\nl = 1\\n\\n1 1 1 1 1\\n\\nl = 2\\n\\n4 256 1.7 (cid:9) 107 1.8 (cid:9) 1019 3.2 (cid:9) 10616\\n\\nl = 3\\n\\n27 3.9 (cid:9) 108 4.4 (cid:9) 1038 3.9 (cid:9) 10154 1.4 (cid:9) 1025043\\n\\nl = 4\\n\\n256 1.8 (cid:9) 1019 3.9 (cid:9) 10115 3.2 (cid:9) 10616 6.7 (cid:9) 10315652\\n\\nl = 10 1010 10200 103000 1040000 10800000000\\n\\n(c) Per state there are now 2n possible actions. Thus there are ð2nÞ\\n\\n‘n\\n\\npolicies.\\n\\nl = 1\\n\\nl = 2\\n\\nl = 3\\n\\nl = 4\\n\\nl = 10\\n\\nn = 1 n = 2 n = 3 n = 4 n = 8\\n\\n2 4 6 8 16\\n\\n4 256 1.7 (cid:9) 106 2.8 (cid:9) 1014 1.8 (cid:9) 10308\\n\\n8 2.6 (cid:9) 109 1.0 (cid:9) 1021 1.4 (cid:9) 1073 1.7 (cid:9) 107900\\n\\n16 4.3 (cid:9) 109 6.3 (cid:9) 1049 1.6 (cid:9) 10231 1.6 (cid:9) 1078913\\n\\n1024 1.6 (cid:9) 1060 1.4 (cid:9) 10778 7.9 (cid:9) 109030 1.8 (cid:9) 10120411998\\n\\n(d) 10120411998 different policies can never be explored combinatorially, even if all of the available computers in the world were to operate on them in parallel. Thus “intelligent” algorithms are necessary to ﬁnd an optimal or nearly optimal policy.\\n\\nReferences\\n\\n[ACM09]\\n\\n[Ada75]\\n\\n[Alp04] [AOJJ89]\\n\\n[APR90]\\n\\n[AR88]\\n\\n[Bar98]\\n\\n[Bat16]\\n\\n[BBBK11]\\n\\n[BBSK10]\\n\\n[BCDS08]\\n\\n[Bel57] [Ben16]\\n\\n[Ber89] [BFOS84]\\n\\n[Bib82] [Bis05]\\n\\nA survey of robot learning from demonstration. Robotics and Autonomous Systems, 57:469–483, 2009 E.W. Adams. The Logic of Conditionals, volume 86 of Synthese Library. D. Reidel Publishing Company, 1975 E. Alpaydin. Introduction to Machine Learning. MIT Press, 2004 S. K. Andersen, K. G. Olesen, F. V. Jensen, and F. Jensen. HUGIN - A Shell for Building Bayesian Belief Universes for Expert Systems. In Proc. of the 11th Intl. Joint Conf. on Artiﬁcial Intelligence (IJCAI-89), 1989 J. Anderson, A. Pellionisz, and E. Rosenfeld. Neurocomputing (vol. 2): directions for research. MIT Press, Cambridge, MA, USA, 1990 J. Anderson and E. Rosenfeld. Neurocomputing: Foundations of Research. MIT Press, Cambridge, MA, 1988. Collection of fundamental original papers R. Bartak. Online guide to constraint programming. http://kti.ms.mff.cuni.cz/bartak/ constraints, 1998 A. Batzill, Optimal route planning on mobile systems (Masterarbeit, Hochschule Ravensburg-Weingarten, 2016) J. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl. Algorithms for hyper-parameter Information Processing Systems, pages optimization. 2546–2554, 2011 Greg Bickerman, Sam Bosley, Peter Swire, and Robert Keller. Learning to create International Conference on jazz melodies using deep belief nets. Computational Creativity, 2010 A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Robot programming by demonstration. In B. Siciliano and O. Khatib, editors, Handbook of Robotics, pages 1371–1394. Springer, 2008 R.E. Bellman. Dynamic Programming. Princeton University Press, 1957 R. Benenson. What is the class of this image? http://rodrigob.github.io/are_we_ there_yet/build/classiﬁcation_datasets_results.html, February 2016 M. Berrondo. Fallgruben für Kopffüssler. Fischer Taschenbuch Nr. 8703, 1989 L. Breiman, J. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and regression trees. Wadsworth, 1984 W. Bibel. Automated Theorem Proving. Vieweg Verlag, 1982 C.M. Bishop. Neural networks for pattern recognition. Oxford University Press, 2005 C.M. Bishop, Pattern recognition and machine learning (Springer, New York, 2006)\\n\\nIn Advances in Neural\\n\\nIn First\\n\\n[Bis06]\\n\\n[BJCdC14] R.C. Barros, P.A. Jaskowiak, R. Cerri, A.C. de Carvalho, A framework for bottom-up induction of oblique decision trees. Neurocomputing 135, 3–12 (2014) C. Beierle and G. Kern-Isberner. Methoden wissensbasierter Systeme. Vieweg, 2000\\n\\n[BKI00]\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4\\n\\n339\\n\\n340\\n\\nReferences\\n\\n[BKNS00] M. Breunig, H.P. Kriegel, R. Ng, J. Sander, Lof: identifying density-based local\\n\\noutliers. ACM sigmod record 29(2), 93–104 (2000) A.G. Barto, S. Mahadevan, Recent advances in hierarchical reinforcement learning. Discrete Event Systems, Special issue on reinforcement learning 13, 41–77 (2003) V. Braitenberg. Vehicles – Experiments in Synthetic Psychology. MIT Press, 1984 B. Brabec. Computergestützte regionale Lawinenprognose. PhD thesis, ETH Zürich, 2001 I. Bratko. PROLOG Programming for Artiﬁcial Intelligence. Addison-Wesley, 4th edition, 2011 Encyclopedia Britannica. Encyclopedia Britannica Verlag, London, 1991 C.J. Burges, A tutorial on support vector machines for pattern recognition. Data Min. Knowl. Discov. 2(2), 121–167 (1998) CADE: Conference on Automated Deduction. http://www.cadeconference.org R. Cubek, W. Ertel, and G. Palm. A critical review on the symbol grounding problem as an issue of autonomous agents. In Proceedings of the 38th German Conference on Artiﬁcial Intelligence (KI), Dresden, Germany, 2015 R. Cubek, W. Ertel, and G. Palm. High-level learning from demonstration with conceptual spaces and subspace clustering. In Proceedings of the 2015 IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, 2015 P. Cheeseman. A method for computing generalised bayesian probability values for expert systems. In Proc. of the 8th Intl. Joint Conf. on Artiﬁcial Intelligence (IJCAI-83), 1983 P. Cheeseman. In defense of probability. In Proc. of the 9th Intl. Joint Conf. on Artiﬁcial Intelligence (IJCAI-85), 1985 C.L. Chang, R.C. Lee, Symbolic Logic and Mechanical Theorem Proving (Academic Press, Orlando, Florida, 1973) W.S. Cleveland, Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association 74(368), 829–836 (1979) T. Cormen, Ch. Leiserson, R. Rivest, Introduction to Algorithms (MIT Press, Cambridge, Mass, 1990) W.F. Clocksin, C.S. Mellish, Programming in Prolog, 4th edn. (Springer, Berlin, Heidelberg, New York, 1994) C. Ohmann, M. Kraemer, S. Jaeger, H. Sitter, C. Pohl, B. Stadelmayer, P. Vietmeier, J. Wickers, L. Latzke, B. Koch, K. Thon, Akuter bauchschmerz - standardisierte befundung als diagnoseunterstützung. Chirurg 63, 113–123 (1992) F.G. Cozman. Javabayes, bayesian networks in java, 1998. http://www.cs.cmu.edu/ javabayes F.T. de Dombal. Diagnosis of Acute Abdominal Pain. Churchill Livingstone, 1991 F.T. de Dombal, D.J. Leaper, J.R. Staniland, A.P. McCann, J.C. Horrocks, Computer aided diagnosis of acute abdominal pain. British Medical Journal 2, 9–13 (1972) The DeepQA Project, 2011. http://www.research.ibm.com/deepqa/deepqa.shtml R.O. Duda and P.E. Hart. Pattern Classiﬁcation and Scene Analysis. Wiley, 1973. Klassiker zur Bayes-Decision-Theorie R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation. Wiley, 2001 D. Diaz. GNU PROLOG. Universität Paris, 2004. Auﬂ. 1.7, für GNU Prolog version 1.2.18, http://gnu-prolog.inria.fr C.L. Blake D.J. Newman, S. Hettich and C.J. Merz. UCI repository of machine learning databases. http://www.ics.uci.edu/mlearn/MLRepository.html, 1998 E. Eder. Relative Complexities of First Order Calculi. Vieweg Verlag, 1991\\n\\n[BM03]\\n\\n[Bra84] [Bra01]\\n\\n[Bra11]\\n\\n[Bri91] [Bur98]\\n\\n[CAD] [CEP15a]\\n\\n[CEP15b]\\n\\n[Che83]\\n\\n[Che85]\\n\\n[CL73]\\n\\n[Cle79]\\n\\n[CLR90]\\n\\n[CM94]\\n\\n[CMS+92]\\n\\n[Coz98]\\n\\n[dD91] [dDLS+72]\\n\\n[Dee11] [DH73]\\n\\n[DHS01] [Dia04]\\n\\n[DNM98]\\n\\n[Ede91]\\n\\nReferences\\n\\n[Elk93]\\n\\n[Ert93]\\n\\n[Ert07] [Ert11]\\n\\n[Ert15]\\n\\n[ES99]\\n\\nC. Elkan. The paradoxical success of fuzzy logic. In Proceedings of the Eleventh National Conference on Artiﬁcial Intelligence (AAAI-93), pages 698–703. MIT Press, 1993 W. Ertel. Parallele Suche mit randomisiertem Wettbewerb in Inferenzsystemen, volume 25 of DISKI. Inﬁx-Verlag, St. Augustin, 1993. Dissertation, Technische Universität München W. Ertel. Grundkurs Künstliche Intelligenz. Vieweg-Verlag, 2007 W. Ertel. Artiﬁcial Intelligence. http://www.hs-weingarten.de/ertel/aibook, 2011. Homepage to this book with materials, demo programs, links, literature, errata, etc W. Ertel. Advanced mathematics for engineers. Lecture notes, Hochschule Ravensburg-Weingarten: http://www.hs-weingarten.de/ertel/vorlesungen/mae/ matheng-skript-1516-v2.pdf, 2015 W. Ertel and M. Schramm. Combining Data and Knowledge by MaxEnt- Optimization of Probability Distributions. In PKDD\\'99 (3rd European Conference on Principles and Practice of Knowledge Discovery in Databases), volume 1704 of LNCS, pages 323–328, Prague, 1999. Springer Verlag\\n\\n[ESCT09] W. Ertel, M. Schneider, R. Cubek, and M. Tokic. The teaching-box: A universal robot learning framework. In Proceedings of the 14th International Conference on Advanced Robotics (ICAR 2009), 2009. http://www.servicerobotik.hs-weingarten. de/teachingbox W. Ertel, J. Schumann, and Ch. Suttner. Learning Heuristics for a Theorem Prover using Back Propagation. In J. Retti and K. Leidlmair, editors, 5. Österreichische Artiﬁcial-Intelligence-Tagung, 208, Springer-Verlag, Berlin, Heidelberg, 1989 C. Felber. Die Gemeinwohl-Ökonomie. Deuticke Verlag, 2014 M. Fitting. First-order logic and automated theorem proving. Springer, 1996 Peter Flach. Machine Learning: The Art and Science of Algorithms that Make Sense of Data. Cambridge University Press, 2012 D. Ferrucci, E. Nyberg, J. Allan, K. Barker, E. Brown, J. Chu-Carroll, A. Ciccolo, P. Duboue, J. Fan, D. Gondek et al. Towards the open advancement of question answer systems. IBM Technical Report RC24789, Yorktown Heights, NY, 2009. http://www.research.ibm.com/deepqa/question_answering.shtml D. Freedman, R. Pisani, and R. Purves. Statistics. Norton, 4th edition, 2007 C. Frayn. Computer chess programming theory. http://www.frayn.net/beowulf/ theory.html, 2005 E. Freuder, In pursuit of the holy grail. Constraints 2(1), 57–61 (1997) V.G. Fischer and M. Schramm. Efﬁcient Compilation of Probabilistic Expressions for Use in MaxEnt Optimization Problems. Technical Report TUM-I9636, Institut für Informatik, Technische Universität München, 1996 B. Fischer and J. Schumann. Setheo goes software engineering: Application of atp to software reuse. In Conference on Automated Deduction (CADE-14), volume 1249 of LNCS, pages 65–68. Springer, 1997. http://ase.arc.nasa.gov/people/ schumann/publications/papers/cade97-reuse.html\\n\\n[ESS89]\\n\\n87–95.\\n\\npages\\n\\nInformatik-Fachberichte\\n\\n[Fel14] [Fit96] [Fla12]\\n\\n[FNA+09]\\n\\n[FPP07] [Fra05]\\n\\n[Fre97] [FS96]\\n\\n[FS97]\\n\\n[GAKW91] M. Greiner, Kölbl A, C. Kredler, and S. Wagenpfeil. Numerical Comparison of Standard SQP-Software with some Second Order Nonlinear Optimization Methods. Report 348, DFG-Schwerpunkt: Anwendungsbezogene Optimierung und Steuer- ung, 1991 I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. Buch in Vorbereitung für MIT Press, http://www.deeplearningbook.org, 2016 L. Gatys, A. Ecker, and M. Bethge. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015. http://www.boredpanda.com/computer-deep- learning-algorithm-painting-masters\\n\\n[GBC16]\\n\\n[GEB15]\\n\\n341\\n\\n342\\n\\n[GHZ14]\\n\\n[GK95]\\n\\n[GK96]\\n\\n[GK09]\\n\\n[Gol94]\\n\\n[Gol97]\\n\\n[GP58] [GR06]\\n\\n[Gra13]\\n\\n[GRS03]\\n\\n[GS15]\\n\\n[GSSD08]\\n\\n[GT96]\\n\\n[Gue02]\\n\\n[GW08]\\n\\n[Göd31a]\\n\\n[Göd31b]\\n\\n[HKP91]\\n\\n[HL04]\\n\\n[HMS+72]\\n\\n[Hon94]\\n\\nReferences\\n\\nP. Gao, R. Hensley, and A. Zielke. A road map to the future for the auto industry. McKinsey Quarterly, Oct, 2014 C. Goller Task-Dependent Distributed Structure-Representations by Backpropagation Through Structure. AR-Report AR-95-02, Institut für Informatik, Technische Universität München, 1995. (a short- ened version will appear in the Proc. of the ICNN-96) C. Goller and A. Küchler. Learning Task-Dependent Distributed Representations by Backpropagation Through Structure. In Proc. of the ICNN-96, volume 1, pages 347–352. IEEE, 1996 J. Grahl and R. Kümmel. Das Loch im Fass – Energiesklaven, Arbeitsplätze und die Interdiziplinär, Milderung des Wachstumszwangs. Wissenschaft und Umwelt 13:195–212, 2009. http://www.fwu.at/assets/userFiles/Wissenschaft_Umwelt/13_ 2009/2009_13_wachstum_5.pdf C. Goller. A Connectionist Control Component for the Theorem Prover SETHEO. In Proc. of the ECAI\\'94 Workshop W14: Combining Symbolic and Connectionist Processing, pages 99–93. ECAI in cooperation with AAAI and IJCAI, 1994 C. Goller. A Connectionist Approach for Learning Search-Control Heuristics for Automated Deduction Systems. PhD thesis, Fakultät für Informatik, Technische Universität München, 1997. (In preparation) Silvio Gesell and Philip Pye. The natural economic order. Owen, 1958 T. Gabel and M. Riedmiller. Learning a partial behavior for a competitive robotic soccer agent. Künstliche Intelligenz, 20(2), 2006. BöttcherIT Verlag Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint http://arxiv.org/abs/1308.0850, 2013. G. Görz, C.-R. Rollinger, and J. Schneeberger, editors. Handbuch der Künstlichen Intelligenz. Oldenbourg Verlag, 2003 J.B. Greenblatt, S. Saxena, Autonomous taxis could greatly reduce greenhouse-gas emissions of us light-duty vehicles. Nature Clim. Change 5(9), 860–863 (2015) R. Geisberger, P. Sanders, D. Schultes, and D. Delling. Contraction hierarchies: In Experimental Faster and simpler hierarchical Algorithms, pages 319–333. Springer, 2008 M. Greiner, G. Tinhofer, Stochastik für Studienanfänger der Informatik (Carl Hanser Verlag, München, Wien, 1996) G. Guerrerio, Spektrum der wissenschaft, spezial 1/2002: Kurt gödel (Spektrum Verlag, Heidelberg, 2002) R.C. González and R.E. Woods. Digital Image Processing. Pearson/Prentice Hall, 2008 K. Gödel, Diskussion zur Grundlegung der Mathematik, Erkenntnis 2. Monatsheft für Mathematik und Physik 32(1), 147–148 (1931) K. Gödel, Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I. Monatsheft für Mathematik und Physik 38(1), 173–198 (1931) J. Hertz, A. Krogh, and R. Palmer. Introduction to the theory of neural computation. Addison Wesley, 1991 P. Hammerer and M. Lein. Stellenwert der PSA-Bestimmung zur Früherkennung des Prostatakarzinoms. Deutsches Ärzteblatt, 101(26):A–1892/B–1581/C–1517, 2004. http://www.aerzteblatt.de/archiv/42497/Stellenwert-der-PSA-Bestimmung- zur-Frueherkennung-des-Prostatakarzinoms J.C. Horrocks, A.P. McCann, J.R. Staniland, D.J. Leaper, F.T. de Dombal, Computer-aided diagnosis: Description of an adaptable system, and operational experience with 2.034 cases. British Medical Journal 2, 5–9 (1972) B. Hontschik. Theorie und Praxis der Appendektomie. Mabuse Verlag, 1994\\n\\nand A. Küchler.\\n\\nLearning\\n\\nrouting in road networks.\\n\\nReferences\\n\\n[Hop82]\\n\\n[HOT06]\\n\\n[HT85]\\n\\n[HTF09]\\n\\n[Hub14]\\n\\n[HW95]\\n\\n[Hüb03] [Jay57] [Jay78]\\n\\n[Jay82a]\\n\\n[Jay82b]\\n\\n[Jay89]\\n\\n[Jay03]\\n\\n[Jen01] [Jor99]\\n\\n[Kal01]\\n\\n[Kan89]\\n\\n[Kan93]\\n\\n[Kar15]\\n\\n[Ken06]\\n\\n[KK92]\\n\\n[KLM96]\\n\\n[KMK97]\\n\\n[Koh72]\\n\\nJ.J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci. USA, 79:2554–2558, April 1982. Wiederabdruck in [AR88] S. 460–464 G. Hinton, S. Osindero, Y. Teh, A fast learning algorithm for deep belief nets. Neural computation 18(7), 1527–1554 (2006) J.J. Hopﬁeld and D.W. Tank. “Neural” computation of decisions in optimization problems. Biological Cybernetics, 52(3):141–152, 1985. Springer T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, Berlin, 3rd. edition, 2009. Online version: http://www-stat.stanford.edu/tibs/ElemStatLearn/ J. Huber. Monetäre Modernisierung, Zur Zukunft der Geldordnung: Vollgeld und Monetative. Metropolis Verlag, 2014. http://www.monetative.de Daniel Heckerman, Michael P. Wellman, Bayesian Networks. Communications of the ACM 38(3), 27–30 (1995) G. Hübner. Stochastik. Vieweg Verlag, 2003 E. T. Jaynes. Information Theory and Statistical Mechanics. Physical Review, 1957 E.T. Jaynes. Where do we stand on Maximum Entropy? In R.D. Rosenkrantz, editor, Papers on Probability, Statistics and Statistical Physics, pages 210–314. Kluwer Academic Publishers, 1978 E.T. Jaynes. Concentration of distributions at entropy maxima. In Rosenkrantz, editor, Papers on Probability, Statistics and statistical Physics. D. Reidel Publishing Company, 1982 E.T. Jaynes, On the Rationale of Maximum Entropy Methods. Proc. of the IEEE 70 (9), 939–952 (1982) E.T. Jaynes. The Well-Posed Problem. In R.D. Rosenkrantz, editor, E.T. Jaynes: Papers on Probability, Statistics and Statistical Physics, pages 133–148. Kluwer Academic Publishers, 1989 E.T. Jaynes. Probability Theory: The Logic of Science. Cambridge University Press, 2003 F.V. Jensen. Bayesian networks and decision graphs. Springer-Verlag, 2001 Michael I. Jordan (ed.), Learning in graphical models (MIT Press, Cambridge, MA, USA, 1999) J.A. Kalman. Automated Reasoning with OTTER. Rinton Press, 2001. http://www- unix.mcs.anl.gov/AR/otter/index.html Th. Kane. Maximum entropy in nilsson’s probabilistic logic. In Proc. of the 11th Intl. Joint Conf. on Artiﬁcial Intelligence (IJCAI-89), 1989 L.N. Kanal, On Pattern, Categories and Alternate Realities. Pattern Recognition Letters 14, 241–255 (1993) Andrej Karpathy. The unreasonable effectiveness of recurrent neural networks, Mai 2015. http://karpathy.github.io/2015/05/21/rnn-effectiveness/ M. Kennedy. Geld ohne Zinsen und Inﬂation. Ein Tauschmittel, das jedem dient. Goldmann Verlag, München, 2006 J.N. Kapur and H.K. Kesavan. Entropy Optimization Principles with Applications. Academic Press, 1992 L.P. Kaelbling, M.L. Littman, and A.P. Moore. Reinforcement Learning: A Survey. Journal of Artiﬁcial Intelligence Research, 4:237–285, 1996. http://www2.cs.cmu. edu/afs/cs/project/jair/pub/volume4/kaelbling96a.pdf H. Kimura, K. Miyazaki, and S. Kobayashi. Reinforcement Learning in POMDPs with Function Approximation. In 14th International Conference on Machine Learning, pages 152–160. Morgan Kaufmann Publishers, 1997. http://sysplan.nams. kyushu-u.ac.jp/gen/papers/JavaDemoML97/robodemo.html T. Kohonen. Correlation matrix memories. IEEE Transactions on Computers, C-21 (4):353–359, 1972. Reprint in [AR88] pp. 171–174\\n\\n343\\n\\n344\\n\\nReferences\\n\\nR. Kowalski and A. Robert. Algorithmic = Logic + Control. Communications of the ACM, 22(7):424–436, Juli 1979 Ch. Kreitz, Formale methoden der künstlichen intelligenz. Künstliche Intelligenz 4, 22–28 (2006) L. Kocsis and C. Szepesvári. Bandit based monte-carlo planning. In European Conference on Machine Learning(ECML) 2006, pages 282–293. Springer, 2006 R. Kümmel. The second law of economics: Energy, entropy, and the origins of wealth. Springer Science & Business Media, 2011 F.D. Laramée. Chess programming, part 1–6. http://www.gamedev.net/reference/ programming/features/chess1, 2000 S.L. Lauritzen. Graphical Models. Oxford Science Publications, 1996 [Lau96] [LBBH98] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to the IEEE, 86(11):2278–2324, 1998.\\n\\n[KR79]\\n\\n[Kre06]\\n\\n[KS06]\\n\\n[Küm11]\\n\\n[Lar00]\\n\\n[LBH15] [Le999]\\n\\n[Let03]\\n\\n[Lif89]\\n\\n[LM88]\\n\\n[Lov78]\\n\\n[LR02]\\n\\n[LSBB92]\\n\\n[MA94]\\n\\n[McC]\\n\\n[McD82]\\n\\ndocument MNIST-Daten: http://yann.lecun.com/exdb/mnist Y. LeCun, Y. Bengio, G. Hinton, Deep learning. Nature 521(7553), 436–444 (2015) Lexmed – a learning expert system for medical diagnosis. http://www.lexmed.de, 1999 R. Letz. Praktikum beweiser. http://www4.in.tum.de/letz/PRAKTIKUM/al-ss05.pdf, 2003 In V. Lifschitz. Benchmark problems for Reinfrank et al, editor, Non-Monotonic Reasoning: 2nd International Workshop, volume 346 of LNAI, pages 202–219. Springer, 1989 Kai-Fu Lee, Sanjoy Mohajan, A Pattern Classiﬁcation Approach to Evaluation Func tion Learning. Artiﬁcial Intelligence 36, 1–25 (1988) D.W. Loveland. Automated Theorem Proving: a Logical Basis. North-Holland, 1978 M. Lauer and M. Riedmiller. Generalisation in Reinforcement Learning and the Use of Obse rvation-Based Learning. In Gabriella Kokai and Jens Zeidler, editors, the FGML Workshop 2002, pages 100–107, 2002. http://amy. Proceedings of informatik.uos.de/riedmiller/publications/lauer.riedml.fgml02.ps.gz R. Letz, J. Schumann, S. Bayerl, and W. Bibel. SETHEO: A High-Performance Theorem Prover. Journal of Automated Reasoning, 8(2):183–212, 1992. http:// www4.informatik.tu-muenchen.de/letz/setheo P.M. Murphy, D.W. Aha, UCI Repository of Machine Learning Databases (University of California at Irvine, Department of Information and Computer Science, 1994) W. McCune. Automated deduction systems and groups. www-unix.mcs.anl. gov/AR/others.html. also http://www-formal.stanford.edu/clt/ARS/systems. html J. McDermott, R1: A rule-based conﬁgurer of computer systems. Artiﬁcial Intelligence 19, 39–88 (1982) D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through reversible learning. arXiv preprint arXiv:1502.03492, 2015\\n\\nrecognition. Proceedings of\\n\\nformal non-monotonic reasoning.\\n\\nsee\\n\\n[MDA15]\\n\\n[MDBM00] G. Melancon, I. Dutour, and G. Bousque-Melou. Random generation of dags for graph drawing. Technical Report INS-R0005, Dutch Research Center for Mathe- matical and Computer Science (CWI), 2000. http://ftp.cwi.nl/CWIreports/INS/INS- R0005.pdf T. Mitchell. Machine Learning. McGraw Hill, 1997. http://www-2.cs.cmu.edu/tom/ mlbook.html\\n\\n[Mit97]\\n\\n[MMZM72] D.L. Meadows, D.H. Meadows, E. Zahn, and P. Milling. Die Grenzen des Wachstums. Bericht des Club of Rome zur Lage der Menschheit. Dt. Verl. Deutsche Verlagsanstalt, Stuttgart, 1972 M. Minsky, S. Papert, Perceptrons (MIT Press, Cambridge, MA, 1969)\\n\\n[MP69]\\n\\nReferences\\n\\nR.E. Neapolitan. Probabilistic Reasoning in Expert Systems. Wiley-Interscience. John Wiley & Sons, Inc., 1990 M. Newborn. Automated Theorem Proving: Theory and Practice. Springer Verlag, 2000 N.J. Nilsson, Probabilistic Logic. Artiﬁcial Intelligence 28(1), 71–87 (1986) N. Nilsson. Artiﬁcial Intelligence – A New Synthesis. Morgan Kaufmann, 1998 T. Nipkow, L.C. Paulson, and M. Wenzel. Isabelle/HOL — A Proof Assistant for Higher-Order Logic, volume 2283 of LNCS. Springer, 2002. http://www.cl.cam.ac. uk/Research/HVG/Isabelle A. Newell, H.A. Simon, Gps, a program that simulates human thought, in Lernende Automaten, ed. by H. Billing (Oldenbourg, München, 1961), pp. 109–124 A. Newell, J. C. Shaw, and H. A. Simon. Empirical explorations with the logic theory machine: A case study in heuristics. In J. Siekmann and G. Wrightson, editors, Automation of Reasoning 1: Classical Papers on Computational Logic 1957-1966, pages 49–73. Springer, Berlin, Heidelberg, 1983. Erstpublikation: 1957 [NWC+11] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 4, 2011. SVHN-Daten:http://uﬂdl.stanford.edu/housenumbers C. Ohmann, C. Franke, Q. Yang, M. Margulies, M. Chan, van P.J. Elk, F.T. de Dombal, and H.D. Röher. Diagnosescore für akute Appendizitis. Der Chirurg, 66:135–141, 1995\\n\\n[Nea90]\\n\\n[New00]\\n\\n[Nil86] [Nil98] [NPW02]\\n\\n[NS61]\\n\\n[NSS83]\\n\\n[OFY+95]\\n\\n[OMYL96] C. Ohmann, V. Moustakis, Q. Yang, K. Lang, Evaluation of automatic knowledge acquisition techniques in the diagnosis of acute abdominal pain. Art. Intelligence in Medicine 8, 23–36 (1996) C. Ohmann, C. Platen, G. Belenky, Computerunterstütze Diagnose bei akuten Bauchschmerzen. Chirurg 63, 113–123 (1994) Oliver Obst and Markus Rollmann. SPARK – A Generic Simulator for Physical Multiagent Simulations. In Gabriela Lindemann, Jörg Denzinger, Ingo J. Timm, and Rainer Unland, editors, Multiagent System Technologies – Proceedings of the MATES 2004, volume 3187, pages 243–257. Springer, September 2004 C. Ohmann, Q. Yang, C. Franke, Diagnostic scores for Acute Appendicitis. Eur. J. Surg. 161, 273–281 (1995) N. Paech, Befreiung vom Überﬂuss – Grundlagen einer Wirtschaft ohne Wachstum, Fromm Forum, volume 20 (Erich Fromm Gesellschaft, Tübingen, 2016), pp. 70–76 G. Palm, On associative memory. Biological Cybernetics 36, 19–31 (1980) G. Palm. Memory capacities of local rules for synaptic modiﬁcation. Concepts in Neuroscience, 2(1):97–128, 1991. MPI Tübingen J. Pearl, Heuristics (Addison-Wesley Publishing Company, Intelligent Search Strategies for Computer Problem Solving, 1984) J. Pearl, Probabilistic Reasoning in Intelligent Systems (Morgan Kaufmann, Networks of Plausible Inference, 1988) T. Piketty, Das Kapital im 21 (CH Beck Verlag, Jahrhundert, 2014) L. Panait, S. Luke, Cooperative multi-agent Autonomous Agents and Multi-Agent Systems 11(3), 387–434 (2005) J. Peters, S. Schaal, Reinforcement learning of motor skills with policy gradients. Neural Networks 21(4), 682–697 (2008) George Pólya and S. Sloan. How to Solve It: A New Aspect of Mathematical Method. Ishi Press, 2009 J.B. Paris, A. Vencovska, A Note on the Inevitability of Maximum Entropy. International Journal of Approximate Reasoning 3, 183–223 (1990)\\n\\n[OPB94]\\n\\n[OR04]\\n\\n[OYF95]\\n\\n[Pae16]\\n\\n[Pal80] [Pal91]\\n\\n[Pea84]\\n\\n[Pea88]\\n\\n[Pik14] [PL05]\\n\\nlearning: The state of\\n\\n[PS08]\\n\\n[PS09]\\n\\n[PV90]\\n\\n345\\n\\nthe art.\\n\\n346\\n\\nReferences\\n\\n[PVS03]\\n\\n[Qui] [Qui93]\\n\\n[Ran12]\\n\\n[Rau96] [RB93]\\n\\n[RDS+15]\\n\\nJ. Peters, S. Vijayakumar, and S. Schaal. Reinforcement learning for humanoid In Humanoids2003, Third IEEE-RAS International Conference on robotics. Humanoid Robots, Karlsruhe, 2003 J.R. Quinlan. C5.0. http://www.rulequest.com J. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993. C4.5 Download: http://www.rulequest.com/Personal, C5.0 Bestellung: http://www.rulequest.com J. Randers. 2052: A Global Forecast for the Next Forty Years. Chelsea Green Publishing, 2012 W. Rautenberg. Einführung in die Mathematische Logik. Vieweg Verlag, 1996 M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning: The rprop algorithm. In Proceedings of the IEEE International Conference on Neural Networks, pages 586–591, 1993 O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., Imagenet large scale visual recognition challenge. International Journal of Computer Vision 115(3), 211–252 (2015)\\n\\n[RGH+06] M. Riedmiller, T. Gabel, R. Hafner, S. Lange, M. Lauer, Die Brainstormers: Entwurfsprinzipien lernfähiger autonomer Roboter. Informatik-Spektrum 29(3), 175–190 (2006) D.E. Rumelhart, G.E. Hinton, and Williams R.J. Learning Internal Representations by Error Propagation. in [RM86], 1986 E. Rich. Artiﬁcial Intelligence. McGraw-Hill, 1983 M. Richter. Fallbasiertes schließen. In Görz et al. [GRS03], chapter 11, pages 407–430 D. Rumelhart and J. McClelland. Parallel Distributed Processing, volume 1. MIT Press, 1986 W. Rödder and C.-H. Meyer. Coherent Knowledge Processing at Maximum Entropy by SPIRIT. In KI-96 (German national conference on AI), Dresden, 1996 M. Riedmiller, M. Montemerlo, and H. Dahlkamp. Learning to drive a real car in 20 minutes. In FBIT \\'07: Proceedings of the 2007 Frontiers in the Convergence of Bioscience and Information Technologies, pages 645–650, Washington, DC, USA, 2007. IEEE Computer Society H. Ritter, T. Martinez, and K. Schulten. Neural computation and self-organizing maps. Addison Wesley, 1992 S. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach. Prentice Hall, 3rd edition, 2010. 1st edition 1995, http://aima.cs.berkeley.edu Robocup ofﬁcial site. http://www.robocup.org The robocup soccer simulator. http://sserver.sourceforge.net J.A. Robinson, A machine-oriented logic based on the resolution principle. Journal of the ACM 12(1), 23–41 (1965) R. W. Robinson. Counting labeled acyclic digraphs. In F. Harary, editor, New Directions in the Theory of Graphs, pages 28–43. Academic Press, 1977 R. Rojas. Neural Networks: a Systematic Introduction. Springer, 1996 F. Rosenblatt. The perceptron : a probabilistic model for information storage and organization in the brain. Psychological Reviews, 65:386–408, 1958. Wiederab- druck in [AR88], S. 92–114 S.M. Ross. Introduction to probability and statistics for engineers and scientists. Academic Press, 2009 P.J. Rousseeuw, Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Computational and Applied Mathematics 20, 53–65 (1987) C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. Mit Press, 2006. Online version: http://www.gaussianprocess.org/gpml/chapters/\\n\\n[RHR86]\\n\\n[Ric83] [Ric03]\\n\\n[RM86]\\n\\n[RM96]\\n\\n[RMD07]\\n\\n[RMS92]\\n\\n[RN10]\\n\\n[Roba] [Robb] [Rob65]\\n\\n[Rob77]\\n\\n[Roj96] [Ros58]\\n\\n[Ros09]\\n\\n[Rou87]\\n\\n[RW06]\\n\\nReferences\\n\\n347\\n\\n[SA94]\\n\\n[Sam59]\\n\\n[Sam67]\\n\\n[SB98]\\n\\n[SB04]\\n\\n[Sch96]\\n\\n[Sch01]\\n\\n[Sch02]\\n\\n[Sch04]\\n\\n[SE90]\\n\\n[SE00]\\n\\n[SE10]\\n\\n[SEP16]\\n\\n[SET09]\\n\\nS. Schaal, C.G. Atkeson, Robot learning. IEEE Control Systems Magazine 14(1), 57–71 (1994) A.L. Samuel, Some Studies in Machine Learning Using the Game of Checkers. IBM Journal 1(3), 210–229 (1959) A.L. Samuel, Some Studies in Machine Learning Using the Game of Checkers. II. IBM Journal 11(6), 601–617 (1967) R. Sutton and A. Barto. Reinforcement Learning. MIT Press, 1998. http://www.cs. ualberta.ca/sutton/book/the-book.html J. Siekmann and Ch. Benzmüller. Omega: Computer supported mathematics. In KI 2004: Advances in Artiﬁcial Intelligence, LNAI 3238, pages 3–28. Springer Verlag, 2004. http://www.ags.uni-sb.de/omega Indifferenz, Unabhängigkeit und maximale Entropie: Eine M. Schramm. wahrscheinlichkeitstheoretische Semantik für Nicht-Monotones Schließen. Number 4 in Dissertationen zur Informatik. CS-Press, München, 1996 J. Schumann. Automated Theorem Proving in Software Engineering. Springer Verlag, 2001 S. Schulz. E – A Brainiac Theorem Prover. Journal of AI Communications, 15 (2/3):111–126, http://www4.informatik.tu-muenchen.de/schulz/WORK/ eprover.html A. Schwartz. SpamAssassin. O\\'Reilly, 2004. Spamassassin-Homepage: http:// spamassassin.apache.org Ch. Suttner and W. Ertel. Automatic Acquisition of Search Guiding Heuristics. In 10th Int. Conf. on Automated Deduction, pages 470–484. Springer-Verlag, LNAI 449, 1990 M. Schramm and W. Ertel. Reasoning with Probabilities and Maximum Entropy: The System PIT and its Application in LEXMED. In K. Inderfurth et al, editor, Operations Research Proceeedings (SOR\\'99), pages 274–280. Springer Verlag, 2000 M. Schneider and W. Ertel. Robot Learning by Demonstration with Local Gaussian Process Regression. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\\'10), 2010 M. Schneider, W. Ertel, and G. Palm. Expected similarity estimation for large-scale batch and streaming anomaly detection. Machine Learning, 2016. accepted T. Segaran, C. Evans, and J. Taylor. Programming the Semantic Web. O\\'Reilly, 2009 M. Schramm and M. Greiner. Foundations: Indifference, Independence & Maxent. In J. Skilling, editor, Maximum Entropy and Bayesian Methods in Science and Engeneering (Proc. of the MaxEnt\\'94). Kluwer Academic Publishers, 1995\\n\\njuggling:\\n\\nimplementation of memory-based\\n\\n2002.\\n\\n[SG95]\\n\\n[SHM+16] D. Silver, A. Huang, C.J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., Mastering the game of go with deep neural networks and tree search. Nature 529(7587), 484–489 (2016) E.H. Shortliffe, Computer-based medical consultations (MYCIN. North-Holland, New York, 1976) Mysteries of the Mind. Speciel Issue. Scientiﬁc American Inc., 1997 Exploring Intelligence, volume 9 of Scientiﬁc American presents. Scientiﬁc American Inc., 1998 B. Staehle, S. Pﬁffner, B. Reiner, W. Ertel, B. Weber-Fiori, and M. Winter. Marvin, ein Assistenzroboter für Menschen mit körperlicher Behinderung im praktischen Einsatz. In M.A. Pfannstiel, S. Krammer, and W. Swoboda, editors, Digitalisierung von Dienstleistungen im Gesundheitswesen. Springer Verlag, 2016. http://asrobe.hs- weingarten.de\\n\\n[Sho76]\\n\\n[Spe97] [Spe98]\\n\\n[SPR+16]\\n\\n348\\n\\n[SR86]\\n\\n[SS02]\\n\\n[SS06]\\n\\n[SS16]\\n\\n[SSK05]\\n\\n[Ste07] [SW76]\\n\\n[SZ15]\\n\\n[Sze10]\\n\\n[Tax01]\\n\\n[Ted08]\\n\\n[TEF09]\\n\\n[Tes95]\\n\\n[Tok06]\\n\\n[Tur37]\\n\\n[Tur50] [TZ16]\\n\\n[vA06]\\n\\n[VLL+10]\\n\\n[VTBE15]\\n\\n[Wei66]\\n\\nReferences\\n\\nT.J. Sejnowski and C.R. Rosenberg. NETtalk: a parallel network that learns to read aloud. Technical Report JHU/EECS-86/01, The John Hopkins University Electrical Engineering and Computer Science Technical Report, 1986. Wiederabdruck in [AR88] S. 661–672 S. Schölkopf, A. Smola, Learning with Kernels: Support Vector Machines (Optimization, and Beyond. MIT Press, Regularization, 2002) G. Sutcliffe and C. Suttner. The State of CASC. AI Communications, 19(1):35–48, 2006. CASC-Homepage: http://www.cs.miami.edu/tptp/CASC K. Schwab and R. Samans. The future of jobs – employment, skills and workforce strategy for the fourth industrial revolution. World Economic Forum, http://reports. weforum.org/future-of-jobs-2016, January 2016 P. Stone, R.S. Sutton, and G. Kuhlmann. Reinforcement Learning for RoboCup-Soccer Keepaway. Adaptive Behavior, 2005. http://www.cs.utexas.edu/ pstone/Papers/bib2html-links/AB05.pdf J. Stewart. Multivariable Calculus. Brooks Cole, 2007 C.E. Shannon and W. Weaver. Mathematische Grundlagen der Informationsthe- orie. Oldenbourg Verlag, 1976 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2015 C. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool Publishers, 2010. draft available online: http://www.ualberta.ca/szepesva/RLBook. html D.M.J. Tax. One-class classiﬁcation. PhD thesis, Delft University of Technology, 2001 R. Tedrake. Learning control at intermediate reynolds numbers. In Workshop on: Robotics Challenges for Machine Learning II, International Conference on Intelligent Robots and Systems (IROS 2008), Nice, France, 2008 M. Tokic, W. Ertel, and J. Fessler. The crawler, a class room demonstrator for the 22nd International Florida reinforcement Artiﬁcial Intelligence Research Society Conference (FLAIRS 09), Menlo Park, California, 2009. AAAI Press G. Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3), 1995. http://www.research.ibm.com/massive/tdl.html M. Tokic. Entwicklung eines Lernfähigen Laufroboters. Diplomarbeit Hochschule Ravensburg-Weingarten, 2006. Inklusive Simulationssoftware verfügbar auf http:// www.hs-weingarten.de/ertel/kibuch A.M. Turing. On computable numbers, with an application to the Entschei- dungsproblem. Proceedings of the London Mathemat. Society, 42(2), 1937 A.M. Turing, Computing Machinery and Intelligence. Mind 59, 433–460 (1950) Y. Tian and Y. Zhu. Better computer go player with neural network and long-term prediction. arXiv preprint arXiv:1511.06410, 2016 L. v. Ahn. Games with a purpose. IEEE Computer Magazine, pages 96–98, Juni 2006. http://www.cs.cmu.edu/biglou/ieee-gwap.pdf P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, P. Manzagol, Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res. 11, 3371–3408 (2010) O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156–3164, 2015 J. Weizenbaum, ELIZA–A Computer Program For the Study of Natural Language Communication Between Man and Machine. Communications of the ACM 9(1), 36–45 (1966)\\n\\nlearning. In In Proceedings of\\n\\nReferences\\n\\n[WF01] I. Witten and E. Frank. Data Mining. Hanser Verlag München, 2001. (DataMining\\n\\n[Whi96] [Wie]\\n\\n[Wie04]\\n\\n[Wik13] [Win]\\n\\n[Zdz05] [Zel94]\\n\\n[ZSR+99]\\n\\n[ZW94]\\n\\nJava Library WEKA: http://www.cs.waikato.ac.nz/ml/weka) J. Whittaker. Graphical models in applied multivariate statistics. Wiley, 1996 U. Wiedemann. PhilLex, Lexikon der Philosophie. http://www.phillex.de/paradoxa. htm J. Wielemaker. SWI-Prolog 5.4. Universität Amsterdam, 2004. http://www.swi- prolog.org Wikipedia, the free enzyclopedia. http://en.wikipedia.org, 2013 P. Winston. Game demonstration. http://www.ai.mit.edu/courses/6.034f/gamepair. html. Java Applet for Minimax- and Alpha-Beta-Search J. Zdziarski. Ending Spam. No Starch Press, 2005 A. Zell. Simulation Neuronaler Netze. Addison Wesley, 1994. Description of SNNS and JNNS: http://www-ra.informatik.uni-tuebingen.de/SNNS A. Zielke, H. Sitter, T.A. Rampp, E. Sch\\'\\'afer, C. Hasse, W. Lorenz, and M. Rothmund. Überprüfung eines diagnostischen Scoresystems (Ohmann-Score) für die akute Appendizitis. Chirurg 70, 777–783 (1999) W.C. Zimmerli, S. Wolf (eds.), Künstliche Intelligenz – Philosophische Probleme (Philipp Reclam, Stuttgart, 1994)\\n\\n349\\n\\nIndex\\n\\nA A⋆-algorithm, 107, 273 Action, 97, 98 Activation function, 248, 258 Actuators, 17 Adaptive Resonance Theory (ART), 286 Admissible, 107, 109 Agent, 3, 17, 289, 290, 293, 296, 297, 300,\\n\\n301, 303, 305\\n\\nB Backgammon, 306 Backpropagation, 279, 298, 322\\n\\nlearning rule, 270 Backtracking, 77, 99 Backward chaining, 35 Batch learning, 217, 266 Bayes formula. See Bayes theorem Bayesian network, 6, 10, 72, 77, 127, 158, 160,\\n\\nautonomous, 10 cost-based, 18 distributed, 10 goal-based, 17 hardware, 17 intelligent, 17 learning, 11, 18, 178 reﬂex, 17 software, 17 utility-based, 18 with memory, 17 Agents, distributed, 18 Alarm-example, 159 Alpha-beta pruning, 115 AlphaGo, 7, 119, 121, 306, 308 And branches, 79 And-or tree, 79 Appendicitis, 133, 145 Approximation, 178, 193 A priori probability, 131, 135 Artiﬁcial Intelligence (AI), 1 Associative memory, 256 Attribute, 118, 199 Auto-associative memory, 250, 257, 261 Autoencoder, 279 Automation, 12 Automotive industry, 14 Autonomous robots, 11\\n\\n198\\n\\nlearning, 241\\n\\nBayes theorem, 134, 163 BDD. See Binary decision diagram Bellman\\n\\nequation, 295 principle, 295\\n\\nBias unit, 187 Bias variance tradeoff, 214 Binary decision diagram, 37 Boltzmann machine, 256 Brain science, 3 Braitenberg vehicle, 2, 10, 193 Branching factor, 91, 95\\n\\naverage, 92 effective, 95 Built-in predicate, 84\\n\\nC C4.5, 198, 216 Calculus, 28\\n\\nGentzen, 49 natural reasoning, 49 sequent, 49\\n\\nCancer diagnosis, 134 Car, 14 CART, 198, 212 CASC, 57\\n\\n© Springer International Publishing AG 2017 W. Ertel, Introduction to Artificial Intelligence, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-58487-4\\n\\n351\\n\\n352\\n\\nCase base, 197 Case-based reasoning, 197 CBR. See Case-based reasoning Certainty factors, 126 Chain rule for Bayesian networks, 132, 168,\\n\\n169 Chatterbots, 5 Checkers, 114, 120 Chess, 114, 117, 120, 121, 306 Church, Alonso, 7 Classiﬁcation, 178 Classiﬁer, 178, 223, 268 Clause, 29\\n\\ndeﬁnite, 34 -head, 34\\n\\nClosed formula, 41 CLP. See Constraint logic programming Cluster, 225 Clustering, 224, 238 hierarchical, 22 Cognitive science, 3 Complementary, 31 Complete, 28 Computer diagnostic, 158 Conclusion, 34 Conditionally independent, 160, 169 Conditional probability, 137\\n\\ntable. See CPT Conditioning, 163, 169 Confusion matrix, 234 Conjunction, 24, 29 Conjunctive Normal Form (CNF), 29 Connectionism, 9 Consistent, 31 Constant, 40 Constraint logic programming, 86 Constraint Satisfaction Problem (CSP), 86 Contraction hierarchies, 111 Convolutional Neural Network (CNN), 277,\\n\\n280, 282, 307\\n\\nCorrelation, 151\\n\\ncoefﬁcient, 182 matrix, 238\\n\\nCost estimate function, 105 Cost function, 95, 107 Cost matrix, 150, 155 CPT, 160, 170, 216, 218 Creativity, 282, 283 Credit assignment, 119, 292 Cross-validation, 212, 213, 279, 281 Curse of dimensionality, 309\\n\\nCut, 79\\n\\nD DAG, 169, 216 Data mining, 179, 180, 197, 198, 211 Data scientist, 277 Decision, 153 Decision tree, 198\\n\\ninduction, 180, 199 learning, 309\\n\\nDeep belief network, 277, 280 Deep learning, 121, 238, 277, 307, 308 Default logic, 71 Default rule, 71 Delta rule, 266, 268 generalized, 270\\n\\nDemodulation, 56 De Morgan, 45 Dempster–Schäfer theory, 127 Dependency graph, 151 Depth limit, 100 Derivation, 28 Deterministic, 97, 114 Diagnosis system, 146 Disjunction, 24, 29 Distance metric, 225 Distributed Artiﬁcial Intelligence (DAI), 10 Distributed learning, 309 Distribution, 130, 148 D-separation, 170 Dynamic programming, 296\\n\\nE Eager learning, 196, 237 Economic growth, 12 Economy, 12 E-learning, 5 Elementary event, 128 Eliza, 5 EM algorithm, 217, 228, 232 Entropy, 202\\n\\nmaximum, 127, 136 Environment, 12, 17, 18\\n\\ncontinuous, 18 deterministic, 18 discrete, 18 nondeterministic, 18 observable, 18 partially observable, 18\\n\\nEquation, directed, 55 Equivalence, 24\\n\\nIndex\\n\\nIndex\\n\\nEvaluation function, 114 Event, 128 Expert system, 145, 158\\n\\nF Fact, 34 Factorization, 31, 54 False negative, 155 False positive, 155 Farthest neighbor algorithm, 230 Feature, 118, 176, 185, 198, 237, 277 Feature space, 177 Feedforward networks, 285 Finite domain constraint solver, 87 First-order sentence, 41 First we solidify, 40 Forward chaining, 35 Frame problem, 71 Free variables, 41 Function symbol, 40 Fuzzy logic, 10, 72, 127\\n\\nG Gaussian process, 195, 236 Generalization, 178 General Problem Solver (GPS), 6 Genetic programming, 83 Go, 114, 120, 122, 306, 308 Goal, 36\\n\\nstack, 36 state, 94\\n\\nGödel\\n\\nincompleteness theorem, 7 Kurt, 7 ’s completeness theorem, 7 ’s incompleteness theorem, 68\\n\\nGoogle DeepMind, 121, 307 Gradient descent, 267 Greedy search, 106, 107, 217, 232 Ground term, 50\\n\\nH Halting problem, 7 Hebb rule, 249, 258, 270\\n\\nbinary, 259 Heuristic, 103 Heuristic evaluation function, 104, 107 Hierarchical learning, 309 Home automation, 15 Hopﬁeld network, 250, 251, 260 Horn clause, 34, 80\\n\\nHugin, 164\\n\\nI ID3, 198 IDA⋆-algorithm, 113 Immediate reward, 292 Implication, 24 Incremental gradient descent, 268 Incremental learning, 266 Independent, 131\\n\\nconditionally, 160, 169\\n\\nIndifference, 140 Indifferent variables, 146 Industry 4.0, 11 Inference machine, 50 Inference mechanism, 19 Information content, 203 Information gain, 200, 203, 237 Input resolution, 55 Internet of Things, 11, 15 Interpretation, 24, 41 Iterative deepening, 100, 102 IT security, 15\\n\\nJ JavaBayes, 164 Jobs, 11\\n\\nK Kernel, 195, 277 Kernel methods, 277 Kernel PCA, 280 K-means, 226 k-nearest neighbor method, 192, 194, 213 KNIME, 199, 233 Knowledge, 19 base, 160\\n\\nconsistent, 31 engineer, 11, 19 sources, 19\\n\\nL Landmark, 109\\n\\nheuristic, 110\\n\\nLaplace assumption, 129 Laplace probabilities, 129 Law of economy, 211 Lazy learning, 196 Learning, 171, 176, 198\\n\\nbatch, 266 by demonstration, 309\\n\\n353\\n\\n354\\n\\nLearning (cont.)\\n\\ndistributed, 309 hierarchical, 309 incremental, 218, 266 machine, 151 multi-agent, 309 one-class, 222 reinforcement, 97, 175, 307 semi-supervised, 236 supervised, 176, 225, 261 unsupervised, 278\\n\\nLearning agent, 178 Learning phase, 178 Learning rate, 249, 267 Least squares, 157, 264, 265, 269 Leave-one-out cross-validation, 214 LEXMED, 127, 136, 145, 207 Limited resources, 104 Linear approximation, 268 Linearly separable, 183, 184 LIPS, 76 LISP, 6, 8 Literal, 29\\n\\ncomplementary, 31\\n\\nLocally weighted linear regression, 197 Logic\\n\\nfuzzy, 127 higher-order, 6 probabilistic, 19 Logically valid, 25 Logic Theorist, 6, 8\\n\\nM Machine learning, 148, 175 Manhattan distance, 112, 226 Marginal distribution, 133 Marginalization, 133, 137, 169 Markov Decision Process (MDP), 17, 293, 305\\n\\ndeterministic, 301 nondeterministic, 305 partially observable, 293 Material implication, 127, 142 MaxEnt, 127, 140, 143, 145, 150, 164, 170\\n\\ndistribution, 140 Memorization, 176 Memory-based learning, 196, 197 Metaparameter, 281 MGU, 53 Minimum cash reserve ratio, 13 Minimum spanning tree, 229 Mining, 179 Model, 25 Model complexity, 213, 214 Modus ponens, 126, 139\\n\\nIndex\\n\\nMomentum, 275 Monotonic, 69 Monte Carlo Tree Search (MCTS), 7, 119, 122,\\n\\n307 Multi-agent systems, 6 MYCIN, 126, 146\\n\\nN Naive Bayes, 157, 159, 171, 180, 189, 218,\\n\\n220, 242\\n\\nclassiﬁcation, 190 classiﬁer, 218, 220 method, 189 Naive reverse, 82 Navier–Stokes equation, 306 Nearest neighbor\\n\\nclassiﬁcation, 190 method, 189\\n\\nNearest neighbor algorithm, 229 Nearest neighbor method, 223 Nearest neighbor data description, 223 Negation, 24 Negation as failure, 80 Neural network, 6, 8, 194, 195, 238, 245\\n\\nrecurrent, 255, 257, 282\\n\\nNeuroinformatics, 255 Neuroscience, 3 Neurotransmitter, 247 Noise, 191 Non-monotonic logic, 144 Normal equations, 265 Normal form\\n\\nconjunctive, 29 prenex, 46\\n\\nNormalization, 223, 280\\n\\nO Object classiﬁcation, 277, 281 Observable, 97, 114 Occam’s razor, 211 OMRk algorithm, 232 One-class learning, 222, 223 Ontology, 63 Or branches, 79 Orthonormal, 258 Othello, 114 Outlier detection, 223 Overﬁtting, 191, 211, 213–215, 217, 263, 265,\\n\\n276\\n\\nOWL, 63\\n\\nP Paradox, 68 Paramodulation, 56\\n\\nIndex\\n\\nPartially Observable Markov Decision Process\\n\\n(POMDP), 293\\n\\nPenguin problem, 85 Perceptron, 192, 196, 249 Phase transition, 254 PIT, 143, 144, 164, 172 PL1, 19, 40 Planning, 83 Plans, 85 Policy, 292\\n\\ngradient method, 305 policy based on its, 292\\n\\nPostcondition, 62 Precondition, 61 Predicate logic, 7\\n\\nﬁrst-order, 20, 40 Preference learning, 180 Premise, 34 Principal Component Analysis (PCA), 277,\\n\\n280\\n\\nProbabilistic\\n\\nlogic, 20, 71 reasoning, 9 Probability, 126, 128 distribution, 130 rules, 150 Product rule, 132 Program veriﬁcation, 61 PROLOG, 6, 9, 75 Proof system, 26 Propositional\\n\\ncalculus, 20 logic, 23\\n\\nProposition variables, 23 Pruning, 206, 212 Pure literal rule, 55\\n\\nQ Q-learning, 300\\n\\nconvergence, 303\\n\\nQuickprop, 275\\n\\nR Random variables, 128 Rapid prototyping, 87 RDF, 63 Real-time decision, 104 Real-time requirement, 114 Receiver operating characteristic, 156 Reinforcement\\n\\nlearning, 119, 292 negative, 292 positive, 292\\n\\nResolution, 8, 30 calculus, 6, 28 rule, 30\\n\\ngeneral, 30, 52\\n\\nSLD, 35 Resolvent, 30 Reward\\n\\ndiscounted, 292 immediate, 292 Risk management, 155 Road transportation, 14 RoboCup, 7, 306 Robot, 17, 289, 293 car, 14, 15 taxi, 14 walking, 289 ROC curve, 157, 235 RProp, 275, 279\\n\\nS Sample, 199 Satisﬁable, 25 Scatterplot diagram, 177 Science ﬁction, 11 Score, 146, 157, 223, 242, 266 Search\\n\\nalgorithm, 94\\n\\ncomplete, 95 optimal, 96 bidirectional, 110 heuristic, 92 space, 31, 35 tree, 94 uninformed, 92 Self-driving car, 14 Self-organizing maps, 285 Semantics\\n\\ndeclarative (PROLOG), 78 procedural (PROLOG), 78, 82\\n\\nSemantic trees, 36 Semantic web, 62 Semi-decidable, 67 Semi-supervised learning, 236 Sensitivity, 135, 156, 162 Sensor, 17 Service robotics, 15 Set of support strategy, 55 Sigmoid function, 249, 264, 269 Signature, 23 Silhouette width criterion, 231 Similarity, 189 Simulated annealing, 256 Situation calculus, 71\\n\\n355\\n\\n356\\n\\nSkolemization, 48 SLD resolution, 38 Software reuse, 61 Solution, 95 Sound, 28 Space, 49, 95 Spam, 220\\n\\nﬁlter, 220 Sparse coding, 279 Speciﬁcity, 135, 156 Stacked denoising autoencoder, 279, 280\\n\\nStarting state, 94 State, 94, 95 space, 95 transition function, 292\\n\\nStatistical induction, 150 Stochastic gradient descent, 308 Subgoal, 36, 77 Substitution axiom, 45 Subsumption, 55 Support vector data description, 223 Support vector machine, 195, 276,\\n\\n280\\n\\nSustainability, 13 SVM. See Support vector machine Symbol grounding, 85\\n\\nT Target function, 178 Tautology, 26 TD, 304\\n\\nerror, 304 -gammon, 306 -learning, 304 Teaching-Box, 310 Temporal difference error, 304 learning, 301\\n\\nTerm\\n\\nrewriting, 56 Test data, 178, 211\\n\\nText\\n\\nclassiﬁcation, 220 mining, 180 Theorem, 134, 169 Theorem prover, 8, 50 Training data, 178, 211 Transition function, 292, 303 Transportation, 12 True, 42 Truth table, 24 method, 27\\n\\nTuring\\n\\nAlan, 7 test, 5\\n\\nTweety example, 144\\n\\nU Uniﬁable, 53 Uniﬁcation, 52 Uniﬁer, 53\\n\\nmost general, 53 Uniform cost search, 99 Unit\\n\\nclause, 55 resolution, 55 Unsatisﬁable, 25\\n\\nV Valid, 25, 43 Value iteration, 308 Variable, 40 Vienna Development Method Speciﬁcation\\n\\nLanguage (VDM-SL), 62\\n\\nVoronoi diagram, 191\\n\\nW Walking robot, 290 Warren Abstract Machine (WAM),\\n\\n76, 82\\n\\nWatson, 20 WEKA, 199, 233 Whitening, 280\\n\\nIndex')],\n",
              " [Document(metadata={'source': '/content/Artificial Intelligence Accelerates Human Learning_ Discussion Data Analytics ( PDFDrive ).pdf'}, page_content='Katashi Nagao\\n\\nArtificial Intelligence Accelerates Human Learning\\n\\nDiscussion Data Analytics\\n\\nArtiﬁcial Intelligence Accelerates Human Learning\\n\\nKatashi Nagao\\n\\nArtiﬁcial Intelligence Accelerates Human Learning\\n\\nDiscussion Data Analytics\\n\\n123\\n\\nKatashi Nagao Nagoya University Nagoya, Japan\\n\\nISBN 978-981-13-6174-6 https://doi.org/10.1007/978-981-13-6175-3\\n\\nISBN 978-981-13-6175-3\\n\\n(eBook)\\n\\nLibrary of Congress Control Number: 2018968397\\n\\n© Springer Nature Singapore Pte Ltd. 2019 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.\\n\\ntrademarks, service marks, etc.\\n\\nThis Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd. The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, Singapore\\n\\nPreface\\n\\nDiscussion is something that often occurs in everyday conversation, for example, sharing your opinion with others and answering questions. However, it is slightly more sophisticated than just chat. People who are good at communicating are generally good at discussion.\\n\\nIn the case of communication ability in general, you might think of someone that considers the feeling of their conversation partner, or thinks about what is appro- priate conversation for the current environment, or maybe even that they are more skilled at making small talk. Although I even believe that these are valid, the most essential skill to acquire is discussion ability. In addition, I would like to explain about three very important technical concepts in this book.\\n\\nThe ﬁrst is data analytics, which is a scientiﬁc system for gathering objective facts (data) in large quantities, analyzing data probabilistically and statistically, and making new discoveries. The second is natural language processing, a technique for analyzing and generating human language. The third type is gamiﬁcation, which is a methodology that introduces game elements (such as scoring points and com- petition, to gradually increase the skill level of the target by giving a sense of accomplishment, etc.) for daily activities.\\n\\nThese three concepts play an important role in efﬁciently improving discussion ability. Data analytics is used in the objective analysis of human behavior, and natural language processing is used to extract features of human behavior, espe- cially to extract and utilize features contained in words. One should think about training for discussion ability in the same way as training for improvement in sports. In other words, we need self-analysis and practice to overcome weaknesses and improve strengths.\\n\\nHowever, different from sports, discussion is an intellectual activity, and those data are composed mainly of words. Although the technology concerning the analysis of words is considerably advanced, it is still very difﬁcult to analyze its semantic contents with high accuracy. So, I will propose a creative idea. That is to attach attributes to people’s remarks. We think about what we want to discuss next, and decide if it is related to the previous speaker’s speech or not, then we categorize the speech. Of course, if you follow conversation properly as a human, you can\\n\\nv\\n\\nvi\\n\\nunderstand without the need for categorization. However, it is quite difﬁcult for a machine to judge the relatedness of remarks in the same manner. Interestingly, if you were to ask most people if they were aware in what way their utterances were related to the ﬂow of the conversation, you would ﬁnd that in most cases they are not completely aware. To tell the truth, it is good for both machines and human beings to listen to other people’s speech and think about the connections.\\n\\nWe now think about how to apply data analytics to a discussion, where the simplest analysis is to examine the number of remarks produced by each partici- pant. After collecting data for around a year, one of the ﬁrst things we found was that people with a higher number of utterances were found to have a higher level of communication ability in comparison to those who produced less utterances. Of course, there are some people that make many utterances with little content. However, we have established practical policies for our meeting system in our lab that encourage thoroughly listening to the speech of others before talking during their turn and to produce utterances that are related to the previous speech or if it is not related to ask clear questions.\\n\\nNext, we examine the relationship between a remark and the preceding utter- ance. This makes it easier for analysis by having metadata on speech. What I learned from the analysis is that people who are good at discussions tend to make related remarks when someone speaks related to their remarks. In other words, people who are capable of making related statements consecutively will have high communication skills.\\n\\nBy using data analytics, you can explain such things with a clear basis. On the other hand, what\\n\\nis gamiﬁcation? Applying game-like elements to everyday activities is not an entirely new concept. We all are familiar with the teacher praising the ﬁrst student to get a question right in front of the class.\\n\\nIt is an important requirement for games to reward achievement of tasks, allow competition with multiple people, and show results in an easy-to-understand manner before the player grows bored of the game, but this also can apply to daily activities where it isn’t rare to gauge motivation.\\n\\nNevertheless, why try to systemize games with a name like gamiﬁcation? To that end, information technology (IT) is also heavily involved. Data are important here as well. In short, IT is a mechanism that processes digitized data to suit the cir- cumstances of human beings, so gaming can also actively utilize the data to expand its effect. For example, a small survey can now be easily ﬁlled out via smartphone with the results being quickly summarized for everyone present to view. If you happen to have past data from a similar survey, you can quickly compare it and carry out statistical processing. In this way, IT is providing humans with new opportunities to think. The fact that we are able to collect and quickly process information for human understanding will change our deﬁnition of what is possible. So, I would like to try to make discussion a game. Some might think that there already exists debate which is like discussion made game-like. Debates have clear rules and frameworks to decide the outcome. Given a problem, such as “whether the Japanese constitution should be revised”, arguing and arguing against and against, we will compete to determine which claim is more convincing. We can imagine that\\n\\nPreface\\n\\nPreface\\n\\nvii\\n\\ndebate will be good training for discussion. However, it is far from the commu- nication which we usually do. We will express our opinion based on our thinking and experience. It is constructive to train people thinking about how to express such opinions effectively to the opponent. Debate is not appropriate for it.\\n\\nThen, what should we do? Rules like debate are not necessary for training to improve discussion. However, there are some restrictions. In order to introduce gamiﬁcation, at least you need to do the following. First, you have the speaker record their name when they speak and whether their remarks are related to previous statements. This also makes it easy to use IT, so that you can enter it automatically for the former speaker. Next, when someone’s remarks are over, we will evaluate that statement. This evaluation is a little difﬁcult, but here it is easy to think that it is whether you were convinced by the remark or not by entering yes or no. In fact, you may have to enter somewhat more complicated things, but that is when the stage in the game has advanced.\\n\\nThe way to train the discussion ability using IT like this is very effective. We have been doing research on this for over 10 years. This book is written to share the research results with readers. In addition, I will explain data analytics and gami- ﬁcation in order to understand the research contents. Of course, I would like to touch on artiﬁcial intelligence which is the cutting edge of IT, especially machine learning and data mining.\\n\\nIt would be an unexpected pleasure if readers would someday become good at\\n\\ndiscussions and communication by utilizing our research.\\n\\nI greatly appreciate the assistance provided by people who contributed to this book. The staff and students of Nagao laboratory of Nagoya University, Shigeki Ohira, Shigeki Matsubara, Katsuhiko Kaji, Daisuke Yamamoto, Takahiro Tsuchida, Kentaro Ishitoya, Hironori Tomobe, Kei Inoue, Kosuke Kawanishi, Naoya Kobayashi, Naoya Morita, Saya Sugiura, Kosuke Okamoto, Ryo Takeshima, Yuki Umezawa, Shimeng Peng, Ryoma Senda, and Yusuke Asai, whom I have worked with and have kindly supported me during the development of the prototype sys- tems introduced in this book. Kazutaka Kurihara, Professor of Tsuda University, provided me with much helpful advice to develop the ideas described in this book. Also, I thank Miku Suganuma, Miyuki Saito, and William Samuel Anderson\\n\\nwho gave me helpful advice on the wording used in the book.\\n\\nThe work described in Chap. 5 was supported by the Real-World Data Circulation Leaders’ Graduate Program of Nagoya University. I thank Kazuya Takeda, program coordinator, and Mehrdad Panahpour Tehrani and Jovilyn Therese B. Fajardo, designated professors of the program, who are co-authors of the journal paper based on Chap. 5.\\n\\nFurthermore, I am indebted to my editor, Mio Sugino, who has been a constant\\n\\nmentor during the writing process.\\n\\nFinally, I am particularly grateful to my wife, Kazue Nagao, and my daughter,\\n\\nSaki Nagao, without whom I could not have undertaken this effort.\\n\\nNagoya, Japan\\n\\nKatashi Nagao\\n\\nContents\\n\\n1 Artiﬁcial Intelligence in Education . . . . . . . . . . . . . . . . . . . . . . . . . . AI and Education . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1 e-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Intelligent Tutoring System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 1.4 Learning Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.5 Machine Learning Accelerates Human Learning . . . . . . . . . . . . . Deep Learning Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2 Discussion Data Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Structure of Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion Mining System . . . . . . . . . . . . . . . . . . . . . . . . . . . . Structuring Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summarization of Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . Task Discovery from Discussion . . . . . . . . . . . . . . . . . . . . . . . . Active Learning for Improving Supervised Learning . . . . . . . . . . Natural Language Processing for Deep Understanding of Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Natural Language Processing for Discussion Structure Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.9 Correctness of Discussion Structure . . . . . . . . . . . . . . . . . . . . . . 2.10 Structuring Discussion with Pointing Information . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2.1 2.2 2.3 2.4 2.5 2.6 2.7\\n\\n2.8\\n\\n3 Creative Meeting Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Meeting Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Machine Learning for Structured Meeting Content . . . . . . . . . . . Post-meeting Assistance to Support Creative Activities . . . . . . . . 3.3\\n\\n1 1 4 7 9 11 13 16\\n\\n19 20 21 23 25 27 33\\n\\n37\\n\\n41 46 51 55\\n\\n57 58 63 64\\n\\nix\\n\\nx\\n\\nEvaluation of Creativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 3.5 Future of Creative Meeting . . . . . . . . . . . . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n4 Discussion Skills Evaluation and Training . . . . . . . . . . . . . . . . . . . . Evaluation of Speaking Ability . . . . . . . . . . . . . . . . . . . . . . . . . Feedback of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation of Listening Ability . . . . . . . . . . . . . . . . . . . . . . . . . Discussion Skills Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Gamiﬁcation for Maintaining Motivation to Raise Discussion Abilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n\\n4.1 4.2 4.3 4.4 4.5\\n\\n5 Smart Learning Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 Environments for Evidence-Based Education . . . . . . . . . . . . . . . 106 Related Work and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . 106 5.2.1 Discussion Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 107 5.2.2 Presentation Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 109 5.2.3 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 Leaders’ Saloon: A New Physical–Digital Learning Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 5.3.1 Discussion Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 5.3.2 Digital Poster Panel . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 5.3.3 Interactive Wall-Sized Whiteboard . . . . . . . . . . . . . . . . . 111 Importing Discussion Mining System into Leaders Saloon . . . . . 112 5.4.1 Discussion Visualizer . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 5.4.2 Discussion Reminder . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 5.4.3 Employing Machine Learning Techniques . . . . . . . . . . . . 116 Digital Poster Presentation System . . . . . . . . . . . . . . . . . . . . . . . 116 5.5.1 Digital Posters Versus Regular Posters . . . . . . . . . . . . . . 116 5.5.2 Authoring Digital Posters . . . . . . . . . . . . . . . . . . . . . . . . 117 5.5.3 Data Acquisition from Interactions with Digital\\n\\n5.1 5.2\\n\\n5.3\\n\\n5.4\\n\\n5.5\\n\\nPosters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 Skill Evaluation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 5.6.1 Discussion Skills . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 5.6.2 Presentation Skills . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 Future Features of Smart Learning Environments . . . . . . . . . . . . 125 5.7.1 Psychophysiology-Based Activity Evaluation . . . . . . . . . . 127 5.7.2 Virtual Reality Presentation Training System . . . . . . . . . . 130 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 6 Symbiosis between Humans and Artiﬁcial Intelligence . . . . . . . . . . . 135 Augmentation of Human Ability by AI . . . . . . . . . . . . . . . . . . . 136 Intelligent Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n\\n5.6\\n\\n5.7\\n\\n6.1 6.2\\n\\nContents\\n\\n73 74 75\\n\\n77 78 83 88 93\\n\\nContents\\n\\nSingularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 6.3 Human–AI Symbiosis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 6.4 Agents Embedded in Ordinary Things . . . . . . . . . . . . . . . . . . . . 145 6.5 Artiﬁcial Intelligence Accelerates Human Learning . . . . . . . . . . . 147 6.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n\\nxi\\n\\nChapter 1 Artiﬁcial Intelligence in Education\\n\\nAbstract This book explains how human learning is promoted by applying artiﬁcial intelligence to education. Before that, let’s ﬁrst look back on how information tech- nology including artiﬁcial intelligence contributed to education. Various technologies have been developed to make it easier for learners to learn and to create an environ- ment where teachers can more easily teach. An example of this is called e-learning or intelligent tutoring systems (ITS). e-Learning is an educational system using online media and has developed together with web technology. ITS was developed using a rule-based system which is an initial result of artiﬁcial intelligence. In the process, user models for learners called learner models and educational contents have been improved. As an application of data science, technology called learning analytics was developed. This is a technique for statistically analyzing learner’s historical data obtained by e-learning, etc. and discovering the characteristics of the learner. This will contribute to personalized learning that adapts the educational system to the learner’s characteristics. Furthermore, the development of learning analytics will clarify the concept of evidence-based education. As with medical care, we should construct a feedback loop that educates in accordance with data-based analysis and the learning strategies obtained from it, and improves if there are problems. Machine learning, which is an important achievement of recent artiﬁcial intelligence, is used for data analysis at this time. In addition, we will use a method that lets machines do the feature extraction from data called deep learning. In this chapter, I will touch them in detail. Keywords Intelligence ampliﬁcation · e-Learning · Intelligent tutoring system · Learning analytics · Evidence-based education · Deep learning\\n\\n1.1 AI and Education\\n\\nResearch on an educational support system using artiﬁcial intelligence has been exploring important questions such as how to support learning and problem-solving which can be considered as the foundation of human intelligence and how to com- municate with people to realize it (Wenger 1987).\\n\\n© Springer Nature Singapore Pte Ltd. 2019 K. Nagao, Artiﬁcial Intelligence Accelerates Human Learning, https://doi.org/10.1007/978-981-13-6175-3_1\\n\\n1\\n\\n2\\n\\n1 Artiﬁcial Intelligence in Education\\n\\nIt is also a research area that has led and impacted critical issues such as nat- ural language understanding and data mining as well as essential technologies for educational support as well as artiﬁcial intelligence research.\\n\\nWhile research on artiﬁcial intelligence focuses on the realization of machine intelligence, research on educational support is of primary interest in raising human intelligence through human learning and support for problem-solving.\\n\\nApproaches to such intelligence are often referred to as intelligence augmentation\\n\\nor intelligence ampliﬁcation (IA).\\n\\nHowever, the approaches are consistent in that they explore human intelligence even if they are different, and research on educational support systems has been trying to elucidate the way of learning, problem-solving, and communication from the viewpoint of IA.\\n\\nThe goal of such research on educational support systems is to realize intellectual\\n\\neducation/learning support for learners.\\n\\nWhat is “intellectual” here? Although the answer is not ﬁxed, it has been aimed\\n\\nat realizing intellectual support from almost two perspectives so far.\\n\\nOne is to adaptively adapt to individual learners, that is, adaptation according to\\n\\nthe characteristics of each learner from the viewpoint of “teaching”.\\n\\nThe other is to provide appropriate learning environments and tools for learn- ers from the viewpoint of “learning”, to build a foothold for problem-solving and learning, called scaffolding.\\n\\nIn order to realize these supports, the design and development methodologies of systems and element technologies have been explored while utilizing theories and knowledge in related ﬁelds such as cognitive science and pedagogy.\\n\\nThe origin of such inquiry dates back to the study of intelligent computer-assisted\\n\\ninstruction from the 1970s to the 1980s.\\n\\nFrom that time on, the composition of the system necessary for realizing intel- lectual educational support with the following four factors (1) teaching knowledge expressing how to teach, (2) teaching material knowledge showing teaching knowl- edge, (3) knowledge state and understanding state of the learner, and (4) the user interface to realize bidirectional dialogue (Carbonell 1970) has been taken into con- sideration, and these elements have directed the research and development of edu- cation and learning support technologies up to today.\\n\\nIn particular, it has been found that it is indispensable for the advancement of intellectual support by the system to represent the various knowledge necessary for support structurally.\\n\\nThis point is consistent with the methodology of expert system development in knowledge engineering (Clancey 1987) and has had a very signiﬁcant impact on subsequent educational support system development.\\n\\nEspecially in the educational support system research, emphasis shifted from educational orientation to learning oriented since the 1990s, and the paradigm of research has shifted to a learner-centered system (Norman 1996).\\n\\nProgresses in information and communication technology pushed this paradigm\\n\\nshift.\\n\\n1.1 AI and Education\\n\\nToday, based on these research backgrounds, a research methodology that sys- tematically constructs support methods by clarifying the way of learning support by modeling learning and its support as an information processing process and express- ing it on the system has been promoted (Kashihara 2015).\\n\\nResearch based on this methodology is called “learning informatics”. In realizing intellectual educational support, the ability of the system to properly understand learners is essential, even though domains, tasks, and learning contexts to be supported diversify in the future, its importance remains unchanged.\\n\\nHowever, even if the human teacher does not fully understand the learner, it is possible to provide accurate support through interactions, so it is not necessary to aim for building a cognitively sophisticated learning model.\\n\\nIt is important to generate learner model information inside the system so as to contribute to the effective execution of educational support or design interaction to compensate for imperfections in learner model information.\\n\\nOn the other hand, it is often difﬁcult to generate adequate learner model infor- mation, such as mastering higher order skills such as problem-solving, learning, and creation in unstructured domains.\\n\\nIn response to such problems, a framework is proposed to disclose learner model information generated by the system and to make learners themselves discover inad- equate/inappropriate points of the model so that a whole system including learners as a part of the system can be modeled (Bull and Kay 2007).\\n\\nSuch a model is called an Open Learner Model. This approach also leads to encouraging learner’s introspection by scrutinizing information on the learner model, and high learning effect is expected.\\n\\nAlso, even if errors and knowledge states found from learners’ inputs are generated\\n\\nas learner model information, learners do not necessarily recognize them.\\n\\nFor such deviations between model expressions and learner’s recognition, meth- ods to promote self-recognition for errors by visualizing errors have been proposed. These can be thought of as an attempt to effectively supplement information\\n\\nlacking as a learner model expression by interaction.\\n\\nFurthermore, in the e-learning environment, enormous learning log data from the learner group is accumulated in learning management system (LMS) and e-portfolios, and this large-scale data can be analyzed. Then, it is possible to discover learning behavior patterns and new attributes that characterize learners that were not supposed beforehand.\\n\\nThis activity is called learning analytics and is an attempt to convert the amount of data into quality data and to ﬁnd a new learner modeling technique (Ochoa et al. 2014).\\n\\n3\\n\\n4\\n\\n1 Artiﬁcial Intelligence in Education\\n\\n1.2 e-Learning\\n\\nEducational support using computer networks has been practiced for quite a long time, but e-learning, which is a form of educational training utilizing the Internet since the latter half of the 1990s, has been developed and is now widespread.\\n\\ne-Learning in a narrow sense refers to an embodiment of education and training\\n\\nusing a computer network.\\n\\nFor example, in asynchronous learning by web-based training (WBT), the learner learns using the teaching materials stored in the WBT server using a web browser, and the instructor grasps the progress of the learner with the learning log accumulated in the server.\\n\\nIn synchronous learning, simultaneous distribution of lectures and acceptance of\\n\\nquestions from learners are realized using a video conference system.\\n\\nIt can be thought that e-learning in a broad sense includes not only implemen- tation of education and training but also IT support for operation management and performance evaluation.\\n\\nFor example, not only WBT and synchronous learning but also learning progress, guidance history, schedule, used assets, etc. of a complex curriculum combining multiple educational forms such as group education, practical training, seminars, etc. are applied to a learning management system (LMS). Supporting the smooth implementation of the curriculum is also a major embodiment of e-learning.\\n\\nAdditionally, in thinking about things in phases for education and training in practice, we need to realize that there is more to be concerned with than just students using the systems; we have a beginning planning phase where we need to think about plans for what and how we will teach and a post-evaluation phase where we need to evaluate our measurements concerning educational effectiveness.\\n\\nIn this way, it is part of e-learning to manage the achievement goal of the cur- riculum and the achievement progress level of individual learners and to support feedback for the implementation of the next curriculum.\\n\\nThe main information in e-learning is related to content/learner/learning sys- tem/learning history. Standardization of data models and representation formats (bindings) on these are underway (Nakabayashi 2002).\\n\\nIn addition, interfaces for linking learning support systems and tools are becoming more standardized. A prime example of this would be the content standard share- able content object reference model (SCORM) which has been decided on by the American standardization organization the advanced distributed learning initiative (ADL).\\n\\nThe SCORM content consists of a course structure loaded into the WBT server, sequencing rules and metadata attached to the course structure, and shareable content object (SCO) executed on the Web client.\\n\\nThe course is described by a hierarchical teaching material structure (correspond- ing to the table of contents of the textbook), and the terminal page of the hierarchy corresponds one-to-one with the SCO.\\n\\n1.2 e-Learning\\n\\nThe LMS sequentially selects pages to be presented to the learner from the course\\n\\nstructure and displays the corresponding SCO on the screen of the Web client.\\n\\nSCO is Web content composed of HTML, JavaScript, various plug-ins, etc., and includes explanation pages of exercise materials, exercise problem pages, simulation pages, and so on.\\n\\nBy describing the sequencing rules, it is possible to create learner adaptive content that dynamically selects the SCO that the LMS presents to the learner according to the solution situation of the exercise problem.\\n\\nThe standard speciﬁes the data model of the course structure and the binding to XML and the API and data model (including learner ID, practical problem acquisition situation, learning time, etc.) of SCO/LMS communication.\\n\\nAccording to the SCORM standard, the same teaching material can be executed by several different LMS (interoperability), and SCO can be used in combination with several different course structures (reusability).\\n\\nAmong the standards of content, there is a question and test interoperability (QTI) standard developed by IMS (IMS Global Learning Consortium, Inc.) as a standard specialized for testing (Nakabayashi 2008).\\n\\nThe QTI standard is roughly divided into standards relating to test content and\\n\\nevaluation results.\\n\\nIn the test content standards, a hierarchical structure of the entire exam question\\n\\nand a description method of the content of each question are stipulated.\\n\\nA question can be categorized in many ways, such as in a written format, as multiple choices, true or false questions, by presentation style, and result evaluation. In the hierarchical structure part, it is possible to designate the method of ordering the question, the method of feedback to the learner, the method of summarizing the result, and so on.\\n\\nIn the standard relating to the evaluation result, a format for recording the results,\\n\\nthe number of trials, the required time, etc. of each question is stipulated.\\n\\nThat is, the entire QTI standard includes not only static descriptions such as question questions of questions, order of correct answers, and problems, but also descriptions of dynamic actions at the time of execution, such as correctness judgment method and resulting aggregation method.\\n\\nIn addition, as a content-related standard, a LOM, which is a metadata standard for describing attributes of LO (learning object), is formulated by the IMS LTSC (Learning Technology Standard Committee).\\n\\nHere, LO refers to all digital/non-digital resources used for educational training. LOM is indexed information for searching and reusing these resources. The index information is composed of general information such as title and expla- nation of contents, education ﬁeld, subject learner, degree of difﬁculty, and other education-related information, intellectual property right information, information showing relation with other LOs.\\n\\nBy constructing a repository of LO using LOM, it becomes possible to classify and extract LO according to educational purposes and to organize LO according to curriculum.\\n\\n5\\n\\n6\\n\\n1 Artiﬁcial Intelligence in Education\\n\\nAs a learner, the information standard IMS has formulated the Learner Infor- mation Package (LIP). LIP is a standard for describing attributes of learners and includes security information with identiﬁcation information such as name and ID, learning purpose information, possession qualiﬁcation information, learning history information, competency information, and password as main items.\\n\\nAccording to the LIP standard, a format for exchanging student information\\n\\nbetween systems is standardized.\\n\\nISO/IEC 20006 Information Model for Competency that Japan proposed from ISO/IEC JTC 1 SC 36 exists as a standard for describing learning systems and skills learned by learners.\\n\\nThe standard speciﬁes a framework for describing the learner’s skills and a method\\n\\nfor expressing standards for expressing skills.\\n\\nAs a standard for describing various learning methods (learning teaching strate- gies), the learning design (LD) standard of IMS standardized based on educational modeling language (EML) developed at the Netherlands Open University. EML and LD describe various learning teaching strategies. These strategies assume that “in learning, people belonging to a speciﬁc group, people with a speciﬁc role are involved in learning activities using an environment consisting of appropriate resources and services.”\\n\\nDescriptions are very diverse from self-taught teaching strategies that perform strict learning control to problem-solving cooperative learning by multiple learners. Regarding the learning history standards, attempts were made to formulate stan-\\n\\ndards related to portfolios, but they have not spread to widespread use.\\n\\nIn recent years, due to popularization of learning using mobile terminals and the like, standards have been proposed that simply deﬁne an interface to transfer learning history between systems.\\n\\nOne of them is the Experience API (xAPI) formulated by ADL. xAPI is a standard for collecting the history of learning activities using mobile\\n\\nterminals and electronic devices such as electronic textbooks.\\n\\nThe target learning activity ranges from very micro-actions such as browsing web pages and page scrolling of electronic textbooks to a range of relatively large grain sizes such as evaluation results of exercise problems handled in SCORM and QTI. We will explain the utility of technical standardization and its future direction. The basic idea of technical standardization is to divide the whole system into parts (modules) and standardize the interface between the modules which enables the exchange of modules without impairing the function of the whole system.\\n\\nThis makes it possible to replace high-performance modules and add modules with new functions, which allows us to autonomously decentralize the evolution and value improvement of the whole system which is the essence of the effectiveness of technical standardization (Nakabayashi 2010).\\n\\nUsers can freely “exchange” and select items of favorite things such as perfor-\\n\\nmance and price from the products of several suppliers.\\n\\nBy giving users freedom of choice, there is a necessity to provide high-quality, low-priced products to suppliers, and competition will accelerate the improvement of value.\\n\\n1.2 e-Learning\\n\\nIn addition, a module developer can easily develop and add modules that have new functions and convenience without knowing the detailed structure of other inter- locked modules in the system.\\n\\nIn order to further promote such evolution, it is desirable that the standard be as\\n\\nsimple and robust as possible.\\n\\nThe success of the Internet, especially the Web, is a good example of the drastic\\n\\nincrease in value brought about by such simple and robust standards.\\n\\nIn the ﬁeld of e-learning, the module that brings about such value improvement\\n\\nwas originally a system, and after that it was contents.\\n\\nAlthough it seems that their importance will not be lost in the future, on the other hand, the value of learner information and learning history information will increase more in the near future.\\n\\nBy standards related to learning history information such as xAPI, development of a learning analytics module for learning portfolio (e-portfolio) and large-scale learn- ing history information is promoted. It is expected that a learning support function will emerge with higher added value than using these results.\\n\\n1.3 Intelligent Tutoring System\\n\\nAn intelligent education system or intelligent tutoring system (ITS) supports learn- ing through adaptive and interactive knowledge exchanges between a teacher’s com- puter and learner using artiﬁcial intelligence technology (Wenger 1987; Woolf 2009, Nkambou et al. 2010). It consists of four modules: a teaching material module, a learner model module, a tutoring strategy module, and an interface module.\\n\\nThe teaching material module not only stores the knowledge that is the object of education but also has functions such as evaluating the learner’s answer and providing information for problem-solving/question generation/explanation.\\n\\nThe learner model module observes the behaviors of learners, constructs models of learning situations such as emotions and motivation, understanding of learner’s teaching materials, and provides necessary information to realize adaptive education. Based on that information, the tutoring strategy module will decide and imple- ment hints appropriate to the state of the learner and actions such as selection and explanation of problems.\\n\\nThe interface module then implements a multimodal interaction between the\\n\\nlearner and the system.\\n\\nThe characteristic of ITS lies in modeling. It has models of teaching materials, models of understanding by learners, and models of educational behavior, and support learners’ learning while utilizing them dynamically.\\n\\nIn particular, since the student model (learner model) is the main source of adaptive behavior of the system, much research has been done. However, it is difﬁcult to learn from a model of human understanding with a small number of observations. One\\n\\n7\\n\\n8\\n\\n1 Artiﬁcial Intelligence in Education\\n\\ncould say that developing necessary and sufﬁcient learning models is an eternal challenge for ITS.\\n\\nA cognitive apprenticeship model (Collins et al. 1987) expressing a process to acquire cognitive skills is one of the inﬂuential models for acquiring and improving skills related to “learning”.\\n\\nThis is an extension of the process of apprenticeship practiced by craftsmen with skills when developing disciples to cognitive skills, and the process is organized as follows:\\n\\n1. Modeling: method of knowledge transfer by experts. 2. Coaching: presentation of hints and feedback by experts. 3. Scaffolding (foothold making): providing clues (scaffolding) according to learner’s proﬁciency level.\\n\\n4. Articulation (clariﬁcation/externalization): linguistic representation and exter- nalization of thought by learner.\\n\\n5. Reﬂection: review of learning process by learners. 6. Exploration (inquiry/practice): preparation of an environment where experts solve problems to learners.\\n\\nIn the early stages of cognitive skill learning, it is important to provide the student with the structure of the skill to be learned at an appropriate level of abstraction according to the level of the learner.\\n\\nIn order to realize this, it is desirable to understand that the structure of the skill to be learned is organized as a combination of a learning curve and an ontology, and the level visualization method by rubric, etc. is deﬁned according to the type of skill. In addition, as a method of presenting to learners, a method such as a concept map that structurizes relationships between concepts has been proposed to support a bird’s-eye view of overall images.\\n\\nAlso in the early stages of cognitive skill learning, immediate feedback according to a learner’s interaction is required to prevent wrong knowledge acquisition and a deadlock situation.\\n\\nSpeciﬁcally, there are issues such as assignment of tasks, division of problems, presentations of points of interest for orienting learning activities, provision of hints to learning activities, visualization of errors, and so on.\\n\\nA method of adaptively recommending or dynamically generating candidates for the next learning activity according to the situation of progress may also be considered.\\n\\nIncreasing the slope of the development stage graph in the learning curve can be\\n\\nregarded as reaching a high skill level with a smaller number of learning times.\\n\\nTo provide such an appropriate “scaffolding”, it is necessary for the system to grasp the skill level of a learner’s cognitive skills and to control the support function provided by that level.\\n\\nTo realize this, it is necessary not only to pay attention to the learning results but to collect data reﬂecting the learning processes. We are utilizing rules like a fading technique which reduces support content based on the user’s proﬁciency and\\n\\n1.3 Intelligent Tutoring System\\n\\nan adaption technique based on learning style and learning method preferences to manage support functions.\\n\\nIt is one of the important learning activities that the learner himself/herself exter- nally (lexically) represents the learning process at the developmental stage of learning of cognitive skills.\\n\\nBy doing so, the learner can objectively grasp his/her own state which is difﬁcult\\n\\nto reproduce in reality.\\n\\nIn order to positively carry out such externalization activities, as an environment for externalization, a method of annotations such as an intention at the time of learning and an outline of learning activity and a method of drawing relations between concepts exist.\\n\\nIt is used in combination with supporting methods such as the externalization\\n\\ndirection of the learning process.\\n\\nIt is not easy for students to notice the necessity of reﬂection (awareness) at the\\n\\ndevelopmental stage.\\n\\nOn the other hand, excessive support that damages the learner’s identity lowers the awareness to the learning process and learning outcomes, which makes it difﬁcult to promote reﬂection.\\n\\nIn order to promote reﬂection, it is required that we attempt to “provide the\\n\\nenvironment in which the student himself/herself reﬂects as much as possible.”\\n\\nIn addition to looking back on their activities, not only capturing changes in their performance but also approaches such as grasping improvement points through comparison with others are often performed.\\n\\n“Learning how to learn” is more implicit than traditional “learning of knowledge,”\\n\\nthe period of learning is long, and the correct answer is not always unique.\\n\\nFor this reason, it is difﬁcult to establish in advance an absolute criterion on how\\n\\nthe proposed function contributed to the learner.\\n\\nSpeciﬁc solutions include an approach that extracts an ideal learning process such as a role model by utilizing social learning context by a large number of learners, and an approach that uses a similar learning process to personalize learning support functions. Also, an approach to evaluate the correspondence relationship between learning process and performance is conceivable. It is necessary to reduce the burden on the teacher and implement an interface that utilizes natural language.\\n\\nIn addition, how to present objective evaluation indexes is a major challenge for\\n\\nthe future in utilizing ITS as an open learning environment.\\n\\n1.4 Learning Analytics\\n\\nIn recent years, methods for visualizing, analyzing, and evaluating learning activities and their practical application are increasingly concerned.\\n\\nLearning analytics is a method of measuring, collecting, analyzing, and reporting evidence-based data on learning and context in order to optimize and improve the learning process and learning environment. It is based on online education such as\\n\\n9\\n\\n10\\n\\n1 Artiﬁcial Intelligence in Education\\n\\nmassive open online courses (MOOCs) (Watanabe et al. 2015), OpenCourseWare, learning management systems, and electronic teaching materials, as well as dissem- ination of various education/learning tools making use of IT. Furthermore, we are gathering great expectations as a means to elucidate through visualization of edu- cation and learning process, improvement of education through measurement and veriﬁcation of learning effect, and as a means to support and promote students’ autonomous and effective learning.\\n\\nAn exemplary practical example of learning analytics is the open learning initiative\\n\\n(OLI) of Carnegie Mellon University which began in 2003 (Lovett et al. 2008).\\n\\nIn OLI, a teaching/learning model based on cognitive learning theory is applied\\n\\nto the design of online lectures.\\n\\nThe material of each lecture is modularized and can be partially used according to the needs of users, and an intelligent tutoring system for learning support and virtual experiment simulation is also incorporated.\\n\\nThe OLI can navigate a learning route that is optimal for individual learners using\\n\\nthe user’s learning history.\\n\\nIn addition, by utilizing OLI before in class meetings as part of the students’ self-study, we can analyze the results of each learning log and ﬁnd which concepts were most difﬁcult for the students to understand. This aspect is similar to ﬂipped learning but still has the beneﬁts of face-to-face learning, creating a more effective blended learning environment.\\n\\nLearning Catalytics, developed by Professor Eric Mazur of Harvard University in 2011, applies a learning method called peer instruction in which “learners mutually teach each other.” There, it provides a function to verify the effect of peer instruction by analyzing the response of each student who used a clicker (personal response system: PRS) in the classroom over time. It also has a recommendation function that presents a combination of students to further enhance the possibility of reaching correct answers.\\n\\nIn this way, learning analytics can be used in various teaching/learning modes ranging from online lecture/teaching materials, blended learning and face-to-face lessons, but the scope of application is not limited to lecture/class level.\\n\\nFor example, at a more macroscopic level, it is possible to analyze learners’ characteristics, studies, learning, performance data, etc. and use them for evaluation and improvement of educational curriculums (Long and Siemens 2011).\\n\\nIt is useful to examine what form of lesson/teaching material is more effective depending on the characteristics of the learner and to improve education from a structural and system viewpoint.\\n\\nIt is also effective to think about what kind of order the lesson will be in chrono-\\n\\nlogically to improve the learning effect.\\n\\nWe ﬁrst mention “personalized learning,” considering that learning analytics has\\n\\nbecome more and more important for education and learning in modern society.\\n\\nAlready in various knowledge and information sectors of society, conversion from “Supply Push” to “Demand Pull” has been aimed at individualization of services. Based on educational big data obtained from a larger number of learners, it is desirable\\n\\n1.4 Learning Analytics\\n\\nto utilize the results in the ﬁeld of artiﬁcial intelligence research and to support diverse and effective education/learning according to individual characteristics and purpose. In doing so, considering how to bring about “adaptive learning” for individuals through each stage of “descriptive”, “diagnostic”, “predictive”, and “prescriptive” learning analytics is an essential subject.\\n\\nLearning analytics is used in an exploratory approach of “understanding what\\n\\neducation and learning-related data are available for the time being.”\\n\\nHowever, it is important to clarify for “what purpose to use learning analytics” and to examine, collect, and analyze the data and evidence required qualitatively and quantitatively.\\n\\nIt is also important to think from the viewpoint of “how to present and use analysis\\n\\nresults in an easy-to-understand manner.”\\n\\nIn modern society which is more complicated and ﬂuidized, the obsolescence of technology and knowledge becomes intense and employment is becoming difﬁcult to stabilize. Therefore, in order to secure the knowledge, skills, and professional foundations necessary for individuals, individuals need a new educational system that allows them to continue learning as necessary for their lifetime.\\n\\nWith the advancement of IT such as the Internet and multimedia technology,\\n\\ndigitization of educational tools and contents is rapidly progressing.\\n\\nOver 1000 paid online degree programs have already been created in the world, and tens of thousands of online lectures/teaching materials are released free of charge. On the other hand, how to effectively utilize these educational resources and envi- ronments for learning throughout life will largely depend on the development and diffusion of learning analytics in the future.\\n\\n1.5 Machine Learning Accelerates Human Learning\\n\\nAt the forefront of learning analytics is “evidence-based education.”\\n\\nThe essential technique for that is machine learning. Records of learners represented by e-portfolios are used for machine learning and\\n\\ncan be used to suggest appropriate guidance.\\n\\nThe educational data gathered by learning analytics will be used by the AI tutor as data that are the basis for the learner’s proper learning. Speciﬁcally, the AI tutor shows the reason for choosing the recommended course of study.\\n\\nThis is to analyze the advantages and disadvantages of learners from the past learning history, to prolong the strengths, to recommend courses to overcome disad- vantages, and to explain the reasons.\\n\\nMachine learning predicts how the learner’s ability will improve when that course is selected. In general, machine learning cannot explain how the prediction result was derived, but research on “explainable AI” to make it possible is being performed.\\n\\nA combination of learning analytics and explainable AI enables evidence-based education. For that purpose, the learner must always be able to conﬁrm the results of analysis and evaluation of various data in the learning process. There are various\\n\\n11\\n\\n12\\n\\n1 Artiﬁcial Intelligence in Education\\n\\nconﬁrmation methods, and it is possible to visualize data by infographics and to read automatically generated sentences. Mutual evaluation by members belonging to the same group as learners is also effective. By evaluating others, you will be able to view your learning state objectively. The results of the mutual evaluation will also be easy to visualize and can be conﬁrmed.\\n\\nWe are incorporating a mechanism of evidence-based education for training stu- dent’s discussion skills. In this mechanism, we use a system (discussion mining sys- tem) that acquires and analyzes discussion data in detail. In addition, in the manner of gamiﬁcation, feedback on the progress of learning is easy to understand (gamiﬁed discussion).\\n\\nRegarding discussion mining, as explained in Chap. 2, it is a research method that records, analyzes, and evaluates discussions in the university laboratory in detail. By expressing the relationship between statements by a tree structure, we structure the discussion, give attributes to individual statements, and make meaning of the discussion data. Since discussion in the laboratory is strongly tied to student research content, strengthening discussion skills will lead to promotion of student’s research activities.\\n\\nRegarding the gamiﬁed discussion as explained in Chap. 4, we applied gamiﬁca- tion to the discussion in which based on the rules, score participants’ actions (that is, statements), and compete within the group to train discussion skills. Since the results are fed back for each action, it is easy to understand the state of the partici- pant himself/herself and others, and it is easy to maintain motivation to continue the action. The goal can be decided for each participant, and the list of goals presented as an option is associated with a system (courseware) for learning discussion skills, depending on the current situation for each participant.\\n\\nMachine learning that predicts the future state based on detailed data also plays an important role in human learning. For example, it is possible to predict the suitability of a learner and recommend a learning course suitable for it.\\n\\nFor that purpose, we must construct a learner model (user model). The recom- mendation system method can be applied to this. A recommendation system is a system that connects products and services with users.\\n\\nThe user model in the recommendation system should always represent the user’s interests and purposes without excess or deﬁciency. What is necessary for this is the detailed data generation of user behavior.\\n\\nWe collect and analyze various data in educational research activities. Speciﬁcally, it is the discussion data described in Chap. 2. This is a record of all\\n\\nthe statements of the student in the meeting (seminar) at the university laboratory.\\n\\nThe created dataset includes audio–visual data exceeding 50,000 times of state- ment data (total number of participants is over 6000) and 1500 h (×number of cameras 3) at about 700 meetings collected from April 2006.\\n\\nWe also collect data on students’ post-meeting research activities using the system called Creative Activity Support System described in Chap. 3. This is a history of actions that the student performed after analyzing the data of the discussion at the meeting to automatically discover candidate tasks to be performed and presented to the students.\\n\\n1.5 Machine Learning Accelerates Human Learning\\n\\nThe students’ activity data include notes organizing the contents of the task, plans for its execution, activity reports, and evaluation results from other students. The plan is designed to clarify the relationship with the overall goal by comparing it with the map that comprehensively covers the research activities (data covering the activities from the start of the research to the presentation of the paper and its goals, achievement criteria, etc.).\\n\\nBy analyzing the data collected by the creative activity support system, the char- acteristics of each student become clear. It will become clear what kind of task can be accomplished efﬁciently and what other tasks cannot. For example, although a student is good at system implementation by programming, the student is not good at summarizing research trends by surveying articles. This feature is effective for customizing the learning support system.\\n\\nAn appropriate learning plan for the learner is generated by accumulating behavior data in the learning process and feature analysis based on machine learning. Human learning is promoted by putting this plan into practice. The plan may be changed dynamically, but it can be explained to the learner who will be able to understand the reason at a fundamental level.\\n\\nHumans understand their characteristics well, set goals, and decide tasks to be performed or subjects to be learned. The support system will guide the learner so as not to deviate from the target while repeating evaluation and adjustment. In this way, human learning is accelerated.\\n\\n1.6 Deep Learning Approaches\\n\\nSince Perceptron was invented in 1957, multilayered neural networks with hidden layers (intermediate layers other than input and output) have been actively researched, but due to technical problems such as local optimal solutions it was not able enough to make it learn well and the performance did not rise, so there were questions about its practicality. However, in 2006, a research team of Geoffrey Hinton who is a researcher of neural networks reported that by restricting the hidden layer in multiple layers in the restricted Boltzmann machine (neural network with recursive structure). By devising a method to efﬁciently learn a multilayered neural network, it came to attract attention again. This research result is seen as a technological breakthrough directly linked to current deep learning. Also, from 2012 onward research has been rapidly active, it is said that a tertiary artiﬁcial intelligence boom has arrived.\\n\\nIn deep learning, learning is performed so that the error is minimized by calcu- lating the output error when inputting a large amount of training data to the multi- layered neural network by the calculation method called backpropagation. At this time, conventionally, feature amounts manually set by researchers and technicians of respective data such as images and sounds are automatically extracted. For this reason, it is one of the great advantages of deep learning that the feature quantity extraction by hand is almost unnecessary.\\n\\n13\\n\\n14\\n\\n1 Artiﬁcial Intelligence in Education\\n\\nFig. 1.1 Example of convolutional neural network\\n\\nMultilayering of neural networks has problems in learning time and calculation cost, but recently it has been difﬁcult to improve the performance of computers and improve the performance of graphics processing unit (GPU), which is superior to parallel processing of operations simpler than a central processing unit (CPU) by general-purpose calculation (GPGPU). By using the GPU, it is said that price per performance and power consumption can be suppressed to 1/100th of a CPU.\\n\\nThe most important techniques in deep learning are convolutional neural networks\\n\\n(CNN) and recurrent neural networks (RNN).\\n\\nConsider, for example, an all-coupled neural network in which all the nodes of\\n\\neach layer are coupled between layers.\\n\\nAt this time, the number of parameters becomes very large, not only the learning efﬁciency is deteriorated but also excessive learning (excessive adaptation to the training data and deterioration of accuracy in the test data) is likely to occur. Also, if it is attempted to treat not only one-dimensional vectors but also three-dimensional matrices as one layer (mainly in the case of images), calculation becomes much more troublesome. Considering a ﬁlter that can be applied to a speciﬁc region of one layer, the value obtained through the ﬁlter is taken as the input to the next layer. Accordingly, it is possible to determine the correspondence relationship between the layers only by the parameters set in the ﬁlter. This operation is called convolution. The general shape of CNN used in actual image recognition is as shown in Fig. 1.1. Detailed explanation will be omitted, but when an input image is given, features of the image are extracted by the operation of convolution and pooling, the extracted features are input to the entire combined network, and the class of the image is ﬁnally estimated. Here, pooling is rough resampling of the convolution output of the previous layer (for example, taking the maximum value of the value in the n × n region of the input image, etc.). This makes it possible to absorb the difference in appearance due to some deviation of the image, and it is possible to acquire the invariable feature amount with a slight shift.\\n\\nThe connection pattern between neurons (nodes) in CNN is inspired by the visual cortex of animals. Individual neurons that respond only to stimuli in the limited ﬁeld\\n\\n1.6 Deep Learning Approaches\\n\\nFig. 1.2 Recurrent neural network\\n\\nof view are called receptive ﬁelds. The receptive ﬁelds of different neurons partially overlap to cover the entire ﬁeld of view.\\n\\nIn this way, CNN is implemented based on various knowledge concerning image processing, and in recent years, CNN has given very high results in object recognition from images.\\n\\nCNN can process classiﬁcations by processing images and sound patterns at a speciﬁc time. However, in order to recognize the state from the moving picture and to understand the meaning of the voice, it cannot be said that identiﬁcation by time is sufﬁcient. Therefore, an RNN that can handle time series information before and after was proposed. An example is shown in Fig. 1.2.\\n\\nThe content of the hidden layer at time t becomes the input at the next time t + 1, the hidden layer at t + 1 becomes the input at t + 2,… and so on. That is, the state of the previous hidden layer is used also for learning the next hidden layer. Since RNN can be considered to be the same as ordinary neural network when it is expanded in time, backpropagation can be applied to parameter learning as with CNN and the like. The temporal expansion of RNN can be brieﬂy shown in Fig. 1.3.\\n\\nThe error (the difference between the teacher signal and the output) propagates from the last time T toward the ﬁrst time 0. Therefore, the error at a certain time t is the sum of “difference between teacher signal and output at time t” and “error propagated from t + 1.” The backpropagation performed in this way is called backpropagation through time (BPTT).\\n\\nSince BPTT cannot perform learning without data up to the last time T, that is, all time series data, it is necessary to handle long time series data by cutting off the latest time section. There are various problems in this BPTT, and various learning methods to deal with it have been devised. One of them is long short-term memory (LSTM).\\n\\n15\\n\\n16\\n\\n1 Artiﬁcial Intelligence in Education\\n\\nFig. 1.3 Temporal expansion of RNN\\n\\nIf T is very large, that is, in the case of long time series data, errors propagated due to calculation problems become very small or conversely very large. Although it can cope with the maximum value restriction for the larger value, it is difﬁcult to deal with it becoming too small. Therefore, LSTM incorporates a method of propagating error so as not to attenuate greatly.\\n\\nDeep learning such as CNN and RNN is very effective mainly in pattern recog- nition such as speech recognition and image processing. As described in Chap. 3, it is used for converting speech into texts and recognizing facial images of partici- pants. Currently, we do not use deep learning techniques to analyze various data in human creative activities. However, when the data are on larger scale and it becomes difﬁcult to manually perform feature extraction for modeling, we must rely on deep learning methods. However, since machines cannot be taught correctly for problems that humans cannot show correct answers, it is necessary for humans to understand the essential mechanism for promoting human learning well.\\n\\nReferences\\n\\nS. Bull, J. Kay, Student models that invite the learner in: the SMILI open learner modelling frame-\\n\\nwork. Int. J. Artif. Intell. Educ. 17(2), 89–120 (2007)\\n\\nJ. Carbonell, AI in CAI: an artiﬁcial-intelligence approach to computer-assisted instruction. IEEE\\n\\nTrans. Man-Mach. Syst. 11(4), 190–202 (1970)\\n\\nW. J. Clancey, Knowledge-Based Tutoring: The GUIDON Program (The MIT Press, 1987) A. Collins, J. S. Brown, S. E. Newman, in Cognitive Apprenticeship: Teaching the Craft of Reading, Writing, and Mathematics. Technical Report 403 (BBN Laboratories, Cambridge, MA, Centre for the Study of Reading, University of Illinois, 1987)\\n\\nA. Kashihara, Modeling learning in engineering. J. Jpn. Soc. Artif. Intell. 30(4), 473–476 (2015).\\n\\n(in Japanese)\\n\\nP. Long, G. Siemens, Penetrating the fog: Analytics in learning and education. EDUCAUSE Rev.\\n\\n46(5), 31–40 (2011)\\n\\nReferences\\n\\nM. Lovett, O. Meyer, C. Thille, The open learning initiative: measuring the effectiveness of the OLI\\n\\nstatistics course in accelerating student learning. J. Interact. Media Educ. 1, 2008 (2008)\\n\\nK. Nakabayashi, Technical standardization trend of educational support systems. J. Jpn. Soc. Artif.\\n\\nIntell. 17(4), 465–470 (2002). (in Japanese)\\n\\nD.A. Norman, Learner-centered education. Commun. ACM 39(4), 24–27 (1996) K. Nakabayashi, e-Testing and standardization, e-Testing (Baifukan Co., Ltd, 2008), pp. 74–94. (in\\n\\nJapanese)\\n\\nK. Nakabayashi, Standardization of e-learning technology and design of learning activities. J. Jpn.\\n\\nSoc. Artif. Intell. 25(2), 250–258 (2010). (in Japanese)\\n\\nR. Nkambou et al., Advances in Intelligent Tutoring Systems (Studies in Computational Intelligence),\\n\\nvol. 308 (Springer, Berlin, 2010)\\n\\nX. Ochoa, D. Suthers, K. Verbert, E. Duval, Analysis and reﬂections on the third learning analytics\\n\\nand knowledge conference (LAK 2013). J. Learn. Analytics. 1(2), 5–22 (2014)\\n\\nF. Watanabe, Y. Mori, C. Kogo, Analyzing learners’ subjective evaluation of peer assessment in\\n\\njapan massive open online courses, Waseda. J. Human Sci. 28(2), 237–245 (2015)\\n\\nE. Wenger, Artiﬁcial Intelligence and Tutoring Systems: Computational and Cognitive Approaches\\n\\nto the Communication of Knowledge (Morgan Kaufmann Publishers Inc., 1987)\\n\\nB. Woolf, Building intelligent interactive tutors (Morgan Kaufmann, Burlington, MA, 2009)\\n\\n17\\n\\nChapter 2 Discussion Data Analytics\\n\\nAbstract Evidence-based research, such as research on big data applications, has been receiving much attention and has led to the proposal of techniques for improving the quality of life by storing and analyzing data on daily activities in large quantities. These types of techniques have been applied in the education sector, but a crucial problem remains to be overcome: it is generally difﬁcult to record intellectual activ- ities and accumulate and analyze such data on a large scale. Since this kind of data is not possible to compress in a manner, such as taking the average, it is necessary to maintain the original data as the instances of cases. Such human intellectual-activity data should be treated as big data in the near future. We have been developing a discussion mining system that records face-to-face meetings in detail, analyzes their content, and conducts knowledge discovery. Looking back on past discussion content by browsing documents, such as minutes, is an effective means for conducting future activities. In meetings at which some research topics are regularly discussed, such as seminars in laboratories, the presenters are required to discuss future issues by check- ing urgent matters from the discussion records. We call statements including advice or requests proposed at previous meetings “task statements” and propose a method for automatically extracting them. With this method, based on certain semantic attributes and linguistic characteristics of statements, a statistical machine learning model is created using logistic regression analysis. A statement is judged whether it is a task statement according to its probability. We also developed a method that maintains the extraction accuracy by using the discussion mining system and its extension on the basis of task statement extraction over a long period. Speciﬁcally, we constructed an initial discriminant model of task statements and then applied active learning to new meeting minutes to improve the extraction accuracy. Active learning also has the advantage of reducing labeling costs in supervised machine learning. We explain the improvement in extraction accuracy and reduction in labeling costs with our method and conﬁrm its effectiveness through simulations we conducted. Keywords Discussion mining · Discussion structure · Summarization · Task discovery · Statistical machine learning · Logistic regression analysis · Active learning\\n\\n© Springer Nature Singapore Pte Ltd. 2019 K. Nagao, Artiﬁcial Intelligence Accelerates Human Learning, https://doi.org/10.1007/978-981-13-6175-3_2\\n\\n19\\n\\n20\\n\\n2 Discussion Data Analytics\\n\\n2.1 Structure of Discussion\\n\\nWe can think of discussion as a form of communication with some form of rules attached. I believe this because in comparison to daily conversation, there are many more things we must think about and decide ahead of time in regards to what we plan on discussing. For example, should I expand on the current topic or conclude my point and prepare to move to another topic. Therefore, we decided to divide the remarks in the discussion into two types such as “start-up” and “follow-up”. Start-up remarks are those that speak about new topics, and follow-up remarks are to continue speaking on the current topic. This classiﬁcation is done manually at ﬁrst, but it can be done automatically by applying a machine learning technique.\\n\\nAs another constraint, we decided to regard the discussion as a collection of tree structures. A tree structure is a data structure that hierarchically derives from a single point called a root to multiple points (nodes). One tree structure expresses the structure of discussion on one topic. At this time, each point of the tree structure, that is, a node is a statement or a remark, and a link connecting a node and another node is a relation between statements.\\n\\nBy using a tree structure, various calculation algorithms can be applied. For exam- ple, by sorting nodes based on the distance from the route or by comparing the number of nodes that have the same node as a parent (sibling nodes having a direct relationship to the route), it is possible to select a certain topic to investigate more deeply.\\n\\nWe also associate multiple discussions by generating a graph structure with links between multiple tree structures, using the materials (presentation slides like Pow- erPoint) used in those discussions. The difference between the tree structure and the graph structure is that there is no route in the graph in general, and there is not necessarily a parent node for each node (to be precise, the tree structure is a graph structure with some restrictions).\\n\\nBy structuring the discussion in this way, it can be used as the data to be subjected to scientiﬁc analysis. There are various things that you miss if you catch the discussion as a collection of just statements. For example, what relationships exist between which statements? If a person asks a question and another person answers it, there is a clear relationship between the question and the answer. Another person sometimes gives a supplementary explanation to someone’s statement. Between these statements, there is a relationship that the latter supplements the former (adds information). Structuring the discussion is to put a relationship between the statements in this way.\\n\\nIt can be said that the basic ability of discussion and thus communication is the ability to have such statements related to other statements. If this ability of someone is low, that person will confuse the surroundings and will lead to spend time wasting just to say things that are not much related to others.\\n\\nHowever, it is inappropriate to say only opinions to other people’s opinions, such as “I think so”. Except for deciding by majority vote, it is often insufﬁcient to express only the intention to agree. Again, you should say something like adding information. In such cases where you only agree, it is better not to speak, but just to add attributes to the remarks to be agreed. The structuring of the discussion includes not only relating\\n\\n2.1 Structure of Discussion\\n\\nother statements to the statement but also attaching attributes to some remarks (e.g., agree or disagree).\\n\\nAttributes for the statements include evaluation of the statements. In other words, we add points (scores) to the statements of others. This is different from showing the attitude of agreeing or opposing the statements, judging whether the quality of the statement is high and attaching evaluation attributes.\\n\\nBy saying something related to a certain statement, or by listening to and evaluat- ing certain statements and so on, the structure of the discussion becomes articulate, and the speakers’ ability of discussion will rise. Therefore, the structure of discussion is effective not only for humans but also for machines, and machines will support humans. There is a technology called data analytics which is used to make machines utilize structured data. It is also a technique for humans to better understand the characteristics of these data.\\n\\nThere is also a technology called natural language processing for analyzing human language. We also use natural language processing and data analytics to analyze and evaluate discussions.\\n\\nNow, let’s take a closer look at the mechanism that we invented.\\n\\n2.2 Discussion Mining System\\n\\nSeminar-style meetings that are regularly held at university laboratories are places where exchanges of opinions on research content occur. Many comments on future work are included in their meeting records. However, as discussions at meetings are generally not recorded in detail, it is difﬁcult to use these for discovering useful knowledge. Our laboratory, on the other hand, has been developing and operating a system that systematically records the content of face-to-face meetings with metadata and achieves support, such as that from reviews of discussion content (Nagao et al. 2005). Although it is essential to review tasks to set new goals in research activities, their existence may be concealed in many other statements in the minutes.\\n\\nTherefore, we have proposed a system to support task execution in student’s research activities by combining a mechanism called discussion mining (hereinafter referred to as DM) that realizes data mining on the contents of the discussion and a machine learning method called active learning, and we have developed a mechanism to operate it for a long time.\\n\\nIn our laboratory at Nagoya University, we have used this DM system to record detailed meetings in the laboratory for over ten years. This system enables all par- ticipants to cooperate to create and use structured minutes. This system is not fully automated, i.e., the secretary manually writes the contents of the speech, and each speaker tags his/her speech. Therefore, we can generate data with high accuracy.\\n\\nAs mentioned at the beginning of this chapter, there are two kinds of tags given to remarks, one tag is a newly introduced topic to introduce new topics, and the other is a tag of continuous talk continuing the topic already discussed. In the case of continuing utterances, it is necessary to clarify from which remarks they continue.\\n\\n21\\n\\n22\\n\\n2 Discussion Data Analytics\\n\\nFig. 2.1 Physical conﬁguration of discussion mining system\\n\\nWe see that discussion takes on a tree structure form as the speakers always attach attributes to their speech and the related speech of others. This tree structure is visualized in real time during the meeting and it is used to overlook the discussion. The meeting style supported by the DM system is that the presenter explains a topic while displaying slides, and question-and-answer with the meeting participants is either conducted during or at the end of the presentation.\\n\\nSpeciﬁcally, using multiple cameras and microphones installed in a discussion room, as shown in Fig. 2.1, and a presenter/secretary tool we created, we record the discussion content. In the center of the discussion room, there is also a main screen that displays the presentation materials and demonstration videos, and on both sides, there are subscreens for displaying information on and images of the participants who are currently speaking.\\n\\nThe DM system records slide presentations and question-and-answer sessions including participants while segmenting them in time. As a result, content (discussion content), as shown in Fig. 2.2, is recorded and generated.\\n\\nEvery participant inputs metadata about his/her speech using a dedicated device that is called a discussion commander, as shown in the lower right of Fig. 2.1. Partici- pants who speciﬁcally ask questions or make comments on new topics assign start-up tags to their statements. Also, if they want to speak in more detail on topics related to the immediately preceding statement, they provide a follow-up tag. Furthermore, the system records the pointer location on slides, designates the location/time for the slide and information, and has a button for agreeing or disagreeing in relation to statements made during the presentation and question-and-answer session. Marking information on important statements is also recorded.\\n\\n2.2 Discussion Mining System\\n\\nFig. 2.2 Structured discussion content\\n\\nFig. 2.3 Discussion browser\\n\\nWe also developed a system for searching and viewing recorded data. In this discussion-content-browsing system, a user can search the contents of the agenda from the date and participant information, view past discussions like the ongoing debate, and effectively visualize the state of the discussion, as shown in Fig. 2.3.\\n\\n2.3 Structuring Discussion\\n\\nMethods of acquiring metadata about meetings include a method using an automatic recognition technique, such as a meeting browser (Schultz et al. 2001) and a method of human input using devices and tools such as a conversation quantizer (Nishida 2007). The amount of human effort when acquiring metadata is very small with\\n\\n23\\n\\n24\\n\\n2 Discussion Data Analytics\\n\\nthe former method, but it is currently difﬁcult to automatically record all necessary information on a computer. Although it is possible to search for the keywords of statements, they do not contain sufﬁcient information on the content to be understood when browsing. Therefore, we adopted a method in this research in which humans and machines cooperatively input these metadata.\\n\\nIn the DM system, the presenter uploads a presentation slide by using a special tool and the system automatically transmits the slide information when the presenter changes the slide page. In addition, all the participants use the portable discussion commander. The start time for the statement and the statement type are recorded, in addition to the speaker ID and the seating position of the speaker, by placing this device on the top. The end time for the statement is input by pressing a button on the device. The system segments video/audio information on each statement by acquiring the start/end times of the statement. In addition, it is possible for a user to express an attitude (agree or disagree) with respect to the statement by pressing a button on the device or to mark a statement that has an important meaning to himself/herself. The DM system is provided with a statement reservation function as a mech- anism for controlling the order of statements by participants to prevent crosstalk. When someone signals his/her intention to speak by holding the discussion comman- der above them, his/her name with the type of statement is added to the statement reservation list, and his/her speaking turn automatically shifts to him/her when the immediately preceding statement ends.\\n\\nThis statement reservation function is not only used to control the order of state- ments but also to create a discussion structure that reﬂects human intentions. If the system only uses the statement type without using the statement reservation function, the discussion structure that is created will be a list structure starting from the start- up statement. However, the discussion structure when multiple participants express opinions from various perspectives on the content of one statement is a tree struc- ture rather than a list structure. Therefore, when a follow-up reservation is added while someone is speaking, the DM system generates link information between the follow-up statement and the ongoing statement. In other words, when multiple reser- vations are added during speaking, a tree structure in which a plurality of follow-up statements is repeated is automatically created for one statement.\\n\\nThe root of the tree structure in this case is a start-up statement, where all the others are follow-up statements. If several follow-up statements are added at the same time for one statement, the branches of the tree increase. Follow-up statements are attached to the preceding statement, which deepens the tree structure.\\n\\nIn addition, the secretary records the contents of the statements using a special tool. This tool is linked with the discussion commander of each speaker, and when the speaker starts speaking, the “speaker and speech type” node is automatically generated in the secretary tool. By selecting this node, the secretary can efﬁciently record the contents of the speech during the discussion.\\n\\n2.4 Summarization of Discussion\\n\\n2.4 Summarization of Discussion\\n\\nWe consider important utterances to have a large impact on the thoughts and opinions of participants, to encourage active discussion and to skillfully summarize ideas from the discussion up until that point. What can be considered as factors that affect the importance of statements are the number of branches of that statement (the number of follow-up statements following the statement), whether it is on a longer thread, the social position of the speaker, and the cumulative number of follow-up statements. Although we wanted to determine what statements were important, start-up state- ments were not always important, and statements by speciﬁc speakers were not always consistently important.\\n\\nTherefore, we propose an algorithm to discover important statements by spreading activation (Nagao and Hasida 1998). This is based on a network where nodes are statements and links are introduced based on the structure of the discussion segment (start-up/follow-up tags) and links based on the pointer referents (links of statements having the same referred object are linked).\\n\\nSpreading activation is a model in which the activity value of a node diffuses to a neighboring node through a link in a network represented by nodes and links, as outlined in Fig. 2.4. In other words, the activation values of nodes closer in distance to a node with a higher activation value are also higher, and conversely, a node whose distance is farther is also lower in activation value. By applying this method, it is possible to calculate the relative activation value of each node for all the nodes in the network.\\n\\nSpeciﬁc methods of calculation are as follows. First, we created a matrix A that represents the presence or absence of a link between each node by assuming that the number of nodes existing in the network is n.\\n\\nHere, the value of (i,j) is one if there is a link between nodes i and j, and zero if it does not exist. We now create matrix W that is related to the weight of spreading activation by using matrix A. Matrix W is an n × n matrix obtained by dividing the value of each row of A by the number of nonzero elements of that row (this is done to\\n\\nFig. 2.4 Spreading activation\\n\\n25\\n\\n26\\n\\n2 Discussion Data Analytics\\n\\nconverge spreading activation). Matrix C that represents a constant activation value spreading from the respective nodes to the whole network is next introduced (red arrow in Fig. 2.4).\\n\\nHere, the value of ci is the external input value of the constant given to node i. Then, by solving the recurrence formula, X 0 (cid:2) C X t+1 (cid:2) W · X t +C, the activation value in the network at the time of step t (=0, 1, 2) is obtained. Here, xt,i represents the activation value of node i at step t. Finally, the activation value of each node in the network is calculated as t → ∞. Also, when t → ∞, the above recurrence formula can be expressed as X (cid:2) W · X + C so that X (cid:2) (E − W )−1 · C, where E represents an identity matrix).\\n\\nWe implemented a mechanism on the browser that selects important statements according to the purpose of browsing to efﬁciently browse the recorded discussion. We speciﬁcally constructed a network where nodes from each statement from the link information were obtained from the discussion structure and pointing acts, calculated the activation value given to each node from the agreement/opposite button and marking, and used the spreading activation algorithm. We then ranked importance ranking and ﬁltered the statements.\\n\\nFor example, if the follow-up statement of statement i is j, there is a link between statement i and statement j. By repeating this, a network of statements is formed for each discussion segment starting with the start-up statement. Further, the links of all the start-up statements in each discussion segment are linked with a certain virtual node. All the discussion segments recorded from this are represented by one network structure. We can then execute the spreading activation algorithm on this network structure and ﬁnd the relative activation value for each statement. Values that take into consideration various metadata acquired at a meeting are used as external input values to be given to each node. Speciﬁc metadata include statement types, button information for agree/disagree, marking information, speaker names, and keywords included in the object. Since the start-up statement is a premise of follow- up statements in the same discussion segment, it is set so that the activation value is higher than that of the follow-up statements.\\n\\nThe users of the discussion browser (Fig. 2.3) use the user interface to ﬁnd how much importance is given to each item of metadata (Fig. 2.5). They can interactively use this interface and discover a set of statements that is suitable for their own reading purposes. For example, if a user increases the importance of the start-up statements, he/she can efﬁciently discover the statement set that has the same topic as the start- up statements, and the statement set that gained numerous agreements when the importance of the agree button information was increased by the user.\\n\\nBy using this system, the user can view the summary of the discussion and the video of each statement. We believe that the discussion that we usually do is very important, so we will try to make effective use of minor content as much as possible. However, as content increases, it will become more difﬁcult to look at each discus- sion in detail. Even though it is possible to automatically extract important parts by summarization, it is still troublesome to look across multiple meeting contents. Therefore, it is necessary to have a mechanism that automatically discovers useful information from the discussion.\\n\\n2.5 Task Discovery from Discussion\\n\\nFig. 2.5 Parameter setting window in discussion browser\\n\\n2.5 Task Discovery from Discussion\\n\\nRemembering past discussion content helps us to seamlessly carry out future activ- ities. For example, presenters in laboratory seminars can remember suggestions and requests about their research activities from discussion content that has been recorded in detail. The meeting content contains useful information for the presenters, but it is onerous to read the information. As necessary information is concealed in a large quantity of statements, it is not easy to ﬁnd. This is problematic if past discussions are not being reviewed, even for other speakers, and not only presenters. Therefore, it is necessary to extract information that concerns unsolved issues from previous discussions. We have called statements that include future tasks “task statements”. We developed a method of statistically determining whether statements were about future tasks, i.e., task statements (Nagao et al. 2015). Some attributes, including linguistic characteristics, structures of discussions, and speaker information, were used to create a probabilistic model.\\n\\nA task statement can include any of three types of content:\\n\\n1. Proposals, suggestions, or requests provided during the meeting: the presenter has determined that they should be considered.\\n\\n2. Problems to be solved: the presenter has determined problems that should be solved.\\n\\n3. Tasks not yet carried out before the meeting: the presenter has occasionally already noted such tasks.\\n\\nCandidates of task statements are fragments of a discussion chunk, which was explained earlier. A typical chunk is created from one or more of the questions and\\n\\n27\\n\\n28\\n\\n2 Discussion Data Analytics\\n\\nFig. 2.6 Candidates for task statements\\n\\ncomments of the meeting participants and the presenter’s responses to them. A coher- ent piece of discussion content related to tasks consists of questions/comments and their responses. Thus, “participants’ questions/comments + presenter’s responses” are a primary candidate and a target of retrieval. “Participants’ questions/comments and no response” is a secondary candidate. Figure 2.6 has an example of candidates for task statements.\\n\\nBelow, as an example of task statements manually discovered from our discussion,\\n\\nwe list participants’ statements and their response to them:\\n\\n(1a)\\n\\n(1b)\\n\\n(2a)\\n\\n(2b)\\n\\n(3a)\\n\\n(3b)\\n\\nI think that there is a mechanism of dynamic prediction, such as using the phrase in the PowerPoint material. I really want to use the current slide. The statement 1a advises the presenter and the speaker in the statement 1b expresses his willingness to do “I really want to …”, which applies to Condi- tion 1 described earlier. I am glad if there is a function that can move immediately by pressing a speciﬁc number. If you enter a number as it is, it conﬂicts with other inputs and when pressed together with Shift, etc., it seems to conﬂict with other shortcuts. In statement 2a, participants inform the presenter of the request, while in state- ment 2b the speaker mentioned the problem in realizing the request and it has not been decided that it should be implemented at this point. Condition 2 is applicable because it includes content to consider whether to implement or how to implement it. If you are targeting students of Nagao Lab, you think that you are not accus- tomed to typing in predictive transformations, so make opportunities to get used to that predictive transformation, and think that if you use the secretary tool with familiarity it will be used so much. How about that? I plan to have a chance to get used to it. The statement 3a states that the participant suggests to the presenter in remark 3a and the speaker in statement 3b states that the proposal is scheduled to execute, and Condition 3 can be applied to it.\\n\\nDiscovery of task statements is the result of the so-called supervised learning where a computer learns by giving information for discrimination (called teacher\\n\\n2.5 Task Discovery from Discussion\\n\\nFig. 2.7 Overall process of task statement extraction\\n\\nsignals) by humans, and a set of sample data for discrimination of past discussions (called a training dataset).\\n\\nUsing the past data of the discussion as training data, we will discover the task statement by the following algorithm. More precisely, we assign the probability of judging that it is a target utterance of a set of candidate utterances.\\n\\n1. Probabilistic model based on logistic regression analysis is created using correct answer data (teacher signal) created by hand. Model parameter estimation is performed in the manner described in the following section.\\n\\n2. Calculate the probability that the set of statements in the latest discussion is a task statement by using the created probabilistic model.\\n\\n3. Extract statements whose probability value exceeds a threshold (for example, 0.5) as task statements.\\n\\nThis procedure is shown in Fig. 2.7. In generalized linear regression methods including logistic regression analysis, there are concepts such as regularization for suppressing over learning (also called overﬁtting, over adaptation to training data and failing to adapt well to other data). In addition, we adopted the logistic regression analysis for this problem because it has high scalability and can be easily combined with other active-learning methods I will mention later.\\n\\nSome of the past meeting content was manually analyzed to ﬁnd characteristics that could be used as clues to extracting task statements. The survey data included 11 types of meeting content and 598 groups of statements (candidates). Each presenter at the meetings manually selected task statements from each type of content.\\n\\nAs a result of manually extracting task statements in the survey data, 246 task statements were found, which corresponded to 41.1% of all candidates. We analyzed the characteristics of the task statements by comparing such percentages. For exam- ple, statements made by teachers had a higher overall percentage of task statements. Therefore, speaker attributes were helpful in calculating the probabilities of task statements.\\n\\nAs was explained earlier, presenters used their discussion commanders to mark statements that they wanted to check later during the meetings. We investigated\\n\\n29\\n\\n30\\n\\n2 Discussion Data Analytics\\n\\nthe effectiveness of marking for discriminating task statements by calculating the percentage of marked task statements in all task statements. The percentage of task statements that were marked were 73.4%, which was higher than that of the task statements for all candidates.\\n\\nWe found distributions for the respective characters of a presenter’s and partic- ipants’ statements to examine whether there were characteristic tendencies in the number of letters (characters) in task statements. We divided the number of charac- ters into ﬁve groups and calculated the percentages of task statements in each group. The percentage of task statements in the participants’ statements increased when the number of characters increased. This was because the number of characters in their statements increased when the participants were making concrete requests and giving advice. In comparison, the number of characters with higher percentages of task statements was 20 or less for the presenter’s statements. The more characters there were, the smaller the percentage of task statements. We believe that if a pre- senter accepted the requests or advice from participants who were present, his or her responses would have tended to be brief.\\n\\nWe also investigated the types of sentences included in the task statements. The percentage of task statements in the participants’ statements was higher when sen- tences were in present tense and in declarative form (56.1%). This was due to the fact that a large amount of advice or requests were in the pattern of “should be …” or “I want to …”. The percentage of task statements for the presenter’s statements in past tense and in declarative form was low (29.2%). This was because presenters did not tend to use sentences in past tense when they talked about future tasks. In addition, the percentage of task statements made by presenters in past tense and in interrogative form was 0%.\\n\\nMorphemes and collocations of morphemes in statements were also important features. We generated a morpheme bigram of nouns, verbs, adjectives, and auxiliary verbs in the survey data by calculating the number of occurrences of the morphemes. We detailed this topic later.\\n\\nWe then determined a feature of morphemes and their bigrams of statements if their occurrences had exceeded certain thresholds. The selected nouns speciﬁcally had a percentage of occurrences that was greater than or equal to 0.5% of all nouns, and the selected verbs also had a percentage that was greater than or equal to 0.5% for all verbs. Morpheme bigrams were selected if their percentages were greater than 0.05% for all morpheme bigrams. These selected morphemes and bigrams were used as features for discriminating task statements.\\n\\nThree main features were selected based on these results obtained from the survey\\n\\nto create a prediction model.\\n\\n1. Attributes of presenter. 2. Features of participant’s statements:\\n\\n(a) Start time and duration of statements, (b) Speaker type (teacher or student), (c) Statement types (start-up or follow-up), (d) Marking (zero or one),\\n\\n2.5 Task Discovery from Discussion\\n\\n(e) Length (no. of characters), (f) Sentence types, (g) Morphemes and morpheme bigrams, and (h) Responses by presenter (zero or one).\\n\\n3. Features of presenter’s response:\\n\\n(a) Marking (zero or one), (b) Length (no. of characters), (c) Sentence types, and (d) Morphemes and morpheme bigrams.\\n\\nWe used answers (zero or one) to ﬁve questions for the values of sentence type\\n\\nfeatures:\\n\\n1. Does the statement include a sentence in past tense and in declarative form? 2. Does the statement include a sentence in present tense and in declarative form? 3. Does the statement include a sentence in past tense and in interrogative form? 4. Does the statement include a sentence in present tense and in interrogative form? 5. Does the statement include a sentence of another type? Regarding the start time of the statements, we divided the time of the whole meet- ing into ﬁve sections every 20% and used the value in which section the participant’s speech start time occurred.\\n\\nRegarding the number of words in the statements, the distribution of the number of characters of each participant’s speech and the speaker’s statements were found and divided into ﬁve sections every 20%, and the value for the number of characters in the section the speech is in is used.\\n\\nIn addition, we used the answers of the following items as values for the types of\\n\\nsentences included in the statement.\\n\\nWhether the statement only appears in other sentences. Whether it is a statement that ends with a noun or noun phrase or not. Whether or not it is a statement that is just composed of a noun or noun phrase Whether it is a statement that doesn’t end on a noun or noun phrase, or does end on a noun phrase, or is just a statement.\\n\\nWhether or not it is a sentence that does not end on a noun phrase. One feature is represented as a set of variables with values of 0 or 1 when vector- izing. This variable is called a dummy variable. In other words, when a feature has m categories (cases), it is represented by m variables. This (i (cid:2) 1, …, m) is a dummy variable. If it corresponds to the ith category, it is represented by (i (cid:5)(cid:2) j). Since m dummy variables have redundancy, m − 1 dummy variables are usually used. For example, when we characterize gender, there are only two cases of male or female, so we have 1 dummy variable, 0 for male and 1 for female. An arbitrary real number can be divided into several areas and can be represented by the number of the area minus one dummy variable.\\n\\nThe creation of a probabilistic model for judging such task statements is based on supervised learning where human beings learn by giving discrimination classes\\n\\n31\\n\\n32\\n\\n2 Discussion Data Analytics\\n\\nteaching signals to machines. And its accuracy depends on the training dataset used for learning. Of course, if there is a clear difference compared with general statements, the task statement may be able to create a discrimination rule without doing machine learning. However, if the content of the statements is diverse, it is difﬁcult to apply the statement to a speciﬁc pattern.\\n\\nTenfold cross-validation was applied to the extraction results to test and conﬁrm the effectiveness of the proposed method. The data used for veriﬁcation included 42 types of meeting content and 1637 groups of statements (candidates). Each presenter created correct data for task statements for each type of meeting content as well as the survey data provided earlier. The data used for veriﬁcation were completely different from the survey data.\\n\\nWe conﬁrmed the effectiveness of the proposed method in terms of high precision (index for extraction accuracy), recall (index for extraction leakage), and the F- measure (harmonic mean of precision and recall).\\n\\nThe extracted results from the task statements with the proposed method were a precision of 75.8%, a recall of 64.2%, and an F-measure of 69.5%. In comparison, the results for the three alternative methods of extraction were as follows. The selection of statements that were marked by the presenter had the highest precision (68.9%), and that of the statements from teachers or statements that were marked by the presenter had the highest recall (44.1%) and F-measure (48.7%). The approach we proposed obtained the highest values compared with these other extraction methods.\\n\\nAs was previously explained, the proposed method could calculate the probabil- ities of candidates for a task statement by using a generated probabilistic model. A candidate whose probability value exceeded a certain threshold was extracted as a task statement. We ﬁrst set the threshold value to 0.5. It was not guaranteed that this value would be optimal. Therefore, we reevaluated the outputs of the system by low- ering the threshold by 0.1 from 0.5. As we found that the F-measure at a threshold of 0.4 was highest (71.4%), task statements should be extracted in the future by setting the threshold to 0.4.\\n\\nSince data analytics is aimed at deriving generality that cannot be simply found from actually obtained data, human beings teach answers to machines on a steady basis as training data, and machines learn gradually by more accurate parameter estimation. We must wait for it to become smarter.\\n\\nTherefore, to improve discrimination accuracy, it is necessary to increase the amount of training data. However, it is very labor intensive to give a teacher signal to all statements of the enormous discussion recorded by the discussion mining system. Also, it would be desirable for the presenters themselves to be responsible for teacher signaling tasks that should best understand the content of the discussion. In this way, giving a teacher signal is a task requiring speciﬁc human knowledge, and it takes time. I will explain how to solve this problem later.\\n\\n2.6 Active Learning for Improving Supervised Learning\\n\\n2.6 Active Learning for Improving Supervised Learning\\n\\nGenerating a discrimination model of task statements is based on supervised learning, where a machine learns from humans who provide a discrimination class of teacher signals to the machine, and the extraction results depend on the training dataset on which machine learning was executed. Therefore, it is necessary to increase the amount of training data to improve extraction accuracy especially when the amount of usable data is relatively small, as it was in this research; however, the assignment of teacher signals to the statements of all minutes recorded by DM is very costly. In addition, it is preferable for the presenter who best understands the content of the presentation to be in charge to minimize the number of misjudgments from teacher signals, i.e., it is also a task that requires speciﬁc human knowledge. Teacher signals of task statements cannot easily be generated.\\n\\nAnother issue in supervised learning is feature changes in objects to be extracted over time. There is no problem if the characteristics of the extraction target are completely invariant, but as new students enter the lab and progress with research activities transforms each year, the characteristics of task statements to be extracted change over time.\\n\\nThe problem with feature changes over time has often been discussed. A spam mail ﬁlter is a good example. The techniques of spam mail are becoming increasingly more sophisticated, and unless the discrimination model is updated, it cannot adapt to the characteristics of new spam mail, which decreases the accuracy of discrimination (Georgala et al. 2014).\\n\\nOne concern with text minutes in DM is that there are differences in wording due to there being different secretaries who manually input text. Since strict rules are not deﬁned for secretaries, the wording depends on the discretion of each person in charge, and the degree of sentence summarization also differs. Since the lan- guage information included in statements mostly affects extraction accuracy in the task statement extraction model, it is necessary to focus on feature changes due to differences in wording.\\n\\nThe discriminant model should always be updated when the amount of data to be analyzed is increased. However, it is very difﬁcult to label all data when there is an increasing amount of data, and labeling incurs large costs. We used active learning (Settles 2010, Shimodaira 2000, Sugiyama and Kawanabe 2012, Liu et al. 2015) to solve this problem, which was used to attempt to improve extraction accuracy for the limited dataset we obtained.\\n\\nActive learning can be implemented as an algorithm in which a sample that makes the greatest contribution to updating a discrimination model from a large number of samples without teacher signals is selected by a machine, and a teacher signal is given to it to minimize human effort. This process efﬁciently updates models. All statements in task statement extraction in the minutes of a seminar correspond to samples without teacher signals; some groups of statements are selected using an active-learning method, and the selected statement group is assigned teacher signals by students who are presenters at the corresponding seminar. The research-activity-\\n\\n33\\n\\n34\\n\\n2 Discussion Data Analytics\\n\\nsupport system that will be described later encourages students to reﬂect on the seminar after their presentations and obtain feedback on the automatically selected task statements.\\n\\nApplying this active-learning method to task extraction in this way both simulta- neously solves problems of cost in teacher-signal assignment and feature changes in task statements over time.\\n\\nActive learning is a technique frequently used in several ﬁelds such as natural language processing and biostatistics since expert knowledge concerning the assign- ment of teacher signals is required and the costs of data collection and teaching-signal extraction are very high. The technique is also suitable for improving the accuracy of task statement extraction in which the cost of teacher-signal assignment is also very high.\\n\\nA ﬁve-step procedure is repeated to apply the active-learning technique to DM:\\n\\n1. Create a task statement extraction model using a set of statements with teacher signals to determine whether or not it is a task statement.\\n\\n2. After a presentation has been made at a seminar, carry out task statement extrac- tion by using the recently created minutes and calculate the probability value that each statement is a task statement.\\n\\n3. Select the target set of statements to which the teacher signal is to be assigned by active learning.\\n\\n4. Display the results for the selected statement set and ask the system to provide teacher-signal feedback to the user.\\n\\n5. Add the statement set with teacher signals that have been obtained to the training data of machine learning.\\n\\nBy updating the task statement extraction model by repeating these ﬁve proce- dures, it is not only possible to improve the accuracy of extraction by increasing the amount of training data but also to constantly adapt to feature changes of task statements over time.\\n\\nInformation density is a representative algorithm of the active-learning technique that takes into account the density of the feature space as a sampling reference (Settles and Craven 2008). The strategy with this algorithm is to consider that there is a large amount of information with higher density data; therefore, the beneﬁts of providing a teacher signal are thought to be great.\\n\\nUncertainty sampling in giving a teacher signal to the most ambiguous classiﬁca- tion is also a typical method of active learning (Lewis and Gale 1994). For example, considering the classiﬁcation problem as shown in Fig. 2.8, the data to which the teacher signal should be given are different between information density and uncer- tainty sampling. In other words, data close to the classiﬁcation boundary are selected for uncertainty sampling, but with information density, data in which there are many data around (data with high data density) are selected.\\n\\nSince information on teacher signals is not used for density calculation, it is possible to use a large number of samples without teacher signals for learning. As the DM project has been continuing for about 10 years, and the accumulated content of discussion is huge, it is very convenient to apply this method.\\n\\n2.6 Active Learning for Improving Supervised Learning\\n\\nFig. 2.8 Two typical methods of active learning\\n\\nFig. 2.9 Feature histogram\\n\\nAlthough the practicality of information density is high when there is a large amount of noise data, it may adversely affect the weighting of model parameters. Morpheme information on statements is used in the task statement extraction model, and extremely long or short statements can be noise. For example, short statements such as “I will consider it” and “I do understand” that frequently appear in responses by presenters have fewer morphemes that are selected as modeling features, and very long statements that have many morphemes also tend to be noisy samples.\\n\\nOur proposed method used a weighting algorithm in this research to cope with such problems that was based on a histogram of feature information. Since all the features used in the task statement extraction model are binary variables, we can deﬁne a histogram in which the number of features that have the value one in a sample is classiﬁed as a class. We call it a feature histogram. Figure 2.9 plots the feature histogram created for the past 492 meeting minutes.\\n\\n35\\n\\n36\\n\\n2 Discussion Data Analytics\\n\\nFig. 2.10 Comparison of accuracy transition\\n\\nOur weighting algorithm was based on the ratio of the frequency of the number\\n\\nof samples without teacher signals as a weight as\\n\\narg max x∈U\\n\\nφx ×\\n\\n(cid:2)\\n\\n1 U\\n\\nU(cid:3)\\n\\ni(cid:2)1\\n\\nsim(x, xi ) × freq(bin(x))\\n\\n|U |\\n\\n(cid:4)\\n\\n(2.1)\\n\\nHere, φx is the score of sampling, U is the set of samples without teacher signals, and sim(x, xi ) is the cosine similarity between x and xi . The freq() is the frequency of a suggested bin, and bin(x) is the bin to which x belongs.\\n\\nfreq(bin(x)) |U |\\n\\nis a new weighting, and the other part of Formula (2.1) is the same as the calculation for information density. This weighting makes it possible to give a lightweight to statements of extreme length that can have a negative effect on information density, and more effective sampling to improve extraction accuracy can be expected.\\n\\nSince teacher-signal feedback improved the accuracy of task extraction, we per- formed a simulation of what kind of accuracy transition could be observed depending on different data-sampling methods for active learning for a situation in which ten statements (for full sampling of all statements) were added as training data when one minute of the meeting was created. The transitions in the value of the F-measure (or F1 score that is the harmonic mean of precision and recall) were compared with six methods in a sevenfold cross-validation (Fig. 2.10) for the data of the minutes recorded by the DM system (data group: 1637). • Proposed sampling: formula (1) (proposed method). • Uncertainty sampling: conventional method (Lewis and Gale 1994).\\n\\n2.6 Active Learning for Improving Supervised Learning\\n\\nInformation density: conventional method (Settles and Craven 2008). • Expected error reduction: conventional method (Roy and Mccallum 2001). • Random sampling: statements are randomly selected for sampling. • Full sampling: all statements are selected to assign teacher signals. Full sampling indicates the limits of improving extraction accuracy as a reference value, where the amount of training data is about four times that of the other methods. The proposed method can maintain high extraction accuracy compared with the method excluding full sampling by reducing noise on the feature space, which has not been considered in conventional methods.\\n\\n2.7 Natural Language Processing for Deep Understanding\\n\\nof Discussion\\n\\nIn the analysis of discussion, morpheme unigram and morpheme bigram are each used as features. Speciﬁcally, for morpheme unigrams, the number of occurrences of each of nouns, verbs, adjectives, and auxiliary verbs was calculated from past minutes, and those that exceeded a certain value were taken as features. Speciﬁcally, I used as features, nouns and verbs that occurred with a ratio of more than 0.5% out of all nouns and verbs, and adjectives and auxiliary verbs that occurred with a ratio of more than 1.0% out of all adjectives and auxiliary verbs. For morphological bigrams, we used morpheme pairs (two consecutive morphemes) with a percentage of total morphological bigram total of 0.05% or more as features. This is an input vector by using a dummy variable and setting its value to 1 when a morpheme or a morpheme pair as a feature exists in the statement, and setting the value to 0 if it does not exist.\\n\\nN gram is a co-occurrence relationship (collocation) of a language unit in which a certain language unit (character, morpheme, etc.) in the document occurs adjacent to each other such as two linguistic units, three linguistic units (generally N languages unit). It can be thought of as representing a kind of feature of the document. The N grams of two languages and three languages are called bigrams and trigrams, respectively. Also, the histogram (frequency distribution) of a single language unit is called unigram.\\n\\nN grams when language units are morphemes are called morpheme N grams. Again, as N increases, it becomes enormous data, which makes management difﬁcult. Therefore, bigrams and trigrams are mainly used.\\n\\nWhen M is the total number of linguistic units (e.g., characters), the total number of possible combinations of consecutive N units is M N . In the method of preparing a combination table of M N , the number of occurrences (frequency) is checked from the text. In this case, the computation of N grams takes time with increasing N, and as a result, the data become extremely huge. It is extremely difﬁcult to store the resulting data created by this method.\\n\\n37\\n\\n38\\n\\n2 Discussion Data Analytics\\n\\nAll morphemes are always given the attribute of a part of speech (except for words that are not in the dictionary, that is, unknown words). In most cases, the part of speech is not enough to express the meaning of words. For example, it is understood that the verb expresses “some kind of action”, but to know the speciﬁc meaning, it is not enough with the part of speech information alone.\\n\\nThen, how about using words themselves as meaning units? A word whose mean- ing is like another word is called a synonym. We think that the set of synonyms represents a certain meaning (or concept). It is unavoidable to use it as a unit for expressing the meaning of a word (or morpheme) that decomposes a word, since it is only a word to tell others what he is thinking to others.\\n\\nHowever, of course, that might be a problem. It may be more convenient for you to be able to deﬁne similarities such as similar, not similar, very similar, slightly similar, and not very similar. For example, how much the meaning of “writing letters” and “speaking words” is similar, which is more similar to “drinking water” “eating fruit” or “reading a book”, etc.\\n\\nIn other words, it is necessary to consider the question of whether the meaning of words whose classiﬁcation is too rough in parts of speech is not expressed as a set of words themselves, but whether it can be expressed in a manner of high abstraction to the extent that similarity can be considered.\\n\\nRecently, machine learning has proposed a way to treat words as vectors rather than letters. Context information is used to create that vector. In other words, it characterizes the meaning of the target word by another word around the word where the word appears. Such a vector is called a distributed representation of a word. In the case of characters, I care about whether or not they match, but if you have a vector, you can calculate the distance between vectors and handle it numerically as similarity. By introducing an indicator of distance to what is difﬁcult to capture the meaning of words, it means that we can handle things more ﬂexibly. Calculation increases, but computers will get faster over time, so you will not have to worry about this indeﬁnitely.\\n\\nLet’s adopt the machine learning method. First, let’s make a word vector so that it can be applied to the learner. There is a method called one-hot that uses a dictionary. Figure 2.11 shows the mechanism.\\n\\nAs shown in this ﬁgure, numbers are added in the order of posting of dictionaries, a vector matching the size of the dictionary is created, and the value of the dimension corresponding to the index of a certain word becomes 1, and everything else becomes 0. Now you can create a one-to-one vector for every word. It is called a one-hot vector. Vectors made by one-hot are not suitable for calculating similarity because they do not use context information at all, such as co-occurrence relations. So, we devise the following ingenuity. This is a method known as word2vec, and there is not much theoretical background, but since it can be easily implemented and its effectiveness is conﬁrmed to a certain extent, it is one example of vectorization.\\n\\nWord2vec is a method of expressing all words by 200-dimensional vectors (num- ber of dimensions can be changed), and it is possible to add and subtract words by using vector representation (Mikolov et al. 2013a, b). For example, if you enter “Tokyo—Japan + France” the answer will be “Paris” and if you enter “King—Male\\n\\n2.7 Natural Language Processing for Deep Understanding of Discussion\\n\\nFig. 2.11 One-hot word vectors\\n\\nFig. 2.12 Skipgram\\n\\n+ Female” the answer will be “Queen”. This allows you to create a vector from a word by entering a corpus (actual document training data) into the neural network called skipgram shown in Fig. 2.12 and learning it.\\n\\nA skipgram takes as input w(1),…, w(T ) (w(i) is a vectorized word with one- hot; T is the total number of words appearing in the document). Skipgram is the neural network for predicting words appearing before and after of the input w(t) (t is a position of occurrence in the document). We use the stochastic gradient descent method to learn using the following formula:\\n\\n39\\n\\n40\\n\\n2 Discussion Data Analytics\\n\\nT(cid:3)\\n\\n(cid:3)\\n\\n1 T\\n\\nlog p(wt+ j |wt ) (cid:6)\\n\\narg max\\n\\nP\\n\\nt(cid:2)1\\n\\n−c≤ j≤c, j(cid:5)(cid:2)0 (cid:5) vT vwI exp wO (cid:5) vT w∈W exp w\\n\\n(cid:6)\\n\\np(wO |wI ) (cid:2)\\n\\n(cid:7)\\n\\nvwI\\n\\nHere, C is called the context size; it indicates how many words before and after the input are predicted, set to about 5. wO is the output word, and wI is the input word. In addition, vw is a vector representing the word w, and it is assumed that it is calculated considering the regularity of the context in which the word appears. The dimension of vw is set to about 200.\\n\\nThe result of the experiment is that the vector vw representing each word has the following properties. The ﬁrst is that the cosine similarity (a value obtained by dividing the inner product of two vectors by the product of the lengths of both vectors) between vectors of words that are likely to appear in the same context, such as “mathematics” and “physics”, is larger than the cosine similarity between vectors of other words such as “mathematics” and “cooking”. This can be thought of as approximating the semantic similarity between words as cosine similarity expresses the similarity between vectors in this case.\\n\\nThe other is a property called linear regularity between words. For example, as shown on the left side of Fig. 2.13, the cosine similarity between the difference vector between a vector representing “woman” and a vector representing “male” and the difference vector between the vector of “aunt” and the vector of “uncle”, or the difference vector between the vector of “queen” and the vector of “king” was very high. This predicts that the semantic commonality between these words is high. Also, the right side of Fig. 2.6 shows that the relationship between singular and plural forms of words is common among words.\\n\\nThe third property is that the vector reﬂects the “degree” of the meaning of the word. For example, as shown in Fig. 2.14, taking an average vector of good and best vectors, there is better in a word with the highest cosine similarity (in other words, better is in the middle of good and best). Likewise, it turns out that unhappy\\n\\nFig. 2.13 Linear regularity\\n\\n2.7 Natural Language Processing for Deep Understanding of Discussion\\n\\nFig. 2.14 Degree of meaning of words\\n\\nis roughly in between pleased and angry, which can be thought of as reﬂecting the degree of a certain meaning of the word.\\n\\nThe last property of a word vector is language independence. Even if the language (such as English or Japanese) differs or the dictionary for which one-hot is calculated differs depending on sufﬁciently large training data (collection of natural language documents), similar regularity was obtained. This will need to be veriﬁed in various languages (the author of the original paper conﬁrmed in English and Spanish), but it will be necessary to wait for sufﬁcient data to gather for learning.\\n\\nIn this way, the mechanism of handling the meaning of a word on a computer is greatly developed by machine learning. From now on, more appropriate vectorization method will be devised, I think that subtle nuances that were difﬁcult to handle with traditional systems are handled well and human intention will be recognized more deeply.\\n\\n2.8 Natural Language Processing for Discussion Structure\\n\\nAnalysis\\n\\nThere is a context in the discussion as well, so if you use it well, you can handle the meaning of the statement more accurately. In other words, the statement has another statement as a trigger for that, and it is related to that statement. Regarding the relationship between the statements, as already mentioned, the discussion concerning a topic is a tree structure, and that node is each statement. We apply natural language processing for this structured data of discussion.\\n\\nAs a result, you can see, for example, the following:\\n\\nWhether someone’s statements were appropriate in that context. Whether a statement by a person still contains content to be considered. Whether someone’s statements were comprehensive and were good statements.\\n\\nBased on the data obtained by discussion mining, we have constructed a machine learning model that classiﬁes statements from the viewpoint of newly introducing\\n\\n41\\n\\n42\\n\\n2 Discussion Data Analytics\\n\\ntopics or continuing previous topics. This allows you to classify speech recognition results in real time and segment them into several topics.\\n\\nFeatures used for machine learning model construction are as follows:\\n\\n(1) Whether or not the displayed slide image matches between a certain statement\\n\\nand the preceding utterance.\\n\\n(2) Whether the two previous speeches are the same as the speaker. (3) How long has passed since the last statement. (4) Similarity with the preceding utterance (cosine similarity between vectors\\n\\nwhen speaking is represented as a vector expression).\\n\\n(5) Morpheme unigram and bigram. (6) Whether the slide was changed during speaking. (7) Entity Grid (described later). (8) Length of sentence included in statement. (9) Time elapsed when pointed in the slide with the discussion commander. (10) Whether or not it coincides with the pointed object pointed to by the immedi-\\n\\nately preceding utterance when using the pointer.\\n\\n(11) Number of sentences included in the statement. (12) Whether the reference includes a referring expression (such as a pronoun).\\n\\nEntity Grid is a model based on the hypothesis that there is regularity in the dis- tribution of appearance of the elements described in the consistent text, letting the sentences in the line correspond to the elements in the sentence (Barzilay and Lapata 2008). It is expressed as a matrix. For each term, the elements of the sentence corre- spond to what is called the syntactic role in that sentence of that sentence. Speciﬁcally, there are four types: Subject (S), Object (O), Other (X), and Not Emerging (–).\\n\\nFor example, an Entity Grid like Fig. 2.16 is created from the four sentences\\n\\nshown in Fig. 2.15.\\n\\nHere, we decided to put “BELL INDUSTRIES Inc.” and “Bell” of s1 and s4, respectively, into the same column named “BELL” as we concluded that they refer to the same thing through a co-reference analysis method. Both are the subject of the sentence, so the syntax role is S. Even when referring to the same target using pronouns or the like, make similar analyzes so that they are in the same column.\\n\\nIn order to use this Entity Grid as a feature vector, we investigate the change of syntactic role of a noun in two consecutive sentences and calculate transition proba- bility. The transition probability of a syntactic role is calculated as the probability of occurrence of that transition in the entire transition. For example, in the example of the Entity Grid of the sentence of Fig. 2.15 (Fig. 2.16), since “BELL” is S in s1 and does not appear in the next sentence s2, we consider it as –, which is a transition of\\n\\nFig. 2.15 Example text (from Paper Barzilay and Lapata (2008)\\n\\n2.8 Natural Language Processing for Discussion Structure Analysis\\n\\nFig. 2.16 Entity Grid (from Paper Barzilay and Lapata 2008)\\n\\nS → –. Transitions of syntactic roles in such two sentences are all 10 (the number of elements) × 3 (the number of transitions in successive sentences, i.e., s1–s2, s2–s3, s3–s4) (cid:2) 30 times. The number of occurrences of the transition S → – is three times since “BELL” has s1 to s2 once, “rate” is s2 to s3 once, and “date” is s3 to s4 once. That is, the transition probability of S → – is 3/30 (cid:2) 0.1.\\n\\nIn order to obtain the transition probability of this syntactic role between two consecutive utterances, we focused on Japanese case particle “ha”. That is because the noun phrase before “ha” often represents the subject of the sentence. So, with respect to all the sentences included in the utterance, if the noun phrase immediately preceding the case particle “ha” is gathered and the existence of the noun phrase or the noun phrase that points to the same is conﬁrmed in the next statement, we examined the syntactic role of a noun phrase and calculated its transition probability. The total number of transitions was analyzed and counted for the entire discussion content of the same meeting.\\n\\nMachine learning was carried out by logistic regression analysis, and the training\\n\\ndata used the minutes accumulated from discussion mining described earlier.\\n\\nThe F-measure when experimenting with the test data is also listed in Fig. 2.17. The F-measure to the right of the feature in the table represents the value when the feature was not used for learning. This indicates that statements were classiﬁed with sufﬁcient precision.\\n\\nWe then aimed at extracting more advanced information from the minutes. It was necessary to analyze the language content of statements and materials in detail to achieve that purpose.\\n\\nBy the way, it can be said that inconsistent statements in the discussion are state- ments that describe topics that are different from topics up to that point. So, consider how to categorize follow-up statements as comments deviating from topics or not. Logistic regression analysis described earlier is used for classiﬁcation. In this case, we calculate the probability value that the topic is diverted and use this value for the consistency evaluation of the statement. For this purpose, in addition to the lin- guistic features obtained from the minutes, we use the meta-information given to the minutes. The features used in this method are shown below:\\n\\n43\\n\\n44\\n\\n2 Discussion Data Analytics\\n\\nFig. 2.17 Features for classiﬁcation of statements\\n\\n(1) Features based on linguistic features of text:\\n\\n– Relevance to parent statement. – Single sentence or compound statement. – Number of characters of statements. – Morpheme unigram and morpheme bigram. – Presence of subject and referring word. – Entity Grid.\\n\\n(2) Features based on meta-information attached to the minutes:\\n\\n– Whether the speaker is a student or not, whether it is a presenter or not. – Whether the speaker of the parent statement is the presenter. – Presence of marking/agreement/disagreement buttons. – Depth from the root in the discussion tree structure. – Whether or not the visual referent of the parent statement matches that of the\\n\\ntarget statement (detailed later).\\n\\n– Presence or absence of slide operation during speaking. – Time for reservation of speaking. – Presence or absence of different statements in time series between the parent\\n\\nstatement and the target statement.\\n\\n– Alternation of questioner.\\n\\n2.8 Natural Language Processing for Discussion Structure Analysis\\n\\nFor morphemes and morpheme pairs that appear during speech, the number of occurrences of nouns, verbs, adjectives, auxiliary verbs, and morpheme pairs is cal- culated by preliminary survey as in the analysis of the task statements described above, we used those exceeding a certain value for the feature. Also, since there is a report that Entity Grid is effective for evaluating text consistency (Barzilay and Lapata 2008), it is directly related to topic transformation among the syntactic role of the Entity Grid. We focused on only the transition of the theme considered as a transition probability and used it for the feature.\\n\\nThe alternation of the questioner, which is the last feature, as a matter of whether or not the questioner is different from the preceding statement pair when considering participant’s question and presenter’s response as a statement pair.\\n\\nWe implemented the above method and conducted experiments on discrimination of inconsistent statements. As a dataset, we used 53 min (discussion content) of seminar in our laboratory (number of statements: 3553). However, since the start- up statements are not subject to this case, the follow-up statements (the number of statements: 2490 cases) are subject to discrimination. As correct answer data (teacher signal), we decided manually whether a certain statement lacked consistency, and gave that attribute. 202 consecutive statements were determined to be lacking in consistency.\\n\\nIn order to evaluate the proposed method, the case where learning was carried out without using features based on the meta-information of the minutes was taken as a comparative method. For the evaluation, we used the precision and recall described above, the F-measure, and carried out the tenfold cross-validation.\\n\\nThe results of this experiment are shown in Fig. 2.18. The results of the consistency judgment by the method we proposed are higher than the case where the feature information given to the minutes is not used; an advantage was conﬁrmed in the precision, the recall, and the F-measure.\\n\\nIn addition, when learning by removing each feature by the meta-information of the minutes, the precision, the recall, and the F-measure declined in all the features, and the effectiveness of the used feature was conﬁrmed. Figure 2.19 shows the results of the top ﬁve cases where the F-measure drops greatly.\\n\\nFig. 2.18 Experiment results of consistency checking\\n\\n45\\n\\n46\\n\\n2 Discussion Data Analytics\\n\\nFig. 2.19 Feature contribution to learned model\\n\\n2.9 Correctness of Discussion Structure\\n\\nEach statement in a discussion generally contains several sentences. Therefore, when considering the structure of the discussion, it would be appropriate to think separately into the structure within the statement and the structure between the statements. The structure in the statement consists of the internal structure of the sentence contained in the statement and the relation between the sentences.\\n\\nRegarding the internal structure of a sentence, there is another analysis method for the relationship between sentences. The structure that spans multiple sentences is called a discourse structure, but there is no decisive way to analyze how to analyze it. In one example, we use a conjunction connecting sentences as a clue. If it is connected with “and” like “it is A, and it is B”, the two sentences are in a relationship of adjacency, that is, the later sentence supplements the information of the previous sentence. It is content which is semantically contradictory. In addition, “it is A, but it is B”, the two sentences are inverse relations; there is a semantically contradictory content in the sentence to be expected from the previous sentence. In addition to this, there are parallel, reason, assumption, limitation, time lapse, topic expansion, and topic convergence among others in relation to such sentences.\\n\\nIf such a conjunction is not described, it is necessary to predict the relationship between the sentences by using the contents of the two sentences (or the sentences before and after them). As for this also, a machine learning method can be considered. However, in this case, unlike syntactic analysis, it depends heavily on semantic content, so it is better to use features reﬂecting the meaning in the word context, such as the vector expression by word2vec described earlier.\\n\\nWell, next is the relation between the statements, as in the ﬁrst part of this chapter, we classify the statement with two tags “start-up” and “follow-up”, and in the case of “follow-up” statement, it is structured in a way to link to the original (parent)\\n\\n2.9 Correctness of Discussion Structure\\n\\nFig. 2.20 Discussion chunk\\n\\nstatement. We use a discussion structure tagged and linked by the speakers as a correct answer, but in the ﬁrst place, we need to verify how valid these tags and links are.\\n\\nBy the way, in the discussion mining (DM) system we developed, we are recording and analyzing all the contents of the discussion on presentations using presentation slides created with Microsoft PowerPoint. During the meeting, with the presentation slide displayed by the presenter, when the participant makes a start-up statement and the presenter or another participant makes a follow-up statement, a series of steps until the next start-up statement is made. We call this structure a discussion chunk. An example is shown in Fig. 2.20. A discussion chunk is considered a discussion on one topic.\\n\\nIn the DM system, there is a part depending on participant’s judgment when creating discussion content, so there is a possibility that human error such as miss- ing information or inputting wrong information may occur. In addition, since we developed a minute input tool that does not hinder discussion as much as possible, there is a possibility that the intention of the speakers may not be fully reﬂected in the acquired metadata. Among the metadata acquired by the DM system, the tree structure of the discussion chunk and the text summary of the statement are greatly affected by human judgment, in particular. Whether or not the statement text cor- rectly reﬂects the spoken content is thought to be able to be checked mechanically by using speech recognition which is also described in the next chapter, but as for the structure, if a human does not give a correct answer, machine learning does not work well. So, its accuracy is a big issue.\\n\\nTherefore, we conducted the following experiment to evaluate the validity of the tree structure of the discussion chunks (Nagao et al. 2005). In this experiment, we ranked the top 18 segments with many statements out of discussion chunks in discussion content created after 2007. The total number of statements included in the 18 chunks was 199 (of which 181 statements were follow-up statements), the average\\n\\n47\\n\\n48\\n\\n2 Discussion Data Analytics\\n\\nof the number of statements per chunk was 11.1, and the average of the number of speakers in the chunk was 4.6. In this experiment, the accuracy of data was evaluated by comparing the statement type and the tree structure generated by the DM system with the correct answer data created manually regarding the following items:\\n\\n(1) Follow-up statements presenting new topics:\\n\\nAs an empirical problem, it often happens that the topic gradually diverges as the discussion becomes longer. Therefore, as examining individual speech contents included in a discussion chunk with a large number of statements, there is a possibility that the contents and intent of the start-up statement as the root of that segment may include a follow-up statement that does not continue the topic. Therefore, by examining the number of follow-up statements presenting new topics in the chunk, we veriﬁed the validity of follow-up statements.\\n\\n(2) Follow-up statements with incorrect parent statements:\\n\\nThe statement reservation function of the DM system mentioned earlier gives link information between the statements based on the input time of a reserva- tion when the target statement is being made. Therefore, if the user makes a reservation after the statement has ended, the correct link information may not be given. For that reason, we evaluated the validity of the statement reservation function by the number of follow-up statements of incorrect parent statements (previous statements directly linked with the follow-up statements).\\n\\n(3) Follow-up statements with multiple links:\\n\\nIn the current statement reservation function, the number of parent statements is limited to one, but when making statements that summarize the ongoing dis- cussion in the chunk, more than one statement may be referred to. Therefore, in order to check whether the link information acquired by the statement reser- vation function is sufﬁcient, we examined the number of follow-up statements with multiple links.\\n\\nFigure 2.21 shows an example of a comparison between the discussion structure generated by the system and correct answer data. In this example, the statements cor- responding to (1) above is 6, the statements corresponding to (2) are two statements 3 and 5, the statement corresponding to (3) is the statement 4 that has links to 1 and to 3.\\n\\nCorrect answer data were created by the following procedure. First of all, in order to conﬁrm whether the contents and intention of the start-up statement serving as the root of the discussion chunk are correctly reﬂected in the subsequent follow-up statements, we asked the speaker of the start-up statement of the target chunk to select statements that deviated from the intention of the start-up statement. Next, we asked several university students that participated in the target discussion to make the correct answer for the above items (1), (2), and (3) while viewing the contents with the discussion browser mentioned earlier. Finally, we reviewed the students’ answers and decided the ﬁnal correct answer data.\\n\\nFigure 2.22 shows the results of comparing the structure of the discussion gener-\\n\\nated by the DM system with the correct answer data.\\n\\n2.9 Correctness of Discussion Structure\\n\\nFig. 2.21 Comparison of discussion chunks\\n\\nFig. 2.22 Experimental results\\n\\n49\\n\\n50\\n\\n2 Discussion Data Analytics\\n\\nThe consideration obtained by this experiment is as follows:\\n\\n(1) Tendency of follow-up statements presenting new topics:\\n\\nFrom Fig. 2.16, there were some follow-up statements presenting new topics in almost all discussion chunks. In other words, we can see that there are multiple topics in one chunk. The most frequent follow-up statement presenting a new topic was a statement that presents a topic that was inspired from the discussion up to the present. For example, until the point just before, the participant was discussing “can you split semantic units?”, the statement that cuts out a topic from another viewpoint “can you unite semantic units in reverse?” was made. With the current statement types, we cannot divide the discussion in this ﬂexible way. The longer the length of one segment is, the longer the time required for browsing becomes. Therefore, in order to realize efﬁcient content browsing, ﬁner segment information is acquired by combining a technique different from the statement types. If the speaker of the start-up statement can judge whether a follow-up statement is not related to the content or intention of his/her statement, it is thought that ﬁner segment information can be acquired.\\n\\n(2) Tendency of follow-up statements with incorrect parent statements:\\n\\nFour follow-up statements with incorrect parent statements were conﬁrmed. It was conﬁrmed that it is equivalent to 2.2% in all follow-up statement (181 state- ments). From this, it can be thought that the link information of the statement acquired by the statement reservation function is valid. Also, when examin- ing the corresponding statements in detail, it turned out that it was caused by delayed timing of speaking. The reservation function gives link information to the statement that was being done at the input time of a reservation, but when the statement is not made at the time of reservation, the reservation function is designed to give link information to the last one (of course, this rule is not applied to the start-up statements). Therefore, the system cannot attach link information to the statements made before the last statement. As a solution to this problem, there is a method of specifying the statements to be targeted when the speaker speaks. For that reason, we added a function to change the linked statement (parent statement) to the discussion commander that is a device held by each participant.\\n\\n(3) Tendency of follow-up statements with multiple links:\\n\\nAs you can see from Fig. 2.22, many follow-up statements with multiple links existed. As checking the individual statements, there are a lot of statements to express opinions and summaries based on statements and discussions until then, such as “to arrange because discussions are complicated” or “even though you were saying …”. In addition, many chunks that do not have follow-up statements with multiple links repeatedly asked questions and requested. From this, it can be said that “the statements with more links are more important in the discussion,” “the discussion is more active as the number of follow-up statements with multiple links increases,” the link information can be thought of as an index for the importance and activity level of discussion.\\n\\n2.9 Correctness of Discussion Structure\\n\\nIt is impossible to acquire multiple link information only with the current state- ment types and the statement reservation function, but it can be dealt with by using the discussion commander to add the statement to be referred to as well as the correction of the parent statement. In addition to the method of inputting in real time, it is possible to consider how to input it after the meeting. For example, by realizing a mechanism for describing ideas by quoting multiple statements that are considered important, the system can give link information that shows relevance between quoted statements at the same time.\\n\\n2.10 Structuring Discussion with Pointing Information\\n\\nAs described in the previous section, the structuring of discussion chunks in our discussion mining system is problematic. For example, more than one topic may be included in a discussion chunk. Experiments showed that the speakers who con- tributed to a discussion chunk were not always talking about the same topic. As the discussion continued, several opinions from various viewpoints tended to be given. We attribute this to two main reasons. The ﬁrst is that a new topic was introduced to enrich the ongoing discussion. It would be unusual for all the remarks made during a discussion to be about a single topic; it is more common for supplemental topics to be introduced during a discussion. The other reason is that the statement types (start-up and follow-up) used were not sufﬁcient to classify all the statements in the discus- sions. When a different topic was introduced in a single statement of a discussion chunk, the system could not create a sufﬁciently detailed semantic structure.\\n\\nWhen a user reviews the discussions of previous meetings by using the discussion browser, the statements made during the meetings are numerous, and even if metadata is available to help in the search effort, it may take a long time for him or her to select and read statements of interest. Therefore, further structuring focusing on topic segmentation as well as structuring on the basis of “start-up” and “follow-up” is required.\\n\\nIn our research, we focus on pointing and “referring to” behaviors during meetings. Speakers usually refer to something when making a statement, e.g., “this opinion is based on the previous comment” or “this is about this part of the slide (while pointing to an image or text in the slide).” We assume that a statement with a reference to an object in a slide is strongly related to the topic corresponding to the object. We also assume that two statements during which the speakers point to the same object are about the same topic. Therefore, we concluded that acquiring and recording information about pointing to objects in a slide would facilitate topic segmentation and lead to more precise semantic structuring of discussions. We call a pointed object in a presentation material a visual referent.\\n\\nWe thus developed a system for pointing to and selecting objects in slides that uses the discussion commander mentioned earlier and created a mechanism for acquiring and recording pointing information related to participants’ statements.\\n\\n51\\n\\n52\\n\\n2 Discussion Data Analytics\\n\\nThis system can also extract any part of the ﬁgure in a slide and refer to it. In addition, selected or extracted image objects can be moved and magniﬁed by using the discussion commander.\\n\\nThe system also gathers various types of metadata about pointing such as identi- ﬁers of objects and slides, starting and ending times of pointing, IDs of participants who are pointing at objects, and IDs of statements during which objects are referred to. The signiﬁcant features of our pointing system are data correctness and clear visualization of “pointed to” objects. In a previous study (Nakano et al. 2006), the pointing target was estimated automatically by using a conventional laser pointer and visual processing. However, it was difﬁcult to accurately acquire the pointing target. In this study, the pointing target is deﬁned using information on objects in slides generated by Microsoft PowerPoint. This enables the attainment of references that correctly reﬂect the speaker’s aim.\\n\\nIn our system, discussions are structured on the basis of metadata acquired by using our pointing system. We hypothesize that statements with the same pointing target are on the same topic with a high probability. Similarly, it is highly probable that the topic changes when the pointing target changes. Therefore, when a statement in a discussion chunk has the same pointing target as the previous statement, the system provides new link information between the two statements. This enables tightly connected statements to be distinguished from normally connected statements in a discussion chunk. This reference-sharing link information, which adds new meaning to the discussion structure, contributes to structuring discussions in more detail. Of course, there are still statements on the same topic without any visual referents. Since the reference is not visualized, it is difﬁcult to deﬁne a target topic. Therefore, this type of topic sharing is removed from the scope of this research.\\n\\nWe evaluated our improved structuring method by using data from actual meet- ings. First, we veriﬁed our hypothesis that a statement with the same pointing target as the previous statement is, with a high probability, continuing the discussion of the current topic. Then we estimated the validity of structuring using pointing informa- tion by evaluating the validity of this hypothesis.\\n\\nThe discussion commander enables a user to point at and select objects shown on the screen. The user interface for selecting screen objects supports two methods as shown in Fig. 2.23.\\n\\n1. Sectional detail drawing (left of Fig. 2.18): a rectangular area on the screen and chosen objects in the slide touched its rectangular range by using drag-type operations of a pointer cursor on the screen.\\n\\n2. Underline drawing (right of Fig. 2.18): text in a slide is pointed at and selected by underlining it.\\n\\nThe references to various targets such as image objects and text portions in the slide are made possible by using the method appropriate for the target. These methods can also be used to acquire rich metadata about pointing.\\n\\nSeveral types of objects are selectable: itemized text, grouped diagrams, images formatted in JPEG, BMP, AutoShapes (illustrations such as polygons and ovals that can be used by Microsoft Ofﬁce applications), and tables and graphs.\\n\\n2.10 Structuring Discussion with Pointing Information\\n\\nFig. 2.23 Two methods for pointing in presentation slide\\n\\nFour functions are provided that enable correct pointing to these objects using the\\n\\nsectional detail drawing method:\\n\\n1. Temporary selection: a reference to an object is temporarily reserved. 2. Persistent visualization: the currently selected object and by whom are visualized. Independentization: the function by which it is cut down as a target of pointing 3. which doesn’t depend on slide layout for enabling its movement and zooming.\\n\\n4. Extraction: any subarea in the slide can be made independent.\\n\\nThe pointing information is used to structure discussions, i.e., to provide new link data between those statements referring to the same object, as shown in Fig. 2.24. Statements 3–6 in the ﬁgure are assumed to refer to the same object. Therefore, links are added to the discussion tree so as to make a complete graph consisting of statements 3–6. Statements referring to the same object (i.e., statements about the same topic) are connected by such links. By using these links, the system can distinguish a set of statements that are strongly connected with each other from a discussion chunk.\\n\\nThe speakers of statements 3–6 should be able to see that pointing to the same object created these links. However, they may ﬁnd it difﬁcult to ascertain whether their current statements are related to the “pointed at” object during the meeting. This is because they are concentrating on speaking when they are making statements even if the pointing target is visualized and can easily be conﬁrmed. Of course, a speaker may point to an object and make a question consciously about it. As the discussion continues, the speaker may tend to forget to decide whether his or her statements are related to the “pointed at” object at that time.\\n\\nWe thus included in our system a conﬁrmation dialogue: “is the present pointing target related to your statement?”, and the next speaker should answer “Yes” or “No”. A speaker operates his or her discussion commander and decides whether the “pointed at” object is related to his or her statement content. He or she selects “Yes”\\n\\n53\\n\\n54\\n\\n2 Discussion Data Analytics\\n\\nFig. 2.24 Links to shared visual referent\\n\\nto continue referring to it or selects “No” to cancel the reference. This function can prompt the speaker to consider whether his or her statements should continue to refer to the object pointed at by the previous speaker when beginning a statement.\\n\\nIt is possible to add links that reﬂect the speaker’s aim more accurately by using this function. The system can thus structure discussion chunks so that the speaker’s aim is reﬂected correctly and thereby support browsing of the discussion contents more efﬁciently.\\n\\nAs described above, our proposed method for identifying statements in a dis- cussion chunk with strong connections is based on the assumption that statements referring to the same pointing target are very likely about the same topic. However, this assumption was unveriﬁed. We thus conducted an experiment to conﬁrm whether the assumed correlation exists and whether the derived links are correct. Speciﬁcally, we veriﬁed the hypothesis that “a follow-up statement that refers to the same pointing target as its parent statement is about the same topic, and it is rare that a new topic is introduced in such a statement.”\\n\\nWe used two types of data in our experiment to verify this hypothesis: (1) pointing data captured by our discussion mining system, and (2) questionnaire data about the relationships between follow-up statements and topics acquired using the discussion browser. We collected the ﬁrst type of data by using the discussion commanders of the participants in three laboratory seminars. The participants in each seminar were the professor and three or four students (ten students in total). Each student made a presentation with slides.\\n\\nThe participants were asked to make statements and to use their discussion com- manders to point to objects on the slides during their statements. To point to paragraph text or the entire slide, they used the sectional detail drawing function. To point to a part of the text, they used the underline drawing function. To point to a particular region in the image, they used the independentization function. We removed the data corresponding to when they pointed to unintended objects by mistake.\\n\\n2.10 Structuring Discussion with Pointing Information\\n\\nWe classiﬁed their statements into three types: statements with no related pointing, statements accompanied by a status change in pointing (start or end pointing), and statements accompanied by no status change in pointing (pointing at an already selected object, i.e., inheritance).\\n\\nAfter each seminar, the participants were asked to complete a questionnaire on the discussion browser. However, we ﬁrst had the participant who inputted the meeting contents by hand check to see if there was anything missing in the minute text since the minute text was an important source for the participants to complete the questionnaire.\\n\\nThe questionnaire consisted of three types of questions: (1) ones asking about the relationship between the statement of interest and the preceding start-up statement (the ﬁrst statement of the discussion chunk in which the statement of interest was included); (2) ones asking about the relationship between the statement of interest and its parent statement; and (3) ones asking about the number of topics included in the statement of interest.\\n\\nOur analysis of the questionnaire data revealed that, when a participant was refer- ring to the same pointing target as the previous speaker, there was a high probability that he or she was speaking about the same topic. Our ﬁnding that there were few cases in which more than one topic was introduced in a statement validates our hypothesis that “speakers who make statements while referring to the same pointing target are very likely to be speaking about the same topic in contrast to statements without a shared reference, and it is rare that a new topic is introduced in such a statement”.\\n\\nOur experiment revealed that some follow-up statements were about a topic dif- fering from that of the start-up statement. The discussion may thus become unsettled and then be abandoned because the participants do not know whether the discus- sion about the previous topic reached a conclusion. We may be able to develop a mechanism that can automatically identify such unsolved topics and suggest to the participants that they discuss them again.\\n\\nWe aim to achieve more semantic structuring of discussions by creating connec-\\n\\ntions between statements including linguistic co-references and anaphora.\\n\\nReferences\\n\\nR. Barzilay, M. Lapata, Modeling local coherence: an entity-based approach. Comput. Linguist.\\n\\n34(1), 1–34 (2008)\\n\\nK. Georgala, A. Kosmopoulos, and G. Paliouras, Spam Filtering: An Active Learning Approach using Incremental Clustering, in Proceedings of the 4th International Conference on Web Intel- ligence, Mining and Semantics, No. 23 (2014)\\n\\nD. D. Lewis, W. A. Gale, A Sequential Algorithm for Training Text Classiﬁers, in Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 3–12 (ACM/Springer, 1994)\\n\\nA. Liu, L. Reyzin, B. D. Ziebart, Shift-Pessimistic Active Learning using Robust Bias-Aware Pre-\\n\\ndiction, in Proceedings of the AAAI Conference on Artiﬁcial Intelligence (2015)\\n\\n55\\n\\n56\\n\\n2 Discussion Data Analytics\\n\\nT. Mikolov, I. Sutskever, K. Chen, G. Corrado, J. Dean, Distributed Representations of Words and Phrases and their Compositionality, in Advances in Neural Information Processing Systems, ed by Burges, C.J.C., Bottou, L., Welling, M., Ghahramani, Z., Weinberger, K.Q., pp. 3111–3119 (2013)\\n\\nT. Mikolov, K. Chen, G. Corrado, J. Dean, Efﬁcient Estimation of Word Representations in Vector\\n\\nSpace. arXiv preprint, arXiv:1301.3781 (2013)\\n\\nK. Nagao, K. Hasida, Automatic Text Summarization Based on the Global Document Annota- tion, in Proceedings of the Seventeenth International Conference on Computational Linguistics (COLING-98), pp. 917–921 (1998)\\n\\nK. Nagao, K. Inoue, N. Morita, S. Matsubara, Automatic Extraction of Task Statements from Structured Meeting Content, in Proceedings of the 7th International Conference on Knowledge Discovery and Information Retrieval (KDIR 2015) (2015)\\n\\nK. Nagao, K. Kaji, D. Yamamoto, H. Tomobe, Discussion Mining: Annotation-Based Knowledge Discovery from Real World Activities, in Advances in Multimedia Information Processing—PCM 2004, LNCS, Vol. 3331, pp. 522–531 (Springer, 2005)\\n\\nW. Nakano, T. Kobayashi, Y. Katsuyama, S. Naoi, H. Yokota, Treatment of Laser Pointer and Speech Information in Lecture Scene Retrieval, in Proceedings of the 8th IEEE International Symposium on Multimedia 2006, pp. 927–932 (2006)\\n\\nT. Nishida, Conversation quantization for conversational knowledge process. Int. J. Comput. Sci.\\n\\nEng. 3(2), 134–144 (2007)\\n\\nN. Roy, A. Mccallum, Toward Optimal Active Learning through Monte Carlo Estimation of Error Reduction, in Proceedings of the 18th International Conference on Machine Learning, pp. 441–448 (ICML 2001)\\n\\nB. Settles, M. Craven, An Analysis of Active Learning Strategies for Sequence Labeling Tasks, in Proceedings of the Conference on Empirical Methods in Natural Language Processing (Associ- ation for Computational Linguistics, 2008)\\n\\nB. Settles, Active Learning Literature Survey, Computer Sciences Technical Report 1648, University\\n\\nof Wisconsin-Madison (2010)\\n\\nH. Shimodaira, Improving Predictive Inference under Covariate Shift by Weighting the Log-\\n\\nLikelihood Function. J. Stat. Plann. Infer. 90, 227–244 (2000)\\n\\nM. Sugiyama, M. Kawanabe, Machine Learning in Non-Stationary Environments: Introduction to\\n\\nCovariate Shift Adaptation (The MIT Press, 2012)\\n\\nT. Schultz, A. Waibel, M. Bett, F. Metze, Y. Pan, K. Ries, T. Schaaf, H. Soltau, W. Martin, H. Yu, K. Zechner, The ISL Meeting Room System, in Proceedings of the Workshop on Hands-Free Speech Communication (HSC-2001) (2001)\\n\\nChapter 3 Creative Meeting Support\\n\\nAbstract A face-to-face meeting is one of the basic social activities; it is necessary to analyze it in order to understand human social interaction in detail. This research is a scientiﬁc analysis of human social interactions. We are researching a mecha- nism to promote innovation by supporting discussions based on the premise that innovations result from discussions. Ideas are created and developed during conver- sations in creative meetings like those in brainstorming. Ideas are also reﬁned in the process of repeated discussions. In our previous research of discussion mining, we speciﬁcally collected various data on meetings (statements and their relationships, presentation materials such as slides, audio, and video, and participants’ evalua- tions of statements). We developed a method to automatically extract important statements to be considered after the meetings by using the collected data. Actions such as investigations and implementations are performed in relation to these state- ments. Here, we present an idea that automatically extracted statements leading to innovations facilitate creative activities after meetings. Our research was aimed at deeply analyzing face-to-face meetings and supporting human creative activities by appropriately feeding back knowledge discovered in the meetings. We particularly analyzed the features of statements made during discussions. We developed a sys- tem called a “meeting recorder” for that purpose. The meeting recorder consists of a 360° panoramic video camera that records meetings in audio–visual scenes, a tablet application that allows users to browse meeting materials and add various notes to them with a stylus, speech recognition that identiﬁes speakers and transcribes speech contents of all meeting participants, and a minute server that integrates all meeting- related information and creates the meeting minutes. We also developed a system that supports the activities after meetings called the “creative activity support sys- tem.” This system supports users in quoting statements extracted from the minutes, in writing notes and reports, in creating activity plans, in managing schedules to accomplish tasks, and in evaluating other members’ results within the group. Keywords Meeting analytics · Meeting recorder · Creative activity support · PDCA cycle · Evaluation of creativity\\n\\n© Springer Nature Singapore Pte Ltd. 2019 K. Nagao, Artiﬁcial Intelligence Accelerates Human Learning, https://doi.org/10.1007/978-981-13-6175-3_3\\n\\n57\\n\\n58\\n\\n3 Creative Meeting Support\\n\\n3.1 Meeting Analytics\\n\\nAlong with the remarkable development of machine learning technology, data ana- lytics, natural language processing, and pattern recognition technology, which I will describe later, have greatly improved. Of course, that did not solve all the prob- lems of intelligence, but at least from several years ago, the work we can automate has increased considerably. Recent artiﬁcial intelligence introduces machine learn- ing technology (or improves machine learning mechanism previously introduced) to various human-supported information systems which have been researched and developed for quite some time. The systems with recent artiﬁcial intelligence tech- nology are a system that can become smarter by collecting data.\\n\\nThere is also a place where artiﬁcial intelligence can be active even in the area of meeting support. It is not a story of automating the meeting itself to make human beings unnecessary but to try to make artiﬁcial intelligence take over work of the surroundings so that human beings can fully demonstrate creativity and improve productivity. Even at the meeting, various data can be gathered by incorporating various ideas into the system, which can be used for machine learning. However, as explained in Chap. 2, in the case of supervised learning, a teacher signal (correct answer data) is necessary. Also, by using the active-learning method also described in Chap. 2, human beings can update their learning models by making appropriate feedback with little effort. The learning model here is a predictive probabilistic model that is useful for appropriately advancing the meeting, efﬁciently reviewing the contents of the discussion, and appropriately reﬂecting it in subsequent activities. Speciﬁcally, this model contributes to improving the accuracy of recognition and summarization and ﬁnding important issues more accurately.\\n\\nThe mechanism to promote the improvement of discussion ability will be described in the next chapter of this book, but here I will explain another meeting support system and its functions we developed.\\n\\nFor creative meeting support, we applied machine learning techniques for discus- sion activity data obtained using a discussion mining system described in Chap. 2 that structures the meaning of a meeting’s content and records it. We call this dis- cipline of analyzing meetings and automatically mining valuable information from them “meeting analytics.”\\n\\nAs an important tool for meeting analytics, we developed a system called the meeting recorder that records and analyzes small-scale and face-to-face meetings in detail. It consisted of an Internet of Things device with a 360° all-around panoramic camera, tablets for all participants, and a server computer. This was aimed at achiev- ing a simpliﬁed portable version of the discourse mining system and it was also aimed at fully automating the creation of the minutes.\\n\\nThe system records meeting scenarios with a panoramic camera and tracks par- ticipants’ faces by assuming that there are people walking around in meetings. The system also simultaneously collects all the participants’ voices with small wear- able microphones. The speech recognition results and meeting materials are syn- chronously displayed on the tablets of participants. In addition, users can freely\\n\\n3.1 Meeting Analytics\\n\\nFig. 3.1 Meeting recorder in action\\n\\ndraw and mark ﬁgures on documents with stylus pens. The stylus inputs are also shared on all the participants’ tablets in real time.\\n\\nThere is a photograph of the meeting recorder we developed in use in Fig. 3.1. The voices of meeting participants that are input for the wearable microphones are recorded on the meeting server together with the IDs of the speakers along with when their speech started and ended. These are simultaneously transcribed using speech recognition on the cloud and displayed on the tablets of all participants. The facial images of the speakers obtained from the panoramic camera are then also simultaneously displayed.\\n\\nThe material displayed on tablets are images of a ﬁle (mainly in PDF format) trans- mitted beforehand to the meeting server, which can be marked with a pen linked to the tablets. Materials and markings are synchronously displayed on the tablets of all participants, and the statements and the materials (including markings) are auto- matically associated and used for structuring the discussion, which will be described later.\\n\\nThe meeting recorder is a system developed to support human creative activities by analyzing more general meetings not being limited to seminars of university laboratories and by appropriately feeding back information (Nagao 2018).\\n\\nIn particular, this system can analyze the characteristics of utterances in a voice discussion and create discussion content like the discussion mining (DM) system described in Chap. 2.\\n\\nThe biggest difference between this system and the DM system is that it is portable. In the DM system, it was necessary to install a pan-tilt camera and a device called\\n\\n59\\n\\n60\\n\\n3 Creative Meeting Support\\n\\nan IR array that sent infrared rays to the ceiling. As a result, meetings could only be held in certain rooms. In order to take a lot of data, it is necessary for everyone to be able to use it easily, so it was necessary to redesign to make installation easier.\\n\\nThe meeting recorder includes a panoramic camera that records meeting land- scapes, a tablet application that allows users to browse slide materials and freely add them with a pen, a speech recognition system that identiﬁes speakers and transcribes the contents of utterances. The tablet and the panoramic camera are connected to the minutes server by wireless network. In other words, if you install a server in the same facility, you can carry out a meeting using the system by carrying only the tablet and the camera. In addition, all the participants are wearing a small wearable (wearable) microphone on one ear. This is so that participants can record and recognize that person’s voice without fail even if they walk around. This microphone is connected wirelessly to the tablet, voice is transmitted to the speech recognition cloud (e.g., Google or Amazon) in real time via the tablet, and the recognition result is collected in the minutes server.\\n\\nOn the minutes server, machine learning is used to estimate statements that change the topic in discussion. By doing so, the system can divide the minutes by topic, and consider the number of statements and the number of speakers in that topic, then the system can decide the importance of each topic.\\n\\nIf the discussion can be further structured, more sophisticated processing can be realized. In order to structure it, it is necessary to gather a lot of data. This system collects and analyzes various data including audio and video about meetings in the simplest possible way.\\n\\nFor example, as described in Chap. 2, we construct a tree structure using the relationship between statements and between statements and slide materials, and then perform spreading activation on it. Spreading activation is a mechanism that gives a high activation value to a node with a high degree of reference on the graph, and by using this activation value, it is possible to calculate the importance of the statements.\\n\\nThe system can select topics with high priority and statements with high impor- tance and present them as a summary of the meeting minutes. Also, the system will be able to remind the participants to discover statements that describe important issues to be solved in the near future, such as the task statements mentioned in Chap. 2.\\n\\n360° spherical panoramic cameras (or simply panoramic cameras) are now being sold for the general public. This is a camera that can shoot pictures and images of 360° around the camera. Because it is a mechanism with two ﬁsheye lenses overlapped, a distorted image as shown in Fig. 3.2 is taken, so use PC to convert it to an image like a normal camera shot and display it. In Fig. 3.2, we also show the results of recognizing the persons and things in the image.\\n\\nIn the DM system, the face of the speaker was photographed using a pan-tilt camera (a camera that can be turned by PC) on the ceiling, but when the speaker walks around and moves to a place, the camera cannot follow. On the meeting recorder, we assume that there are people walking around while meeting and recording meeting scenes with a panoramic camera and tracking the participants’ faces.\\n\\n3.1 Meeting Analytics\\n\\nFig. 3.2 Image captured by panoramic camera\\n\\nThis is so that you can easily search the speaker’s current video from the speech when browsing the recorded content later. By analyzing the position of the partici- pant’s face in the panoramic image, it is possible to ﬁnd the image of the speaker at that time in the panorama from the speaker ID and the speaking start time.\\n\\nAlso, as mentioned above, participants all wearing a small microphone in their ears and participate in the conference, and the voice of all the participants is recorded separately. Then, as shown in Fig. 3.3, the speech recognition result and meeting materials such as presentation slides are displayed synchronously on the tablet of each participant. On the right side of the screen shown in Fig. 3.3, the speech recognition results are displayed in a chat log format. The user’s own utterance is displayed right aligned, the other utterances are displayed left aligned, and color coded for each person. This color also matches the pen stroke color of the corresponding user. Meeting materials are displayed on the left side of the screen, so that writing can be done with a pen. The user can also use it like a whiteboard by making the background white, without displaying the document. At the top of the screen, a part of the utterance content that represents the topic at that time is displayed.\\n\\nThe minutes created by the meeting recorder include images, sounds, images, texts, and can be searched and viewed using a Web browser. The user interface of the minutes is shown in Fig. 3.4. Images shot with panoramic camera, slide image displayed on tablet marked with pen and recognized speech text are displayed. When viewing video, the speaker automatically scrolls left and right when the speaker changes and the speaker is displayed. The slide image and the speech content are associated with each other, and an ordered list of statements is displayed for each slide image. Also, based on the discussion structuring method described in the next section, the statements are segmented into topics and are displayed in a tree structure format.\\n\\n61\\n\\n62\\n\\nFig. 3.3 Tablet interface of meeting recorder\\n\\nFig. 3.4 Browser view of discussion content\\n\\n3 Creative Meeting Support\\n\\n3.2 Machine Learning for Structured Meeting Content\\n\\n3.2 Machine Learning for Structured Meeting Content\\n\\nAlthough the online minute of the meeting recorder has the same function as the discussion browser mentioned in Chap. 2, the discussion does not constitute a tree structure at this point. That is because the meeting recorder is not designed to manu- ally tag the statement types and to specify the parent statement in the case of follow-up statements in order to reduce the burden on participants. Therefore, we need to struc- ture the discussion recorded by the meeting recorder based on the training data of the discussion content created and accumulated in the discussion mining system. This makes it possible to use various functions used in discussion mining. The func- tions include summarization of discussion content and discovery of task statements described in Chap. 2.\\n\\nRegarding how to construct a tree structure, we begin by classifying the set of statements by topic. However, to understand what topics are, we need a fairly sophisti- cated semantic processing, so we modeled the tags (“start-up” and “follow-up”) given by discussion mining and employed a machine learning technique as described in Chap. 2. If a statement that is supposed to be start-up type appears, the system consid- ers that the topic has changed. In this way, since tagged statement data of discussion contents accumulated in the DM system can be used as it is as training data. Of course, using the data of the meeting recorder, if we perform active learning as mentioned in Chap. 2, we will be able to improve classiﬁcation accuracy with minimal effort. Assuming that tags of “start-up” and “follow-up” could be estimated for the state- ments, the next thing to do is to investigate which statement before a follow-up statement is related to the follow-up statement. In order to solve this problem, we apply a technique called conditional random ﬁeld (CRF) (Qu and Liu 2012). This method gives attributes in a probabilistic way to each data included in the sequence data (the data arranged in order). As in speech recognition, the one-dimensional conditional random ﬁeld is suitable for problems such as allocating phonemes to morphemes and assigning phonemes to divided waveforms. In addition, the two- dimensional conditional random ﬁeld is suitable for the problem of estimating the dependency between data in series data. Therefore, in order to estimate the depen- dency between the statements and construct the tree structure of the discussion, we will use the two-dimensional CRF (2D CRF).\\n\\nThe 2D CRF has the structure as shown in Fig. 3.5. In this ﬁgure, the dependent element (i.e., follow-up statement) is called the source, and the depended element (i.e., parent statement) is called the target. The CRF is one of stochastic graphical models (graphical representations in which nodes are random variables) represented by undirected graphs (graphs in which nodes are connected by undirected links), and the link represents the dependency between random variables.\\n\\nIn Fig. 3.5, the case where the same statement has different parent statements is represented by nodes in the vertical direction, and the case where several follow- up statements depend on the same parent statement are represented by nodes in the horizontal direction. Like other machine learning methods, we estimate the parameter θ that maximizes the conditional probability of using the stochastic gradient descent\\n\\n63\\n\\n64\\n\\n3 Creative Meeting Support\\n\\nFig. 3.5 Two-dimensional conditional random ﬁeld\\n\\nmethod. X is called the observation value (objective variable) and Y is called the latent variable (explanatory variable). Although details are omitted here, this mechanism makes it possible to structure the discussion recorded by the meeting recorder as a tree structure similar to the DM system.\\n\\nIn the case of content recorded with the meeting recorder, we regard the region marked with a pen stroke within the presentation material instead of a pointer as a visual referent (it is not necessarily a slide if it is a meeting material). Since the marking is shared by all the tablets, the user can select other people’s marking area and make a statement about it, and the system can check the match of the visual referent between the statements.\\n\\nThis makes it possible to apply the model learned with discussion mining data to the data of the meeting recorder. Of course, depending on the content of the meeting, the prediction accuracy by the machine learning model may not be sufﬁcient, so we need to update the machine learning model using the active-learning method described in Chap. 2. In order to do that, after the meeting, the system needs to get the user feedback which is the information equivalent to the teacher signal.\\n\\nI will next explain how the system that supports the users’ actions after the meeting\\n\\nand acquires data to improve the machine learning model at the same time.\\n\\n3.3 Post-meeting Assistance to Support Creative Activities\\n\\nWe developed another system to extract important statements and task statements and to reﬂect them in subsequent activities to make effective use of the meeting results. This system consists of a function that involved writing notes and reports by quoting statements, a function that involves managing schedules concerning the execution of tasks, a function that involves disclosing and sharing notes and reports in a group, and a function that involves group members mutually evaluating reports. We call the system a “creative activity support system.”\\n\\nThe creative activity support system is a system that guides subsequent activities from immediately after a meeting has been held based on the results obtained from discussions. As there is a possibility of forgetting content for a while after meetings,\\n\\n3.3 Post-meeting Assistance to Support Creative Activities\\n\\nparticularly for task statements, and there is a possibility of neglecting this, it is necessary to associate this with the activities as soon as possible. We implemented functions that users could easily remember to quote task statements, formulate action plans, reﬂect on schedules, and be appropriately reminded of the task execution plan for that reason.\\n\\nIt is not easy to steadily complete various tasks that arise in work and research. Indeed, there have been many arguments on how to manage tasks to achieve success, and many scheduling support systems, such as Google Calendar, have been developed to support this need.\\n\\nMost conventional scheduling support systems have only been focused on man- aging the schedule of plans, and not on understanding how an established schedule is processed and what state it is in before proceeding to the next task. There is no support in implementing tasks that have causal relationships over the long term, such as setting guidelines. One major issue related to graduation and completion studies is addressed by carrying out individualized tasks that have been segmentalized in a long-term time series, so the plan–do–check–act (PDCA) cycle in business execution is recommended for education as well (Osone and Uota 2015). It is necessary to have a mechanism to support the smooth execution of a series of performance cycles of planning, executing, and evaluating tasks.\\n\\nBy using the creative activity support system, the users quote the statements extracted from the minutes, write notes and reports, form action plans and sched- ules them. The reports on achieved tasks are published to view and evaluate in the group. Furthermore, in the future, we will be able to model the process of devel- opment of ideas by machine learning. To that end, what kind of evaluation was used/reconsidered by the idea presented at the meeting, what kind of opinion came out when presented at the meeting again, what kind of evaluation was given when the task derived from the idea was achieved. Based on the results of this analysis, we will construct a machine learning model to estimate the creativity of ideas.\\n\\nBy applying this model to the initial idea, you can predict the future development of the idea. Evaluation of idea creativity can be used to set priorities for multiple ideas for similar problems. I believe that people can accelerate innovation by working according to this priority. I will talk more about this in Chap. 6 of this book.\\n\\nAs we will return to in Chap. 6, our purpose is not just to develop the system to support humans by recording meetings and discovering knowledge. Another goal is to support the general creative activities of humans and to create an environment that enables humans to perform more creative work more efﬁciently. At the same time, we are thinking about a mechanism that humans can extend their potential and that the system can improve its accuracy simultaneously. Then, the expansion of human capability will be a positive cycle.\\n\\nIn other words, post-meeting assistance contains two main items. One is to make the participants utilize the knowledge gained at the meeting for the subsequent activ- ities, and the other is to evaluate and cultivate discussion ability of the participants through meetings.\\n\\n65\\n\\n66\\n\\n3 Creative Meeting Support\\n\\nFig. 3.6 Flow of task achievement\\n\\nAs we discuss the evaluation of discussion ability and its improvement support in Chap. 4 in detail, we would like to talk about the mechanism to make use of the results of the meeting for subsequent actions.\\n\\nThe ﬂow of task achievement is outlined in Fig. 3.6, and the underlined parts are executed by the system. First, the user creates notes for a performance plan to complete the tasks for the task statements extracted by using the learned extraction model and then manages the execution schedule with the scheduling components of the system. He/she then carries out the tasks according to the planned schedule, receives evaluations from other users (support for the check step) on the basis of the results added to the performance plan notes, and considers the evaluations to improve subsequent activities.\\n\\nAs an information tool to use after the meeting, there is an activity note creation application as shown in Fig. 3.7. This is a system to select and review the task state- ments estimated by the machine learning model trained by the contents structured by the meeting recorder and the DM system, to conﬁrm the tasks, and to create, edit and organize the activity notes. The user can also classify past meetings by topic and select and review some important statements.\\n\\nActivity notes can include quotation of related task statements and organize items to achieve tasks found during and after the meeting. The task statement is associated with the discussion content, so the user can also view the surrounding statements. The task statements quoted by this tool are registered as correct answer data and are used for relearning. Active learning as described in Chap. 2 allows the user to select a statement to which a teacher signal should be given. By emphasizing the candidates of task statements on the tool, it can prompt the user to browse and quote them. Teacher signals for incorrect answers will be given to statements that were not quoted even after some time.\\n\\n3.3 Post-meeting Assistance to Support Creative Activities\\n\\nFig. 3.7 Activity note creation tool\\n\\nFig. 3.8 Task scheduler\\n\\nWhen creating an activity note and planning to do items in the note, the user needs to check his/her schedule. Figure 3.8 is a tool called the task scheduler that allows the user to incorporate the contents of activity notes, to correlate them to the appropriate dates and times, and to check other activities and tasks underway. The pie charts on the upper left of Fig. 3.8 show the percentage of time required for the activities of this week and this month. In addition, the vertical bar chart below shows the number of accomplishments per month, and the right bar chart shows the cumulative task achievement status for all tasks. These data can be used not only to plan activities, but also to evaluate activities and to know their own skill level.\\n\\n67\\n\\n68\\n\\n3 Creative Meeting Support\\n\\nResearch is a typical example of an intelligent creative activity. In many cases, research will ﬁrst decide the theme, think about the next goal in that theme and the issues related to that subject. The important thing in solving the problem is that it achieves some goal. In other words, it is necessary to take into consideration which goal is set in detail in advance, which element of that goal is tied to the tasks to be carried out and achieved by that solution.\\n\\nThe state of awareness of students with little experience in research activities who only use the number of tasks is disadvantageous because of the uncertainty regarding execution. Therefore, after the existence of tasks is identiﬁed, each task should be well organized and scheduled to use time efﬁciently. The task scheduler works well for scheduling the duration of task executions.\\n\\nThere is a graph at the top of the screen of the task scheduler that can roughly organize the proportion of information by type (e.g., surveillance, development, and experiment) of tasks and their situation with achievement. The user can arrange and update the time intervals of tasks scheduled to be executed on the timetable at the bottom of the screen.\\n\\nAlso, if multiple activities are to be performed in parallel, it would be better if the planning is done after checking whether there are dependencies on tasks to be performed and considering priorities. For example, if it is assumed that implemen- tation of program X is necessary to carry out task A, and that program X can be implemented by modifying program Y, which is a result of another task B, then task A has a dependency to task B. In that case, it is more efﬁcient to achieve task B earlier than task A. In order to discover such dependency between tasks, we analyze the activity notes and the quoted task statements and the discussion contents, because they are good resources to ﬁnd clues about task dependency.\\n\\nAs a way to gain insight into dependency, we analyze the similarity of topics, the commonality of speciﬁcations or functions of anticipated results, relationships between the original idea and derived discussions, and so on. Anyway, if we continue discussion at meetings, the discussion content should deﬁnitely include data that can be used for reasoning of task dependency.\\n\\nThe most important thing in executing a task is that it achieves some goal. In other words, it is necessary to ﬁrst consider what target is set, which part of the goal is associated with the task to be carried out, and what will be achieved by the solution. The creative activity support system has a map that roughly describes the entire activity in tree structure form [called mind map (Buzan 1990, Beel and Langer 2011)] and can search which part of the target is related to the problem to be solved from the map as shown in Figs. 3.9 and 3.10 (Fig. 3.10 is an enlarged center of Fig. 3.9). Then, the user can search on the map which part of the goal is the issue to be solved. By doing so, the users can continue their activities while conﬁrming that they are heading toward the goal by doing so, and not just executing the task in front of them.\\n\\nWe must begin by being aware of the problems that confront us at the moment to facilitate creative activities. As mentioned above, task statements extracted with the proposed system are presented in Fig. 3.11.\\n\\n3.3 Post-meeting Assistance to Support Creative Activities\\n\\nFig. 3.9 Activity mind map\\n\\nFig. 3.10 Central part of activity mind map in Fig. 3.9\\n\\nEach statement that is presented is determined to be a task statement with high levels of possibility, and it is necessary for the user to ﬁnally assess whether the task should be achieved. Therefore, since the statement immediately after extraction has a blue icon marked “Task?”, which can be clicked, it determines whether the statement indicates a task to be achieved.\\n\\nBy using the activity note creation tool mentioned above, the user creates a note citing the task statement after deciding the task and describes the details of the plan and results related to the task. By setting the attribute of this note to “Finished,” a completion icon is displayed in the related task statement. The user can then easily be aware of which task is in a completed state.\\n\\n69\\n\\n70\\n\\n3 Creative Meeting Support\\n\\nFig. 3.11 Presentation of tasks\\n\\nAfter the tasks are carried out, we should assess their progress, comprehend the level of achievement, and use it as a source for future activities (Nagao et al. 2017). We implemented a function to evaluate task execution content in the proposed system by obtaining other students’ assessments of the content of execution by publishing task execution notes in the lab.\\n\\nWe conﬁrmed that mutual evaluation was reasonable by referring to the activity notes that described the content of task execution. However, since the task execution notes were primarily private, they are not supposed to be published. The sharing function of the notes provided an opportunity to receive a more accurate evaluation when there was motivation to tell others about the results of their task execution.\\n\\nWhen completing task execution, the system accesses the level of achievement of the research goals. The user can then publish related research notes and associate them with the previously shared notes to motivate others to update evaluations. The associated notes are displayed on the right of the evaluation window, as seen in Fig. 3.12, when receiving an evaluation from another user. The user considers them as a supplemental resource to evaluate another’s research results.\\n\\nIn the creative activity support system, there is also a function that can receive evaluation according to the content by publishing the activity notes as a task execution report in the group as the evaluation function of the user’s activity concerning task execution. The screen example shown in Fig. 3.12 is an interface for that, by clicking on one of the ﬁve stars, it evaluates the degree of achievement and evaluates the effort of the user (even if the result is not good to evaluate), the user can click on the hand icon (this is called stamp evaluation).\\n\\n3.3 Post-meeting Assistance to Support Creative Activities\\n\\nFig. 3.12 Task evaluation tool\\n\\nThe notes describing the activity contents are generally private, and do not assume that they are made public. The note publication function is to provide opportunities to receive evaluation from others when there is an intention to tell others about the result of their task execution. This is a mechanism that adopts the gamiﬁcation described in the next chapter.\\n\\nIn addition, the creative activity support system has a function to automatically evaluate the activity process based on task statements, activity notes, and related goals. For this, several numerical targets are set in the abovementioned map, and the note has a column for describing the progress status corresponding to those numerical targets. A numerical target is like, for example, investigating more than ﬁve related instances of research. At this time, it is possible to notice the user by displaying at all times what percentage of the target is achieved. Also, by calculating the degree of relevance between the task statements and the content of the activity notes, it is possible to evaluate the degree of relevance to the contents of the activity with the contents of the challenge that triggered it. Tasks and activities are linked correctly, and furthermore, by using numerical targets as clues, we will be able to progress creative activities more steadily.\\n\\nBy conﬁrming the goal based on the map that bird’s-eyed the activity and con- tinuing activities while utilizing the PDCA cycle, you will be able to achieve results steadily. However, creation activities that are extremely difﬁcult to create maps for are likely to exist. As aiming at the development and practical application of IT products that make commoditization (to become necessities for daily life) next to smartphones, it will be difﬁcult to clarify what kind of map to work on. For research activities, there is some standard way to proceed, but if you try to make innovative inventions, it may be difﬁcult in the same way as other people. Sometimes there is a doubt that activities that can make maps are not so creative in the ﬁrst place. However,\\n\\n71\\n\\n72\\n\\n3 Creative Meeting Support\\n\\nFig. 3.13 Top page of creative activity support system\\n\\nwhat we can say from our experience is that maps and guidelines (procedures of what to do) are effective in doing research, and that you can demonstrate creativity as well. In our laboratory, the members logged into the creative activity support system once a day, conﬁrmed the activities of the day, checked the information published by others, they did it without forgetting they are trying to describe things. Figure 3.13 shows the top page of the system after login. From this page, the user can check their own and others’ tasks and their situation and view the details. It is also linked to the discussion browser mentioned in Chap. 2 and the personal page of gamiﬁed discussion explained in Chap. 4 (detailed conﬁrmation page of students’ discussion ability), and the state of themselves at the meeting. So, the students can learn more about the current situation with it.\\n\\nWe are planning to develop a system to discover creative ideas that lead to future innovation by efﬁciently recording and analyzing meetings and subsequent activities, and to develop it efﬁciently. Future plans also include clariﬁcation of the true value of this research by gathering data on a large scale, building a machine learning model and operating it.\\n\\nWe predict that most of the intellectual creative activities will be carried out by the group, and it will be in the form of activities being promoted during discussion. In other words, it is the idea that support of group activities centering on discussion will directly support human creative activities.\\n\\nAs it is no longer an age that some genius thinks of everything, it will be that people will share wisdom, invent and discover. At that time, what is important is a system for recording and effectively utilizing discussions and results.\\n\\n3.3 Post-meeting Assistance to Support Creative Activities\\n\\nIn this way, in order for an artiﬁcial intelligence system that combines machine learning and natural language processing to support human creative activities, it is necessary for human beings not to hide themselves and to enter information about their activities into the system.\\n\\n3.4 Evaluation of Creativity\\n\\nAlthough how to capture creativity is varied, we regarded it as the degree of develop- ment of ideas in this research. In other words, the extent to which ideas were reﬁned and materialized after discussion was used as an indicator of creativity.\\n\\nThe ideas should appear in the statements in the discussions and be extracted from the minutes and cited in the creative activity support system. Their further development will therefore be possible. Then, after the reports are published and the evaluations are received, the values of the ideas are augmented, and they can be further developed by making them the topics of the next meeting.\\n\\nCreativity can be represented by using concrete data and modeled by machine learning in this way by tracing the generation of ideas through to their development. The idea seems to have something of value at the time of its creation, but it is very difﬁcult to evaluate, so by presenting it at a meeting and placing it on the top of the discussion, the idea of later development will leave the possibility and direction to the judgment of participants. Of course, depending on the experience and knowledge of the participants, it is also good that the idea creator compares reactions by showing similar ideas at meetings attended by other participants.\\n\\nIn this way, by tracing the development process of ideas, it is possible to represent\\n\\ncreativity with concrete data, and it can be modeled by machine learning.\\n\\nThe development of ideas is not necessarily carried out by a speciﬁc individual. Currently, only the activities of individual users registered in the creative activity support system can analyze the development process of the idea in detail, and only by discussion, multiple users can develop ideas (whether idea should be developed or not can involve judgment). However, there are cases where multiple people will integrate and develop ideas that come up separately. It is necessary to analyze collaborative idea development of multiple users in the same way.\\n\\nWhen it becomes possible to predict the creativity of ideas by machine learning, and when multiple ideas to solve the same problem are conceived, it becomes possible to rationally decide which one to develop preferentially. This is done by presenting multiple ideas at a meeting and by automatically evaluating extracted statements after discussions.\\n\\nIt is thought that innovation will be accelerated by repeating choices based on the creativity of the ideas. Innovation cannot be achieved unless it develops ideas to a level that can affect society, but it will be possible to efﬁciently use time to carry out creative activities by considering which ideas to preferentially develop.\\n\\nThis is presently not an era when innovation is generated from a few outstanding ideas. However, a plethora of ideas that can create innovation in the future can be\\n\\n73\\n\\n74\\n\\n3 Creative Meeting Support\\n\\ncreated from discussions by numerous people. True innovation can be achieved by carefully developing such a profusion of creative ideas.\\n\\nIt might be the case that humans might become more creative through the support of artiﬁcial intelligence. Whether or not creativity itself can be entrusted to artiﬁcial intelligence is a matter that is likely up for debate, but artiﬁcial intelligence can be used in places where humans are not very good and will help humans in various ways, for example, to discover high-priority items from a large amount of information and many others.\\n\\nThe thing that humans should do is to record the discussion ﬁrmly and make analysis possible. After that, we will conduct creative activities based on goals, issues and plans, and keep track of the details as much as possible. I think that recording of activity is very troublesome unless the person taking the record is in the habit of doing it (such as a person writing a diary every day), but if they are trying to make their activities more meaningful and creative, it would be better to adapt to that way.\\n\\n3.5 Future of Creative Meeting\\n\\nSince a meeting is an important means of communication and a place for multiple people to share ideas, it is natural to try to acquire various knowledge using the minutes, which is the record, as a knowledge source. The more knowledge the par- ticipants can earn, the higher value of their meeting can produce. One way to use the minutes for multiple purposes is a mechanism that creates answers to any ques- tion of the user by combining information obtained from the minutes. This is called an interactive minute. In order to realize the interactive minutes, detailed semantic structuring of the minutes is necessary, and it is expected to realize advanced artiﬁcial intelligence technology (e.g., question answering system).\\n\\nIn order to facilitate collaborative work, a mechanism called a shared editor has been proposed based on the idea that all the participants need to be able to manage any information which everyone is seeing in the shared space. In this mechanism, since multiple participants write and view the information at the same time, exclusive control and so on need to be devised. Therefore, it is possible to set a writable layer for each participant, make it overlap, and at the same time use the color-coded pointers to grasp the points indicated by each participant.\\n\\nIn the future, pseudo-face-to-face meetings using virtual space will be heavily used. In the meeting, participants are held by wearing a virtual reality headset and virtually gathering in a place simulating real meeting rooms or halls. There, vari- ous materials will be presented using video/audio/3D data, and discussion will be carried out by voice accompanied by text subtitles. Of course, the minutes will be automatically created and their summaries will be generated.\\n\\nAn anthropomorphic agent as a facilitator emerges in the virtual space, in some cases proposing to prevent divergence of the discussion. It also proposes to make an appropriate break during the meeting, and extracts keywords that seem to be important. The agent then arranges the keywords in order and shows their diagram-\\n\\n3.5 Future of Creative Meeting\\n\\nmatical representation. It also supports comparisons with past meetings and supports or disapproves the current proposal based on the comparison results.\\n\\nHowever, the main participants of the discussion will still be humans. Human creativity cannot be imitated sufﬁciently by artiﬁcial intelligence yet. This is because the current artiﬁcial intelligence is a mechanism to learn from the data. Humans do not necessarily think based on data. For that reason, there are things that could lead to a mistake, but on the contrary, there is a chance to make a revolutionary idea that cannot be deduced from existing data.\\n\\nBy making a meeting in the virtual space, physical contact becomes difﬁcult, but the advantage of not having to travel long distances is very great. Moreover, almost the same situation as the face-to-face meeting is reproduced, so the nuance of words is easily transmitted. Although it is difﬁcult to recognize the face wearing the headset, appropriate (sometimes deformed) facial expressions are expressed by avatars trained by machine learning in advance.\\n\\nAt such a meeting, the technology we developed will maximize its function. The techniques of analyzing and structuring meeting and discovering knowledge by discussion mining lead the essence of discussion and function for users to look back past discussions efﬁciently. In addition, it is possible to reproduce the discussion as faithfully as possible with the technology that automatically collects and uses various data concerning meetings such as a meeting recorder. The contents of past creative activities are not lost, they are reused appropriately. The gamiﬁed discussion method explained in Chap. 4 works not only to improve motivation for discussion but also to promote qualitative evaluation of the user’s discussion ability. The mechanism for improving the ability of the discussion will also be described in detail in Chap. 4.\\n\\nReferences\\n\\nJ. Beel, S. Langer, An Exploratory Analysis of Mind Maps, in Proceedings of the 11th ACM\\n\\nSymposium on Document Engineering, pp. 81–84 (2011) T. Buzan, Use Both Sides of Your Brain (Plume Books, 1990) K. Nagao, Meeting Analytics: Creative Activity Support Based on Knowledge Discovery from Discussions, in Proceedings of the 51st Hawaii International Conference on System Sciences (HICSS 2018) (2018)\\n\\nK. Nagao, N. Morita, S. Ohira, Evidence-based education: case study of educational data acquisition\\n\\nand reuse. J. Syst. Cybern. Inform.: JSCI. 15(7), 77–84, ISSN: 1690–4524 (Online) (2017)\\n\\nT. Osone, K. Uota, An Approach to Teaching Basic Information Education based on PDCA Cycle.\\n\\nBus. Rev. Senshu Univ. 100, 1–14 (2015)\\n\\nZ. Qu, Y. Liu, Sentence Dependency Tagging in Online Question Answering Forums, in Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pp. 554–562 (2012)\\n\\n75\\n\\nChapter 4 Discussion Skills Evaluation and Training\\n\\nAbstract There must be as many concrete indicators as possible in education, which will become signposts. People will not be conﬁdent about their learning and will become confused with tenuous instruction. It is necessary to clarify what they can do and what kinds of abilities they can improve. This paper describes a case of evidence- based education that acquires educational data from students’ study activities and not only uses the data to enable instructors to check the students’ levels of under- standing but also improve their levels of performance. Whether a meeting is executed smoothly and effectively depends on the discussion ability of the participants. Evalu- ating participants’ statements in a meeting and giving them feedback can effectively help them improve their discussion skills. We developed a system for improving the discussion skills of participants in a meeting by automatically evaluating statements in the meeting and effectively feeding back the results of the evaluation to them. To evaluate the skills automatically, the system uses both acoustic features and linguis- tic features of statements. It evaluates the way a person speaks, such as their “voice size,” on the basis of the acoustic features, and it also evaluates the contents of a statement, such as the “consistency of context,” on the basis of linguistic features. These features can be obtained from meeting minutes. Since it is difﬁcult to evaluate the semantic contents of statements such as the “consistency of context,” we built a machine learning model that uses the features of minutes such as speaker attributes and the relationship of statements. We implemented the discussion evaluation system and used it in seminars in our laboratory. We also conﬁrmed that the system is effec- tive for improving the discussion skills of meeting participants. Furthermore, with regard to skills that are difﬁcult to evaluate automatically, we adopted a mechanism that enables participants to mutually evaluate each other by applying a gamiﬁcation method. In this chapter, I will also describe the mechanism in detail. Keywords Discussion ability · Speaking ability · Listening ability · Evaluation of discussion skills · Training of discussion skills · Gamiﬁcation\\n\\n© Springer Nature Singapore Pte Ltd. 2019 K. Nagao, Artiﬁcial Intelligence Accelerates Human Learning, https://doi.org/10.1007/978-981-13-6175-3_4\\n\\n77\\n\\n78\\n\\n4 Discussion Skills Evaluation and Training\\n\\n4.1 Evaluation of Speaking Ability\\n\\nAnalyzing discussion scientiﬁcally provides an evaluation of the appropriateness of the statements and the discussion ability of the speaker. Evaluating the discussion ability of the speaker leads to evaluating the person’s communication ability. As I mentioned at the beginning of this book, the ability to conduct discussions can be said to be the essence of communication skills, so having a high discussion capability necessarily means a high communication skill.\\n\\nTo evaluate discussion ability, we ﬁrst evaluate individual statements and evaluate the speaker’s abilities based on the evaluation of his/her statements. Our ﬁrst step is to evaluate an individual’s speaking ability.\\n\\nAs explained in Chap. 3, the meeting support system we developed records the statements of each participant during the meeting as the discussion content including video/audio data and text minutes. Therefore, we can automatically evaluate the statements based on their acoustic features and linguistic features.\\n\\nAt the meeting, participants need to discuss a topic, analyze the story of the other persons’ statements, and communicate their argument in an easy-to-understand manner. “Pronunciation,” “speech speed,” “pause,” “conciseness,” etc. are mentioned as easy-to-understand way of speaking (Kurihara et al. 2007). Based on this, we set eight evaluation indicators. The evaluation indicators are based on acoustic features and those based on linguistic features.\\n\\nThe indicators that evaluate only by acoustic features are as follows:\\n\\nA. Voice size: voice should be large enough for the speaker to hear enough, while on the other hand it is better not to be emotional and too loud. Therefore, we measure and evaluate the volume [dB] of each statement.\\n\\nB. Voice intonation: speech without intonation is a factor that makes a listener bored. We measure the height of the voice in the statement (fundamental frequency F0 described later) [Hz], and evaluate those with high standard deviation values as good evaluations.\\n\\nC. Speed of talking: it will be hard to hear even if the statement is too fast or too slow. Therefore, if the speech speed (the number of syllables per hour, the syllable is described later) is within the appropriate range, it is evaluated as good.\\n\\nD. Fluency: speech with a lot of silence and disﬂuency is difﬁcult to understand. A good evaluation is given to statements with few ﬁlled pauses (vowel extension) such as “eh” during speaking and few silence periods of more than 2 seconds. E. Tempo: it seems easy to understand the speech when the emphasized part is clear. It is effective that the statements are not monotone such as speaking slowly the part that you want to emphasize and setting a pause before the emphasized part. Therefore, the tempo of the statement is evaluated based on the standard deviation of the speech speed and the number of “pauses” (“pause” is deﬁned as a silence period of less than 2 s).\\n\\nHere, the fundamental frequency (generally written as F0) is a value expressing the periodicity of sound, which is the acoustic feature quantity that governs the pitch\\n\\n4.1 Evaluation of Speaking Ability\\n\\nof sound. There is periodicity in voiced sound (vibrating the vocal cord), so the reciprocal of that period (basic period) is the fundamental frequency.\\n\\nF0 is a very important index to consider for the intonation of a voice, but (1) a speech waveform is quasiperiodic signal (due to quasiperiodicity of vocal fold vibration), periodicity is not clear, (2) speech is mixed with noise, and (3) the range of change of F0 in voiced sounds is difﬁcult to limit because of the wide range. Accurate extraction of F0 is very difﬁcult. Therefore, several estimation methods have been proposed. The acoustic analysis program called speech signal processing toolkit (SPTK) has been released (http://sp-tk.sourceforge.net/), in which an algo- rithm called pitch extraction is implemented to estimate F0.\\n\\nIn addition, the syllable used to calculate the speech speed is a type of segmental unit that separates consecutive voices, and is a group of sounds heard. Typically, it is a voice (group of voices) consisting of one vowel and its vowel alone or with one or more consonants before and after the vowel. In the case of Japanese, syllables may use a segment unit called a mora (beat) that does not necessarily match. Strictly speaking, the mora is used instead of a syllable. The main difference between syllable and mora is that the long vowel, the geminate consonant, and the syllabic nasal are integrated with the preceding vowel in the case of the syllable, but in the case of the mora it is 1 mora.\\n\\nNext, evaluation indicators based on linguistic features are as follows:\\n\\nF. Conciseness: it is easier to understand if the statement is concise. Therefore, for the sake of conciseness evaluation, we compare the number of syllables of statements (strictly mora) in the meeting by speech recognition and the number of syllables of the corresponding statements in the minute of the meeting. Since the secretary describes the content of the statements in summary, if the number of syllables of the statements and the number of syllables of the corresponding statements of the minute are close, it can be considered that the statements can be regarded as being concise.\\n\\nG. Relevance with start-up statements: statements should be relevant with the sub- ject of discussion as much as possible. If the content of follow-up statements is in common with the content of the topic raising statement (i.e., start-up statement), it can be considered that it is relevant with the theme. Therefore, by evaluating the degree of relevance to the start-up statements (described later), the relevance of statements is evaluated.\\n\\nH. Consistency with parent statements: follow-up statements need to be united or consistent with their parent statements. In other words, the content of the follow- up statement and the content of its parent statement must be semantically related, so it is important to evaluate the degree of the consistency. We use a machine learning technique to decide whether the statement is a “consistent statement or not” which determines the evaluation. The method is described later.\\n\\nWe calculate the degree of relevance between statements in the following way. First, we calculate term frequency–inverse document frequency (TF-IDF) values of words in each statement by using to following formula:\\n\\n79\\n\\n80\\n\\n4 Discussion Skills Evaluation and Training\\n\\nt f id fi, j (cid:2) t fi, j · id fi\\n\\nt fi, j (cid:2)\\n\\nid fi (cid:2) log\\n\\nni, j(cid:2)\\n\\nk∈T nk, j |D| |{d : d(cid:4)ti }|\\n\\nHere, ni,j is the number of occurrences of word ti in document dj. (cid:2)\\n\\nk∈T nk, j is a summation of the number of occurrences of all words in document\\n\\ndj.\\n\\n|D| is the number of all documents and |{d : d(cid:4)ti }| is the number of documents\\n\\nthat contain the word ti.\\n\\nIDF works as a kind of general language ﬁlter. If words (generic words) appear in many documents, their IDF values decrease. If words appear only in speciﬁc documents, their IDF values raise.\\n\\nUsing the TF-IDF value with each statement of one meeting as one document, we\\n\\nweight the word t with the following formula to obtain the degree of relevance:\\n\\n(cid:3)\\n\\n(cid:4)\\n\\nf(t, d1, d2) (cid:2)\\n\\n(cid:2)\\n\\nt f (t, d1)\\n\\nt f (s, d1) +\\n\\ns∈d1\\n\\n(cid:2)\\n\\nt f (t, d2)\\n\\ns∈d2\\n\\nt f (s, d2)\\n\\n∗ id f (t)\\n\\nFor words that appear commonly in two documents, add that value, subtract the value for words that appear only in one side, and sum over all the words. Let this value be the degree of relevance between statements. An example of the calculation is shown in Fig. 4.1.\\n\\nIn Fig. 4.1, t1, … t6 are words and d1, d2 are statements. In this example, t1 and t2 appear in common, other words appear only in one. For common words, the weighted values by the TF-IDF value are summed to 6.0, otherwise the total weighted value by the TF-IDF value is added to 2.5, so the relevance is 6.0 − 2.5 (cid:2) 3.5.\\n\\nBy the way, it can be said that inconsistent statements in the discussion are state- ments that describe topics that are different from topics up to that point. So, consider how to categorize follow-up statements as statements deviating from topics or not. Logistic regression analysis described in Chap. 2 is used for this classiﬁcation. In this case, calculate the probability value that the statement is deviated from the topic, and use this value for the consistency evaluation of the statement. For this purpose, in addition to the linguistic features obtained from the minutes, we use the meta- information given to the minutes. The features used in this method are as follows:\\n\\n(1) Features based on linguistic features of statements.\\n\\n– Relevance to parent statement. – Number of sentences of statements. – Number of characters of statements. – Morpheme unigram and morpheme bigram (see Chap. 2). – Presence of subject word and referring word. – Entity Grid (see Chap. 2).\\n\\n4.1 Evaluation of Speaking Ability\\n\\nFig. 4.1 Relevance between statements\\n\\n(2) Features based on meta-information attached to the minutes\\n\\n– Whether the speaker is a student or not, whether it is a presenter or not. – Whether the speaker of the parent statement is the presenter. – Presence of marking/agreement/disagreement buttons. – Depth from the root in the tree structure (i.e., discussion chunk). – Whether or not the visual referent of the parent statement matches that of the\\n\\ntarget statement.\\n\\n– Presence or absence of slide operation during speaking. – Time for reservation of speaking. – Presence or absence of different statements in time series between the parent\\n\\nstatement and the target statement.\\n\\n– Alternation of questioner.\\n\\nFor morphemes and morpheme pairs that appear during speech, the number of occurrences of nouns, verbs, adjectives, auxiliary verbs, and morpheme pairs is cal- culated by preliminary survey as in the analysis of the task statements described in Chap. 2, we used those exceeding a certain value for the feature. Also, since there is a report that Entity Grid mentioned in Chap. 2 is effective for evaluating text con- sistency (Barzilay and Lapata 2008), it is directly related to topic transition among the syntactic role of the Entity Grid. We focused on only the transition of the theme considered as a transition probability and used it for the feature.\\n\\n81\\n\\n82\\n\\n4 Discussion Skills Evaluation and Training\\n\\nFig. 4.2 Experimental results\\n\\nThe alternation of the questioner, which is the last feature where we separate into questions groups by whether or not the same person asks two questions in a row to the presenter, or whether there are two different people in a row that ask the presenter a question.\\n\\nWe implemented the above method and conducted an experiment on discrimina- tion of inconsistent statements. As a dataset, we used 53 min (discussion content) of seminar in our laboratory (number of statements: 3553). However, since the start-up statements are not the subject in this case, the follow-up statements (the number of statements: 2490) are subject to discrimination. As correct answer data (teacher signals), we decided manually whether a certain statement lacked consistency, and gave the attribute of inconsistency to the statement. 202 follow-up statements were determined to be lacking in consistency.\\n\\nIn order to evaluate the proposed method, the case where learning was carried out without using features based on the meta-information of the minutes was taken as a comparative method. For the evaluation, we used the precision, the recall, and the F-measure that is a harmonic mean of these two values, and additionally carried out the tenfold cross-validation.\\n\\nThe results of this experiment are shown in Fig. 4.2. The results of the consistency judgment by the method we proposed are higher than the case where the feature information given to the minutes is not used, for all the precision, the recall, and the F-measure. The advantage was conﬁrmed.\\n\\nIn addition, when learning by removing each feature by the meta-information of the minutes, the precision, the recall, and the F-measure declined in all the features, and the effectiveness of the used feature was conﬁrmed. Figure 4.3 shows the results of the top ﬁve cases where the F-measure drops greatly.\\n\\nWe will automatically evaluate all statements of the meeting participants using the evaluation indicators mentioned above. Then, let the weighted average value of the value of each indicator be the evaluation of one statement, and let the sum of the evaluation values of all statements of the participant be the numerical value expressing that participant’s speaking ability in discussions at that meeting. By looking at the changes for each discussion at each meeting, the participants will be able to judge whether their discussion skills are rising or stagnating.\\n\\n4.2 Feedback of Evaluation Results\\n\\nFig. 4.3 Effectiveness of features\\n\\n4.2 Feedback of Evaluation Results\\n\\nThe evaluation indicators as described in the previous section are indices for mea- suring the participants’ speaking ability, but of course it should be used not only to measure but also to extend their ability. One of the way to do this is to visualize the results in an easy-to-understand manner and feed back to the participants at just the right time.\\n\\nThe participants should make efforts to raise their speaking ability. For that pur- pose, the system we developed evaluates their statements during the meeting, points out the problems, and encourages improvement. There are various ways this is pointed out. One is to display a message on the main screen during or shortly after speaking or to display the icons next to the name of each participant in the member table of the subscreen. There is another way to display feedback including somewhat detailed information like the icons and their descriptions shown on the tablet used by all the participants. Let me explain each.\\n\\nThe evaluation indicators through using the acoustic features described in the previous section can automatically calculate the evaluation values and feedback dur- ing the meeting. Speciﬁcally, they evaluate in real time each of “voice size,” “voice intonation,” and “speech speed”, and when it is a value lower than a certain threshold value, that is, a “bad” evaluation value, the system pops up a warning message imme- diately on the main screen (normally displaying the presentation slide) as shown at the bottom right of Fig. 4.4. This display will be hidden after 2 s.\\n\\nIn order to measure the effect of this simple direct feedback on participants, we evaluated the participants’ “voice size,” “voice intonation,” and “speech speed” at ﬁve meetings. Results of examining the change in the evaluation values are shown in Fig. 4.5. For “voice size,” the message to be displayed differs depending on whether the evaluation value is smaller or smaller than the reference value. Regarding “speech speed,” it may be faster or slower than the reference value, but in the preliminary experiments, it was extremely rare when it was later than the reference value, and it\\n\\n83\\n\\n84\\n\\n4 Discussion Skills Evaluation and Training\\n\\nFig. 4.4 Feedback message pop-up on main screen\\n\\nFig. 4.5 Experimental results of effects of feedback on main screen\\n\\nwas overwhelmingly more in the case of the faster case. Only when the evaluation value is larger than the reference value, the message is displayed.\\n\\nImprovement numbers shown in Fig. 4.5 are counted as the total number of improvements for all participants. Improvement means that the evaluation value concerning the indicator of a certain participant becomes better after a message on the indicator is displayed while the same person is speaking or immediately after speaking. The improvement rate is the number of improvements for a certain indica- tor divided by the total number of messages in the indicator. In the ﬁve meetings, we could not collect sufﬁcient data, but we found trends of improvement by feedbacking the evaluation results of the statements based on the acoustic features in real time.\\n\\nIn addition to the main screen, there are two subscreens for displaying meeting metadata in discussion mining so that participants can view one of them at any time during the meeting. In addition to the statement content (summary) described by the secretary, the tree structure of the discussion currently being created and the reservation status of the statements are displayed.\\n\\n4.2 Feedback of Evaluation Results\\n\\nFig. 4.6 Feedback display on subscreen\\n\\nThen, as shown in the part surrounded by the red frame line in Fig. 4.6, we present the evaluation result on the latest statement with the icons next to the participant’s name on the panel on which the meeting participant list is displayed. The icon type corresponds to each evaluation indicator (voice size, speech speed, voice intonation, ﬂuency, and tempo), the icon color corresponds to the result (good: green, normal: yellow, and bad: red). The icons to be displayed are limited to two in terms of space and visibility. One of the combinations that are two poor (or ordinary) evaluation indicators or one good evaluation indicator and poor (or ordinary) evaluation indicator is displayed.\\n\\nUnlike the main screen, the subscreen does not always come into view, so it seems that there is not a direct effect. However, the feedback display to the main screen disappears immediately, whereas the display on the subscreen is kept until the next statement by the same participant, it is considered to be effective if the participants want to know their evaluation results compared with the others.\\n\\nIn the discussion mining (DM) system and the meeting recorder, each participant in the meeting is using the tablet. In the DM system, the tablet displays the same thing as the presentation slide displayed on the main screen, and the user can mark it with a pen or ﬁnger.\\n\\n85\\n\\n86\\n\\n4 Discussion Skills Evaluation and Training\\n\\nFig. 4.7 Feedback display on tablet\\n\\nOn the tablet, as shown in Fig. 4.7, using the ﬁve icons, the result of each eval- uation indicator for each statement is displayed immediately after the end of the statement. As with the above-described subscreen, the type of icon corresponds to each evaluation indicator (voice size, speech speed, voice intonation, ﬂuency, and tempo), the color of the icon corresponds to the result value. The evaluation indica- tors based on the linguistic features are still difﬁcult to analyze in real time, so they are not shown on the tablet.\\n\\nSince the display on the main screen and the subscreen is carried out in advance of the display on the tablet, the independent effect on display on the tablet is not measured. However, in order to improve ﬂuency and tempo, we found that it is necessary for the participants to practice their speaking style.\\n\\nAs mentioned in Chap. 2, discussion mining uses a system called a discussion browser for viewing discussion content. Also, in the meeting recorder of Chap. 3, there is a viewing system for the minutes as well. Even on these systems, the user can check the content of each statement and the evaluation result of the statement.\\n\\nAs shown in Fig. 4.8, eight icons are displayed together with other metadata directly under the part where the statement content is displayed. At this time, the content is also analyzed, and the results are also displayed for the evaluation indicators based on linguistic features. Three icons at the right end of the icon row correspond to them. From the left, it shows the result of simplicity (square icon), relevance with start-up statement (icon of “subject”), and consistency with parent statement (icon of “ﬂow”).\\n\\nIcons are superior in quick overview, but are not appropriate to know the details, so if the user moves the mouse cursor over the icon, a description pop-up will be displayed (such a user interface is generally called a tooltip). An example is shown in Fig. 4.9. In this example, if the user moves the cursor to the icon in the form of a wave, the description “voice intonation” and the advice “let’s give a little more inﬂection” are displayed.\\n\\nAlthough it seems that there is immediate effect in the feedback of the evaluation results during the meeting, it may be difﬁcult for the participants to continue speaking at the next meeting while remembering their weaknesses pointed out last time. That’s because the participants are not always trying to raise their speaking ability which\\n\\n4.2 Feedback of Evaluation Results\\n\\nFig. 4.8 Feedback display on discussion browser\\n\\nFig. 4.9 Tooltip for description of evaluation\\n\\neventually improves discussion skills, and they must pay attention to other issues to be considered among meetings (e.g., achievement of tasks).\\n\\nFor this reason, a mechanism to remind the user of the problems of the statement at the last meeting is required. Of course, if the user reviews the minutes, he or she can reconﬁrm the evaluation results of the statements as well as the contents of the previous meeting, but it is unlikely that he/she will frequently review the minutes unless it is a very important agenda.\\n\\nIn the past, we implemented a mechanism of mail notiﬁcation to let participants know that the minutes were completed and accessible. Apart from that, this time, we added a mechanism to notify the participants by the result of the evaluation on the statements at the last meeting and the points to be noticed in the next meeting by e-mail.\\n\\nAn example of feedback by this mail is shown in Fig. 4.10. This is called HTML mail, and the receiver can display contents including images and links to Web pages on the mail application. The sentences and graphs shown in Fig. 4.10 were automat- ically generated based on discussion data. Compared with the evaluation results of the previous meetings, the mail comments on the items that show little improvement with referring the data.\\n\\nThe timing to send this mail is just after the minutes are ready to browse and around noon of the day when the meeting is held the next day. We have a meeting using the DM system every Wednesday, so we will receive two e-mails within a week.\\n\\n87\\n\\n88\\n\\n4 Discussion Skills Evaluation and Training\\n\\nFig. 4.10 Example of feedback mail\\n\\n4.3 Evaluation of Listening Ability\\n\\nOf course, only the speaking ability that I mentioned in the previous section is not the ability of discussion. Since discussion ability is a complex ability, it can be analyzed from various viewpoints.\\n\\nWell, while considering “speaking ability,” we will need to consider “listening ability” as well. There are also indicators that do not raise the evaluation value unless the participants cautiously listen to the statements of other participants such as rele- vance with start-up statements and consistency with parent statements. However, for evaluating from the viewpoint of understanding the entire discussion, it is considered that these indicators are insufﬁcient.\\n\\nTherefore, we propose “summarization ability” as ability equivalent to “listening ability.” This is the ability to hear people’s statements and summarize them. In the DM\\n\\n4.3 Evaluation of Listening Ability\\n\\nsystem, students who are in charge of secretaries during the meeting write content of all statements. However, since it is very difﬁcult to enter all the contents of the statements, it is inevitably the way that the secretary enters while summarizing the statements. We assume that summarization ability is an approximation of the ability of understanding discussions.\\n\\nThe evaluation indicator of the summarization ability is not mere description quantity. Even if a person with a fast keyboard input can enter many characters in a short time, it does not mean that their summarization ability is high. Besides that, it is said that a person who enters only the contents considered necessary and does not input contents judged to be redundant is more summarized.\\n\\nSo, we conducted an experiment to check the following items on the text of the\\n\\nstatements entered by the secretary (Tsuchida et al. 2010).\\n\\n(1) (2) Relationship between the type of discussion and the description quantity.\\n\\nIndividual differences in description quantities by secretary.\\n\\nIn this experiment, we used the discussion content which the six undergraduate and graduate students of our laboratory created for the discussion content, we compared the statement text (hereinafter referred to as the secretary text) entered by the secretary and the full spoken text (hereinafter referred to as transcription text) which was generated by using the speech recognition of the meeting recorder and then corrected for recognition errors by humans in terms of the proportion of keywords.\\n\\nTo take account of individual differences in the written amount of words by the secretary, we sorted six secretaries into three groups (two each for each) according to the average number of input characters per statement. Groups A, B, and C are grouped from the order of the average number of input characters. And, we chose two contents for each secretary in charge. In other words, we targeted 2 discussion contents for each secretary, 4 for each group, and 12 discussion contents overall.\\n\\nThe total number of statements included in all discussion contents was 661 state- ments (55.1 statements per meeting), and the total number of discussion chunks (discussion trees with start-up statements as the root) was 137 (11.4 per meeting).\\n\\nThe speech recognition results of the discussion content include speech disﬂuency such as “um” and “ah,” but it is deleted in the transcription text. Next, in order to obtain the proportion of keywords in statement texts, morphological analysis was performed on each secretary text and transcription text. The type and number of parts of speech included in the result are shown in Fig. 4.11.\\n\\nThe TF-IDF value as described above was calculated using a self-contained word (a word making a meaning by itself) obtained by morphological analysis as a keyword candidate, and one having a threshold value or more was taken as a keyword of a statement.\\n\\nThe total number of characters in the discussion content used in this experiment was 41,946 characters (3495 characters per meeting) in the secretary text and 188,816 characters in the transcription text (15,734 characters per meeting). Also, the number of morphemes (including duplicates) obtained by morphological analysis was 23,950 in the secretary text (1995 per meeting) and 105,843 in the transcription text (8820 per meeting). From the viewpoint of the number of characters and the number of\\n\\n89\\n\\n90\\n\\n4 Discussion Skills Evaluation and Training\\n\\nFig. 4.11 Type and number of parts of speech in statement texts\\n\\nmorphemes, we found that the amount of descriptions necessary to transcribe all statement contents is about four times the descriptive quantity entered by the secretary in real time.\\n\\nFor each group of secretaries, the number of independent words per statement in the secretary text (1), the number of keywords per statement in the secretary text (2), the number of independent words per statement in the transcription text (3), the number of keywords per statement in the transcription text (4), and the total number of common keywords (5). The relationship between these data is as shown in Fig. 4.12.\\n\\nFor the following explanation, (2)/(4) shows the ratio of the number of keywords in the secretary text and the number of keywords in the transcription text which is used as an indicator for measuring how much the keywords transcribed the statements. We call it the “transcription rate” and also treat (5)/(4) which shows the content ratio of common keywords for keywords in the transcription text as an indicator to measure how accurately the secretary grasped the content of the statements, we will call it the “grasping rate.” Likewise, we call (2)/(1) which indicates the content rate of independent words for the keywords in the secretary text as an indicator to measure how much the secretary has described redundant content. We call it the “reduction rate.”\\n\\nIn the experiment, the average of the grasping rate was 43.7%. From this, we can see that more than half of the keywords of the speech contents are missing when the secretary enters them.\\n\\n4.3 Evaluation of Listening Ability\\n\\nFig. 4.12 Relationship between independent words, keywords, common keywords in secretary text and transcription text\\n\\nFig. 4.13 Difference in description amount of statement text for each secretary\\n\\nFigure 4.13 compares the average and standard deviation of the transcription rate (2)/(4) for each secretary group. Similarly, the content ratio of the common keywords in the transcription text (grasping ratio) (5)/(4), the content ratio of the common keywords in the secretary text (5)/(2), and the content rate of the keywords in the secretary text (reduction rate) (2)/(1) are also shown in the same ﬁgure.\\n\\nSince we grouped according to the average number of input characters per state- ment, we found that there is a difference in the transcription rate for each group. In addition, from the graph of the grasping rate, it is understood that as the transcription rate increases, more keywords in the speech contents are described. On the other hand, when looking at the graph showing the ratio of common keywords in the sec- retary text, there was not so much individual difference. In other words, regardless of the amount of text entered by the secretary, the percentage of keywords in the secretary text can be thought of as nearly constant.\\n\\nFrom this, it seems that there was not much individual difference in the quality of the minutes as it is, although the amount of text to be entered depends on individual\\n\\n91\\n\\n92\\n\\n4 Discussion Skills Evaluation and Training\\n\\nFig. 4.14 Relationships between transcription rate and grasping rate\\n\\nsecretaries. Also, from the graph of the reduction rate (2)/(1), it can be seen that there is a possibility that the secretary with few average input characters may have entered the secretary text more concisely compared to the secretary that has many. In other words, while a secretary with many average input characters was trying to describe the details of the speaker’s speech as much as possible, the secretary with a small average number of input characters was thought to have entered input while paraphrasing the speech content as short as possible.\\n\\nAt the meeting, there are various types such as discussion with only question and answer, discussion such as argument on a certain subject like brainstorming. Therefore, for example, if a meeting has a seminar style that includes a question and answer session, a questioner can refer to a presentation slide, so it is easier to understand the meaning and it is easy to create a secretary text. On the other hand, a discussion including conceptual opinions which cannot be referenced easily through materials can be difﬁcult to understand, which makes it difﬁcult to create a secretary text. It seems that there is a relation between the discussion type and the description quantity of the secretary text.\\n\\nFor this reason, a transcription ratio and a grasping rate are calculated for discus- sion chunks within each discussion content, a reference value SSi expressed by the following equation is calculated for each value, and these tendencies are compared. Where X i is the value of X (X is the transcription rate or the grasping rate), E[X] is the arithmetic average of X, and SD[X] is the standard deviation of X.\\n\\nSSi (cid:2) Xi − E[X ] SD[X ]\\n\\nThe result is shown in Fig. 4.14. As shown in this ﬁgure, in most discussion\\n\\nchunks, the transcription rate and the grasping rate show similar tendencies.\\n\\nHowever, as in the sixth discussion chunk in the graph on the right side of Fig. 4.14, there was the case where the grasping rate was small despite the large transcription rate. When we examine this discussion chunk in detail, we found out that it is a discussion about the ﬁgure in the presentation slide. In such a case, since the partici- pants are pointing at the ﬁgure while speaking, there was a tendency to frequently use referring words. Therefore, while the secretary often inputs a word that supplements the contents of the referring words, it was found that the grasping rate decreases because the words are not directly included in the speech content.\\n\\n4.3 Evaluation of Listening Ability\\n\\nAs mentioned at the end of Chap. 2, the discussion mining system allows the user to select arbitrary character strings in the slide using a device called a discussion commander, so that characters in slides are automatically inserted into the editing window of the secretary tool. This solves the problem of the reference to the slide, so there is no big difference in the description amount of the secretary by the type of discussion.\\n\\nAs there is no dependence on the discussion type and the amount of description of the secretary, the summarization ability can be deﬁned as generically as possible regardless of the content of the meeting. As mentioned earlier, it is considered that there are three types of individual differences that are likely to occur such as the transcription rate, the grasping rate, and the reduction rate.\\n\\nTherefore, we will use these three values as indicators of the summarization ability (i.e., listening ability) of the corresponding secretary. Each time the meeting is over, these values are calculated and fed back to the participants in charge of the secretary.\\n\\n4.4 Discussion Skills Training\\n\\nWell, in this chapter, to evaluate the discussion ability, we have considered the ability to make statements as a speaking ability, the ability to summarize statements as a listening ability as its constituent elements. Of course, we need to consider other abilities as well, but we focus on discussion ability based on automatically evaluable indices and give feedback to meeting participants quickly. This is to make it possible for each participant to give better results at the next opportunity based on the feedback. Any training starts with grasping your current state ﬁrst. And, by being conscious of the current state, the direction of the effort will be decided. Like sports, we improve by accumulating practice. In the case of discussion, at the meeting, the participants make as many statements as possible and make sure that the value of the speaking ability is raised. Also, when the participants are responsible for acting as the secretary, they should act as a secretary with consciousness to increase the value of the indicators of their listening ability.\\n\\nBy the way, our laboratory decides the scores of the seminar based on the data obtained by the discussion mining system (Nagao et al. 2015). However, since we are now thinking of evaluating the process in terms of effort rather than the result, we evaluate the quantity more than the quality of the statements, so it is not always necessary for the outstanding grades to have a particularly high discussion ability. However, the ability of those who are not making efforts will not rise, so people with low scores clearly have low discussion skills.\\n\\nIn order to train the discussion ability and communication ability, it is necessary to record the evaluation results with a considerably long span. Changes in the short-term evaluation results are effective as a clue to evaluate and improve the performance of the developed system, but it will not be enough to judge whether a person certainly has that capability. This is similar to the fact that local optimal solutions do not necessarily become true optimal solutions in optimizing parameters of machine learning models.\\n\\n93\\n\\n94\\n\\n4 Discussion Skills Evaluation and Training\\n\\nIt is often said that it takes time for human education, I think that discussion skills as well as basic academic ability need to be ﬁrmly acquired based on long-term perspectives. To that end, I think that we must have a clear guide to become a signpost. With clear and unfounded guidance, people will lose conﬁdence in themselves. The technique we developed is useful for clarifying what kind of ability improves what to do. I believe that “evidence-based education” will be possible by such a mechanism. To the reader, I think that the ability of discussion is a fundamental and important skill for human beings to do intellectual activities. Improving this ability is a task that can be said to be essential for many people. However, if visible growth does not appear, people will get bored with that training. In the next section, I would like to introduce one approach to that problem.\\n\\n4.5 Gamiﬁcation for Maintaining Motivation to Raise\\n\\nDiscussion Abilities\\n\\nIn recent years, the word gamiﬁcation has begun to be widely used. Gamiﬁcation is said to be “movement to incorporate elements of the game into development of social activities and service applications” (McGonigal 2011). Speciﬁcally, to introduce various game elements, such as level and reward, into the system to improve the motivation to use the system. The concept of gamiﬁcation is not new at all, it has been used for various services before. For example, point cards of shops, access ranking of blogs, etc. can be regarded as a type of gamiﬁcation. In the beginning of the 2010s, it seems that general methods of utilizing such game elements have become generically called gamiﬁcation.\\n\\nMany applications for gamiﬁcation aim to increase the number of times of service used by unspeciﬁed number of users and to improve work efﬁciency, and in most cases, the effect will appear immediately after introduction of gamiﬁcation. However, in the meetings we are targeting, it is still difﬁcult to improve the discussion ability in a short period of time, although it is possible to facilitate activities within a limited time and temporarily increase motivation. In order for participants to be able to conduct a high-quality discussion, it is necessary to record, evaluate, and support in detail the process of gradually growing and experiencing.\\n\\nThe gamiﬁcation framework for discussion consists of seven elements as shown in Fig. 4.15. Students unfamiliar with the discussion at the starting point can be motivated by each game element and ﬁnally reach the point where their discussion ability has improved.\\n\\nThe elements of this gamiﬁcation framework are described below.\\n\\n1. Goal\\n\\nThe purpose of the discussion conducted at the university’s laboratory is divided into two categories: deciding future policies by improving the content of research by exchanging views and improving the student’s discussion ability.\\n\\n4.5 Gamiﬁcation for Maintaining Motivation to Raise Discussion …\\n\\nFig. 4.15 Gamiﬁcation framework for discussion\\n\\nBased on the premise that students’ abilities will improve, discussion for improv- ing research will be effectively done. We will pay attention to improving student’s discussion skills. Therefore, we will make this a ﬁnal goal.\\n\\nAs a goal element to reach the ﬁnal goal step by step, it is possible to decompose discussion ability into several components. This decomposed capability has advanced abilities such as “can ﬁnally summarize stories” from basic ability such as “can say with loud voice” for example. Meeting participants will voluntarily target the ability they want to improve from the ﬁnely disassembled discussion abilities and aim to improve their abilities by speaking to achieve each goal element.\\n\\n2. Visualization\\n\\nMeeting participants will be aware of the improvement in capacity and gain a sense of accomplishment by grasping the process of mastering the decomposed discus- sion abilities independently one by one. For that purpose, it is necessary to have a mechanism to inform the improvement of the discussion ability in real time and a mechanism to conﬁrm the degree of improvement of their own ability later.\\n\\n3. Rule\\n\\nAs mentioned in the previous sections, when evaluating the discussion ability auto- matically, semantic evaluation of the contents of the speech is difﬁcult and only quantitative evaluations such as the number of statements and the volume of voice can be done in real time. So, we are making semantic evaluations of the content of each other’s statements by participants. Also, instead of evaluating others throughout the discussion, we evaluate each statement at a time aiming to increase the number of statements. Those that regulate the behavior of such participants are called rules.\\n\\n95\\n\\n96\\n\\n4 Discussion Skills Evaluation and Training\\n\\n4. Design\\n\\nIt is a mechanism to improve the motivation for students to participate by attractive interfaces and various compensations. By giving a virtual remuneration, it is neces- sary to carefully decide factors that become extrinsic motivation, paying attention not to signiﬁcantly lower the intrinsic motivation.\\n\\n5. Social\\n\\nWe introduce a mechanism that enables us to check the degree of mastery of discus- sion skills using Web pages, the users compare their abilities and motivate each other to compete. In addition, it is possible to look back on the statements of other par- ticipant’s high discussion skills and use them as reference for their own statements. Furthermore, at the end of the meeting, we aim to further motivate competition and satisfy self-esteem desires by ranking and praising participants who demonstrated excellent discussion abilities at that time.\\n\\n6. Tutorial\\n\\nFor students who join the meeting for the ﬁrst time belonging to the laboratory, it is necessary to understand not only the discussion mining system described in Chap. 2, but also the elements of the new gamiﬁcation, which is a heavy burden. Therefore, to understand how to use each system is also one goal in gamiﬁcation, and we will promote system understanding by presenting detailed guidelines.\\n\\n7. Difﬁculty adjustment\\n\\nThe users are roughly divided into two types: teachers and students. Also, from the viewpoint of discussion ability, among students, there is a big difference between students belonging to the laboratory and newly assigned students. By visualizing the difﬁculty and complexity of the target, participants with a wide range of discussion ability will be able to select appropriate targets for themselves.\\n\\nWe will refer to the mechanism that introduced this gamiﬁcation framework for\\n\\ndiscussion as gamiﬁed discussion (GD, for short).\\n\\nThe purpose of GD is to keep motivation to improve the discussion skills of students participating in the meeting. In order to evaluate the improvement of the discussion ability, manual evaluation information is required besides the information recorded by discussion mining, but at the same time, by evaluating others during the meeting, the risk of hindering concentration on the discussion.\\n\\nTherefore, it is necessary to lower the cost of evaluation as much as possible. First, we will break down discussion ability, which is a comprehensive skill of complex manners, so that we can evaluate it more concretely.\\n\\nFor example, the Japan Debate Association classiﬁes abilities necessary for dis- cussion or skills acquired by discussion into the following four categories: (http:// www.debate-association.org/).\\n\\n1. Comprehension ability. 2. Analytical ability. 3. Constitutional ability.\\n\\n4.5 Gamiﬁcation for Maintaining Motivation to Raise Discussion …\\n\\nFig. 4.16 Discussion ability graph (a part)\\n\\n4. Transmission ability.\\n\\nComprehension ability is the ability to understand the background of the agenda, the content and the intention of the statements by the participants. Analytical ability is the ability to analyze whether there is contradiction or ambiguity in the participant’s statements after understanding the contents of the discussion. Constitutional ability is the ability to constitute its own statement content so that it is easy to understand and convincing. Transmission ability is the ability to devise ideas for speech, attitudes, gestures, etc. so that the other participants can easily understand.\\n\\nIn addition, participants in the meeting using the discussion mining system must have the ability to reserve the statement and to select the object in the slide by the operation of the discussion commander. In order to conduct a discussion without delay, it is necessary to perform these operations accurately and smoothly, so we decided to classify the ability to deal with systems related to discussion mining as one class of discussion skills. This is called DM ability, and the abilities belonging to the DM ability category such as “I can speak without spending time to reservation” and “speak by referring elements on the screen” have been newly added.\\n\\nAs discussion abilities, there are many inclusion relationships such as “I can say opinions” and “I can say brieﬂy”. Therefore, we investigated whether there is an inclusion relation among all the discussion abilities, set the ability to be a subset of the two abilities in inclusion relation as a lower level ability, and make the other the higher level ability. Based on this, we created a graph in which each discussion ability is a node and nodes of two abilities in inclusion relation are linked by links. A part of it is shown in Fig. 4.16. This is called a discussion ability graph. In this graph, discussion ability nodes are color coded for each category.\\n\\n97\\n\\n98\\n\\n4 Discussion Skills Evaluation and Training\\n\\nFig. 4.17 Personal page (goal setting view)\\n\\nIn GD, we hope that ﬁnally the comprehensive discussion ability will be improved by acquiring subdivided discussion abilities one by one based on the discussion ability graph. Therefore, we use the process of acquiring each subdivided ability as a goal element in gamiﬁcation.\\n\\nThe automatic evaluation of the discussion ability described in previous sections was to evaluate whether the way of speaking was appropriate or whether the content being talked was appropriate, but the evaluation of the discussion ability in GD is performed by evaluating the achievement degree of a discussion goal set by one participant by another participant. These are considered to evaluate different aspects of discussion skills. The latter is more difﬁcult to evaluate automatically because there are no clear evaluation criteria. In other words, since the methods based on automatic recognition are suitable for automated evaluation, no human evaluator is necessary, but GD is a method in which multiple participants mutually evaluate their statements during discussion.\\n\\nIn order to minimize the burden on the evaluator, the speaker sets only one of the discussion abilities (goal elements) as a target, while the other participants who are the evaluators listen to the statements and evaluate only with respect to one goal. The meeting participants access the Web page called Personal Page as shown in Fig. 4.17 beforehand and set targets while checking the discussion ability graphs, categories of each ability and acquisition difﬁculty level.\\n\\nWe also created an evaluation interface to evaluate the speaker’s goal during the meeting as shown in Fig. 4.18. The evaluation interface was created as a browser application that can be used with a Web browser so that it can be used from various\\n\\n4.5 Gamiﬁcation for Maintaining Motivation to Raise Discussion …\\n\\nFig. 4.18 Statement evaluation interface\\n\\ndevices. In particular, it is a user interface that makes it easier to evaluate by touch operation, it is recommended to use a portable device with a built-in touch panel such as a tablet.\\n\\nAlso, when the evaluator determines that the content of the statement is excel- lent irrespective of the degree of achievement of the target, there are cases where the evaluation of the target includes the evaluation of the quality of the statement. Therefore, a function to evaluate the quality of the statement independently of the speaker’s goal was added to the statement evaluation interface.\\n\\nThe evaluator listens to the speaker’s statement while checking the speaker’s goal displayed on the statement evaluation interface and evaluates whether or not the target has been achieved in ﬁve stages. In the ﬁve grades evaluation, I made it “it is not done,” “it is done a little,” “it is half done,” “it is done well,” and “it is done very well” in order from 1. Evaluation of the quality of the statement makes it possible to input plus 1 or plus 2. This is not to be strictly evaluated in ﬁve stages like the goal, but if you think “good,” it is positive 1, if you think it is “very good” plus 2. The granularity of these evaluations is the result of adjustment by the authors in order to enable as rapid and precise evaluation as possible.\\n\\nAfter entering the evaluation and pressing the submit button (tap for tablet, mouse click for PC) the score will be sent to the server and recorded. At this time, the interface screen switches to the presentation slide display. If the user wants to redo the evaluation, he or she can revise the evaluation of the current statement by displaying the statement evaluation interface again by selecting “Evaluation” from the tab list displayed at the top of the screen.\\n\\n99\\n\\n100\\n\\n4 Discussion Skills Evaluation and Training\\n\\nFig. 4.19 User information view\\n\\nWhen evaluations are made on statements, all the evaluation scores are totaled on the server, and the results are displayed in the statement evaluation interface of the speaker. The ﬁnal evaluation score for the target of the speaker is the average score of all the evaluations and the total of the evaluation of the quality of the statement. The score for the statement is displayed in the user information view at the upper right of the screen as shown in Fig. 4.19 so that it can be conﬁrmed immediately. In the user information view, only the score for the last speech is displayed.\\n\\nThe speaker can check the evaluation score after the end of the statement and make decisions such as whether to speak again with the same goal or change the goal.\\n\\nThe average of all the evaluation scores with respect to the target goal of the speaker is the point at which the speaker achieves that goal. If the average of the evaluations from all participants is equal to or greater than the “well-done” evaluation, we judge that the speaker has achieved the goal adequately.\\n\\nAt the end of the meeting, participants can check the degree of accomplishment of each discussion ability category on the Personal Page on the Web, while comparing it with other people on the radar chart. As shown in Fig. 4.20, since the current achievement level is visualized easily in graphs, it is possible to set the next goal while grasping their ﬁeld of strength/weakness. The degree of achievement for each category is the average of all the historical high scores of all targets belonging to that category.\\n\\nIn addition, details of the elements of each category are displayed as a table as\\n\\nshown in Fig. 4.21.\\n\\nIn addition, the Personal Page has a history display function that allows the user to look back at all the past statements of the past meetings, and the user can check the history of statements he or she evaluated in the past and the history of his/her statements evaluated in the past. As shown in Fig. 4.22, for each statement, the contents of the statement and the target goal when he or she speaks are displayed, and when the user press the “Look at the score” button, all evaluation scores for\\n\\n4.5 Gamiﬁcation for Maintaining Motivation to Raise Discussion …\\n\\nFig. 4.20 Current status of discussion ability\\n\\nFig. 4.21 Details of discussion ability\\n\\n101\\n\\n102\\n\\n4 Discussion Skills Evaluation and Training\\n\\nFig. 4.22 Past statement data page\\n\\nthat statement are displayed. In order to increase the usefulness of the score while maintaining the anonymity of the evaluator, the evaluator’s name is expressed as “teacher” or “student” only. By browsing the history, it is possible to check the difference between the contents of the statement and the target goal and to make a plan to achieve the goal.\\n\\nIn this way, by applying gamiﬁcation to discussion, the participants can positively improve their discussion abilities. Although the effectiveness of this system has not been fully veriﬁed yet, when comparing the average number of statements per unit time of participants in each year before and after GD introduction, it increased from 0.99 to 1.73. In the questionnaire survey, all the participants said that the\\n\\n4.5 Gamiﬁcation for Maintaining Motivation to Raise Discussion …\\n\\nintroduction of GD raised the motivation for discussion. Of course, this alone cannot prove that gamiﬁcation contributes to improving the discussion ability, but I think that by examining the long-term execution data, we can verify the effect more precisely.\\n\\nReferences\\n\\nR. Barzilay, M. Lapata, Modeling local coherence: an entity-based approach. Comput. Linguist.\\n\\n34(1), 1–34 (2008)\\n\\nK. Kurihara, M. Goto, J. Ogata, Y. Matsusaka, T. Igarashi, Presentation Sensei: A Presentation Training System using Speech and Image Processing, in Proceeding of ICMI 2007, pp.358–365 (2007)\\n\\nJ. McGonigal, Reality Is Broken: Why Games Make Us Better and How They Can Change the World\\n\\n(Penguin Books, 2011)\\n\\nK. Nagao, M. P. Tehrani, J. T. B. Fajardo, Tools and evaluation methods for discussion and presen-\\n\\ntation skills training, SpringerOpen J. Smart Learn. Environ. 2(5) (2015)\\n\\nT. Tsuchida, S. Ohira, K. Nagao, Creation of contents and visualization of metadata during face-\\n\\nto-face meetings. Transac. Inf. Process. Soc. Jpn.51(2), 404–416, 2010. (in Japanese)\\n\\n103\\n\\nChapter 5 Smart Learning Environments\\n\\nAbstract Our university is currently developing an advanced physical–digital learn- ing environment that can train students to enhance their discussion and presentation skills. The environment guarantees an efﬁcient discussion among users with state- of-the-art technologies such as touch panel discussion tables, digital posters, and an interactive wall-sized whiteboard. It includes a data mining system that efﬁciently records, summarizes, and annotates discussions held inside our facility. We also developed a digital poster authoring tool, a novel tool for creating interactive digital posters displayed using our digital poster presentation system. Evaluation results show the efﬁciency of using our facilities: the data mining system and the digital poster authoring tool. In addition, our physical–digital learning environment will be further enhanced with a vision system that will detect interactions with the digital poster presentation system and the different discussion tools enabling a more auto- mated skill evaluation and discussion mining. In addition, we argue that students’ heart rate (HR) data can be used to effectively evaluate their cognitive performance, speciﬁcally the performance in a discussion that consists of several Q&A segments (question-and-answer pairs). We collected HR data during a discussion in real time and generate machine learning models for evaluation. HR data are used to estimate the degree of self-conﬁdence of speech while speakers participate in Q&A sessions. We checked whether there is a correlation between the degree of conﬁdence and the appropriateness of statements. So, we can evaluate the mental appropriateness of the statements. Furthermore, we realized a presentation rehearsal system using virtual reality technology. Based on the system, students can easily experience the act of presentation in front of many audiences with a wide auditorium in virtual space. In this chapter, I will also describe them in detail. Keywords Leading graduate school · Leaders’ saloon · Presentation skills · Digital poster · Psychophysiology-based activity evaluation · Virtual reality presentation rehearsal system\\n\\n© Springer Nature Singapore Pte Ltd. 2019 K. Nagao, Artiﬁcial Intelligence Accelerates Human Learning, https://doi.org/10.1007/978-981-13-6175-3_5\\n\\n105\\n\\n106\\n\\n5 Smart Learning Environments\\n\\n5.1 Environments for Evidence-Based Education\\n\\nWe have been developing an advanced physical–digital learning environment that can train students to enhance their discussion and presentation skills. The environment guarantees an efﬁcient discussion among users with state-of-the-art technologies such as touch panel discussion tables, digital posters, and an interactive wall-sized whiteboard. It includes a data mining system that efﬁciently records, summarizes, and annotates discussions held inside our facility. We also developed a digital poster authoring tool, a novel tool for creating interactive digital posters displayed using our digital poster presentation system. Evaluation results show the efﬁciency of using our facilities: the data mining system and the digital poster authoring tool. In addition, our physical–digital learning environment will be further enhanced with a vision system that will detect interactions with the digital poster presentation system and the different discussion tools enabling a more automated skill evaluation and discussion mining presented in Chap. 2.\\n\\nAs I mentioned earlier, a lot of attention has been paid to evidence-based research, such as life-logging (Sellen and Whittaker 2010) or big data applications (Armstrong 2014), that proposes techniques to raise the quality of human life by storing and analyzing data of daily activities in large quantities. This technique has been applied in the education sector, but a key method has not been found yet because it is generally hard to record intellectual activities, accumulate, and analyze data in a large scale, and compare it with things like a person’s physical activities, position, and movement information. Although there are some recent studies on the automated recording of intellectual activities in more detail, their techniques are not sufﬁcient to be applied to an automated evaluation of a person’s intellectual activities. Thus, this study aims to develop a new environment to empower the skills of students not only in real time but also ofﬂine based on the abundant presentation and discussion data analyses.\\n\\nOur study focuses on the new graduate leading program of Nagoya University that aims to cultivate future industrial science leaders. This leading graduate program has a new physical–digital environment for facilitating presentations and discussions among the selected students of the program. In particular, the presentations and discussions of the students are recorded in detail, and the mechanism of knowledge emergence is analyzed based on a discussion mining system. Furthermore, we have evaluated the performance of some students with respect to their skill in creating a digital poster using our recently developed tool.\\n\\n5.2 Related Work and Motivation\\n\\nThis section has two parts: discussion evaluation and presentation evaluation. For each evaluation system, there are also two parts based on the type of system: a fully automatic system and a semi-automatic system. A fully automatic system calculates\\n\\n5.2 Related Work and Motivation\\n\\nthe scores of discussion or presentation quality in an automated fashion, while a semi- automatic system supports the people in evaluating the discussion or presentation with some evidential data.\\n\\n5.2.1 Discussion Evaluation\\n\\n5.2.1.1 Fully Automatic\\n\\nOne of the most familiar types of intellectual and creative activities is discussion at meetings. There is great signiﬁcance in analyzing discussion in a scientiﬁc way and evaluating its quality. Therefore, we proposed and implemented a method for evaluating the discussion ability of students in meetings in a university laboratory setting. There are roughly two kinds of evaluation methods as mentioned in Chap. 4:\\n\\n(1) One based on acoustic information, that is, the manner of speaking. (2) One based on language information, that is, the contents of speech.\\n\\nMethod (1) evaluates the appropriateness of utterances in a discussion by using the acoustic characteristics of speech. The characteristics are automatically evaluated in real time and fed back to speakers during a meeting. For example, we measure the voice size (loudness), voice intonation, speech speed, ﬂuency, tempo, and other vocal aspects of a speaker and automatically evaluate the acoustic appropriateness of the statements. If anything is determined to be inappropriate, the system provides feedback to the speaker in several ways, such as with a message popping up on a screen.\\n\\nMethod (2) analyzes linguistic characteristics in consideration of context. For example, we estimate the consistency of the context of statements by using machine learning techniques. Then, the linguistic appropriateness of the statements is auto- matically evaluated.\\n\\nWe believe that carefully examining these methods over a period of time will result in a more detailed analysis that helps us focus on more appropriate training for students.\\n\\nStudents’ improvement in discussion ability is evaluated in subsequent training. Discussion–skill training is carried out through a repeat cycle that consists of notify- ing a person of a problem and giving advice via e-mail prior to a meeting, evaluating statements during the meeting, and the person reﬂecting and making improvements after the meeting.\\n\\nThis section reviews other methods for discussion evaluation including online\\n\\ndiscussions such as text discussion on bulletin boards.\\n\\nWith the abundant data in discussions, there is difﬁculty in searching for good qual- ity posts. An automatic rating of postings in online discussion forums was presented based on a set of metrics (Wanas et al. 2008). This set of metrics was used to assess\\n\\n107\\n\\n108\\n\\n5 Smart Learning Environments\\n\\nthe value of a post and includes the following: relevance, originality, forum-speciﬁc features, surface features, and posting-component features. With these metrics used to train a nonlinear support vector machine classiﬁer, the posts were then categorized to their corresponding levels (High, Medium, or Low).\\n\\nAnother system called Auto-Assessor used natural language processing tools to assess the responses of students to short-answer questions (Cutrone et al. 2011). The system utilized a component-based architecture with a text preprocessing phase and a word/synonym matching phase to automate the marking process. In their system evaluation, they compared the assessment results of the Auto-Assessor and Human Graders to verify the possibility of applying the proposed system in practical situations.\\n\\nHowever, these fully automatic systems still have some drawbacks. Some methods are language independent resulting in poor performance in relevance and originality (Wanas et al. 2008); thus, other additional techniques should be employed in their assessment of discussions. Also, even with additional NLP techniques, the weights given to words are not varied (Cutrone et al. 2011) hindering the system from iden- tifying words that are more signiﬁcant than others.\\n\\n5.2.1.2 Semi-automatic\\n\\nAside from fully automatic systems, some studies employed a semi-automatic approach. One such study is the implementation of a group discussion evaluation method and a discussion evaluation support system that focused on ex-post evalua- tion (Omori et al. 2006). The system provided a Web-based interface to display the evaluation item and the evaluation criteria so that users can easily make a score to each of the discussion remarks based on clearness of remarks, proposal of issues, and logicality of remarks. Results conﬁrmed the effectiveness of both their evaluation method and support system.\\n\\nWith the abovementioned systems, there was no mention about one problem in discussions, which is the difﬁculty in getting students to actively participate. Thus, a gamiﬁcation framework was integrated into a discussion support system for enhanc- ing and sustaining motivation in student discussions (Ohira et al. 2014). Besides sustaining student motivation, the system also evaluates and visualizes improvement of the students’ capacity to discuss. It also supports the users to evaluate the quality of each discussion statement.\\n\\nHowever, with the two semi-automatic systems, more experiments are needed to determine the effect of teachers’ feedback to the students (Omori et al. 2006) and its performance in real-world settings.\\n\\n5.2 Related Work and Motivation\\n\\n5.2.2 Presentation Evaluation\\n\\n5.2.2.1 Fully Automatic\\n\\nA presentation training system called Presentation Sensei was implemented to observe a presentation rehearsal and give presentation feedback to the speaker (Kurihara et al. 2007). The system is equipped with a microphone and camera to analyze the presentation by combining speech and image processing techniques. Based on the results of the analysis, the system provides the speaker with recommen- dations for improving presentation delivery such as speed and audience engagement. During the presentation, the system can alert the speaker when some of the indices, speaking rate, eye contact with the audience, and timing, exceed predeﬁned warning thresholds. After the presentation, the system generates summaries of the analysis results for the user’s self-examination. Although this system focuses on self-training, it still needs to be tested in a real presentation environment.\\n\\n5.2.2.2 Semi-automatic\\n\\nAnother presentation training system called PitchPerfect was implemented to develop conﬁdence in presentations (Trinh et al. 2014). From interviews with presenters, the authors uncovered mismatches between best rehearsal practices as recommended in the presentation literature, the actual rehearsal practices, and support for rehearsal in conventional presentation tool. Thus, they developed the proposed system, an integrated rehearsal environment that supports users to evaluate their presentation performance during preparation for structured presentation in PowerPoint. Their user study with 12 participants demonstrated that PitchPerfect led to small but signiﬁcant improvements in perceived presentation quality and coverage of prepared content after a single hour of use, arising from more effective support for the presenter’s content mastery, time management, and conﬁdence building.\\n\\n5.2.3 Motivation\\n\\nIn the initial phase of our research, we selected a semi-automatic approach to evaluate the discussion and presentation. However, our proposed system can acquire several kinds of student activity-related data to make evaluation automated in the near future. We understand that current technologies that analyze human activity data which are fully automated are still insufﬁcient to realize our purposes so we focused on data acquisition by using our new environment for discussion and presentation.\\n\\n109\\n\\n110\\n\\n5 Smart Learning Environments\\n\\nFig. 5.1 Leaders’ saloon environment\\n\\n5.3 Leaders’ Saloon: A New Physical–Digital Learning\\n\\nEnvironment\\n\\nThe Leaders’ Saloon shown in Fig. 5.1 is capable of creating discussion contents using discussion tables, digital poster panels, and an interactive wall-sized white- board.\\n\\n5.3.1 Discussion Table\\n\\nEach student uses a tablet to connect with the facilities including the discussion table shown in Fig. 5.2. The content and operation history of the discussion table are automatically transferred and shared to the server, the meeting cloud. Previous table content can be easily retrieved, and any texts or images can be reused. Such reference and quotation operations are recorded and analyzed to discover semantic relationships between discussions. Furthermore, software that analyzes temporal changes of table contents with the corresponding users is also being developed.\\n\\n5.3.2 Digital Poster Panel\\n\\nFor poster presentations, a digital poster panel system, shown in Fig. 5.3, is used for content and operation analyses. The system helps the users create digital posters and analyze their creation process. The system also supports the retrieval of previously presented posters and allows users to annotate them. Annotations are automatically sent to the author and are analyzed by the system to evaluate the quality of the poster. Poster presentations as well as the regular slide-based presentations are also broadcasted by streaming on the Web. The system collects and analyzes the feedbacks\\n\\n5.3 Leaders’ Saloon: A New Physical–Digital Learning Environment\\n\\nFig. 5.2 Students using discussion table\\n\\nbased on comments and reviews given by Internet viewers (e.g., Twitter users can associate their tweet messages with any scenes from the presentation based on the starting and ending timestamps).\\n\\n5.3.3 Interactive Wall-Sized Whiteboard\\n\\nAs shown in Fig. 5.4, our facility houses a wall-sized whiteboard. Unlike the tradi- tional whiteboards, we are able to physically and digitally write on the whiteboard. We use a special projector equipped with an infrared sensor to detect the location of the digital pen with respect to the wall. The writings and interaction on the white- board can then be recorded by cameras. The captured data using the camera can identify the physical interaction in combination with the given digital interaction information. This system is under development, and we are working on proposing a new evaluation system that can enhance the presentation and discussion performance of students using this system.\\n\\n111\\n\\n112\\n\\n5 Smart Learning Environments\\n\\nFig. 5.3 Poster presentation using digital poster panel system\\n\\n5.4 Importing Discussion Mining System into Leaders\\n\\nSaloon\\n\\nWe developed an extended version of the discussion mining system presented in Chap. 2 working at the Leaders’ Saloon. The discussion tables are used to operate and visualize discussion structures. The users also use discussion commanders and the previously described discussion mining system.\\n\\nIn this section, we explain two systems implemented on the discussion tables to visualize real information recorded by the discussion mining system: (1) discussion visualizer, a system to visualize the structure of an ongoing discussion, and (2) discussion reminder, a system to retrieve and visualize past discussions.\\n\\n5.4.1 Discussion Visualizer\\n\\nThe discussion visualizer shown in Fig. 5.5 is a system to visualize the structure of meeting discussions shown in the discussion table (Sect. 3.1). This visualizer consists of a meeting view, a slide list, a discussion segment view, and a discussion segment list.\\n\\n5.4 Importing Discussion Mining System into Leaders Saloon\\n\\nFig. 5.4 Capturing data using interactive wall-sized whiteboard\\n\\nFig. 5.5 Discussion visualizer\\n\\n113\\n\\n114\\n\\n5 Smart Learning Environments\\n\\nThe meeting view provides a preview of camera records showing the participants, a list of all attendances, and the elapsed time of the presentation. A list of slide thumbnails displayed in the presentation are also shown, and the thumbnail of the currently displayed slide is emphasized in the slide list. Speakers can operate the slideshow by selecting the thumbnail in this view using the touch panel.\\n\\nThe discussion segment view shows the information about the discussion segment, which contains the current statement. The texts of the start-up statement, which was the trigger of the discussion, and the parent statement of the current statement (if it is a follow-up statement) are shown at the upper side of this view. The structure of the discussion segment is shown at the bottom side of this view. Users can also make corrections of parent statements. Participants conﬁrm the stream of discussion at the meeting through the discussion segment list. In this list, the nodes representing main topics are shown as rectangle nodes while the subtopics are shown as circle nodes. These discussion segment topics are displayed as a chain structure in the middle, the keywords of multiple discussion segments are displayed on the left, and the keywords of the main topics or subtopics are displayed on the right. Moreover, the nodes that involve questions and answers are represented by the speciﬁc character Q. The amount of agreements on the statements input by the discussion commanders are represented as a density of the color of the nodes. The icons are displayed next to the node containing the statements marked by discussion commanders. Therefore, it enables participants to conﬁrm when important discussions occur.\\n\\nThere are various kinds of discussion segments created by the discussion mining system, for example, short segments with only comments on the presentation and long segments that contain a lot of statements resulting from intense debate. There is also a possibility that long discussion segments have follow-up statements whose content derives from the topic of the start statement. Thus, we think that the start statement is the root node of the discussion segment and some subtopics derive from this root node.\\n\\n5.4.2 Discussion Reminder\\n\\nA review and sharing of previous discussion contents lead to a uniformed knowl- edge level among all participants, wherein low-level participants can make remarks actively. This will also prevent redundant discussion. From here, we can then think about topics from a new point of view and ﬁgure out solutions to problems that have not been solved due to lack of technology. Therefore, we develop a system to retrieve and browse past discussions on time, called the discussion reminder.\\n\\nThere are two concerning issues in the development of the discussion reminder. One is an accurate understanding of discussion contents, and another issue is the quick retrieval of discussion contents preventing any disruption in the ongoing discussion. Unclear and inadequate sharing of discussion content will inhibit the achievement of a uniformed knowledge level and will lead to misunderstandings and confusion. Thus,\\n\\n5.4 Importing Discussion Mining System into Leaders Saloon\\n\\nFig. 5.6 Discussion reminder\\n\\nthe discussion reminder provides a function to browse videos of past discussions for accurate understanding.\\n\\nHowever, all participants need to interrupt the ongoing discussion for a review of discussion content, and thus it is desirable to ﬁnish the review in no time using the above method to ﬁnd the things required in the audiovisual information. For an efﬁcient review, the discussion reminder provides an interface to narrow down the browsing information, such as discussion content matched with queries, slides matching discussion content, and statements associated with matched slides, and to cooperative retrieval by participants. A participant who notices the existence of the discussion, which he/she wants to review, inputs queries to the discussion reminder. Various types of information, such as names of presenters, dates of meetings, and keywords, are available as queries. The contents of retrieved results are displayed on the discussion table as shown in Fig. 5.6.\\n\\nParticipants conduct various operations using the touch panel in this interface. This interface consists of a discussion content list, a slide list, and a discussion segment view. The discussion content list displays titles of the discussion contents, which contains the discussion matched queries. When a participant selects a title using the touch panel, slide thumbnails comprised in the selected discussion content are shown at the bottom of the slide list. Participants can preview the larger slide thumbnail at the top of the slide list.\\n\\n115\\n\\n116\\n\\n5 Smart Learning Environments\\n\\nThe discussion segment view shows information about the discussion segments associated with the slide selected in the slide list. Examples of discussion segment information include structures of discussion segments, speaker’s ID, keywords of statement, and so on. In the discussion segment view, full text of the statement can also be previewed. Participants can browse videos in the video view displayed on the table from the start time of the selected statement in the discussion segment view.\\n\\n5.4.3 Employing Machine Learning Techniques\\n\\nIn this study, machine learning techniques are employed to obtain deep structures of presentation and discussion content. Techniques like deep neural networks (Bengio 2009) integrate several contexts of information such as operation histories of users. By integrating the results of subject experiments on presentations and discussions, different methods to evaluate the quality of students’ intellectual activities and to increase their skills are discovered. The system tries to perform some consensus- building processes to make evaluation results appropriate for each student.\\n\\n5.5 Digital Poster Presentation System\\n\\nThe digital poster presentation system consists of an authoring tool for digital posters, an interactive presentation system with digital posters, and an online sharing system for digital posters. Poster presentations can be considered as a close communication with the audience, and are also ideal for training in discussion not only for presen- tation. The digital poster presentation system makes the poster presentation easier. Tools such as PowerPoint slides can be integrated into the poster presentation. Addi- tionally, the system will be extended for an interactive data acquisition. Hence, we believe that this system would signiﬁcantly change the way of poster presentations.\\n\\n5.5.1 Digital Posters Versus Regular Posters\\n\\nA digital poster is an interactive multimodal version of regular papers. The advantage of digital posters includes retrieval and reuse of content. However, one of the biggest problems is portability since a digital poster needs special hardware such as a digital poster panel and these devices cannot be carried elsewhere. Perhaps, in the near future, large and thin ﬁlm-type screen devices, such as organic electroluminescence displays, will be available and tools for digital posters will be commodities and easily acquired.\\n\\n5.5 Digital Poster Presentation System\\n\\nFig. 5.7 Main screen of digital poster authoring tool\\n\\n5.5.2 Authoring Digital Posters\\n\\nAuthoring of digital posters is very simple but some preparation is needed. The users should prepare resources such as images, videos, and slides in advance. We also developed an online resource management system for memos, images, videos, and slides. The digital poster authoring tool can import any resources submitted or shared in the resource management system.\\n\\nThe digital poster authoring tool shown in Fig. 5.7 has three parts: the main menu,\\n\\nthe resources menu, and the poster ﬁeld.\\n\\nThe main menu provides the basic functionalities of the tool such as creating, opening, and saving of poster ﬁles, setting up the desired preferences, and choosing different creation modes. The digital authoring tool is also able to create both portrait and landscape orientation posters as needed.\\n\\nThe resources menu shown in Fig. 5.8 lets the users add different types of blocks to the poster ﬁeld. Each block automatically downloads a certain type of resource depending on the selected block from the online resource management system, except for the layout and text blocks. Selecting an image block will automatically scan for images in the resource management system, while selecting a video block will automatically scan for videos in the resource management system. For the slide block, existing PowerPoint slides will be selected.\\n\\nWhen the user taps a block in the resource menu, a list of thumbnail images is displayed in the window that appears from the right edge of the screen as shown in Fig. 5.9. The user can easily arrange the layout of the poster using a layout block and interactively change a position of a block’s borderline. When the user wants to place any resource in the block, he/she should just drag and drop the thumbnail image from the resource list to the target block as shown in Fig. 5.10.\\n\\n117\\n\\n118\\n\\nFig. 5.8 Resource menu\\n\\nFig. 5.9 Image resource menu\\n\\n5 Smart Learning Environments\\n\\n5.5 Digital Poster Presentation System\\n\\nFig. 5.10 Image resource placement in layout block\\n\\nOther resources, such as videos and slides, are inserted in the blocks in a similar way. An example of a created poster using the described authoring tool is shown in Fig. 5.11. When the user ﬁnished editing the digital poster, the ﬁnal poster can be stored in the online poster sharing system. It can be used for presentations by searching the digital poster at any time. During presentation time, the enlargement of images and the playback of videos and slides in the poster can be done.\\n\\n5.5.3 Data Acquisition from Interactions with Digital Posters\\n\\nDigital posters are not only for a presenter to make a presentation but also for an audi- ence to view in detail by interacting with the poster. Posters are unlike slides, where the complete content is summarized in one piece, which is more suitable to under- stand the content quickly. At the Leaders’ Saloon, visitors can easily retrieve and view the digital posters using the digital poster panel whenever they like. Interaction histories when visitors have interacted with the posters are recorded automatically. The number and time of poster views, views of the elements in the poster, and data such as browsing the order of the poster elements can be obtained by this system. These data are used to evaluate the posters and the skills of the poster author.\\n\\n119\\n\\n120\\n\\n5 Smart Learning Environments\\n\\nFig. 5.11 Example of digital poster\\n\\n5.6 Skill Evaluation Methods\\n\\nThe focus of this study is the students of the new Graduate Leading Program at Nagoya University, which aims to nurture future global leaders. To achieve this goal, improving the communication skills of the students must be addressed. In this study, we focus on developing the discussion and presentation skills of the students and this section describes in detail the evaluation method for the discussion and presentation skills of the students.\\n\\n5.6 Skill Evaluation Methods\\n\\nFig. 5.12 Data results of discussion mining system\\n\\n5.6.1 Discussion Skills\\n\\nI presented discussion skills evaluation methods based on acoustic and linguistic features of participants’ statements in Chap. 4. In the Leaders’ Saloon, we employed a simpler version of the evaluation method based on statistic data calculated by the discussion mining system.\\n\\nData acquired by the discussion mining system includes participant types (pre- senter, secretary, and others), number of start-up/follow-up statements of each par- ticipant, and quality scores for each statement. The quality scores are calculated by the agreement/disagreement data input by each participant’s discussion commander during discussions. For each statement, one point is added if someone agrees with it, one point is subtracted if someone disagrees, and then the score is determined. Results of the aggregate data of multiple students in 3 months are shown in Fig. 5.12. The discussion skills of a student are evaluated using the score calculated by the following processes. First, the weight values for every behavior are determined. These weights are going to be rationally determined in the future using machine learning, but for now, the values were decided intuitively based on the difﬁculty of execution. • Number of participants: 3. • Number of presentations: 10. • Number of secretary acts: 5. • Start-up statements except presenter’s cases: 3. • Follow-up statements except presenter’s cases: 2. • Quality (sum of agreement/disagreement values): 4.\\n\\nLet the score be the value of the sum after having applied such weight to the number of each behavior. Additionally, the evaluation of statement quality is also calculated. For the discussion skill score calculation, the presenter’s cases are excluded by start- up and follow-up statements of the students. This is because the situations when the presenters must answer the question from other participants occur naturally and they\\n\\n121\\n\\n122\\n\\n5 Smart Learning Environments\\n\\nshould not be treated the same way as cases in which participants of the discussion make remarks spontaneously.\\n\\nThe students can judge their status for this evaluation as a reference and can analyze their weak points. The student performance increases if the student makes many statements when he/she is not the presenter, and many of the other participants also agree with these statements. It is then possible that these data be a basis to improve a student’s discussion skills. It can also be conﬁrmed that discussion skills are improved by making high-quality statements, that is, a lot of agreements obtained from many participants.\\n\\n5.6.2 Presentation Skills\\n\\nA study on developing oral presentations skills embedded oral presentations and assessment to their curriculum (Kerby and Romine 2009). In their case study, they included at least one oral presentation in three of their courses and used a rubric to assess the oral presentations. Their results indicate that students better understood their weaknesses, strengths, and areas for improvement with their presentations. In our study, we also implemented the same design to improve the presentation skills of our students. We conducted two poster presentation sessions with two groups of students to evaluate their presentation skills. We used the poster presentation format instead of the regular oral presentations because of the interactivity of poster presentations. In poster presentations, the students are able to engage in conversation with people, giving them more opportunities to improve their communication skills. Also, poster presentations enliven the student presentations because students interact with each other more instead of just passively observing like in formal presentations. In the case of oral presentations, we developed a presentation training system in a virtual reality environment. The system also works at the Leaders’ Saloon. I will explain the system at the end of this chapter.\\n\\n5.6.2.1 Poster Presentation Session I\\n\\nIn this poster presentation session, twenty-four (24) inexperienced students were divided into six (6) groups and each group was asked to create a digital poster using the authoring tool discussed in Sect. 5.5. Each group presented their posters in the allotted time of ﬁfteen (15) minutes, while members of the other groups and spectators of the poster session evaluated each poster presentation. The evaluation sheet used for this poster presentation session is shown in Fig. 5.13.\\n\\nEvaluators ﬁll up the feedback form shown in Fig. 5.13 for each presentation. The evaluation criteria include Content, Organization, and Impact. The said criteria are based on the common themes in Brownlie’s 2007 bibliography (Hess et al. 2009). For scoring results, the numerical values for the different ratings are as follows: Bad\\n\\n5.6 Skill Evaluation Methods\\n\\nFig. 5.13 Poster presentation evaluation sheet I\\n\\nFig. 5.14 Poster presentation I score results\\n\\nis 1, Poor is 2, Fair is 3, Good is 4, and Excellent is 5. The average scores of all the evaluators for all group presentations are shown in Fig. 5.14.\\n\\nIn addition, the scores and standard deviations of all groups are shown in Fig. 5.15. Since the standard deviations are not too large, this metric is not far off from human intuition for evaluation. Histories of interactions with digital posters are not analyzed yet. We are planning to combine human metrics and accumulated data such as access counts of posters and their internal elements in the near future.\\n\\nHowever, there were major drawbacks in the evaluation sheet that we used for this session. First, we failed in evaluating the presentation delivery of the students. Second, the evaluators found it hard to judge certain criteria based on the ratings. Thus, for the next poster presentation session, we made a number of changes in our evaluation sheet.\\n\\n123\\n\\n124\\n\\n5 Smart Learning Environments\\n\\nFig. 5.15 Scores and standard deviations of all groups\\n\\n5.6.2.2 Poster Presentation Session II\\n\\nIn this poster presentation session, ﬁve (5) students were asked to create and present ﬁve digital posters using the authoring tool discussed earlier. Spectators of the poster session were asked to evaluate each poster presentation.\\n\\nWe improved our evaluation criteria based on the encountered problems in the former trial. Based on feedback from the evaluators, one major drawback in the previous evaluation sheet shown in Fig. 5.13 is that there are not enough details for the ratings (Bad, Poor, Fair, Good, and Excellent) under certain criteria. Thus, the evaluation sheet was modiﬁed to a rubric with concrete descriptions for each score and criteria. The new evaluation sheet is shown in Fig. 5.16. Using this rubric style of evaluation, the evaluation criteria were clearer and evaluation time were faster for the spectators. Aside from changing the evaluation format, the sets of criteria were also modiﬁed. We added two main sets of criteria: Impact and Presentation.\\n\\nWe added Impact to determine how the poster is able to attract the attention of spectators. It consists of the criteria for evaluating the poster’s title, overall appear- ance, and interest. We also added Presentation, another set of criteria for evaluating the students’ poster presentation skills. It consists of the ability to communicate properly to their audience and the ability to answer questions conﬁdently. Adding these new sets of criteria provided a more effective and complete digital poster eval- uation. Using the new evaluation sheet, the score results of the second round of poster presentations are shown in Fig. 5.17. The evaluation scores of the professors (P) and students (S) were calculated. With these results, we were able to determine the weakness of each poster based on the criteria. For example, with Poster III, its content and organization needed a lot of improvement thus for feedback, the author needs to focus on these sets of criteria when he/she modiﬁes the said poster.\\n\\n5.7 Future Features of Smart Learning Environments\\n\\nFig. 5.16 Poster presentation evaluation sheet II\\n\\n5.7 Future Features of Smart Learning Environments\\n\\nThe current training environment contains a 2D interactive system, such as touch panel discussion tables, digital poster panels, and an interactive wall-sized white- board, facilitating the interactions of users with the system. However, to further enhance the performance of the current learning environment, a vision system will be incorporated to increase the interaction dimension to 3D. The system will consist of a multi-camera system or Kinect that has a camera and range sensor device. More- over, given an intelligent system that recognizes the users by robust face detection algorithm, user interaction will be smooth, and annotations will be automated and\\n\\n125\\n\\n126\\n\\n5 Smart Learning Environments\\n\\nPoster\\n\\nI\\n\\nII\\n\\nIII\\n\\nIV\\n\\nV\\n\\nCriteria/Evaluators Impact Title Overall appearance Interest Content Quality Main points Organiza(cid:415)on Logical structure Text size Text/graphics balance Presenta(cid:415)on Communica(cid:415)on Ques(cid:415)ons Total Score\\n\\nProfessors\\n\\n1.6000 1.0000 1.4000\\n\\n1.0000 1.2000\\n\\n1.0000 1.2000 1.4000\\n\\n2.0000 1.8000 13.6000\\n\\nStudents Professors\\n\\n1.2500 1.0833 1.4167\\n\\n1.5000 1.3333 1.3333\\n\\n0.9167 1.0833\\n\\n1.1667 1.1667\\n\\n1.1667 1.0000 1.4167\\n\\n1.1667 0.8333 0.8333\\n\\n1.4167 1.5000 12.2500\\n\\n1.5000 1.5000 12.3333\\n\\nStudents Professors\\n\\n1.7000 1.4000 1.4000\\n\\n1.4000 1.2000 1.2000\\n\\n1.3000 1.5000\\n\\n0.8000 0.6000\\n\\n1.5000 1.2000 1.5000\\n\\n0.6000 0.8000 0.6000\\n\\n1.0000 1.4000 13.9000\\n\\n1.2000 1.2000 9.6000\\n\\nStudents Professors\\n\\n1.8571 1.0000 1.2857\\n\\n1.0000 1.2500 1.2500\\n\\n1.0000 0.8571\\n\\n1.2500 1.2500\\n\\n0.8571 1.0000 0.7143\\n\\n1.0000 1.2500 1.5000\\n\\n1.0000 1.0000 10.5714\\n\\n2.0000 1.2500 13.0000\\n\\nStudents Professors\\n\\n0.9091 1.3636 1.4545\\n\\n1.3333 1.6667 1.5000\\n\\n1.6364 1.5455\\n\\n1.1667 1.3333\\n\\n1.2727 1.0000 1.4545\\n\\n1.3333 1.5000 1.5000\\n\\n1.4545 1.4545 13.5455\\n\\n1.5000 1.5000 14.3333\\n\\nStudents\\n\\n1.7500 1.2500 1.2500\\n\\n1.0000 1.2500\\n\\n1.3750 1.1250 1.5000\\n\\n1.5000 1.2500 13.2500\\n\\nFig. 5.17 Poster presentation II score results\\n\\npersonalized, thereby creating a more advanced learning environment. We plan to utilize an automated evaluation system and facilitation system for intellectual activ- ities to determine whether students’ skills are improving and whether newly created content is more highly evaluated than previous content.\\n\\nThe current criteria do not evaluate things like body movement, gestures, and pos- ture, which is common in evaluating presentations. However, it is difﬁcult to evaluate these criteria and we will be incorporating evaluating these movements through the planned vision system (the virtual reality system mentioned later has a recognition function of body movement and gestures). In addition, students’ psychophysiological data such as heart rate will also be incorporated in our learning evaluation system. A trial system is explained next. In order to improve the evaluation criteria presented in the previous section, the automated system is expected to receive real-time evaluation from the audience and to provide the presenter with the relative score. The registered audience can input their score to an online sheet with a tablet while attending the poster session. The audio–visual system is also expected to record the visual and audio interactions between the presenter and each audience. The system will be able to match the provided online score by each spectator and his/her interaction with the presenter. We hope to understand the relation between the audio/visual information and the provided online score to train the system. Our eventual goal is to introduce a nearly automatic evaluation system that is regularly trained by the audience.\\n\\nA novel physical–digital learning environment for discussion and presentation skills training has been developed at our university under the leading graduate pro- gram. By using state-of-the-art technologies, the selected students of the program will achieve an effective, interactive, and smooth discussion with the discussion min- ing system simultaneously summarizing and annotating the ongoing discussion. The discussion contents are available to the community or to the faculty for evaluation, feedback, and follow-up activities. With this prototype environment, a new education system may emerge promoting efﬁcient and advanced learning.\\n\\n5.7 Future Features of Smart Learning Environments\\n\\nFig. 5.18 Heart rate acquisition system based on Apple Watch\\n\\n5.7.1 Psychophysiology-Based Activity Evaluation\\n\\nConsidering that the discussion process is a type of cognitive activity, which could result in changes in certain psychophysiological data, such as heart rate (HR) variabil- ity (HRV), several studies have proven that HR is an important index of the autonomic nervous system regulation of the cardiovascular system (Camm et al. 1996). There- fore, there has been increasing focus on observing the correlation between HR data and cognitive activities. A study on measuring the HR during three cognitive tasks (Luque-Casado et al. 2013) revealed the affection of cognitive processing on HRV. The stress level also has been assessed during Trier social stress test tasks, a type of cognitive activity, by using HR and HRV metrics (Pereira et al. 2017). Judging from the large amount of evidence presented, we argue that the HR data of the participants of a meeting can be used to effectively evaluate the answer quality of Q&A sessions in the meeting, which is helpful in improving participants’ discussion skills (Peng and Nagao 2018).\\n\\nSmart watches, such as Apple Watch, the Fitbit series, and Microsoft Bands, contain wearable sensors to accurately detect users’ biological data, such as HR and blood pressure. Such noninvasive detection makes it possible to link users’ biological information with their daily activities. Iakovakis and Hadjileontiadis (2016) used the Microsoft Band 2 to acquire the HR data of users to predict their body postures. In our study, we used the Apple Watch to collect participants’ HR data on the basis of our DM system and to visualize the data during discussions. Through the Health Kit framework on the Apple Watch, which we asked participants to wear on their left wrist during discussions, participants’ HR data were acquired almost in real time in 5–7 s intervals, as shown in Fig. 5.18. The collected HR and participants’ information is displayed on the Apple Watch screen as well as synchronously presented on an HR browser.\\n\\nTo automatically evaluate the discussion performance, we started from analyzing the answer quality of Q&A segments in discussion chunks mentioned in Chap. 2, which are the most important constituent components generated around a discussion topic. Our goal was to validate our argument that the HR of discussion participants\\n\\n127\\n\\n128\\n\\n5 Smart Learning Environments\\n\\nFig. 5.19 Heart rate graphs\\n\\ncan be used to effectively evaluate the answer quality of Q&A segments during discussions.\\n\\nAll HR information of participants during their discussions is displayed in a graph, shown in Fig. 5.19a, that presents a participant’s complete HR detected per minute throughout a discussion. The HR segments in each Q&A segment were then extracted and displayed in a graph, shown in Fig. 5.19b, which shows HR data during a question period (blue line) and answer period (orange line). We then computed 18 h and HRV features from all Q&A segments as well as the question-and-answer periods separately.\\n\\nThe HR and HRV features include mean, standard deviation (std.), and root mean square successive difference (RMSSD) from these two periods (question-and-answer periods), and these metrics have been proven to be important for understanding HRV\\n\\n5.7 Future Features of Smart Learning Environments\\n\\nFig. 5.20 HR and HRV features\\n\\nHR period\\n\\nHR and HRV features\\n\\nBoth periods\\n\\nmean, std., RMSSD, trend, freq. all mean, freq. all std.\\n\\nQuestion period mean, std., RMSSD, trend,\\n\\nfreq. question mean, freq. question std.\\n\\nAnswer period mean, std., RMSSD, trend,\\n\\nfreq. answer mean, freq. answer std.\\n\\ndifferences under cognitive activities (Wang et al. 2009). The trends in the HR of these two periods are also computed by calculating the difference between two adjacent HR points. If the number of positive differences was more than the negative ones, we assumed that the HR period showed an upward trend; if not, it showed a downward trend, as shown in Fig. 5.19b. We used a quadratic curve (red line) to more clearly present the HR trend. We can see that HR during the question period showed a downward trend and upward trend during the answer period.\\n\\nWe also divided the HR data of these two periods into nine ranges: less than 60, 60–70, 71–80, 81–90, 91–100, 101–110, 111–120, 121–130, and more than 130 bpm. The mean and std. were calculated to describe the HR appearance-frequency distri- bution in each range. Figure 5.20 summarizes these 18 features.\\n\\nWe collected discussion data from nine presenters from nine lab-seminar discus- sions held over a period of 4 months. Twelve undergraduate and graduate students and three professors made up the participants. The discussions were carried out fol- lowing the presenters’ research reports, with the participants asking questions related to the discussion topic that were then answered by the presenters. There were 117 complete Q&A segments extracted from these 9 discussions, and the answer quality of these Q&A segments was evaluated by the participants who asked the questions by giving a score based on a ﬁve-point scale: 1 (cid:2) very poor, 2 (cid:2) poor, 3 (cid:2) acceptable, 4 (cid:2) good, and 5 (cid:2) very good. We obtained 66 high-quality answers with scores from 4 to 5 and 51 low-quality answers with scores from 1 to 3.\\n\\nWe adopted three machine learning models, logistic regression (LR), support vector machine (SVM), and random forest (RF), to carry out binary classiﬁcation of the Q&A segments’ answer quality. About 80% of Q&A segments were randomly selected as a training dataset and the remaining 20% as a test dataset.\\n\\nFor the LR model, we obtained a 0.790 F-measure by using an eight-feature candidate subset and an F-measure of 0.740 by using a seven-feature candidate subset; therefore, we used the eight-feature subset to train our LR model. We obtained an F-measure of 0.805 for the SVM model with 10 h and HRV features we selected in advance. For the RF model, when there were 36 trees (submodels of RF) and 19 terminal nodes on each tree, we obtained the highest F-measure of 0.87. In this case,\\n\\n129\\n\\n130\\n\\n5 Smart Learning Environments\\n\\nFig. 5.21 Evaluation results of each learning model\\n\\nEvaluation model F-measure\\n\\nLR SVM RF\\n\\n0.790 0.805 0.870\\n\\nwe chose an eight-feature subset. Figure 5.21 lists the evaluation results for each model.\\n\\nComparing the F-measures of each model, the RF model exhibited superior eval- uation performance compared to the LR and SVM models. Considering all three models, the HRV data of participants showed an outstanding performance in evaluat- ing Q&A segments’ answer quality. Meanwhile, we focused on seven HRV features: all mean, answer trend, all RMSSD, freq. answer std., answer std., question trend, and all trends, which exhibited the largest effect on all three models.\\n\\nOur evaluation method automatically evaluates all statements of meeting partici- pants by using the evaluation indicators mentioned above. Let the weighted average value of the value of each indicator be the evaluation of one statement, and let the sum of the evaluation values of all statements of a participant be the numerical value expressing that participant’s speaking ability in discussions at a meeting. By looking at the changes for each discussion in each meeting, participants will be able to judge whether their discussion skills are rising or stagnating.\\n\\n5.7.2 Virtual Reality Presentation Training System\\n\\nIt is special to perform presentations in front of a large audience, and training such as rehearsal for that is important. However, it is difﬁcult to prepare such a situation in advance. Virtual reality (VR) provides a means for that. In a VR space, you can display a virtual audience who stands on a big stage such as an auditorium, and reacts according to the content of the presentation and the way of presenter’s speaking.\\n\\nWe developed a VR presentation training system as shown in Fig. 5.22. In this system, based on the acoustic features of the speaker’s voice introduced in Chap. 4 and the characteristics analyzing the slide used for the presentation, we control the reaction of the virtual audience. Also, by tracking the movement of the body with the VR system, we evaluate gaze and gesture and score the presentation. Actions such as responding quickly to the reaction of the audience, changing the way of speaking, changing the display slide, and changing the topic ﬂexibly are also subject to evaluation.\\n\\nSuch a VR training system operates in our smart learning environment. The system includes contents of VR environments that precisely reproduce institutions such as actual lecture halls and auditoriums, 3D models, and action scripts that visualize the appearance and reaction of audiences three-dimensionally. A physical device is\\n\\n5.7 Future Features of Smart Learning Environments\\n\\nFig. 5.22 VR presentation training system (The ﬁgure above shows the presenter’s 3D avatar and the ﬁgure below shows the 3D avatars of the audience)\\n\\na headset called head-mounted display (HMD) and a device called a hand tracker (shown in Fig. 5.23) attached to the hand. The HMD incorporates a mechanism for precisely measuring the position and motion of the head and a mechanism for three- dimensionally displaying images and sounds. The hand trackers not only measure the position and orientation of both hands but also the angle at which the ﬁnger is bent can be detected in real time. This allows the training system to recognize eye contact and gestures with the audience.\\n\\nThe conﬁguration of hand tracker is as follows. The bend sensor measuring the bending ﬁts each ﬁnger by the cloth glove. Then, this value is transmitted to the PC using Bluetooth low energy (BLE). At that time, using the inertial measurement unit (IMU) attached to the back of the hand, the values of acceleration/angular velocity of XYZ-axes are also measured and transmitted at the same time.\\n\\n131\\n\\n132\\n\\n5 Smart Learning Environments\\n\\nFig. 5.23 Hand tracker\\n\\nIn addition, the hand tracker is equipped with a position tracking device called the VIVE tracker provided by HTC (a 3D position and orientation tracking device using infrared markers installed in the environment). Therefore, it is possible to know the three-dimensional pose of hands in the tracking area of the VIVE tracker.\\n\\nA noteworthy point of this system is the feedback method of audience reactions and evaluation results by artiﬁcial intelligence. The audience in the VR system is all expressed by 3D animation imitating humans as shown in Fig. 5.22. In the simulation of the reaction, we measure actual human data and use what is modeled by machine learning. The data obtained from the human audiences in the presentation (wearing the HMD and the hand trackers as well as the presenter) are the position and direction of the head and body, the myoelectric potential, the heart rate. They are acquired by dedicated sensors in addition to the VR system. These data are collected by operating a meeting system using VR described below. The audience’s model is accompanied by parameters that adjust the degree and timing of reaction, and ingenuity is given to prevent all people from always showing similar reactions to the same stimulus.\\n\\nFeedback on the presentation is displayed as a reaction of the audience during the presentation. Also, after the presentation, it is displayed as a score for several features mentioned above and can be browsed at an arbitrary timing together with a history of past presentations. The history of the presentation in the VR system is in chronological order of 3D animations with sounds of their own avatar presenting using slides and other materials.\\n\\nLike the evaluation indicators of the discussion mentioned in Chap. 4, the eval- uation indicators of the presentation are based on acoustic features such as voice size, intonation, and speech speed and those based on eye contact between presenter and audiences. In addition, the simplicity of the content of presentation materials\\n\\n5.7 Future Features of Smart Learning Environments\\n\\nsuch as slides, the balance of charts (proportion of the whole document), and the relevance of explanation by voice and the region designated by the pointer are taken into consideration.\\n\\nIn the VR presentation system, the pointer is operated by pointing with a hand tracker, and the pointing part is highlighted. Although it operates differently from a normal laser pointer, it can realize almost the same function as the pointing system using the discussion commander in the discussion mining system described in Chap. 2.\\n\\nOf course, this VR presentation system can be used not only for training but also for meetings such as laboratory seminars. In that case, the audience is not an AI but an actual human avatar, and participation from a remote place is possible. In that case, human avatar can input gesture using hand trackers, and head movement is also reproduced on VR using HMD.\\n\\nAlso, in the case of a meeting by VR, as in the case of the face-to-face meeting, the contents can be recorded in detail and used for searching, knowledge discovery, and so on. Since the VR system is equipped with a voice recognition function, it is possible to convert the voice of the speaker into text, and furthermore, by using the pointer function by the hand tracker, in addition to the movement of the head and the hand, it is possible to record pointing and referring actions. In this system, since the meeting participants are wearing the HMD, it is impossible to recognize the facial expression from the image, but it can be estimated by using the myoelectric sensor brought into contact with the skin of the face. Information on gestures and facial expressions of meeting participants are used to construct a machine learning model of reaction of audience AI in the presentation training system.\\n\\nThe machine learning model represents relationships among various data gathered by the presentation training system, the behavior of the audience (raising the face, cranking the neck, laughing, sleeping, hitting the hands, stomping the feet, etc.), and mental conditions of the audience (interested, not interested, pleased, angry, etc.). Based on the relationships, the VR system controls the reaction of audiences. While seeing this reaction, the presenter examines the presentation state (whether it is well accepted by the audience) and considers how to improve the presentation.\\n\\nWe are not conducting subject experiments of this system yet, but we will demon- strate that students’ presentation skills will improve by operating this VR training system for a certain period of time.\\n\\n133\\n\\n134\\n\\n5 Smart Learning Environments\\n\\nReferences\\n\\nK. Armstrong, Big data: a revolution that will transform how we live, work, and think. Inf. Commun.\\n\\nSoc. 17(10), 1300–1302 (2014)\\n\\nY. Bengio, Learning deep architectures for AI. Found. Trends Mach. Learn. 2(1), 1–127 (2009) A.J. Camm, M. Malik, J. Bigger, G. Breithardt, S. Cerutti, R. Cohen, P. Coumel, E. Fallen, H. Kennedy, R. Kleiger, Heart rate variability: standards of measurement, physiological interpreta- tion and clinical use. Eur. Heart J. 17(3), 354–381 (1996)\\n\\nL. Cutrone, M. Chang, Kinshuk, Auto-assessor: computerized assessment system for marking stu- dent’s short-answers automatically, in 2011 IEEE International Conference on Technology for Education (T4E), pp. 81–88 (2011)\\n\\nG.R. Hess, K.W. Tosney, L.H. Liegel, Creating effective poster presentations: AMEE guide no. 40.\\n\\nMed. Teach. 31(4), 319–321 (2009)\\n\\nD. Iakovakis, L. Hadjileontiadis, Standing Hypotension Prediction Based on Smartwatch Heart Rate Variability Data: A Novel Approach, in Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services, pp. 1109–1112 (ACM, 2016) D. Kerby, J. Romine, Develop oral presentation skills through accounting curriculum design and\\n\\ncourse-embedded assessment. J. Educ. Bus. 85(3), 172–179 (2009)\\n\\nK. Kurihara, M. Goto, J. Ogata, Y. Matsusaka, T. Igarashi, Presentation Sensei: A Presentation Training System using Speech and Image Processing, in Proceedings of the 9th International Conference on Multimodal Interfaces, ICMI’07, pp. 358–365 (ACM, New York, NY, USA, 2007) A. Luque-Casado, M. Zabala, E. Morales, M. Mateo-March, D. Sanabria, Cognitive performance and heart rate variability: the inﬂuence of ﬁtness level. PLoS ONE. https://doi.org/10.1371/ journal.pone.0056935 (2013)\\n\\nS. Ohira, K. Kawanishi, K. Nagao, Assessing motivation and capacity to argue in a gamiﬁed seminar setting, in Proceedings of the Second International Conference on Technological Ecosystems for Enhancing Multiculturality. TEEM’14, pp. 197–204. ACM, New York, NY, USA (2014)\\n\\nY. Omori, K. Ito, S. Nishida, T. Kihira, Study on supporting group discussions by improving discussion skills with ex post evaluation, in IEEE International Conference on Systems, Man and Cybernetics, 2006. SMC’06., vol. 3, pp. 2191–2196 (2006)\\n\\nT. Pereira, P.R. Almeida, J.P. Cunha, A. Aguiar, Heart rate variability metrics for ﬁne-grained stress\\n\\nlevel assessment. Comput. Meth. Programs Biomed. 148, 71–80 (2017)\\n\\nS. Peng and K. Nagao, Automatic Evaluation of Presenters’ Discussion Performance based on Their Heart Rate, in Proceedings of the 10th International Conference on Computer Supported Education (CSEDU 2018) (2018)\\n\\nA.J. Sellen, S. Whittaker, Beyond total capture:a constructive critique of lifelogging. Commun.\\n\\nACM 53(5), 70–77 (2010)\\n\\nH. Trinh, K. Yatani, D. Edge, PitchPerfect: Integrated rehearsal environment for structured presen- tation preparation, in Proceedings of the 32nd Annual ACM Conference on Human Factors in Computing Systems, CHI’14, pp. 1571–1580 (ACM, New York, NY, USA, 2014)\\n\\nN. Wanas, M. El-Saban, H. Ashour, W. Ammar, Automatic scoring of online discussion posts, in Proceedings of the 2Nd ACM Workshop on Information Credibility on the Web. WICOW ’08, pp. 19–26 (ACM, New York, NY, USA, 2008)\\n\\nX. Wang, X. Ding, S. Su, Z. Li, H. Riese, J.F. Thayer, F. Treiber, H. Snieder, Genetic inﬂuences on\\n\\nheart rate variability at rest and during stress. Psychophysiology 46(3), 458–465 (2009)\\n\\nChapter 6 Symbiosis between Humans and Artiﬁcial Intelligence\\n\\nAbstract In the last chapter, I will talk about how artiﬁcial intelligence develops and supports humans in the future, and how humans and machines harmoniously cooperate to create a new society. Symbiosis between humans and artiﬁcial intelli- gence can be said to be a relationship that enhances each other’s abilities. In order to establish such a relationship, one’s learning needs to have a positive inﬂuence on the other’s learning. The current mainstream is to advance artiﬁcial intelligence by machine learning based on human-made data. However, if artiﬁcial intelligence can properly support human learning and human beings can generate useful data for artiﬁcial intelligence as a byproduct of humans’ main activities, positive circu- lation will be established between human and artiﬁcial intelligence learning. At this time, artiﬁcial intelligence needs to understand the situation of human beings, sup- port learning, and actively collect data necessary for their learning. Such artiﬁcial intelligence should be an autonomous existence. Indications of autonomous artiﬁ- cial intelligence have already appeared. One example would be a smart speaker like Google Home. It usually listens to the human calls silently, and when it is called it responds to it. However, if a universal pattern is found in human behavior (happen at the same time every day at the same time, sleep at the same time every day, when we wake up in the morning, ﬁrst check the weather, etc.), artiﬁcial intelligence could also prefetch human demands and provide information. We need to think about evils when such autonomous artiﬁcial intelligence becomes common. It begins with a leak of privacy, which in turn involves human control and management. To ensure that artiﬁcial intelligence does not become a disadvantage to humans, we should estab- lish a symbiotic relationship with artiﬁcial intelligence as soon as possible. In this chapter, I will also discuss that point. Keywords Autonomous artiﬁcial intelligence · Intelligent agent · Chatter bot · Smart speaker · Singularity · Human–computer symbiosis\\n\\n© Springer Nature Singapore Pte Ltd. 2019 K. Nagao, Artiﬁcial Intelligence Accelerates Human Learning, https://doi.org/10.1007/978-981-13-6175-3_6\\n\\n135\\n\\n136\\n\\n6 Symbiosis between Humans and Artiﬁcial Intelligence\\n\\n6.1 Augmentation of Human Ability by AI\\n\\nAs mentioned in the previous chapter, humans will properly extend their abilities, and machines will add new functions to it. Machines will contribute to supplementing human memory and judgment because they are good at search based on large amounts of data and prediction based on them.\\n\\nIn the explanation so far, artiﬁcial intelligence is rather playing a backseat role, and it does not claim any self-assertion, but we can think of a way of artiﬁcial intelligence that is not so. This includes cases where the machine moves away from humans and acts autonomously, for example, a self-driving car that a human driver does not ride. However, strictly human control is supposed to be applied and it will be possible to give instructions to artiﬁcial intelligence by remote control. The machine acts autonomously if human instructions do not make it in time or it cannot be instructed by human ability in the ﬁrst place.\\n\\nSuch autonomous artiﬁcial intelligence may become a dangerous existence. How- ever, it is not a rebellion of artiﬁcial intelligence, as is told in science ﬁction novels. In my opinion, the rebellion of artiﬁcial intelligence (an event that looks like it) is mostly caused by human error (or negligence); I think that it can prevent recurrence. In other words, I think that artiﬁcial intelligence does not directly harm humans by their own judgment (prediction acquired from data). For example, it cannot be said that there are no cases where a person dies due to judgment of artiﬁcial intelligence, such as a death by accident from self-driving car, but it is not a case that artiﬁcial intelligence acts to killing humans, I think that it is caused by unfortunate coincidence because it is impossible to predict everything that could happen.\\n\\nThe real danger lies in losing one’s drive for self-improvement due to reliance on artiﬁcial intelligence. Many people often point to Japan’s adoption of “Yutori education” (relaxed education) as being a cause of the declining scholastic ability in students. However, I believe that the main cause lies in IT devices and applications typiﬁed by smartphones giving children a groundless sense of knowing everything, which leads to their abandoning efforts to learn by themselves. For example, a modern child might wonder what is the purpose of ﬁnding out something on my own, when I can just search for anything I am interested in and ﬁnd it easily. As a result, we now see that the number of young people with low adaptability and lacking fundamental ability has increased.\\n\\nAnyway, what I would like to claim in this book is that humans cannot adapt to the new society unless efforts continue in the future, and that artiﬁcial intelligence has the ability to properly support them. There, we can ﬁnd out the way of new symbiosis of man and machine.\\n\\nI think that what we should consider to expand human ability is what something humans cannot replace with machines. Things that have been very difﬁcult to auto- mate until now are gradually becoming feasible by collecting and analyzing large amounts of data that measured the phenomena. It is the stance of this manual that it is not appropriate to let a machine’s intellectual creation activities act on behalf of machines, but still I think that someday humans will leave many parts of intel-\\n\\n6.1 Augmentation of Human Ability by AI\\n\\nlectual activities to artiﬁcial intelligence, for example, writing a novel, writing a musical script, writing a drama script, designing a product, doing medical diagnosis, determining a company’s management strategy, and so on.\\n\\nOf course, it may not be possible to produce very good results at the beginning, but by constantly preparing the evaluation mechanism, always providing feedback and updating the machine learning model, it is possible to achieve reasonable accuracy. I think that scientiﬁc basis will be gained about whether the features humans have focused on (or the lessons learned in the past) were appropriate.\\n\\nHowever, humans still should continue intellectual creative activities in the future. That is because the current artiﬁcial intelligence is a mechanism to learn from the data. I believe that humans will always be able to create new data from creative activities, as long as they strive diligently to do so. However, as long as artiﬁcial intelligence is always based on historical data, it seems that what they produce will be a repetition of similar things. Humans always seek new things. Therefore, we can create something new. There is also the term “warm innovation.” Therefore, depend- ing on the ﬁeld, there is a possibility that new data can be created by analyzing past data well. However, I think that creative activities have more than what is obtained from historical data.\\n\\nThe machine learning model and the human brain structure are actually quite different. The neural network that machine learning (especially deep learning) uses is originally made with reference to the human brain, but there are few differences between what we know about brain mechanisms at that the time and what we currently know, there is a gap. In terms of the fact that the brain consists of a network of units called neurons, it is similar to the machine learning model, but its input/output relationship is very different.\\n\\nThe relationship between layers of the neural network in machine learning has no particular meaning. It only speciﬁes a nonlinear function to compute parameters. It is said that neurons of the human brain always output forecast values (Hawkins and Blakeslee 2005). This means that upon receiving a signal in one state, it will output a signal in a later state in time. For example, when listening to a song, the neuron is predicting the next note. Also, when getting off the stairs, neurons are predicting when the foot will touch the next area. When throwing the ball to someone else, the neuron is predicting that the ball will approach the other person. Of course, the neuron sees only one element when expressing a phenomenon in a distributed representation (vector expression), but still predicts the next state of the element. In addition, context information is used for prediction, but its description is somewhat complicated, so I omit it here.\\n\\nIncidentally, there is an illusion of the eye to a phenomenon famous for human vision (the same line of length looks like a different length, a parallel line looks bent, etc.). This phenomenon is recognized by the human brain as always making forecasts, as initially recognized as predicted, but it is found that the prediction is incorrect while doing some operation, such as drawing an extension line. It seems to be seen correctly. It is probably due to such a brain’s work that you can see something that should not exist, or you can hear sounds that cannot be heard (which\\n\\n137\\n\\n138\\n\\n6 Symbiosis between Humans and Artiﬁcial Intelligence\\n\\ncannot be detected with a microphone). Naturally, this is not all bad; due to the brain’s forecasting mechanism, recognition has been accelerated and balanced with behavior.\\n\\nAnyway, what I want to say is that artiﬁcial intelligence and the human brain structure are currently different. So, the way creativity comes up is different. We cannot fully automate our creative activities with machines so far, but we can make good use of each other’s characteristics to make it more sophisticated and efﬁcient. This is considered as “ augmentation of human ability” in this book.\\n\\nAugmentation of human ability on the premise of artiﬁcial intelligence is to make human and machines work properly, and they are closely related to each other so that humans can achieve greater results than by themselves. Alternatively, it solves problems that cannot be solved by humans alone. Problems that are difﬁcult to solve include not only that it is difﬁcult to solve but also problems that cannot be solved within a limited time, even if you can afford it if you have time to spare. For example, it would be a problem for me to decide what roads I will walk within a few seconds. Humans must properly communicate their intention (goal) and current state of their actions to the machine. One of Google’s founders who swept the world with search engines said that “searching is an act of communicating what people want to do or things they are interested into machines.” When we live in this era, when we have something to worry about, we immediately started searching the net (inputting keywords into Web search engines) using PCs and smartphones. I do not think that is a bad thing, but it is not a creative activity if it is completed by the act of investigating. I hope to make a habit of utilizing the retrieved results for subsequent activities and communicating the results to the machine again.\\n\\nEven if the act of communicating your intention is a daily search, we still must communicate our current state to the machine. Even if we understand human inten- tions, if we do not know the state of the human, the machine cannot predict what people should do. For example, when considering directions on a map, the destina- tion alone is insufﬁcient and it is necessary to know the current position (or departure point). There are various ways the machine knows the current state of humans. One way to do this is pattern recognition such as image recognition and speech recogni- tion, as explained in Chap. 3. In addition, there is a way to estimate the current state using past history. Pattern recognition as well as estimation from history is current areas of artiﬁcial intelligence, that is, because machine learning works effectively.\\n\\nEspecially pattern recognition processes information of the real world (our living physical world). For that purpose, we use a device called a sensor (a camera and a microphone). And, as a matter of course, information that can be used (input) becomes more diverse as more sensors are added. The sensors we are currently paying attention to are three-dimensional sensors and biosensors (sensors that detect biological signals). Three-dimensional sensors are used by machines to learn more about the physical space around humans. In addition, the biosensor is used by the machine to know the internal state of humans (tired, feeling good, etc.).\\n\\nAlso, machines will be able to recognize what people are watching, what they are listening to, and even voices. By processing such a wide variety of data (called\\n\\n6.1 Augmentation of Human Ability by AI\\n\\nmultimodal data), the machine can better understand the state of humans. And when you know the human intention and state, artiﬁcial intelligence will predict human behavior and give us insight.\\n\\nWhether humans adhere to that advice or not, perhaps humans can do more than now. It is because humans are not good at objectively monitoring their state (espe- cially internal state), as they are often conscious only after being told by others. In general, the humans brain does not work well when he or she just woke up. Likewise, there are things that humans can do well and things that cannot be done properly depending on their condition. By choosing the most successful actions in the current state, humans will be much better at using time. That means exactly that human capabilities will be expanded.\\n\\nJust as artiﬁcial intelligence is useful for improving speciﬁc skills such as discus- sions, the skills of various creative activities will be improved appropriately with the support of artiﬁcial intelligence. The author cannot conﬁrm the effectiveness of all abilities, but we feel that we probably have a rough idea of the things that humans must do in the future. Humans will continue their creative activities in the future, continue to make new ones, and to successfully use artiﬁcial intelligence as a tool, so that humans devise to give as much data as possible to artiﬁcial intelligence.\\n\\n6.2 Intelligent Agents\\n\\nAn artiﬁcial intelligence system actively acting on humans is called an intelligent agent. Intelligent agents can act actively on their own goals. Therefore, for humans, I think that it can be felt as a system with some personality. However, the personality is not naturally occurring; it is just made to imitate the behavior of humans, and it is perhaps better to call it a pseudo-personality. Since it is not clear how to generate self- consciousness, it is only possible to imitate it. Nevertheless, there is great signiﬁcance in humans recognizing an agent as being a person and treating them as such.\\n\\nThis is because humans can interact with it in a way they do not normally inter- act with machines. That is by way of speaking. In Chap. 3, I explained the speech recognition function installed in the meeting recorder, but in this system, artiﬁcial intelligence is listening to what humans are talking about, for the purpose of prepar- ing the minutes. It is, of course, an important function, but unlike an intelligent agent, it was not something that actively encouraged humans. The agent listens to human words in order to return some response to humans. In other words, it is very appreciated by the humans to talk to the agent.\\n\\nA typical example of such a system would be called a chatter bot. This works on a system of sending short messages like Twitter in a textual interactive system that exists on the net. In that case, it interrupts when people are chatting with each other and delivers messages to unspeciﬁed people. Some are running with the intention of deceiving humans, and others have data collection purpose to understand human language. However, as of now, there seems to be no chatter bot that is wise enough to successfully deceive many people.\\n\\n139\\n\\n140\\n\\n6 Symbiosis between Humans and Artiﬁcial Intelligence\\n\\nThere is a system called ELIZA which can be called the ancestor of the chatter bot (Weizenbaum 1966). ELIZA is a system created by Joseph Weizenbaum in 1966 and interacts with humans like a counselor. For example, if a human says, “my head hurts,” ELIZA returns with “Why do you say your head hurts?”. And if he or she says, “my mother hates me,” ELIZA returns “does anyone in your family and others dislikes you?”. ELIZA seems to mimic the reaction of a psychotherapist in a psychiatric interview. Weizenbaum chose psychotherapy to avoid the problem of having the system data on real-world knowledge. Although the situation of psychotherapy is a dialogue between humans, there is a characteristic that almost no knowledge about the content of the dialogue is required. For example, even if a human asked “who is your favorite composer?”, ELIZA can return “who is your favorite composer?” or “is that question important?”. It seems to be because the dialogue system does not need knowledge about the composer.\\n\\nAlthough a chatter bot is a good partner to kill time chatting with, it is not enough to expand human ability, so there is need for more speciﬁc intelligence. As an example of such a system, there is a spoken dialogue robot that the authors previously created (Nagao 2002). As shown in Fig. 6.1, this robot has a face and responds variously according to human speech. In some cases, images and images are displayed on the nearby display, and they refer to it and explain by voice. This was made in 2000, at that time, the mechanism of machine learning was not implemented. Therefore, the types of questions that can be answered were quite limited. In other words, the robot could not learn words and knowledge while talking to humans.\\n\\nMeanwhile, recently, there is a smart speaker in an agent-like system which is attracting much attention. Starting with Amazon Echo, Amazon launched in 2014, Google released Google Home in 2017, and Apple released HomePod in 2018. Figure 6.2 shows the appearance of each product.\\n\\nIf it is placed somewhere in the room, it will retrieve information and manipulate appliances such as lighting and air conditioning according to human calls. Also, since it is a system for playing music, it will play when you request a favorite song. Both the smartphone and the PC operated by touching directly, the distance from the machine becomes a more freeing by being able to operate it by voice. Of course, there was a mechanism to remotely control home appliances, but it was necessary to use a machine (remote control) for that. Since voice can be sent without using tools, there are some deﬁnite merits. For example, it is not necessary to remember the procedure for operating the machine, and you can pay attention only to the operation. In other words, you can directly input requests like “I want to know…” and “what is…?”.\\n\\nThis is something the researchers of speech recognition have been saying for quite a while but ﬁnally it is becoming a reality. For agents, it is a very important function to interact with humans, so it is inevitable that agents become more sophisticated with the development of spoken dialogue technology.\\n\\nBy the way, the biggest difference between the above-spoken dialogue robot and the recent smart speakers is not just whether there is a face. The real difference lies in whether the intelligence system is in the machine before your eyes or whether it exists somewhere else. The mechanism of doing a speciﬁc service on the Internet is called the cloud, but for smart speakers, the body of intelligence is in the cloud.\\n\\n6.2 Intelligent Agents\\n\\nFig. 6.1 Spoken dialogue robot\\n\\nFig. 6.2 Smart speakers (From Left, Amazon Echo, Google Home, Apple HomePod)\\n\\n141\\n\\n142\\n\\n6 Symbiosis between Humans and Artiﬁcial Intelligence\\n\\nThe advantage of having intelligence implemented in the cloud is that it makes machine learning easier. As I have said so far, for machine learning, in most cases, it is advantageous to have as much data as possible. Since the cloud can connect with various machines, the incoming data will be much more than a single machine.\\n\\nThere is not an appropriate place to realize artiﬁcial intelligence as much as the cloud, but there are some evils caused by it. That is, the human’s live voice (in some cases, including the surrounding video data) is sent to the cloud. For learning, it is better to input the signal of the real world itself, but of course it may be a problem. Individuals have privacy. Privacy is a complex concept, in short, it can be said that freedom not to disclose personal life information about individuals and freedom that does not receive interference of others against individual activities. However, unintentionally information on privacy may be leaked to the outside. If it is due to a person’s mistake, such as what happened due to incorrectly setting the disclosure range of information posted to SNS, it likely cannot be helped. However, in many cases, it is not always the responsibility of the person himself/herself that the problem occurs. Many of the issues come from the cloud.\\n\\nThe problem with smart speakers is that some of the conversations you usually talk about are sent to the cloud. It is used as learning data, but it can be used for other purposes as well. For example, it is a clue to estimate the user’s daily behavior and living standards. It may be used as evidence to identify a criminal. In fact, there was a case where Amazon requested that the American court submit data of a certain user’s voice obtained by Amazon Echo as evidence (although Amazon refused the request).\\n\\nThis problem is a new problem born with the advancement of technology, and it seems that problems similar to this will occur from now on. In 2013, Edward Snow- den accused the US government of collecting personal information gathered. It is a so-called “Snowden Incident”. In this way, thinking that collecting and analyzing per- sonal information for the purpose of maintaining public security is a sufﬁcient story, irrespective of its compliance. I do not use SNS and do not unnecessarily disclose personal information. Still, it will be difﬁcult to prevent things like getting involved in some kind of trouble and unfairly exposing personal information. By the way, con- cerning the relationship between privacy and technology, Ann Cavoukian advocates the concept of Privacy by Design (Cavoukian 2011). She insists that in designing and operating an information system, personal information should be controlled by individuals (such as limiting the scope of disclosure and use).\\n\\nWhen using the cloud, there is a possible way to do something and send data with care so as not to leak personal information. In that case, the mechanism of intelligence is distributed among machines and clouds that users directly use. In other words, it means that machines used by individuals also have some intelligence. Such a way of thinking is called edge intelligence. Edge refers to the end of the network. For that, we need to be able to use machine learning to a certain extent even with a small amount of data. In this case, the mechanism of deep learning may not be necessarily effective, but logistic regression analysis and active learning such as explained in Chap. 2 will function effectively.\\n\\n6.2 Intelligent Agents\\n\\nI believe that by implementing intelligent agents based on the concept of edge intelligence, it is possible to realize a personal assistant system that is sufﬁciently intellectual and also capable of privacy protection. However, the agent will help the work done by humans, but it will not take over. There is a possibility that the agent will be in charge of some part of your work, but you will not be able to delegate everything. The agent is an autonomous system, but it is not a self-contained system. The agent is designed to be a human support system to the end; it must be such that it operates under the control of humans. It is an essential requirement for agents to coexist with humans.\\n\\n6.3 Singularity\\n\\nA phenomenon called the singularity or the technological singularity thought to be a possible outcome of the development of artiﬁcial intelligence has been widely discussed (Kurzweil 2005). Current artiﬁcial intelligence cannot be said to have the same intelligence as humans, but it is certain that progress is accelerating due to the large concentration of data in the cloud and technologies like deep learning. It is predicted that when artiﬁcial intelligence surpasses human intelligence and leads technical advancement it will bring about a rapid and large change that cannot be understood at the speed of human thought. This is a phenomenon called the singularity (to be precise at the time that the phenomenon occurs), but I do not know if this is a correct prediction or not. Also, I think that considering the singularity as a precursor of artiﬁcial intelligence against humans is not a very appropriate way of thinking.\\n\\nI think that it is natural that human life will change by advancing technology, and now I think that we as a society depend heavily on IT, especially the Internet. Also, I think that it is no doubt that artiﬁcial intelligence will become more sophisticated, and in certain areas (games such as shogi and go as easy-to-understand examples) will have much better ability than humans. But still, I do not think that artiﬁcial intelligence will be an omnipotent existence that surpasses humans in every aspect. Of course, artiﬁcial intelligence is obviously advantageous because it is an electronic entity without a physical body. Self-propagation of artiﬁcial intelligence is fully considered. However, unless autonomous evolution happens and self-consciousness emerges, it does not seem that it will become threatening humans away from the control of the human who is the designer of artiﬁcial intelligence.\\n\\nIf such a thing happened, it means that the design made by humans was wrong. In addition, although we cannot assert about the possibility of autonomous evolution, I think that it is impossible for humans to understand the mechanism, which ultimately makes it impossible to implement. As it happened that the act is properly performed, of course it is possible to repeat similar acts, but I think that such learning of behavior and self-consciousness are not continuous.\\n\\nIf human created design was wrong, it should be modiﬁed by humans. In other words, even if artiﬁcial intelligence is realized, it is only necessary to realize a mechanism for correcting it and incorporating it in advance. There are people who\\n\\n143\\n\\n144\\n\\n6 Symbiosis between Humans and Artiﬁcial Intelligence\\n\\npredict that it will become very difﬁcult to stop and redo halfway through learning once learning begins, but I do not think there is such a thing. Humans should become smarter with the evolution of artiﬁcial intelligence, because I think that a method of properly stopping artiﬁcial intelligence can be implemented for the human who designed it.\\n\\nThere is a prediction that artiﬁcial intelligence gets smarter and faster and humans cannot go about it. This prediction seems to ignore the possibility of human efforts, and anyway, I cannot agree with it, regardless of its pros and cons. I think it is inevitable to be overtaken by some people and some abilities. However, I think that artiﬁcial intelligence will never be hostile to humans, as artiﬁcial intelligence keeps the role of continuing to support the expansion of human capabilities, and as we are carefully managing it.\\n\\n6.4 Human–AI Symbiosis\\n\\nJ. C. R. Licklider published a paper named Man–Computer Symbiosis in 1960 (Lick- lider 1960). As written in the paper, symbiosis is deﬁned as “living together in inti- mate association, or even close union, of two dissimilar organisms.” In this paper, it is stated as follows. “The hope is that, in not too many years, human brains and computing machines will be coupled together very tightly, and that the resulting partnership will think as no human brain has ever thought and process data in a way not approached by the information-handling machines we know today.”\\n\\nThis can be thought of as connecting the human brain directly to the computer. However, what I call “symbiosis of humans and artiﬁcial intelligence” is not a form in which humans and computers are physically coupled. Instead, we assume a form that is tied in with both senses and language ability.\\n\\nThe reason for assuming in this manner is related to a nature that I call separability. Separability is a property of whether the work is still feasible in a situation where humans and machines cooperate and work, if for some reason it allows the human burden to increase even if the machine fails. If a human can execute the work even if the machine stops, it is said he or she can be separated from the machine.\\n\\nAt this point, it would seem that humans are somewhat inseparable from computers with regard to the task of searching. In the case of humans interacting with an agent while working, even if the agent stops to function, I believe that a human would be able to continue working with their own efforts. This is because even if humans rely on machines to shoulder part of their thinking, it is not to say that humans are incapable of thinking on their own without the machine. In the case of working with a machine, we can say that we are relying on the machine, but we do not unilaterally depend on them.\\n\\nIf a human connects his/her computer directly to the human brain, it will be quite difﬁcult to separate the human and the computer. I think that the collaboration between humans and agents should be very similar to the collaboration between people, including the way of communicating that information. Or, sometimes, I think\\n\\n6.4 Human–AI Symbiosis\\n\\nthat relationship is good so that humans are learners and agents become teachers. At that time, I think that the person who ﬁnished learning as much as possible should be able to do the job without agent assistance or guidance.\\n\\nIn terms of educational psychology, there is scaffolding. This is a tool (or some- one’s support) to raise the learner to a level that can accomplish the task that could not be achieved by one person alone. And when it comes to being able to do a speciﬁc job, it also means to do education aiming to be able to be executed even without that support.\\n\\nI expect that humans can be separated from agents by supporting agents based on the concept of scaffolding. However, agents cannot always be a teacher for humans. In the case that the agent can do better with respect to a speciﬁc job than a human, it will be able to support such that it can be understood naturally well. Then, I think whether it will be possible for that person to do as it is, even if any human is not as good as an agent.\\n\\nWell, in the abovementioned paper, the following things are also mentioned. “However, many problems that can be thought through in advance are very difﬁcult to think through in advance. They would be easier to solve, and they could be solved faster, through an intuitively guided trial-and-error procedure in which the computer cooperated, turning up ﬂaws in the reasoning or revealing unexpected turns in the solution. Other problems simply cannot be formulated without computing-machine aid. Poincare anticipated the frustration of an important group of would-be computer users when he said, “The question is not, ‘What is the answer?’ The question is, ‘What is the question?’” One of the main aims of man-computer symbiosis is to bring the computing machine effectively into the formulative parts of technical problems.”\\n\\nIn other words, in order to solve the problem that we face frequently in reality, the problem is solved earlier by human and computer collaborating with trial and error. Here, trial and error by humans and computers means that the computer repeats the process of verifying the hypotheses considered by humans. This is also “work for computers to solve problems” and “work for humans to understand problems.” This situation can be said to be one example where a symbiotic relationship between humans and computers is established.\\n\\nThe symbiotic relationship between humans and artiﬁcial intelligence (agents) is similar to this idea. Humans and artiﬁcial intelligence should coexist with each other so that they can be separated as much as possible and that humans unilaterally depend on artiﬁcial intelligence.\\n\\n6.5 Agents Embedded in Ordinary Things\\n\\nI would like to show examples where a human and an agent coexist in everyday life. We developed the system called Ubiquitous Talker in 1995 (Nagao and Rekimoto 1995). This is made with the concept of making everyday things as agents and allowing them to interact with humans. Although the term “ubiquitous” has become less used recently, it is based on the idea of Ubiquitous Computing (Mark Weiser\\n\\n145\\n\\n146\\n\\n6 Symbiosis between Humans and Artiﬁcial Intelligence\\n\\nFig. 6.3 Ubiquitous talker in use\\n\\nproposed in 1988), the idea that computers are ubiquitously coordinated to expand the human environment (Weiser 1991). Figure 6.3 shows how this system is used.\\n\\nIn this ﬁgure, when a person holds the portable device (currently smartphone) that he has in hand, he can communicate with that one by voice. At the time of making this, there was no concept of the cloud, so the agent’s intelligence was implemented on the side of the machine used by the user. Currently, in order to incorporate the mechanism of machine learning (especially deep learning), things will talk to humans while communicating with the cloud.\\n\\nRecently, the term Internet of Things (IoT) came to be used. This is an idea of implementing the function of connecting with the Internet to all things operating on electricity and linking it with the cloud. One characteristic is that items themselves may not have a mechanism for exchanging information with users. In that case, the user can receive services from IoT by accessing the cloud with a smartphone or PC. I think that the important function that IoT should have is to discover useful information for humans by collecting and analyzing data on the environment surrounding humans rather than directly interacting with humans. Probably, IoT devices will need to work with the cloud for this learning.\\n\\nOne example of IoT is a smart light bulb. This has a function that connects each LED light bulb installed on the ceiling or other locations to the Internet (or an intranet limited to the building). And the users can freely change brightness and color using\\n\\n6.5 Agents Embedded in Ordinary Things\\n\\na smartphone or PC. It is a function that is not yet available in the current smart light bulb, but considering which one is near the light bulb, what the person is doing right now, and what kind of environment is desirable, and cooperating with other surrounding lighting and clouds, light bulbs will actively create a lighting environment suitable for the surrounding people.\\n\\nThe previously smart speaker is also a kind of IoT. If IoT can communicate with humans by voice, it will be possible to realize the Ubiquitous Talker previously proposed by the authors as more practical. It will further step forward the symbiosis of humans and artiﬁcial intelligence. For example, if you do not know how to use the tools when you want to do the work, ask the tool or the machine how to use that tool. At that time, if your skill level is converted to data, you can make the tool refer to that data. Then the intelligent agent embedded in that tool will guide you how to use it in an easy-to-understand way. If the guide does not go well, you can tell the agent “I want to know more about it.” It is very useful information for agents. It will probably be converted to a teacher signal to update the machine learning model of the agent.\\n\\nAnd we can realize the symbiosis of human and artiﬁcial intelligence with respect to intellectual activities which are very important to humans such as discussions and presentations which we have repeatedly described in this book. In that case, an agent will appear in the place of discussion and presentation. The agent will evaluate humans’ discussions and presentations, and will feed the results back. The agent also contributes to creation of minutes. Also, in some cases, it may make statements that stimulate human creativity. At that time, it can be said that the agent is also participating in intelligent creative activities with humans. Agents will also participate in the gamiﬁed discussion mentioned in Chap. 4. In that case, the agent may be able to make better statements after being evaluated by other participants and may eventually become an excellent trainer for discussion beginners one day.\\n\\nI cannot predict how symbiotic relationships between humans and artiﬁcial intel- ligence will transform with the arrival of technological singularity or the emergence of much higher artiﬁcial intelligence than at present. I do not know whether their symbiotic relationships are becoming more sophisticated symbiotic relationships with higher capabilities from each other or whether they are becoming unilateral subordination so that humans are inferior to artiﬁcial intelligence. Believing that the former will be realized, I will continue my research in the future.\\n\\n6.6 Artiﬁcial Intelligence Accelerates Human Learning\\n\\nFinally, I will describe the topic “artiﬁcial intelligence accelerates human learning” which is also the title of this book. Speciﬁcally, it is about research on an AI trainer which improves student’s discussion and presentation skills.\\n\\nIn the story of the previous chapters, AI was not playing the role of guiding humans but was to support humans from behind. This is because it was difﬁcult for humans\\n\\n147\\n\\n148\\n\\n6 Symbiosis between Humans and Artiﬁcial Intelligence\\n\\nFig. 6.4 Robot trainer (implemented on pepper by SoftBank Robotics)\\n\\nto recognize AI as being equal to humans. However, the future AI is not out of sight as a support system. On the contrary, it seems to actively participate with humans as an agent.\\n\\nThe purpose of our research is to intentionally introduce automation with respect to tasks that are concerned about automation and aims to demonstrate with scien- tiﬁc basis that performance will be higher than humans perform. One such task is education, especially dealing with education on discussion and presentation.\\n\\nSpeciﬁcally, we are developing a robot trainer (shown in Fig. 6.4) that utilizes various sensors, link with information tools used by students, and bringing about a mechanism to make appropriate suggestions especially for discussion between students and presentations by students. In this task, detailed data analysis will clarify that the skill of students improves when AI analyzes, evaluates, and comments on data rather than teaching by a human trainer.\\n\\nDuring the discussion, the robot trainer is always at the side of the student, observ- ing the situation, taking some appropriate timing, and doing some suggestions. For example, as far as the number of utterances is fewer than other participants, it is better to speak more, as to the question better not to return it as a question, better to say the conclusion earlier, and so on.\\n\\nLikewise, when a student makes a presentation, the robot trainer is always listening nearby, and a comment is made after the presentation. For example, the voice was\\n\\n6.6 Artiﬁcial Intelligence Accelerates Human Learning\\n\\ndifﬁcult to hear, the slide was hard to read, the speaker did not talk properly toward the audience, it was not relaxed, and so on.\\n\\nThe robot trainer can collect and analyze data such as video, voice, person’s posture, heart rate, and can decide the contents and timing of the suggestion from various viewpoints. This is similar to the audience AI of the VR presentation training system mentioned at the end of Chap. 5. However, while the audience AI only responds to the human presentation, the robot trainer actively tells humans to promote learning.\\n\\nAutomation in the ﬁeld of education is not necessarily progressing properly. This is not only technically difﬁcult but also because education is done for humans, and that automation seems to be due to a simple intuition that it does not pro- duce very good effects. The e-learning system and Learning Analytics mentioned in Chap. 1 have increased the number of elements that can be converted to data (Daniel 2017). However, it has not been realized at a manageable level a mechanism that automatically analyzes the data and appropriately reﬂects it in the guidance.\\n\\nTherefore, we examine the hypothesis that “the presentation of concrete data on the student’s condition and guidance based on it are more likely to be accepted by students than instructed by humans” and realize automation in education. As a result, human learning is accelerated by AI.\\n\\nThe speciﬁc veriﬁcation method will be described below. Expanding the results so far on the analysis and evaluation of discussion and pre- sentation, we realize a mechanism to improve students’ discussion and presentation skills using humanoid robots, and conduct demonstration experiments for laboratory students. For that purpose, we record discussions and presentations, analyze contex- tual information such as the internal structure of the materials (slides and posters, etc.), visual and auditory scenes of students’ activities, participants’ mental states, and then evaluate the quality of their activities.\\n\\nBased on the analysis results of the data, we propose a way to give effective suggestions to humans. In conjunction with the discussion and presentation support systems mentioned in Chap. 5, we develop a robot trainer that evaluates student activities and gives them a suggestion at an appropriate timing. Then, we compare guidance results with human experts and verify the effectiveness of the robot trainer. In the case of discussion, we will expand the discussion mining system and inte- grate mechanisms that recognize gestures and expressions of participants in real time. In the case of a slide presentation, software for analyzing the internal structure of the slide and analyzing the browsing behavior of the participant who views the slide is developed. On the other hand, in the case of poster presentations, we plan to develop software to analyze the digital poster creation and operation log, as well as software that will record the presentation and analyze the images and sound captured during recording. Recognition of the position and posture of the surrounding people is also performed by means of depth sensors mounted on the ceiling that are capa- ble of tracking the human body. In addition, wearable devices (e.g., Apple Watch) to measure heart rate are attached to all students, and psychophysiological data are also collected in real time and subject to analysis. These systems are the foundation system for operating the robot trainer.\\n\\n149\\n\\n150\\n\\n6 Symbiosis between Humans and Artiﬁcial Intelligence\\n\\nThe robot trainer moves to the venue where the discussion and presentation are being conducted and gives appropriate instructions and advice at appropriate timing near the student who is speaking or presenting. In addition to human facial expression, gaze, and voice, biometric information based on heart rate acquired from wearable devices can also be used, and in collaboration with a discussion/presentation support tool, students’ skills can be analyzed and evaluated. Activity records and comments can be reviewed later through the foundation system.\\n\\nIn order to make the evaluation of the robot trainer universal, it is necessary to analyze the record of the discussion and presentation from various viewpoints and establish the evaluation method. This is done by organizing the mechanisms described in Chaps. 4 and 5 and clarifying the evaluation index and its calculation method. For example, in the case of discussion, the following points are considered. The quality and frequency of statements, the attitude toward others’ opinions (agree, disagree, disinterest), the size of voice of statements, whether the direction of gaze toward the discussion partner, whether the discussion is diverging or converging, whether it is not diverting the topic, whether it properly responds to questions from others, and the like become the point of view.\\n\\nAlso, in the case of a presentation, it is necessary to decide whether the content such as a slide or a poster is easy for the viewer to understand, whether the theme or result has an impact and gets interest, whether the contents are structured in a well- balanced manner, whether the layout is appropriate, whether the way of presentation (how to watch gaze, voice size, speaking speed, time balance, etc.) is appropriate, whether relaxing during presentation, whether the responder answers the question properly, and so on.\\n\\nThis is based on the guidelines of the American Evaluation Association (http:// www.eval.org/). In order to conﬁrm the validity of the automatic evaluation method, students are asked to self-evaluate their own activity records by selecting representa- tive ones. Also, we ask students to evaluate each other’s activities mutually. We are planning to compare and analyze these results and the evaluation by the system.\\n\\nNext, it is necessary to link the evaluation of the discussion/presentation to con- crete guidance. Basically, it is the way that the robot trainer is at the side of the student, giving advice so that the evaluation goes up at an appropriate time, or advis- ing not to lower the evaluation. One of the hypotheses of this research is that advice by the machine is more likely to be accepted than advice by humans in the case of advice based on the concrete evaluation index and speciﬁc data clarifying the score. If similar failures are repeatedly pointed out, perhaps you would feel uncomfortable if this fact were pointed out by another human. However, in the case of machines, it is not so annoying and we can calmly consider their advice.\\n\\nWhether a student feels uncomfortable is judged based on psychophysiological information such as a heart rate. In addition, students should be able to view the evaluation of their past activities at any time, so that they can compare their activities with others’ activities. Furthermore, we compare the content and evaluation of our activities in chronological order, visualize the degree of growth, and make it possible to discover problems by themselves compared to the growth of others. When that problem occurs again, the robot trainer advises by referring to past guidance so\\n\\n6.6 Artiﬁcial Intelligence Accelerates Human Learning\\n\\nthat the problem is solved. If they are aware of the problem, it will be easier to accept instructions from the robot. Applying this mechanism to laboratory students, we conﬁrm that their discussions and presentations are highly evaluated since their skills are improved.\\n\\nBy summarizing the results so far, it is possible to realize a method of automatically evaluating discussion and presentation quality and an effective teaching method for machines. When the student performs a discussion/presentation, the robot trainer evaluates the activities from various viewpoints and makes leading comments. For example, it points out the low number of statements, suggests the timing to speak, and points out the quietness of their voice and encourages the presenter to speak up. Also, it points out if the content of the slide is inappropriate and asks for the presentation of a slide with a more detailed explanation, and so on.\\n\\nIn addition, visualization and presentation of evaluation details and time series data compared with past activities are presented. Next, we compare the degree of the change in the evaluation in the case of direct guidance by human with the case of the robot trainer. In the case of a human trainer, we distinguish between instructing based on the data analyzed by the system and instructing based on the subjective evaluation of the human trainer. And it shows that the degree of improvement of the evaluation is signiﬁcantly higher in the case of using the robot trainer compared with either case of the human trainer. By doing this, it is possible to empirically demonstrate the case of AI accelerating human learning.\\n\\nThe plan is a 3-year schedule and, as of writing of this book, it is still the ﬁrst year, so we cannot state the results of this research here unfortunately. I would like to describe the details of this research in the sequel of this book.\\n\\nReferences\\n\\nCavoukian,\\n\\nin Privacy by Design: 7 Foundational Principles,\\n\\nInformation and Pri- https://www.ipc.on.ca/wp-content/uploads/Resources/\\n\\nvacy Commissioner 7foundationalprinciples.pdf (2011)\\n\\nof Ontario.\\n\\nB.K. Daniel (Ed.), Big Data and Learning Analytics in Higher Education: Current Theory and\\n\\nPractice (Springer International Publishing, 2017) J. Hawkins, S. Blakeslee, On Intelligence, Grifﬁn (2005) R. Kurzweil, The Singularity Is Near: When Humans Transcend Biology, Viking Adult (2005) J.C.R. Licklider, Man-computer symbiosis. IRE Transac. Human Factors Electron. HFE-1, 4–11\\n\\n(1960)\\n\\nK. Nagao, Situated Conversation with a Communicative Interface Robot. In Proceedings of the First International Workshop on Intelligent Media Technology for Communicative Reality (2002) K. Nagao, J. Rekimoto, Ubiquitous Talker: Spoken Language Interaction with Real World Objects, in Proceedings of the Fourteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI- 95), pp. 1284–1290 (1995)\\n\\nM. Weiser, The computer for the 21st century. Sci. Am. 265(3)94–104, September 1991 J. Weizenbaum, ELIZA—A computer program for the study of natural language communication\\n\\nbetween man and machine. Commun. ACM 9(1), 36–45 (1966)\\n\\n151')],\n",
              " [Document(metadata={'source': '/content/Stahl_2021_Artificial_intelligence_for_human_f.pdf'}, page_content='Availableonline16December20200148-2963/©2020TheAuthor(s).PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\\n\\nJournalofBusinessResearch124(2021)374–388\\n\\nContents lists available at ScienceDirect\\n\\nJournal of Business Research\\n\\njournal homepage: www.elsevier.com/locate/jbusres\\n\\nArtificial intelligence for human flourishing – Beyond principles for machine learning\\n\\nB.C. Stahl a,*, A. Andreou e, P. Brey b, T. Hatzakis c, A. Kirichenko d, K. Macnish b, S. Laulh´e Shaelou f, A. Patel d, M. Ryan g, D. Wright c a De Montfort University, UK b University of Twente, the Netherlands c Trilateral Research, UK d F-Secure, Finland e Aequitas, Cyprus f University of Central Lancashire, Cyprus g Wageningen University & Research, the Netherlands\\n\\nA R T I C L E I N F O\\n\\nA B S T R A C T\\n\\nKeywords: Ethics Artificial intelligence Big data Human rights Governance\\n\\nThe technical and economic benefits of artificial intelligence (AI) are counterbalanced by legal, social and ethical issues. It is challenging to conceptually capture and empirically measure both benefits and downsides. We therefore provide an account of the findings and implications of a multi-dimensional study of AI, comprising 10 case studies, five scenarios, an ethical impact analysis of AI, a human rights analysis of AI and a technical analysis of known and potential threats and vulnerabilities. Based on our findings, we separate AI ethics discourse into three streams: (1) specific issues related to the application of machine learning, (2) social and political questions arising in a digitally enabled society and (3) metaphysical questions about the nature of reality and humanity. Human rights principles and legislation have a key role to play in addressing the ethics of AI. This work helps to steer AI to contribute to human flourishing.\\n\\n1. Introduction\\n\\nwhether and how AI should be regulated or whether other ways should be found to address the downsides of AI.\\n\\nThe development of artificial intelligence (AI) is often described in terms of human progress. The recent progress of machine learning, supported by growing amounts of available data combined with rapidly expanding computing capabilities and publicly available tools and li- braries, have led to expectations of increased efficiency but also to new and better services for consumers and citizens. This broadly positive discourse is, however, counterbalanced by a discussion of the downsides and risks of AI.\\n\\nA growing volume of literature suggests that governance mecha- nisms need to be devised for these technologies because existing governance structures are not able to address the issues they raise. As a consequence, one can find numerous suggestions on various ways to develop governance structures that range from the informal, such as voluntary industry codes of conduct, to national and international legislation and the creation of regulators.\\n\\nOne weakness of the current discourse is a disconnect between rigorous academic research on the content and implications of these technologies and the development of governance proposals.\\n\\nThe ethics of AI is a topic of conversation in the disciplines concerned with these technologies including the social sciences, humanities, media and policy. Worries range from discrimination due to biased datasets to the domination of humanity by sentient machines. The social impact of AI-based technologies provides the backdrop and justification for the flurry of activities in public discourse and policy developments about\\n\\nIn order to move beyond the current discourse, gain a deeper un- derstanding of the nature of ethics in AI, and allow for a critical reflection of the current discourse, we conducted multi-method and interdisciplinary research aimed at contributing to empirical and\\n\\nCorresponding author at: De Montfort University, The Gateway, Leicester LE2 9BH, UK.\\n\\nE-mail address: bstahl@dmu.ac.uk (B.C. Stahl).\\n\\nhttps://doi.org/10.1016/j.jbusres.2020.11.030 Received 18 January 2020; Received in revised form 16 November 2020; Accepted 18 November 2020\\n\\nJournalofBusinessResearch124(2021)374–388375\\n\\nB.C. Stahl et al.\\n\\nconceptual clarity of the nature of these technologies, the challenges they raise and the potential of new governance structures to address these issues. The aim of this paper is to contribute to the discussion about how to identify, interpret and address ethical issues arising from AI applications1. The paper critically reflects on the term AI and explores which aspects of AI raise which types of issues and how these are re- flected and addressed in organisational and societal practice. Bringing together conceptual insights and empirical findings, the paper is in a position to propose new ways to think about AI and structure the AI ethics narrative. In order to achieve this aim, the paper first seeks to answer the question of what precisely are the key ethical issues and how best to classify or categorise them. It then explores how existing governance mechanisms may be applied to these issues. This leads to the final question of theoretical and practical next steps.\\n\\norganization, or territory, and whether through laws, norms, power, or language” (Bevir, 2012, p. 1). The term also refers to specific localised ways of organising (or governing) particular issues, as in data gover- nance (Khatri & Brown, 2010) or information governance (ISO, 2008), rendering it suitable to describe ways of dealing with AI that cover many societal actors and activities.\\n\\n2.1. AI and big data\\n\\nThe concept of AI, while much discussed, is not well defined. A typical definition of AI is the one provided by the European Commission (2018, p. 1): “Artificial intelligence (AI) refers to systems that display intelligent behaviour by analysing their environment and taking actions – with some degree of autonomy – to achieve specific goals.” This is consistent with Haenlein and Kaplan (2019) emphasis on the interpre- tation of external data, learning from such data and using data for the achievement of specific goals. While such definitions are sufficient to give an idea of the scope of AI, they are arguably not specific enough to allow the identification of specific ethical issues or the application of governance structures. Definitions such as the EC’s can also be prob- lematic when they seem to imply contentious positions, such as that AI can behave, analyse and act, which can be read as imputing character- istics, notably that of independent agency, that current machine learning technologies do not display. This points to metaphysical as- sumptions about AI to which we return to below.\\n\\nOur analysis shows that the ethical issues that arise in empirical observations are similar to those that the academic literature discusses, which provides reason for the belief that the discourse on ethics in AI is reasonably expansive. At the same time, however, it becomes clear that the meaning of these issues is largely context-dependent. We use our understanding of the ethical issues to categorise them into three broad categories: (1) issues directly related to machine learning, (2) broader social and political issues arising in modern digitally enabled societies and finally (3) metaphysical questions. These categories allow us to map currently existing and discussed mitigation and governance structures to these issues. This is an important starting point for the practical question of what can and should be done to address these issues. This question is beyond the scope of this paper.\\n\\nOne reason for these shortcomings is that the definitions hide the immense breadth and depth of the underlying AI research (Elsevier, 2018). The current prominence of AI is based on long-established principles of machine learning, often implemented through (deep) neural networks. These have recently gained prominence due to the increased availability of computing power and large data sets for training purposes. Ethical discussions of AI therefore need to be sensitive to both the consequences of the application of AI algorithms and tech- niques as well as the ethical aspects of (big) data analytics (B. D. Mit- telstadt, Allo, Taddeo, Wachter, & Floridi, 2016; Nerurkar, Wadephul, & Wiegerling, 2016; Varley-Winter & Shah, 2016). Public discourse on AI especially focuses on machine learning and the empirical work we have undertaken covers technologies in machine learning and big data ana- lytics. However, as we argue below, the ethics of AI debate is broader than this and refers to other concepts of AI, notably that AI technologies have broader human-like cognitive abilities. The concept of general AI goes back to the beginning of AI research and is sometimes referred to as Good Old Fashioned AI (GOFAI) (Moor & Bynum, 2002). General AI technologies do not currently exist, but they figure strongly in the public discourse. In order to be able to make sense of the broader debate, it is important to be aware of the entire breadth of meaning of the term.\\n\\nThe findings presented in this paper are important in several re- spects. The paper makes an academic contribution to the quickly spreading discussion of ethics and AI and research around ethics, values, governance and tools of AI. The categorisation of issues suggested here and the mapping of the categories to different governance mechanisms can help streamline the debate. Due to the high practical importance of the underlying technologies, the paper also has practical importance for stakeholders faced with the practical challenge of proactively engaging with the ethics of AI. The paper can help organisations developing, deploying or using AI to identify issues they are likely to face and engage with governance mechanisms that can address these issues.\\n\\nIn order to develop the argument, the paper proceeds as follows. In the next section, we discuss the governance of AI, looking first at defi- nitions, followed by a discussion of ethical issues and currently proposed governance structures. We then describe our multi-dimensional empir- ical study of AI. The findings and discussion give rise to our catego- risation of issues, which we then map to governance structures and stakeholders. Our discussion and conclusions demonstrate the novelty and relevance of our findings while we propose next steps.\\n\\nThis paper therefore does not attempt to offer a comprehensive definition of AI or of any of its constituent technologies such as machine learning. Instead, it aims to bring greater clarity to the question what people refer to when they talk about AI and, more importantly, when they talk about the ethics of AI or about the ethical issues of AI. These conceptual questions are crucial to dealing with the ethics of AI and questions of governance. They pose the problem of delineating which ethical issues are related to or caused by AI and it complicates questions of governance, where the application area of governance mechanisms is often not clear, as we will show in more detail below.\\n\\n2. Governance of AI\\n\\nThis section provides the conceptual basis of tthis article and gives an overview of current discussions regarding AI, its ethical implications and possible governance structures. The term ’governance’ as developed in political sciences traditionally refers to alternatives to formal govern- ment on a societal or state level. In business research, it frequently refers to structures and processes within organisations, whereas on a higher level the term ’regulation’ is used (Braithwaite & Drahos, 2000). How- ever, ’governance’ is increasingly used to describe a much broader array of “[…] processes of governing, whether undertaken by a government, market, or network, whether over a family, tribe, formal or informal\\n\\n2.2. Ethics and AI\\n\\nThe concept of ethics is even more contested and open than that of AI. In everyday English, it denotes questions of right or wrong, of good or bad. Following Stahl (2012), we argue that this everyday under- standing of ethics constitutes the basis of explicit reasoning and aca- demic reflection, which are the subject matter of philosophical ethics. Answering the question of why a particular action can be seen as good or bad or which processes would allow answering such a question is the\\n\\n1 We use the term \"issue\" in an open sense, accepting as issues whatever our respondents or the literature describes using the term. As a consequence, the issues we cover vary greatly in terms of scope and impact. Some are very precise and focused whereas other are large and fuzzy and cover entire areas where issues arise.\\n\\nJournalofBusinessResearch124(2021)374–388376\\n\\nB.C. Stahl et al.\\n\\nfirst and most prominent purpose is to improve processes and efficiency. For organisations using AI, this translates into lower costs, higher pro- ductivity and, eventually, higher profits. The second purpose is the use for social control. AI techniques are the enablers for voice and face recognition and can therefore be used for surveillance and tracking in- dividuals. This is the basis for controlling individuals to ensure they follow specific requirements. This is the underlying idea of the Chinese Social Credit System (Liu, 2019). The third purpose of using AI is to promote human flourishing. Flourishing is an ethical principle typically associated with virtue ethics, which has a well-established history of application to digital technologies (T. W. Bynum, 2006) and which has been used to frame the AI debate more recently (ALLEA & Royal Society, 2019). It is not always trivial to determine what constitutes flourishing or how technology can contribute to it. However, any attempts to use ‘AI for Good’, as the title of the series of summits organised by the Inter- national Telecommunication Union suggests (https://aiforgood.itu.int/ ), can count in this category.\\n\\nrole of philosophical ethical theories. These include classical theories such as virtue ethics (Aristotle, 2007) which determines the ethics quality often action based on the character of the individual undertaking it. Other frequently used ethical theories include deontology, which focuses on the agent’s duty (Kant, 1788, 1797), or teleology which looks at the consequences and outcomes of an action to determine its ethical status (Mill, 1861). In addition to these well-established traditional ethical theories, there are more recent ones like the ethics of care (Adam, 2001; Gilligan, 1990) and specific ethical theories aimed at technolog- ical applications, such as computer ethics (Terrell Ward Bynum & Rogerson, 2003; D. G. Johnson, 2001), information ethics (Capurro, 2006; L. Floridi, 1999; Luciano Floridi, 2010) or disclosive ethics (Brey, 2000).\\n\\nDespite a rich history of discussing the relative merits of various ethical positions, the current discourse around ethical issues of AI makes little reference to philosophical ethical theories. Instead, the generally accepted approach to AI ethics seems to define mid-level ethical prin- ciples, an approach pioneered by biomedical ethics (Beauchamp & Childress, 2009; The National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research. (1979), 1979). This approach, sometimes referred to as principlism, is not without criticism (Clouser & Gert, 1990). It has the practical advantage of sidestepping long-standing ethical debates. But it is open to the charge that it fails to solve practical ethical issues, due to the apparent consensus on ethical principles that then fail to guide practical action (B. Mittelstadt, 2019). Nevertheless, the creation and compilation of ethical principles form key aspects of the ethics of AI debate (Anabo, Elexpuru-Albizuri, & Villard´on-Gallego, 2019; Asilomar Conference, 2017; Boden et al., 2017). In addition, most of the high-level interventions into the ethics of AI discussion are principle-based, such as the guidelines produced by the European High Level Expert Group on AI (HLEG on AI, 2019). For our purposes, it is sufficient to understand the prevalence of ethical princi- ples in the AI discourse. We agree with Mittelstadt (2019), however, in seeing the focus on principles as limiting and will return to the question of an appropriate ethical theoretical basis for the ethics of AI below.\\n\\nWe realise that this is a strong simplification and that these in- tentions and purposes of AI are not necessarily mutually exclusive and do not comprehensively cover all possibilities. The need for contact tracing to fight a pandemic, for example, shows that social control can be conducive to human flourishing. Similarly, the optimisation of pro- cesses and resulting profit maximisation leads to higher income and welfare, which can (but do not have to) contribute to broader human flourishing. The Venn diagram in Fig. 1 indicates that the three different purposes can intersect and overlap. However, they are recognisably different ways of approaching AI and have different ethical implications and connotations.\\n\\nIntentions behind promoting AI are important to understand and evaluate perceptions of ethics and possible governance mechanisms employed to address ethical issues. These intentions do not develop in isolation but form part of a larger socio-economic, cultural and political context that influences the way a ‘good society’ is perceived and the role AI can play in it (Cath, Wachter, Mittelstadt, Taddeo, & Floridi, 2016). We do not wish to overstate differences between regions or underesti- mate levels of disagreement within political cultures, but we think it is probably safe to say that the European approach to AI aims to promote human flourishing, even where this may lead to trade-offs with effi- ciency or access, which may result from specific interventions, such as the EU’s General Data Protection Regulation (GDPR) (General Data Protection Regulation, 2016; see also Kaplan & Haenlein, 2019).\\n\\nDiscussions of the ethics of AI tend to cover particular ethical issues. These are typically particular features of the technology or consequences of its use that the authors see as problematic. Many of these have long- standing histories in ethics of technology or ethics of computing, such as security, privacy or access. Some of them seem to be particularly linked to the algorithms that drive AI, such as problems of algorithmic biases (CDEI, 2019; K. Johnson, Pasquale, & Chapman, 2019; B. D. Mittelstadt et al., 2016) and many of them are linked to the compilation and manipulation of large data sets that are required for many of the current AI techniques (Metcalf et al., 2016; Nerurkar et al., 2016; Taylor, 2016). Some ethical issues are specific to particular application areas, such as finance or autonomous vehicles, whereas others are seen as broadly relevant to all AI areas.\\n\\nIn order to assess whether a response to an ethical issue is appro- priate or likely to be successful, we need to not only understand the purpose of AI, but also the range of possible options used to address the issue. This paper does not offer the space to review all governance ar- rangements or tools that are available to implement them (see Hagen- dorff, 2019; Morley, Floridi, Kinsey, & Elhalal, 2019). For the purposes of this paper, we seek to understand the types and levels of activity that aim to provide governance mechanisms for AI. Below, we distinguish between measures aimed at the individual, the organisation and society.\\n\\n2.3. Purpose of AI and governance proposals\\n\\nThe number and reach of ethical issues linked to AI is enormous, in particular, when considering the breadth of possible application areas. Addressing them is therefore a challenge that has attracted much attention. One key question that needs to be answered before any mitigation measures can be developed is the role that AI has and is meant to have in society. AI, along with most other information and communications technologies (ICTs), has a particularly high level of interpretive flexibility (Doherty, Coombs, & Loan-Clarke, 2006), which means that it is difficult to predict how it will be used. This has been a key driver for thinking about ethical aspects of ICTs for decades, sometimes discussed under the heading of “logical malleability” (Moor, 1985). What this means is that even in cases where a technology is designed for a particular purpose, it is difficult to foresee whether and to what degree it will be used for this purpose.\\n\\nWe distinguish between different purposes of making use of AI. The\\n\\nFig. 1. Possible purposes of AI.\\n\\nJournalofBusinessResearch124(2021)374–388377\\n\\nB.C. Stahl et al.\\n\\nAcademy & Royal Society, 2017; Khatri & Brown, 2010; OECD, 2017), and ensure these cover AI. Similarly, many organisations have estab- lished mechanisms for dealing with ethical and broader societal con- cerns, often discussed under the heading of corporate social responsibility (CSR) (Garriga & Mel´e, 2004), which can be extended to include AI and emerging technologies.\\n\\nGovernance structures that support the identification and mitigation of possible ethical issues of AI cover all levels, notably the individual, organisational and political / societal. Individual researchers and de- velopers can make use of a quickly growing number of AI ethics frameworks originating from companies, governments or other organi- sations. The EU’s High Level Expert Group is a pertinent example (HLEG on AI, 2019) but many others exist. Individual developers can make use of professional guidance, for example, from bodies such as the ACM or BCS (Brinkman et al., 2017). Standardisation initiatives, such as the ISO/IEC JTC 1/SC 42 - Artificial intelligence or the IEEE P7000 family of standards, consider the ethics of AI and, once agreed, can provide guidance. Development methodologies can be created or adapted to pay attention to ethical issues, for example, by integrating specific issues into the design process, such as privacy by design (Cavoukian, 2017; Hansen, 2016; Information Commissioner’s Office, 2008) or more broadly by adopting an ethics by design stance (Beard & Longstaff, 2018; Iphofen & Kritikos, 2019; Martin & Makoundou, 2017).\\n\\nThe final level is the societal and policy level, covering national and international policy and regulation. These drive a lot of individual and organisational activity and therefore play a role in governing AI. It is, therefore, not surprising that policy and regulatory mechanisms play a prominent role concerning the ethics of AI. There are existing statutory instruments, such as the GDPR (General Data Protection Regulation, 2016), that clearly address some of the issues that AI raises. Similarly, there are principles of human rights, that are addressed and safeguarded in international agreements, such as the Universal Declaration of Human Rights or the European Convention on Human Rights, that cover rele- vant rights such as the right not to be discriminated against that have relevant applications to AI. Similarly, legislation in areas such as competition law, product liability or intellectual property can have consequences for AI. One type of regulatory instrument with regard to AI is the creation of a regulator to oversee AI development and use. This can be achieved by extending the remit of existing regulators, such as data protection authorities, or by creating new bodies.\\n\\nThe second level of measures provides guidance for organisations to follow or adopt. According to Clarke (Clarke, 2019b, 2019a), established mechanisms of risk management can go a long way in allowing orga- nisations to address the ethics of AI. Organisations can employ existing impact assessment approaches such as privacy (or data protection) impact assessments (CNIL, 2015), technology assessment (Grunwald, 2009), ethics impact assessment (Wright, 2011), social impact assess- ment (Becker & Vanclay, 2003) or human rights impact assessment (Latonero, 2018). They can extend existing governance mechanisms, such as those used for quality assurance or data governance (British\\n\\nOn all three of these levels, the individual, the organisational and the national / international, there can be different focus areas. AI can be looked at in general terms or specific application areas can be emphasised, such as AI in health, finance, politics, public services or\\n\\nFig. 2. Overview of proposals for governance mechanisms of AI ethics.\\n\\nJournalofBusinessResearch124(2021)374–388378\\n\\nB.C. Stahl et al.\\n\\nothers. In many cases, organisational or technical tools may exist or could be developed to support governance structures.\\n\\napproach (See Fig. 3).\\n\\nA single paper such as this one cannot hope to do justice to the complexity of five different major components of a complex multi- dimensional study as presented here. Each of these has been described in detail elsewhere (Andreou et al., 2019; Macnish, Ryan, Gregory, et al., 2019; Macnish & Ryan, 2019; Patel, Hatzakis, Macnish, Ryan, & Kir- ichenko, 2019; Wright et al., 2019). Instead of a detailed account of all methodological considerations, we focus here on a brief overview of the different methods and why they provided the insights we required for our research objective.\\n\\nThe aim of this very brief overview of ethics and governance of AI, graphically represented in Fig. 2 (above), was to demonstrate the complexity of the topic area. The discourse of ethics and AI is currently characterised by a cacophony of voices and contributions. There is no lack of ideas or proposals. The challenge is to synthesise a manageable approach from the multitude of activities. This synthesis should start with a useful and manageable categorisation of ethical issues that lends itself to an analysis of suitable governance structures that can be applied to these issues. This paper attempts such a synthesis based on a multi- dimensional research approach, as described in the next section.\\n\\nThe motivation for undertaking a set of case studies arose from the lack of rigorous empirical academic research of AI across application areas. While there are numerous studies of the impact of AI in particular areas, a broader understanding of AI required a set of comparable in- sights in different settings. We chose to undertake a set of interpretive case studies (Walsham, 1995) because they allowed us to develop a detailed understanding based on the views of individuals and organi- sations involved. There are certainly many more application areas than 10, but undertaking 10 studies and doing a comparative analysis (Yin, 2003) gave us the confidence of being able to develop a strong under- standing across applications. We developed a case study protocol and pilot tested it during the summer of 2018. The empirical work was un- dertaken in 2018 and 2019. We interviewed a total of 22 stakeholders across the 10 case studies. For each case, we furthermore undertook background research on the organisation in question as well as the field in which the case study was undertaken (e.g., AI in finance, agriculture). The analysis was undertaken collaboratively using NVivo Server 11. The partners wrote up case studies following an agreed template, cross- reviewed and published on our website (Macnish, Ryan, & Stahl, 2019). All case studies were furthermore developed as stand-alone publications whose references are listed in Table 1.\\n\\n3. Methodology: A multi-dimensional approach\\n\\nWhile there is a large and quickly growing literature on ethical issues of AI, much of it is anecdotal or speculative. In our research, we there- fore aimed to combine academic rigour with detailed insights into the way in which AI is realised in society and a broad and conceptual overview of the field. No one single established methodology can ach- ieve this. We therefore decided to use a multi-dimensional approach that involved using and combining several methods to collect and interpret data and develop an understanding of the field of ethics and AI.\\n\\nThe geographical focus of our study is Europe. We wanted to know whether the influence of AI on human flourishing is noticeable in the way AI is developed and deployed. In order to understand this, to gain an in-depth understanding of the social reality of AI across different application domains at present and in the future and to understand the technical, ethical and human rights implications, we undertook a multi- dimensional study comprising the following:\\n\\n1 10 interpretive case studies of AI application in particular applica-\\n\\nIn order to broaden our understanding further, but also to go beyond the description of current technologies, we decided to develop a set of five scenarios. The social domain in which AI is employed was discussed simultaneously for case studies and scenarios (see table below). This served to broaden the range of insights.\\n\\ntion domains and organisations\\n\\n2 Five policy-oriented scenarios exploring near term (<5 years) use of\\n\\nemerging AI applications\\n\\n3 An ethical impact analysis of AI 4 A human rights analysis of AI 5 A technical analysis of threats and vulnerabilities connected with AI.\\n\\nThe scenarios were constructed with a specific focus on providing applicable insights that could help decision-makers, notably those working in policy development, to develop and implement governance mechanisms. Based on the rich history of scenario methods (Andersen &\\n\\nThe following figure is a graphical representation of the research\\n\\nFig. 3. Components of the multi-dimensional research approach underpinning this research.\\n\\nJournalofBusinessResearch124(2021)374–388379\\n\\nB.C. Stahl et al.\\n\\nto identify, interpret and address ethical issues arising from AI appli- cations. In order to achieve this, we need a comprehensive overview of both conceptual and empirical insights into AI and its use. We therefore draw on a range of research activities outlined below. The underlying empirical studies were not re-analysed but taken as the starting point for compiling and, more importantly, for categorising these issues. The important contribution to the AI ethics discourse that this paper makes is in the conceptualisation and proposed narrative which is based on the overall findings. It is therefore only possible to provide a high-level overview of the empirical work, all of which is published elsewhere.\\n\\nTable 1 Social domains of case studies and scenarios (CS:= Case Study; SC:= Scenario).\\n\\nNo.\\n\\nSocial Domain of Case Study / Scenario\\n\\nReference\\n\\n(Antoniou & Andreou, 2019)\\n\\nCS01\\n\\nEmployee monitoring and administration Government Agriculture Sustainable development Science Insurance Energy and utilities Communications, media and cybersecurity CS09 Retail and wholesale trade CS10 Manufacturing and natural\\n\\nCS02 CS03 CS04 CS05 CS06 CS07 CS08\\n\\n(Ryan, 2019a) (Ryan, 2019b) (Ryan & Gregory, 2019) (Jiya, 2019b) (Kanceviˇcien ˙e, 2019) (Hatzakis, Rodrigues, & Wright, 2019) (Macnish, Inguanzo, & Kirichenko, 2019) (Macnish & Inguanzo, 2019) (Jiya, 2019a)\\n\\nOur research showed that the breadth of ethical issues discussed in the literature is reflected to a large extent in organisational practice. A cross-case analysis extracting the moral issues that were raised in the different case studies showed a large number of ethical issues, many of them recurring, as shown in the following table (See Table 2).\\n\\nresources Social care Information warfare Education Transportation Predictive policing\\n\\nThis table listing the ethical issues we encountered is the result of our data analysis and represents our interpretation and categorisation of what respondents shared with us. The terms used to denote the indi- vidual ethical issues were discussed and agreed during the data analysis. The results of the analysis from the case studies were supported by the first round of our Delphi Study, which targeted experts in AI and big data (Santiago, 2020). Implemented as an online survey, the first set of questions was e-mailed to 231 experts, 50 per cent of whom were women. We received 145 responses.Following review of the data and data cleansing, 41 responses contained sufficient information to warrant analysis. The first (open-ended) question covered the same ground, asking “What do you think are the three most important ethical or human rights issues raised by AI and / or big data?” Fig. 4 shows the most frequently given answers:\\n\\nSC01 SC02 SC03 SC04 SC05\\n\\nAll scenarios are described in detail in ( Wright et al., 2019)\\n\\nJaeger, 1999; Boenink, Swierstra, & Stemerding, 2010; Cairns & Wright, 2017; Ramirez, Mukherjee, Vezzoli, & Kramer, 2015), we applied a modified methodology which we called “policy scenarios” (Wright, Stahl, & Hatzakis, 2020). At its core, a policy scenario aims at a rela- tively short time horizon that is of relevance to policymakers caught up in election cycles. We aimed to develop scenarios that are relevant in several years. A second important aspect of policy scenarios is close stakeholder engagement. For each scenario, we organised a face-to-face workshop that included stakeholders and involved them in the revision and refinement of the scenarios. All scenarios were peer reviewed and made publicly available in June 2019 (Macnish, Wright, & Jiya, 2020; Wright et al., 2019).\\n\\nOne important insight from our empirical work was that the many ethical issues discussed in the literature are reflected in practice. Some issues are almost ubiquitous, such as those related to privacy and data protection. This paper does not offer a detailed analysis of the issues, nor of the exact differences between the case study and Delphi study find- ings. One key point from our findings is that clearly recognisable issues are well covered are roughly consistent. The Delphi study is under- standably broader than the case studies that focused on organisational practice, including issues such as ’awakening’ of AI that do not play a role in current implementations of AI. Our conceptual review of the ethics of AI and the analysis of our empirical work agreed to a large extent, showing that the literature covers the same issues of individuals and organisations working with AI (See Fig. 4).\\n\\nWhile the work on case studies and scenarios gave us the confidence to be in a position to understand a broad range of ethical aspects of AI in social situations, this knowledge had to be based on and complemented by current academic and other debates about ethical issues and human rights implications. We furthermore realised that it was important to understand the technical aspects of AI, notably questions of security and vulnerabilities of AI systems that have potentially large implications for the use and social impact of these technologies.\\n\\nThe ethical and human rights analyses were undertaken as desk research drawing on appropriate sources of their various disciplines (i. e., philosophy and human rights law). We also conducted a study of security issues, dangers, and implications of the use of data analytics and artificial intelligence. We examined:\\n\\nHowever, while it was possible to analyse our data using widely accepted terms, we note that the local meaning of these terms varied widely. The meaning of the term privacy, for example, in a medical diagnostic context, in the use of social media for logistics prediction or in the case of agricultural optimisation, differs greatly. Privacy is “an inherently heterogenous, fluid and multidimensional concept” (Finn, Wright, & Friedewald, 2013, p. 26) and can be divided up into a number of sub-types (Koops et al., 2017; Solove, 2002). In general, privacy in the AI ethics discourse seems to refer to data privacy or information privacy which touches on various other types of privacy (ibid). Other theoretical positions on privacy (Tavani, 2008) are not widely employed in the AI discourse. While this connection is rarely made explicit, there is an underlying assumption that implementing data protection mechanisms is the way to ensure privacy, although this has been contested (Macnish, 2020). This position is reflected in our findings where the frequent references to privacy as an issue were accompanied by a strong emphasis on often technical data protection measures. However, it is important to note that privacy risks take very different forms in these contexts, requiring different technical and organisational measures to ensure compliance with data protection legislation, but also to ensure that broader ethical issues are covered.\\n\\nways in which machine-learning systems are commonly mis- implemented or mis-used (and recommendations on how to pre- vent this from happening);\\n\\nways in which machine-learning models and algorithms can be attacked (and mitigations against such attacks);\\n\\nhow artificial intelligence and data analysis methodologies and technologies might be used for malicious purposes.\\n\\nThis initial review of security-related issues and possible sources of harm provided the starting point for further and ongoing studies of specific vulnerabilities of AI. The purpose of this task was to provide a baseline understanding of the current capabilities and applications of machine learning, including examples of potential malicious uses of machine learning techniques, and the implications of attacks against systems powered by machine learning.\\n\\n4. Findings and Discussion\\n\\nInterestingly, our analysis of human rights concerns, undertaken in parallel with the case studies, found that the human rights issues that can be found in the literature are closely related to and overlap with the\\n\\nThe purpose of this paper is to contribute to the discussion about how\\n\\nJournalofBusinessResearch124(2021)374–388380\\n\\nB C\\n\\n.\\n\\n.\\n\\nS t a h l\\n\\ne t\\n\\na l .\\n\\nTable 2 Distribution of ethical issues across case studies.\\n\\nEthical Issues\\n\\nCS01 Employee monitoring and administration\\n\\nCS02 Government\\n\\nCS03 Agriculture\\n\\nCS04 Sustainable development\\n\\nCS05 Science\\n\\nCS06 Insurance\\n\\nCS07 Energy and utilities\\n\\nCS08 Communications, media and cybersecurity\\n\\nCS09 Retail and wholesale trade\\n\\nCS10 Manufacturing and natural resources\\n\\n\\n\\n●\\n\\n● ●\\n\\n● ●\\n\\n\\n\\n\\n\\n●\\n\\nAccess to SIS Accuracy of Data Accuracy of\\n\\n●\\n\\n●\\n\\n\\n\\nRecommendations\\n\\n●\\n\\n●\\n\\n● ● ●\\n\\n\\n\\n●\\n\\nAlgorithmic Bias Discrimination Economic Employment Fairness Freedom Human Contact Human Rights Individual Autonomy Inequality Informed Consent Integrity Justice Ownership of Data Military, Criminal, Malicious Use Power Asymmetries Privacy Responsibility Security Sustainability Transparency Trust Use of Personal Data\\n\\n\\n\\n\\n\\n● ●\\n\\n● ●\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n●\\n\\n\\n\\n\\n\\n●\\n\\n●\\n\\n●\\n\\n\\n\\n\\n\\n\\n\\n●\\n\\n\\n\\n●\\n\\n●\\n\\n● ●\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n● ● ●\\n\\n●\\n\\n● ● ● ● ● ● ●\\n\\n●\\n\\n●\\n\\n● ● ●\\n\\n● ● ● ● ● ●\\n\\n● ●\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n● ●\\n\\n\\n\\n● ●\\n\\n●\\n\\n● ●\\n\\n\\n\\n\\n\\n●\\n\\n\\n\\n\\n\\nJournalofBusinessResearch124(2021)374–388381\\n\\nB.C. Stahl et al.\\n\\nFig. 4. Delphi survey responses covering the most important issues of AI and big data (Santiago, 2020).\\n\\nethical issues described above. The human rights gap analysis was based on a combination of methodological approaches emanating from the interim results of case studies and scenarios, preliminary in-depth in- terviews on cyberthreats and ethics as well as desktop research. Initial findings were cross-checked and combined to identify the most burning\\n\\nchallenges to human rights in the digital world and start formulating solutions. The findings were mindful of the diversity and breadth of data collected and expected results. A list of commonly perceived human rights issues and challenges in the digital world was derived from the process, encompassing general and specific considerations as follows:\\n\\nFig. 5. Key human rights issues in AI.\\n\\nJournalofBusinessResearch124(2021)374–388382\\n\\nB.C. Stahl et al.\\n\\n(See Fig. 5)\\n\\nTable 3 Selection of Rights and Freedoms (EU) with potential relevance to AI.\\n\\nIn recognition that the relationship between ethics and human rights is complex (van Est & Gerritsen, 2017) and that human rights have an ethical core that must be safeguarded, the human right considerations were analysed on a scale of concepts and notions: (See Fig. 6)\\n\\nRights\\n\\nECHR\\n\\nEUCFR\\n\\nRight to human dignity Right to the integrity of the person Right to liberty and security Right to respect for private and family life Protection of personal data Freedom of thought, conscience and\\n\\nArticle 1 Article 3 Article 6 Article 7 Article 8 Article 10\\n\\nFormalising human rights at different levels is based on the recog- nition that humans have ethical rights that need to be explicitly safe- guarded. Human rights declarations such as the Universal Declaration are political statements, but they have long found their way into positive law. Europe, for example, has the European Convention on Human Rights (ECHR) at the international level, which originates from the Council of Europe but is authoritative in the EU legal order. The EU has its own legally binding human rights instrument, the EU Charter of Fundamental Rights (EUCFR), whose scope can be said to be wider and deeper than the ECHR, as evidenced in the table below. This is relevant because it means that many of the ethical issues identified by our research are not just ethical issues, but, where they relate to human rights questions, may well be open to adjudication in a court of law. The following overview of human rights that may be affected by AI is therefore interesting, as it shows where and how these human rights are enshrined in existing human rights instruments, namely the ECHR and the EUCFR (See Table 3).\\n\\nArticle 5 Article 8\\n\\nArticle 9\\n\\nreligion\\n\\nFreedom of expression and information Prohibition of discrimination\\n\\nArticle 10 Article 14; Article 1, Protocol 12 Article 1, Protocol 1 Article 1, Protocol 2 Article 3, Protocol 1\\n\\nArticle 11 Article 21\\n\\nRight of property Right to education Right to free election\\n\\nArticle 17 Article 14 Articles 39–40 Article 45 Article 15\\n\\nFreedom of movement Freedom to choose an occupation and\\n\\nArticle 2, Protocol 4\\n\\nright to engage in work\\n\\nFreedom to conduct a business Rights of the child Rights of the elderly Integration of persons with disabilities Right to health care Consumer protection Right to good administration Right of access to documents Freedom of movement and residence\\n\\nArticle 16 Article 24 Article 25 Article 26 Article 35 Article 38 Article 41 Article 42 Article 45\\n\\nAll of these rights and freedoms were raised as a concern in at least one of our case studies or scenarios, thus further demonstrating the broad range of concerns. The reason for including this table in the text was that for readers not familiar with the detail of our case studies, it shows a list of human rights that are easy to associate with AI use and demonstrates that there are legal instruments that could deal with these. As several overlaps were identified in terms of, for example, relevant legal instruments and types of solution proposed, these were intertwined in the establishment of a ‘blueprint’ of current and future practice in relation to the interrelationship of human rights, law, ethics and smart information systems, those systems that have AI and big data analytics at their core (Stahl & Wright, 2018).\\n\\napplication of human rights to AI as a key way of addressing these issues (Access Now Policy Team, 2018; BSR, 2018; Committee on Bioethics (DH-BIO), 2019; Council of Europe, 2019; Latonero, 2018; World Eco- nomic Forum, 2019).\\n\\n4.1. Classification of ethical issues\\n\\nThis paper has so far confirmed that there are many potential ethical issues related to AI. We have furthermore shown that there are numerous governance approaches, including human rights legislation, that can address many of these issues. An ongoing problem, however, is the complexity of the landscape, the fact that there are too many ethical issues and ways of addressing them to allow scholars or practitioners to keep an overview.\\n\\nHuman rights touch on or are directly part of many of the issues related to AI. Our analysis shows the breadth of human rights concerns using the European human rights framework. It seems plausible that a more direct application of human rights legislation to AI can provide some clarity on related issues and point the way to possible solutions. It is therefore not surprising that there are many voices that point to the\\n\\nFig. 6. Concepts and notions employed for human rights analysis of AI.\\n\\nJournalofBusinessResearch124(2021)374–388383\\n\\nB.C. Stahl et al.\\n\\nthe case of Cambridge Analytica (Isaak & Hanna, 2018) is the most visible case in point. But while Cambridge Analytica represents a rela- tively clear-cut case of misuse of AI and big data, there are broader questions about the economic and resulting political power amassed by the tech industry (Macnish & Galliott, 2020). The market capitalisation and therefore economic power of the big tech companies is now such that many observers are increasingly worried about the mere possibility of oversight of these actors. One resulting question is that of justice of distribution of costs and benefits. Big tech companies have the technical infrastructure and know-how to create ever-larger data sets and benefit from these whereas smaller competitors lack the means to catch up. The role of consumers and end users at present is predominantly passive; they produce data and consume services, but have little control over the use of their data.\\n\\nIn the course of our research and engagement with the ethics of AI, it emerged that there are types of issues that can be clustered in a way conducive to finding appropriate responses. We propose three types of issues that have significant specificities to allow them to be clustered: specific issues of machine learning, general questions about living in a digital world and metaphysical questions.\\n\\n4.1.1. Specific issues of machine learning\\n\\nMachine learning and the various techniques used to achieve it have some characteristics at the core of particular ethical issues. Two of these characteristics seem most likely to raise ethical issues: First, many of the current machine-learning techniques are opaque, which means that even experts with relevant equipment cannot determine why and how inputs are transformed into outputs, e.g. how exactly a personal profile leads to a classification in terms of a mortgage application or parole decision. Second, these systems require access to big amounts of data for training and validation purposes.\\n\\nOther larger-scale societal concerns have to do with the question about what technology can and should do. A high-profile example of this is the use of AI-driven autonomous weaponry that has the potential to change the face of modern warfare (Defense Innovation Board, 2019; Sparrow, 2009). But there are many other examples where it is not clear- cut what machines can and should do, e.g., with regard to autonomous vehicles or care robots (Decker, 2008; Sharkey & Sharkey, 2010; Bernd Carsten Stahl & Coeckelbergh, 2016).\\n\\nResulting ethical issues are, for example, those having to do with bias and discrimination (Johnson et al., 2019; Macnish, 2012), which can arise on the basis of undetected biases included in the training data. Further specific issues are linked to the use of data, which may be per- sonal data and which may allow additional insights into individual personal behaviour through AI and big data analysis. Questions around privacy and data protection therefore arise on this level as do concerns about security and integrity of systems, algorithms and data (Stahl & Wright, 2018).\\n\\nOur empirical findings support the relevance of these issues. In fact, and somewhat surprisingly, ethical issues in this category constitute the majority of issues. Economic consequences, employment, fairness, freedom, the ability of having human contact, individual autonomy, inequality, integrity, justice, ownership, military use, power asymmetry, responsibility and sustainability all fall into the category.\\n\\nThese issues are common to most specific applications of AI. Algo- rithmic biases, discrimination, security and transparency are issues that are directly linked to the characteristics of machine learning. Others, such as quality and accuracy of data are closely related. Our case study analysis suggests that these issues are prominent among the AI users in organisations. They materialise and present themselves in specific and context-dependent ways. From the perspective of this paper, an inter- esting feature is that these issues can become visible and are – at least to some degree – capable of being addressed on the project or organisa- tional level. While these ethical issues represent some of those most frequently mentioned in our empirical research, it is important to see that many of the ethical issues do not seem to be linked to the technical properties of AI as machine learning but point to the broader socio- economic context in which these technologies are used.\\n\\n4.1.3. Metaphysical questions\\n\\nThe final set of questions concerns what machines should be allowed to do and points to some of the deeper philosophical and metaphysical questions about the future of AI and autonomous machines and their relationship with humans. There has been a long-standing discussion about the change of human nature due to machines, the emergence of cyborgs (Latimer, 2017) and transhumans (Livingstone, 2015). In par- allel, there has been discussion whether machines can ever become sentient or conscious (Carter et al., 2018; Dehaene, Lau, & Kouider, 2017), whether there will be a singularity (Kurzweil, 2006) at which point machines will develop superintelligence (Bostrom, 2016).\\n\\nThese developments are controversially discussed and highly contentious. They are based on the idea of general AI (Baum, 2017). In this paper, we do not take a position on whether these developments are likely or even possible. We also refrain from taking a position on whether current narrow AI can lead to general AI or whether a funda- mentally different approach would be needed. In our case studies, we found no evidence of their being considered a current priority, but in our scenarios, there are examples of technologies that come close to this category. In the Delphi study, there was reference to ’awakening of AI’, which falls into this category. The reason for including them here is thus less their current practical relevance and more the fact that they are prominent and highly visible in science fiction, the media and increas- ingly in policy discussions – which, of course, in no way reduces the legitimacy of the concerns they raise.\\n\\n4.1.2. General questions about living in a digital world\\n\\nOur second category of ethical issues includes those that have less to do with the specific capabilities of AI and more with the way societies use technologies many of which incorporate elements of machine learning or other AI techniques. These issues play to a general feeling of unease with the way in which industrialised societies develop and the role that technology plays in promoting certain developments and inhibiting others. These issues are currently discussed in the context of AI because the expectation is that AI will greatly influence them, but they are better understood as questions that relate to how modern so- cieties organise themselves using technologies such as AI. A key char- acteristic of these technologies seems to be autonomy, i.e., the ability to act without direct human input. This, combined with higher levels ability to detect patterns and act accordingly, can lead to the replace- ment of humans by machines. Another key feature of digital technolo- gies with high ethical relevance is that they increasingly constitute the environment in which humans live. The malleability of digital tech- nologies means that our realities can easily be changed. Maybe even more importantly, the constitutive element of digital technologies in modern social reality means that the owners and controllers of these technologies become immensely powerful in many different ways.\\n\\nThe following figure aims to summarise these three types of ethical\\n\\nissues of AI (See Fig. 7).\\n\\nThe classification suggested in the above figure serves as a starting point to explore how governance structures are positioned to address ethical issues.\\n\\n4.2. Governing the three types of issues\\n\\nKey examples of these issues are the influence of technology on economic and political power, the future of warfare or distribution of costs and benefits of AI. The high-profile example of the misuse of social media data for purposes of the manipulation of democratic elections in\\n\\nThe purpose of classifying ethical issues in the previous section was to render the broad array of ethical issues more manageable and impose some order on the chaos of AI ethics. In this section, we now look at how this can help identify suitable governance structures. Before we start\\n\\nJournalofBusinessResearch124(2021)374–388384\\n\\nB.C. Stahl et al.\\n\\nFig. 7. Classification of ethical issues of AI.\\n\\nthis, we should make it clear that we understand that the above classi- fication is analytic in nature and represents one possible way of thinking about AI ethics among many. We hope that the distinction makes sense and is plausible, but we are happy to concede that it is not exclusive. Privacy, to take an example, materialises on the local and project level, but is seen as a societal question (Roessler & Mokrosinska, 2015), sub- ject to legislation and regulation and arguably based on human needs arising from human nature (Locke, 2010). With this contingent nature of the classification in mind, we now look at how this translates to governance mechanisms.\\n\\nissues, general questions, metaphysical questions).\\n\\nIn many cases, the issues that we classified as specific arising from machine learning appear easiest to address. They can be subject to the application of existing governance mechanisms. Data protection or se- curity issues provide a good example. Data protection is furthermore governed by legislation and regulation, even though the details of these legislative requirements differ significantly between jurisdictions. Not only is there ample regulation, there are also structures such as data protection impact assessments (CNIL, 2017a, 2017b) or standards such as the ISO 27000 family that allow structured approaches. Similar ob- servations apply to other examples of the specific issues arising from machine learning, such as algorithmic biases and subsequent discrimi- nation. Unlike data protection, these are less comprehensively defined and regulated. A key problem here is that it may be difficult to under- stand what exactly the issue is. There are several high-profile examples of these problems, e.g., where a system evaluating job applications leads to discrimination on the basis of gender or where predictive policing or parole decisions lead to discrimination on the basis of race. There is broad consensus that discrimination on the basis of gender or race is immoral and to be avoided. However, it is not clear whether other systems discriminate on the basis of other properties that are less high- profile and of which individuals are unaware. It is conceivable that a medical diagnostic system might lead to discrimination on the basis of blood type or a dating system would prefer individuals on the basis of their height. This raises questions about how we would even know how to check for relevant characteristics and, even if confirmed, whether and why this would constitute an ethical issue.\\n\\nIn our discussion of governance, we take the position that the desirable purpose of AI should be to support human flourishing. As discussed earlier, this does not preclude uses of AI for efficiency max- imisation or control but implies that the overall aim of development and deployment of the technology should be to improve human lives. This position has the disadvantage of muddying the waters, raising difficult questions of what counts as flourishing and who determines this. It is also likely to require the balancing of competing goods, values and in- terests. But it is arguably the purpose of AI research, development, funding and application on which most citizens in democratic states, and maybe most human beings, can agree. It is also aligned with the EU’s AI policies. Based on an agreement that AI is to support human flourishing, the question then is how ethical issues can be addressed and which governance structures can help us deal with them.\\n\\nTo return to the suggested classification of ethical issues (Fig. 6) and compare this with the earlier description of governance proposals (Fig. 2), it is easy to see that there is no simple and/or linear relation- ship. The two sets of classification do not align, but can be better un- derstood as a matrix, where all levels of governance (individual, organisational, policy) can refer to all types of ethical issues (specific\\n\\nThese issues can be described in terms of transparency and explainability. At the heart of the problem is the fact that humans expect explanations that they can understand, whereas machine learning\\n\\nJournalofBusinessResearch124(2021)374–388385\\n\\nB.C. Stahl et al.\\n\\nalgorithms classify individual cases on the basis of complex statistical calculations that defy simple translations into forms that are accessible to humans (Mittelstadt et al., 2019).\\n\\nreligious characteristics [overcoming the body, the distinction between (mortal) body and (immortal) essence of the human], they may be most suitably dealt with by theology. While it may be difficult to deal these issues, they did not receive much attention in our empirical findings. But that does not mean that proactive attention to these issues would be misplaced or that it might not be advisable to think about early warning signs or trigger points where more attention should be paid to such issues.\\n\\nThis tension between human reasoning and machine-learning data processing is a reason why transparency is among the most visible ethical issue linked to AI. The prominence of transparency as a perceived issue may explain why the same term is used to denote an ethical principle that pervades the AI ethics guidelines (Jobin, Ienca, & Vayena, 2019). The ethical principle of transparency is described as a way to minimise harm, improve AI, foster trust and improve AI (ibid). It is also linked to dialogue, participation and democracy. Elsewhere we have tried to unpack the normative implications of ethical principles, including the principle of transparency (Ryan & Stahl, 2020). Of course, while transparency may be perceived as obligatory, it does not in itself resolve the ethical or privacy risks at issue. Transparency only gets us part of the way to solutions.\\n\\nInsights from our case studies suggest that this way of looking at ethical issues and governance clarifies current activities. This paper does not provide the space for a comprehensive analysis, so suffice it to say that the organisations in our case studies focused on the issues arising from machine learning and on established and practical ways of addressing them. The dominant topics were security and data protec- tion, which companies dealt with in ways to ensure they met their legal responsibilities. They focused on technical approaches to achieve these. In addition, companies developed oversight mechanisms and reflective capabilities, e.g., by instituting ethics boards or engaging with stake- holders. Respondents were aware of the policy issues but not actively engaged with them. Similarly, as noted, the metaphysical issues did not play a role in the social reality of the case studies.\\n\\nThe example of transparency indicates that the ethical issues arising from AI are not clearly defined. The perception that something consti- tutes an ethical issue does not by itself clarify whether this perception is accurate or on which grounds such an evaluation would be possible. It says little about the exact nature of the issue and its status from a theoretical ethical perspective.\\n\\nOne interesting point to observe is that in our case study research, there was little reference even to most of the governance mechanisms that we categorised as aimed at the organisational level. Activities like risk management, impact assessments or human rights integration into company policies were not mentioned by our respondents. This does not mean that the companies did not engage in them or that they were unsuccessful, but that in the responses to our questions, the respondents did not associate them with ethics of AI or big data. It will therefore be important for future studies to evaluate the effectiveness of these governance approaches.\\n\\nIn the case of transparency, there is now a quickly growing research community that focuses on such questions of transparency and explainability of AI (USACM, 2017). While these questions remain open, there is at least reason to hope that they can be addressed on the local level by designing, testing and reviewing systems using appropriate methods, relying on professional expertise of developers and users and organisational capacity to develop the relevant expertise. The ISO is developing guidance for developers (e.g., TR 24368). Discussion within ISO has recognised that AI is not neutral, that values embedded in al- gorithms are intentionally or inadvertently shaped by the pro- grammers’, operators’, and third-parties’ own worldviews and cognitive biases and that ethical violations could also result from AI deployed or developed prematurely, applied without proper consideration of the ways it could negatively impact individuals or society.\\n\\n5. Conclusion\\n\\nIn this paper, we have reported some findings and their implications from a multi-dimensional research study into the ethics of AI. Our starting point was the highly complex and often overwhelming AI ethics discourse that motivated our attempt to bring better empirical insights but also conceptual clarity to the discussion.\\n\\nThe second set of issues, those relating to general questions about living in a digital world, are not capable of being dealt with on the micro or meso level of the individual or organisation. This does not mean that there are no existing governance mechanisms that could be applied to AI. Nation states can make use of policy, legislative and regulatory op- tions and this is happening at large scale with regard to AI. At the same time, there are many existing structures that may well be suited to dealing with at least some of the challenges related to AI, even if such structures can be criticised as having failed to deal adequately with the ethical, privacy and societal issues. Consumer protection legislation, competition law, anti-trust law, data protection law as well as human rights legislation may well be capable of regulating AI and its conse- quences, at least in part. Similarly, existing regulators may make a claim that they can address the challenges of AI. This suggests a close affinity between this type of ethical issue and the policy level of governance options. The key to the resolution of these issues does indeed seem to be on a policy level, as they touch on constitutive questions of modern societies (e.g., who can own what, how do we distribute wealth and risks, the asymmetries in the power wielded by the big tech companies through their algorithms). Ethical issues of AI may be subject to existing regulation, but it is entirely possible that additional regulation and legislation will be required to facilitate human flourishing. Nevertheless, the policy level needs to be supplemented by actions on the organisa- tional and individual levels.\\n\\nOne outcome of this work was the classification of ethical issues into specific issues arising from machine learning, general questions about living in a digital world and metaphysical questions. We hope that this classification is helpful in identifying issues and finding or developing appropriate governance mechanisms.\\n\\nOur discussion and mapping of existing proposals for AI governance to the three classes of ethical issues give rise to suggestions for further research as well as organisational practice. One important observation refers to the concept of AI and the implications of the use of a particular concept for subsequent insights. In this study, we started with an in- clusive view of AI that covers not only narrow AI and machine learning, but also broader socio-technical systems incorporating AI techniques and artificial general intelligence. Our empirical findings show that the focus of attention in current use of AI is on machine learning and, to some degree, on broader socio-technical systems. This raises the ques- tion whether it would be desirable to limit the scope of reflection, e.g., by focusing exclusively on machine learning. We suggest that the term AI is broader than machine learning and a rich conceptualisation of AI and its ethical consequences needs to take this into consideration.\\n\\nThis paper has shown that a key next step in the AI ethics discourse needs to be a more detailed and thorough mapping exercise that not only lists and clearly defines ethical issues but explores to what degree these are novel or manifest inadequate regulatory structures and in need of novel solutions. As we have suggested above, the broad array of extant governance structures may be able to cover many of the issues raised by AI. Whether they do so adequately is another question. Where no rem- edies currently exist, or existing ones are insufficient, further and novel\\n\\nThe most difficult area to govern is the metaphysical. Questions of general AI and its consequences are partly technical and empirical in terms of what technology can achieve in the hands of the big tech companies, intelligence agencies and cyber attackers (for example). They are also partly philosophical insofar as they refer to basic concepts and arguments. To some extent, e.g., where transhumanism displays\\n\\nJournalofBusinessResearch124(2021)374–388386\\n\\nB.C. Stahl et al.\\n\\nsolutions may well be required. The work presented here suggests that the AI ethics debate can benefit from a different perspective, which we have offered above. This new perspective is based on a novel catego- risation of ethical issues that point towards next steps and the bigger question about how the different types of issues can be addressed, which can be used to generate recommendations for organisations. Many companies are interested in exploiting the advantages of AI but are unsure about how to deal with societal and ethical issues. Our research shows that there are numerous existing activities, many of which are related and interlinked, that will go a long way towards showing that a company is serious about engaging with these questions. The integration of CSR structures into organisations in a way that takes into account the organisation’s research and development is one such step. An explicit commitment to human rights and the adoption of processes designed to integrate human rights into corporate processes would be another such step. These suggestions are, of course, no guarantees that nothing will go wrong, but they are serious steps in ensuring that the corporate culture supports human flourishing, through AI and otherwise.\\n\\nmost suitable ways of moving on. It also shows that AI ethics is not a problem to be ‘solved’ in the sense that there are clear solutions that will make problems go away. Instead, uncertainties around definitions, ethics and values can form the basis for a creative but unpredictable journey towards a desirable future where new technologies including AI are conducive to human flourishing.\\n\\nThis paper provides a clearer, more structured and inclusive narra- tive of ethics of AI. It draws on a set of different research activities to develop a way of thinking about AI and ethics that covers the broad range of technologies that fall under the heading of AI, that covers the manifold ethical issues associated with AI and that provides a theoretical ethical position that can be used to reflect on all of these. It is clear, however, that this is only the basis for further work. The difficult questions about what stakeholders need to do, about which laws and regulations need to be updated or developed, about definitions of pro- fessional or organisational responsibilities etc. still need to be addressed. Undertaking these next steps is important and urgent. It will also be greatly helped by an empirically based conceptual view of ethics and AI as we offer in this paper.\\n\\nA similar conclusion offers itself to policymakers and decision- makers. Just as companies can look through a portfolio of existing governance mechanisms, policymakers should take stock of the ade- quacy of existing policy and regulatory options while developing spe- cific steps to address AI. Some of the issues discussed in the context of AI and the general questions arising in our increasingly digital world have little to do with AI in the narrow sense. Questions of justice, distribution and power may be exacerbated by particular technologies but exist in- dependent of them. As a consequence, it may well be that existing legislation or regulatory bodies are in a good position to deal with at least some of these questions and the task for policymakers is to ensure that existing policy and the adequacy of its application are taken into account in the rush to develop new policy.\\n\\nAcknowledgments\\n\\nThis research has received funding from the European Union’s Ho- rizon 2020 Framework Programme for Research and Innovation under the Specific Grant Agreement No. 786641 (SHERPA).\\n\\nReferences\\n\\nAccess Now Policy Team. (2018). The Toronto Declaration: Protecting the right to\\n\\nequality and non-discrimination in machine learning systems. Access No. https:// www.accessnow.org/cms/assets/uploads/2018/08/The-Toronto-Declaration_ENG_ 08-2018.pdf.\\n\\nAdam, A. (2001). Computer ethics in a different voice. Information and Organization, 11\\n\\nThe nature of integrating human rights into organisations and the creation of policy provides another pointer to lessons learned from our work. The way we deal with the ethics of AI will need to be sensitive to the conceptually challenging and changing nature of the technologies in question and the social perceptions they engender. We should simply not assume that we can provide a permanently stable definition of AI or of the ethical issues related to it. Instead, we need to embrace a world where concepts are changing and contested, where moral preferences change over time, where scientific, media and political discourses dynamically interact and where impacts of new technologies such as AI need to be adequately assessed. Ethical positions based on process and exchange, such as discourse ethics (Mingers & Walsham, 2010; Rehg, 2014), are well suited to reflect the need for the ongoing negotiations of facts and values needed to make new technologies work in society.\\n\\n(4), 235–261.\\n\\nALLEA & Royal Society. (2019). Flourishing in a Data-enabled Society (ALLEA Discussion\\n\\nPaper No. 4). https://www.allea.org/wp-content/uploads/2019/06/ DiscussionPaper_DataGov_Digital.pdf.\\n\\nAnabo, I. F., Elexpuru-Albizuri, I., & Villard´on-Gallego, L. (2019). Revisiting the Belmont\\n\\nReport’s ethical principles in internet-mediated research: Perspectives from disciplinary associations in the social sciences. Ethics and Information Technology, 21 (2), 137–149. https://doi.org/10.1007/s10676-018-9495-z.\\n\\nAndersen, I.-E., & Jaeger, B. (1999). Scenario workshops and consensus conferences: Towards more democratic decision-making. Science and Public Policy, 26(5), 331–340.\\n\\nAndreou, A., Shaelou, S. L., & Stahl, B. (2019). D1.5 Current Human Rights Frameworks.\\n\\nhttps://doi.org/10.21253/DMU.8181827.v1.\\n\\nAntoniou, J., & Andreou, A. (2019). Case Study: The Internet of Things and Ethics. ORBIT\\n\\nJournal, 2(2), Article 2.\\n\\nAristotle. (2007). The Nicomachean Ethics. Filiquarian Publishing, LLC. Asilomar Conference. (2017). Asilomar AI Principles. Future of Life Institute. https://\\n\\nfutureoflife.org/ai-principles/.\\n\\nOur work immediately suggests further areas of study. The case studies, for example, while covering a breadth of activities, could be expanded to be even more comprehensive. Our approach was open and exploratory. Future empirical work could also test the adequacy of existing governance mechanisms as well as proposals for new ones, their impacts and possible side effects (e.g., a loss of social trust in existing mechanisms). A direct mapping of ethical issues and governance pro- posals would be helpful in assessing the effectiveness of the governance proposals, an important factor in developing these further. Further research should also be undertaken to broaden the geographical scope of the study. Our European focus means that we simply took for granted certain aspects of the AI ecosystem that may not be applicable else- where. This includes the strong legal data protection regime of the Eu- ropean Union or the existing regulatory, industrial and professional settings that shape the way in which AI is developed, deployed and used. The international nature of AI renders necessary such further work.\\n\\nBaum, S. (2017). A Survey of Artificial General Intelligence Projects for Ethics, Risk, and\\n\\nPolicy (SSRN Scholarly Paper ID 3070741). Social Science Research Network. https:// papers.ssrn.com/abstract=3070741.\\n\\nBeard, M., & Longstaff, S. (2018). Ethical By Design: Principles For Good Technology.\\n\\nTHE ETHICS CENTRE.\\n\\nBeauchamp, T. L., & Childress, J. F. (2009). Principles of Biomedical Ethics (6th ed.).\\n\\nOUP USA.\\n\\nBecker, H. A., & Vanclay, F. (2003). The International Handbook of Social Impact\\n\\nAssessment: Conceptual and Methodological Advances. Edward Elgar Publishing.\\n\\nBevir, M. (2012). Governance: A Very Short Introduction. OUP Oxford. Boden, M., Bryson, J., Caldwell, D., Dautenhahn, K., Edwards, L., Kember, S., …\\n\\nWinfield, A. (2017). Principles of robotics: Regulating robots in the real world. Connection Science, 29(2), 124–129. https://doi.org/10.1080/ 09540091.2016.1271400.\\n\\nBoenink, M., Swierstra, T., & Stemerding, D. (2010). Anticipating the Interaction\\n\\nbetween Technology and Morality: A Scenario Study of Experimenting with Humans in Bionanotechnology. Studies in Ethics, Law, and Technology, 4(2). https://doi.org/ 10.2202/1941-6008.1098.\\n\\nBostrom, N. (2016). Superintelligence: Paths. Dangers, Strategies (Reprint edition): OUP\\n\\nOxford.\\n\\nBraithwaite, J., & Drahos, P. (2000). Global Business Regulation. Cambridge University\\n\\nThe high-level view of the AI ethics debate presented in this paper should make an important contribution to theory and practice. We believe that our proposed categorisation is helpful for both scholars and practitioners. It offers a way to navigate the complexities of the AI debate and helps individuals, organisations and policymakers to find the\\n\\nPress.\\n\\nBrey, P. (2000). Disclosive Computer Ethics: Exposure and Evaluation of Embedded\\n\\nNormativity in Computer Technology. CEPE2000 Computer Ethics: Philosophical Enquiry. CEPE2000 Computer Ethics: Philosophical Enquiry, Dartmouth College. http://ethics.sandiego.edu/video/CEPE2000/Responsibility/Index.html.\\n\\nJournalofBusinessResearch124(2021)374–388387\\n\\nB.C. Stahl et al.\\n\\nGilligan, C. (1990). In a Different Voice: Psychological Theory and Women’s Development\\n\\nBrinkman, B., Flick, C., Gotterbarn, D., Miller, K., Vazansky, K., & Wolf, M. J. (2017).\\n\\nListening to Professional Voices: Draft 2 of the ACM Code of Ethics and Professional Conduct. Commun. ACM, 60(5), 105–111. https://doi.org/10.1145/3072528. British Academy, & Royal Society. (2017). Data management and use: Governance in the 21st century A joint report by the British Academy and the Royal Society. https:// royalsociety.org/~/media/policy/projects/data-governance/data-management- governance.pdf.\\n\\n(Reissue). Harvard University Press.\\n\\nGrunwald, A. (2009). Technology Assessment: Concept and Methods. In D. M. Gabbay, A. W. M. Meijers, J. Woods, & P. Thagard (Eds.), Philosophy of Technology and Engineering Sciences. North Holland, 9 (pp. 1103–1146).\\n\\nHaenlein, M., & Kaplan, A. (2019). A brief history of artificial intelligence: On the past,\\n\\npresent, and future of artificial intelligence. California Management Review, 61(4), 5–14.\\n\\nBSR. (2018). Artificial Intelligence: A Rights-Based Blueprint for Business Paper 3:\\n\\nHagendorff, T. (2019). The Ethics of AI Ethics—An Evaluation of Guidelines. ArXiv:\\n\\nImplementing Human Rights Due Diligence [Working paper]. BSR.\\n\\nBynum, T. W. (2006). Flourishing Ethics. Ethics and Information Technology, 8(4),\\n\\n1903.03425 [Cs, Stat].\\n\\nHansen, M. (2016). Data Protection by Design and by Default `a la European General Data Protection Regulation. In A. Lehmann, D. Whitehouse, S. Fischer-Hübner, L. Fritsch, & C. Raab (Eds.), Privacy and Identity Management. Facing up to Next Steps (pp. 27–38). Springer International Publishing. https://doi.org/10.1007/978-3-319- 55783-0_3.\\n\\n157–173.\\n\\nBynum, Terrell Ward, & Rogerson, S. (2003). Computer Ethics and Professional\\n\\nResponsibility: Introductory Text and Readings. WileyBlackwell.\\n\\nCairns, G., & Wright, G. (2017). Scenario Thinking: Preparing Your Organization for the\\n\\nFuture in an Unpredictable World. Springer.\\n\\nCapurro, R. (2006). Towards an ontological foundation of information ethics. Ethics and Information Technology, 8(4), 175–186. https://doi.org/10.1007/s10676-006-9108- 0.\\n\\nHatzakis, T., Rodrigues, R., & Wright, D. (2019). Smart Grids and Ethics. ORBIT Journal,\\n\\n2(2), Article 2.\\n\\nHLEG on AI, H. E. G. on A. I. (2019). Ethics Guidelines for Trustworthy AI. European\\n\\nCarter, O., Hohwy, J., van Boxtel, J., Lamme, V., Block, N., Koch, C., & Tsuchiya, N. (2018). Conscious machines: Defining questions, 400 400 Science, 359(6374). https://doi.org/10.1126/science.aar4163.\\n\\nCommission - Directorate-General for Communication. https://ec.europa.eu/digital- single-market/en/news/ethics-guidelines-trustworthy-ai.\\n\\nInformation Commissioner’s Office. (2008). Privacy by design. http://www.ico.gov.uk/\\n\\nCath, C. J. N., Wachter, S., Mittelstadt, B., Taddeo, M., & Floridi, L. (2016). Artificial Intelligence and the “Good Society”: The US, EU, and UK Approach (SSRN Scholarly Paper ID 2906249). Social Science Research Network. https://papers.ssrn.com/abstr act=2906249.\\n\\nupload/documents/pdb_report_html/privacy_by_design_report_v2.pdf.\\n\\nIphofen, R., & Kritikos, M. (2019). Regulating artificial intelligence and robotics: Ethics\\n\\nby design in a digital society. Contemporary Social Science, 1–15.\\n\\nIsaak, J., & Hanna, M. J. (2018). User Data Privacy: Facebook, Cambridge Analytica, and\\n\\nPrivacy Protection. Computer, 51(8), 56–59.\\n\\nCavoukian, A. (2017). Global privacy and security, by design: Turning the ‘privacy vs. security’ paradigm on its head. Health and Technology, 1–5. https://doi.org/ 10.1007/s12553-017-0207-1.\\n\\nISO. (2008). BS ISO/IEC 38500:2008—Corporate governance of information technology.\\n\\nhttps://bsol.bsigroup.com/Bibliographic/BibliographicInfoData/ 000000000030162049.\\n\\nCDEI. (2019). Interim report: Review into bias in algorithmic decision-making. Centre for Data Ethics and Innovation. https://www.gov.uk/government/publications/interim- reports-from-the-centre-for-data-ethics-and-innovation/interim-report-review-into- bias-in-algorithmic-decision-making.\\n\\nJiya, T. (2019a). Ethical Implications of Predictive Risk Intelligence. ORBIT Journal, 2(2),\\n\\nArticle 2.\\n\\nJiya, T. (2019b). Ethical Reflections of Human Brain Research and Smart Information\\n\\nClarke, R. (2019a). Principles and Business Processes for Responsible AI. Computer Law &\\n\\nSystems. ORBIT Journal, 2(2), Article 2.\\n\\nSecurity Review, 35(4), 410–422.\\n\\nJobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature Machine Intelligence, 1(9), 389–399. https://doi.org/10.1038/s42256-019- 0088-2.\\n\\nClarke, R. (2019b). Regulatory Alternatives for AI. Computer Law & Security Review, 35\\n\\n(4), 398–409.\\n\\nClouser, K. D., & Gert, B. (1990). A Critique of Principlism. Journal of Medicine and\\n\\nJohnson, D. G. (2001). Computer Ethics ((3rd ed.).). Prentice Hall. Johnson, K., Pasquale, F., & Chapman, J. (2019). Artificial Intelligence, Machine\\n\\nPhilosophy, 15(2), 219–236. https://doi.org/10.1093/jmp/15.2.219.\\n\\nCNIL. (2015). Privacy Impact Assessment (PIA) Good Practice. CNIL. http://www.cnil.fr/\\n\\nLearning, and Bias in Finance: Toward Responsible Innovation. Fordham Law Review, 88(2), 499.\\n\\nfileadmin/documents/en/CNIL-PIA-3-GoodPractices.pdf.\\n\\nKanceviˇcien ˙e, N. (2019). Insurance, Smart Information Systems and Ethics. ORBIT\\n\\nCNIL. (2017a). How Can Humans Keep the Upper Hand? The Ethical Matters Raised by\\n\\nAlgorithms and Artificial Intelligence.\\n\\nJournal, 2(2), Article 2.\\n\\nCNIL. (2017b). Algorithms and artificial intelligence: CNIL’s report on the ethical issues. (The Ethical Matters Raised by Algorithms and Artificial Intelligence). CNIL. https:// www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_ai_gb_web.pdf.\\n\\nKant, I. (1788). Kritik der praktischen Vernunft. Reclam, Ditzingen. Kant, I. (1797). Grundlegung zur Metaphysik der Sitten. Reclam, Ditzingen. Kaplan, A., & Haenlein, M. (2019). Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence. Business Horizons, 62(1), 15–25.\\n\\nCommittee on Bioethics (DH-BIO). (2019). Strategic Action Plan on Human Rights and Technologies in Biomedicine (2020-2025) (CM(2019)198). Council of Europe. https://search.coe.int/cm/Pages/result_details.aspx?ObjectId=0900001680994df7. Council of Europe. (2019). Unboxing artificial intelligence: 10 steps to protect human\\n\\nKhatri, V., & Brown, C. V. (2010). Designing data governance. Communications of the\\n\\nACM, 53(1), 148–152.\\n\\nˇ Skorv´anek, I., Chokrevski, T., & Galiˇc, M. (2017).\\n\\nrights. https://www.coe.int/en/web/commissioner/view/-/asset_publisher/ ugj3i6qSEkhZ/content/unboxing-artificial-intelligence-10-steps-to-protect-human- rights.\\n\\nKoops, B.-J., Newell, B., Timan, T.,\\n\\nA Typology of Privacy. University of Pennsylvania Journal of International Law, 38(2), 483.\\n\\nKurzweil, R. (2006). The Singularity is Near. Gerald Duckworth & Co Ltd. Latimer, J. (2017). Donna J Haraway, Manifestly Haraway: The Cyborg Manifesto, The Companion Species Manifesto, Companions in Conversation (with Cary Wolfe). Theory, Culture & Society, 34(7–8), 245–252.\\n\\nDecker, M. (2008). Caregiving robots and ethical reflection: The perspective of interdisciplinary technology assessment. Ai & Society, 22(3), 315–330.\\n\\nDefense Innovation Board. (2019). AI Principles: Recommendations on the Ethical Use of Artificial Intelligence by the Department of Defense. US Department of Defense. https://media.defense.gov/2019/Oct/31/2002204459/-1/-1/0/DIB_AI_ PRINCIPLES_SUPPORTING_DOCUMENT.PDF.\\n\\nLatonero, M. (2018). Governing artificial intelligence: Upholding human rights & dignity. Data&Society. https://datasociety.net/wp-content/uploads/2018/10/ DataSociety_Governing_Artificial_Intelligence_Upholding_Human_Rights.pdf. Liu, C. (2019). Multiple Social Credit Systems in China (SSRN Scholarly Paper ID\\n\\nDehaene, S., Lau, H., & Kouider, S. (2017). What is consciousness, and could machines\\n\\nhave it? Science, 358(6362), 486–492.\\n\\nDoherty, N. F., Coombs, C. R., & Loan-Clarke, J. (2006). A re-conceptualization of the\\n\\n3423057). Social Science Research Network. https://papers.ssrn.com/ abstract=3423057.\\n\\ninterpretive flexibility of information technologies: Redressing the balance between the social and the technical. European Journal of Information Systems, 15(6), 569–582.\\n\\nLivingstone, D. (2015). Transhumanism: The History of a Dangerous Idea. CreateSpace\\n\\nElsevier. (2018). ArtificiaI Intelligence: How knowledge is created, transferred, and used—Trends in China, Europe, and the United States. Elsevier. https://www. elsevier.com/?a=827872.\\n\\nIndependent Publishing Platform.\\n\\nLocke, J. L. (2010). Eavesdropping: An Intimate History (Illustrated Edition). OUP\\n\\nOxford.\\n\\nCommission, European (2018). COMMUNICATION FROM THE COMMISSION TO THE EUROPEAN PARLIAMENT, THE EUROPEAN COUNCIL, THE COUNCIL, THE EUROPEAN ECONOMIC AND SOCIAL COMMITTEE AND THE COMMITTEE OF THE REGIONS Artificial Intelligence for Europe (COM(2018) 237 final). European Commission. http://ec.europa.eu/transparency/regdoc/rep/1/2018/EN/COM-201 8-237-F1-EN-MAIN-PART-1.PDF.\\n\\nMacnish, K. (2012). Unblinking eyes: The ethics of automating surveillance. Ethics and Information Technology, 14(2), 151–167. https://doi.org/10.1007/s10676-012-9291- 0.\\n\\nMacnish, K. (2020). Mass Surveillance: A Private Affair? Moral Philosophy and Politics, 7\\n\\n(1), 9–27. https://doi.org/10.1515/mopp-2019-0025.\\n\\nMacnish, K., & Galliott, J. (2020). Big Data and Democracy. Edinburgh University Press. Macnish, K., & Inguanzo, A. F. (2019). Customer Relation Management, Smart\\n\\nGeneral Data Protection Regulation, (2016) (testimony of European Parliament & European Council). http://www.dhealth.co.uk/resources/Documents/Event% 20Content/CONSIL-ST_5419_2016_INIT-EN-TXT.pdf.\\n\\nInformation Systems and Ethics. ORBIT Journal, 2(2), Article 2.\\n\\nMacnish, K., Inguanzo, A. F., & Kirichenko, A. (2019). Smart Information Systems in\\n\\nFinn, R. L., Wright, D., & Friedewald, M. (2013). Seven Types of Privacy. In S. Gutwirth, R. Leenes, P. de Hert, & Y. Poullet (Eds.), European Data Protection: Coming of Age (pp. 3–32). Netherlands: Springer. https://doi.org/10.1007/978-94-007-5170-5_1.\\n\\nCybersecurity. ORBIT Journal, 2(2), Article 2.\\n\\nMacnish, K., & Ryan, M. (2019). SHERPA Deliverable 1.4 Report on Ethical Tensions and Social Impacts (Online Resource Project deliverable). SHERPA project. https://doi. org/10.21253/DMU.8181827.v2.\\n\\nFloridi, L. (1999). Information ethics: On the philosophical foundation of computer\\n\\nethics. Ethics and Information Technology, 1(1), 33–52.\\n\\nMacnish, K., Ryan, M., Gregory, A., Jiya, T., Antoniou, J., Hatzakis, T., Andreou, A.,\\n\\nRodrigues, R., Kirichenko, A., & Stahl, B. C. (2019). SHERPA Deliverable 1.1 Case studies (Online Resource Project deliverable). SHERPA project. https://doi.org/ 10.21253/DMU.8181827.v2.\\n\\nFloridi, Luciano (Ed.). (2010). The Cambridge Handbook of Information and Computer\\n\\nEthics. Cambridge University Press.\\n\\nGarriga, E., & Mel´e, D. (2004). Corporate Social Responsibility Theories: Mapping the Territory. Journal of Business Ethics, 53(1–2), 51–71. https://doi.org/10.1023/B: BUSI.0000039399.90587.34.\\n\\nMacnish, K., Ryan, M., & Stahl, B. (2019). Understanding Ethics and Human Rights in\\n\\nSmart Information Systems. ORBIT Journal, 2(2), Article 2.\\n\\nJournalofBusinessResearch124(2021)374–388388\\n\\nB.C. Stahl et al.\\n\\nMacnish, K., Wright, D., & Jiya, T. (2020). Predictive Policing in 2025: A Scenario. In\\n\\nVarley-Winter, O., & Shah, H. (2016). The opportunities and ethics of big data: Practical priorities for a national Council of Data Ethics. Phil. Trans. R. Soc. A, 374(2083), 20160116. https://doi.org/10.1098/rsta.2016.0116.\\n\\nH. Jahankhani, B. Akhgar, P. Cochrane, & M. Dastbaz (Eds.), Policing in the Era of AI and Smart Societies (pp. 199–215). Springer International Publishing. https://doi. org/10.1007/978-3-030-50613-1_9.\\n\\nWalsham, G. (1995). Interpretive case studies in IS research: Nature and method.\\n\\nEuropean Journal of Information Systems, 4(2), 74–81. https://doi.org/10.1057/ ejis.1995.9.\\n\\nMartin, C. D., & Makoundou, T. T. (2017). Taking the high road ethics by design in AI.\\n\\nACM Inroads, 8(4), 35–37.\\n\\nMetcalf, J., Keller, E. F., & boyd, danah. (2016). Perspectives on Big Data, Ethics, and Society. Council for Big Data, Ethics, and Society. http://bdes.datasociety.net/wp- content/uploads/2016/05/Perspectives-on-Big-Data.pdf.\\n\\nWorld Economic Forum. (2019). Responsible Use of Technology [White paper]. WEB.\\n\\nhttp://www3.weforum.org/docs/WEF_Responsible_Use_of_Technology.pdf. Wright, D. (2011). A framework for the ethical impact assessment of information\\n\\ntechnology. Ethics and Information Technology, 13(3), 199–226. https://doi.org/ 10.1007/s10676-010-9242-6.\\n\\nMill, J. S. (1861). Utilitarianism ((2nd Revised edition).). Hackett Publishing Co Inc. Mingers, J., & Walsham, G. (2010). Towards ethical information systems: The\\n\\ncontribution of discourse ethics. MIS Quarterly, 34(4), 833–854.\\n\\nWright, D., Rodrigues, R., Hatzakis, T., Pannofino, C., Macnish, K., Ryan, M., &\\n\\nMittelstadt, B. (2019). Principles alone cannot guarantee ethical AI. Nature Machine\\n\\nAntoniou, J. (2019). SHERPA Deliverable 1.2: SIS Scenarios. De Montfort University.\\n\\nIntelligence, 2019. https://doi.org/doi:10.1038/s42256-019-0114-4.\\n\\nWright, D., Stahl, B., & Hatzakis, T. (2020). Policy scenarios as an instrument for\\n\\nMittelstadt, B. D., Allo, P., Taddeo, M., Wachter, S., & Floridi, L. (2016). The ethics of algorithms: Mapping the debate. Big Data & Society, 3(2), 2053951716679679. Mittelstadt, B., Russell, C., & Wachter, S. (2019, January). Explaining Explanations in AI. FAT*’19, Atlanta, Georgia. https://www.academia.edu/37701175/Explaining_ Explanations_in_AI.\\n\\npolicymakers. Technological Forecasting and Social Change, 154, Article 119972. https://doi.org/10.1016/j.techfore.2020.119972.\\n\\nYin, R. K. (2003). Case Study Research: Design and Methods (Third Edition). Sage\\n\\nPublications Inc.\\n\\nMoor, J. H. (1985). What is computer ethics. Metaphilosophy, 16(4), 266–275. Moor, J. H., & Bynum, T. W. (2002). Introduction to cyberphilosophy. Metaphilosophy, 33\\n\\nBernd Carsten Stahl is Professor of Critical Research in Technology and Director of the Centre for Computing and Social Responsibility at De Montfort University, Leicester, UK. His interests cover philosophical issues arising from the intersections of business, tech- nology, and information. This includes ethical questions of current and emerging of ICTs, critical approaches to information systems and issues related to responsible research and innovation.\\n\\n(1/2), 4–10.\\n\\nMorley, J., Floridi, L., Kinsey, L., & Elhalal, A. (2019). From What to How- An Overview of AI Ethics Tools. ArXive: Methods and Research to Translate Principles into Practices. https://arxiv.org/abs/1905.06876.\\n\\nNerurkar, M., Wadephul, C., & Wiegerling, K. (2016). Ethics of Big Data: Introduction.\\n\\nInternational Review of Information. Ethics, 24.\\n\\nAndreas Andreou works as a Researcher and Project Manager at the human rights non governmental organisation Aequitas which is based in Cyprus. He holds a BA Humanities from the University of Essex and an LLM Master of Laws from the University of Central Lancashire. He has a research interest and experience in artificial intelligence and human rights, LGBTQI rights and hate speech/hate crime. He published academic articles and a research book.\\n\\nOECD. (2017). Recommendation of the OECD Council on Health Data Governance.\\n\\nhttp://www.oecd.org/health/health-systems/Recommendation-of-OECD-Council- on-Health-Data-Governance-Booklet.pdf.\\n\\nPatel, A., Hatzakis, T., Macnish, K., Ryan, M., & Kirichenko, A. (2019). SHERPA\\n\\nDeliverable D1.3: Security Issues, Dangers and Implications of Smart Information Systems. SHERPA project. https://dmu.figshare.com/articles/D1_3_Cyberthreats_ and_countermeasures/7951292.\\n\\nRamirez, R., Mukherjee, M., Vezzoli, S., & Kramer, A. M. (2015). Scenarios as a scholarly methodology to produce “interesting research”. Futures, 71, 70–87. https://doi.org/ 10.1016/j.futures.2015.06.006.\\n\\nPhilip Brey (PhD, University of California, San Diego, 1995) is professor of philosophy of technology at the Department of Philosophy, University of Twente, the Netherlands. He is on the editorial board of eleven leading journals and book series in his field, including Ethics and Information Technology, Nanoethics, Philosophy and Technology, Techn´e, Studies in Ethics, Law and Technology and Theoria. He is also former president of the International Society for Ethics and Information Technology (INSEIT), and former presi- dent of the Society for Philosophy and Technology (SPT). His research focuses on ethics and philosophy of emerging technologies, in particular digital technologies, AI, robotics, biomedical technology and sustainable technology. He currently coordinates the EU H2020 SIENNA project on the ethical and human rights dimensions of emerging tech- nologies, and the NWO-funded 10-year research programme Ethics of Socially Disruptive Technologies.\\n\\nRehg, W. (2014). Discourse ethics for computer ethics: A heuristic for engaged dialogical reflection. Ethics and Information Technology, 17(1), 27–39. https://doi.org/10.1007/ s10676-014-9359-0.\\n\\nRoessler, B., & Mokrosinska, D. (Eds.). (2015). Social Dimensions of Privacy:\\n\\nInterdisciplinary Perspectives. Cambridge University Press.\\n\\nRyan, M. (2019a). Ethics of Public Use of AI and Big Data. ORBIT Journal, 2(2), Article 2. Ryan, M. (2019b). Ethics of Using AI and Big Data in Agriculture: The Case of a Large\\n\\nAgriculture Multinational. ORBIT Journal, 2(2), Article 2.\\n\\nRyan, M., & Gregory, A. (2019). Ethics of Using Smart City AI and Big Data: The Case of\\n\\nFour Large European Cities. ORBIT Journal, 2(2), Article 2.\\n\\nRyan, M., & Stahl, B. C. (2020). Artificial intelligence ethics guidelines for developers\\n\\nTally Hatzakis is a senior research analyst at Trilateral Research with more than 15 years of research experience in academia as well as in the public and private sectors. Her domains of interest include smart cities, future tansport, big data and smart technologies, AI and robotics, personal wearables and privacy-by-design business models, as well as open innovation, green technologies and circular economy.\\n\\nand users: Clarifying their content and normative implications. Journal of Information, Communication and Ethics in Society, ahead-of-print(ahead-of-print). https://doi.org/10.1108/JICES-12-2019-0138.\\n\\nSantiago, N. (2020). SHERPA Delphi Study—Round 1 Results [Project Deliverable].\\n\\nSHERPA project. https://www.project-sherpa.eu/wp-content/uploads/2020/03/she rpa-delphi-study-round-1-summary-17.03.2020.docx.pdf.\\n\\nSharkey, A., & Sharkey, N. (2010). Granny and the robots: Ethical issues in robot care for the elderly. Ethics and Information Technology. https://doi.org/10.1007/s10676-010- 9234-6.\\n\\nAlexey Kirichenko is research collaboration manager at F-Secure, coordinating F-Secure’s participation in European and Finnish national research projects. He also represents the company in the Steering Board of WG6 (SRIA) of European Cyber Security Organisation (ECSO). His interests are mainly in applications of Machine Learning to cybersecurity.\\n\\nSolove, D. J. (2002). Conceptualizing Privacy. California Law Review, 90(4), 1087–1156. Sparrow, R. (2009). Predators or plowshares?: Arms control of robotic weapons. IEEE\\n\\nTechnology and Society Magazine, 28(1). https://doi.org/10.1109/MTS.2009.931862. Stahl, B. C., & Wright, D. (2018). Ethics and Privacy in AI and Big Data: Implementing Responsible Research and Innovation. IEEE Security & Privacy, 16(3), 26–33. https:// doi.org/10.1109/MSP.2018.2701164.\\n\\nKevin Macnish is Assistant Professor in Ethics and Technology at the University of Twente. His interests include ethical questions regarding privacy, surveillance, security, and AI.\\n\\nStahl, Bernd Carsten (2012). Morality, Ethics, and Reflection: A Categorization of\\n\\nAndrew Patel is a researcher in F-Secure Corporation’s Artificial Intelligence Center of Excellence. His main research areas include natural language processing, disinformation hunting and analysis, graph analysis and reinforcement learning. He is also a frequent contributor to F-Secure’s blog.\\n\\nNormative IS Research. Journal of the Association for Information Systems, 13(8), 636–656. https://doi.org/10.17705/1jais.00304.\\n\\nStahl, Bernd Carsten, & Coeckelbergh, M. (2016). Ethics of healthcare robotics: Towards responsible research and innovation. Robotics and Autonomous Systems. https://doi. org/10.1016/j.robot.2016.08.018.\\n\\nMark Ryan works as a Digital Ethics Researcher at Wageningen Economic Research, Wageningen University & Research. His interests are around topics in the ethics of tech- nology, with a particular focus on ethical issues pertaining to the use of artificial intelli- gence and Big Data. He has published on topics, such as the ethics of smart cities, self- driving vehicles, agricultural data analytics, social robotics, and trusting AI.\\n\\nTavani, H. (2008). Informational Privacy: Concepts, Theorie and Controversies. In\\n\\nJ. V. D. Hoven, & J. Weckert (Eds.), Information Technology and Moral Philosophy (pp. 131–164). Cambridge University Press.\\n\\nTaylor, L. (2016). The ethics of big data as a public good: Which public? Whose good?\\n\\nPhil. Trans. R. Soc. A, 374(2083), 20160126. https://doi.org/10.1098/ rsta.2016.0126.\\n\\nThe National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research. (1979). The Belmont Report—Ethical Principles and Guidelines for the Protection of Human Subjects of Research. Department of Health, Education, and Welfare. http://videocast.nih.gov/pdf/ohrp_belmont_report.pdf. USACM. (2017). Statement on Algorithmic Transparency and Accountability. ACM US\\n\\nDavid Wright is Director of Trilateral Research (London and Waterford), a company he founded in 2004. The company has partnered in more than 60 EU-funded projects. He has published four books on ambient intelligence, privacy and surveillance and more than 60 articles in peer-reviewed journals. He coined the term ethical impact assessment and published the first article on EIA. The ISO privacy impact assessment standard is based on the methodology he developed as is the CEN Workshop Agreement on ethical impact assessment. His interests include scenario construction, horizon scanning, impact assess- ments, cybersecurity, artificial intelligence, privacy, ethics and surveillance.\\n\\nPublic Policy Council.\\n\\nvan Est, R., & Gerritsen, J. (2017). Human rights in the robot age—Challenges arising from the use of robotics, artificial intelligence, and virtual and augmented reality [Report to the Parliamentary Assembly of the Council of Europe (PACE)]. Rathenau Instituut. https://www.rathenau.nl/en/file/9605/download?token=OQgFllZS.')],\n",
              " [Document(metadata={'source': '/content/ai and the future of humans.pdf'}, page_content=\"FOR RELEASE DECEMBER 10, 2018\\n\\nArtificial Intelligence and the Future of Humans Experts say the rise of artificial intelligence will make most people better off over the next decade, but many have concerns about how advances in AI will affect what it means to be human, to be productive and to exercise free will\\n\\nBy Janna Anderson, Lee Rainie and Alex Luchsinger\\n\\nFOR MEDIA OR OTHER INQUIRIES:\\n\\nJanna Anderson, Director, Imagining the Internet Center Lee Rainie, Director, Internet and Technology Research Shawnee Cohn, Communications Manager\\n\\n202.419.4372\\n\\nwww.pewresearch.org\\n\\nRECOMMENDED CITATION Pew Research Center, December, 2018, “Artificial Intelligence and the Future of Humans”\\n\\n1\\n\\nPEW RESEARCH CENTER\\n\\nAbout Pew Research Center\\n\\nPew Research Center is a nonpartisan fact tank that informs the public about the issues, attitudes and trends shaping America and the world. It does not take policy positions. It conducts public opinion polling, demographic research, content analysis and other data-driven social science research. The Center studies U.S. politics and policy; journalism and media; internet, science and technology; religion and public life; Hispanic trends; global attitudes and trends; and U.S. social and demographic trends. All of the center’s reports are available at www.pewresearch.org. Pew Research Center is a subsidiary of The Pew Charitable Trusts, its primary funder. For this project, Pew Research Center worked with Elon University’s Imagining the Internet Center, which helped conceive the research and collect and analyze the data.\\n\\n© Pew Research Center 2018\\n\\nwww.pewresearch.org\\n\\n2\\n\\nPEW RESEARCH CENTER\\n\\nArtificial Intelligence and the Future of Humans\\n\\nExperts say the rise of artificial intelligence will make most people better off over the next decade, but many have concerns about how advances in AI will affect what it means to be human, to be productive and to exercise free will\\n\\nDigital life is augmenting human capacities and disrupting eons-old human activities. Code-driven systems have spread to more than half of the world’s inhabitants in ambient information and connectivity, offering previously unimagined opportunities and unprecedented threats. As emerging algorithm-driven artificial intelligence (AI) continues to spread, will people be better off than they are today?\\n\\nSome 979 technology pioneers, innovators, developers, business and policy leaders, researchers and activists answered this question in a canvassing of experts conducted in the summer of 2018.\\n\\nThe experts predicted networked artificial intelligence will amplify human effectiveness but also threaten human autonomy, agency and capabilities. They spoke of the wide-ranging possibilities; that computers might match or even exceed human intelligence and capabilities on tasks such as complex decision-making, reasoning and learning, sophisticated analytics and pattern recognition, visual acuity, speech recognition and language translation. They said “smart” systems in communities, in vehicles, in buildings and utilities, on farms and in business processes will save time, money and lives and offer opportunities for individuals to enjoy a more-customized future.\\n\\nMany focused their optimistic remarks on health care and the many possible applications of AI in diagnosing and treating patients or helping senior citizens live fuller and healthier lives. They were also enthusiastic about AI’s role in contributing to broad public-health programs built around massive amounts of data that may be captured in the coming years about everything from personal genomes to nutrition. Additionally, a number of these experts predicted that AI would abet long- anticipated changes in formal and informal education systems.\\n\\nYet, most experts, regardless of whether they are optimistic or not, expressed concerns about the long-term impact of these new tools on the essential elements of being human. All respondents in this non-scientific canvassing were asked to elaborate on why they felt AI would leave people better off or not. Many shared deep worries, and many also suggested pathways toward solutions. The main themes they sounded about threats and remedies are outlined in the accompanying table.\\n\\nwww.pewresearch.org\\n\\n3\\n\\nPEW RESEARCH CENTER\\n\\nAI and the future of humans: Experts express concerns and suggest solutions\\n\\nCONCERNS\\n\\nHuman agency: Individuals are experiencing a loss of control over their lives\\n\\nDecision-making on key aspects of digital life is automatically ceded to code-driven, “black box” tools. People lack input and do not learn the context about how the tools work. They sacrifice independence, privacy and power over choice; they have no control over these processes. This effect will deepen as automated systems become more prevalent and complex.\\n\\nData abuse: Data use and surveillance in complex systems is designed for profit or for exercising power\\n\\nMost AI tools are and will be in the hands of companies striving for profits or governments striving for power. Values and ethics are often not baked into the digital systems making people’s decisions for them. These systems are globally networked and not easy to regulate or rein in.\\n\\nJob loss: The AI takeover of jobs will widen economic divides, leading to social upheaval\\n\\nThe efficiencies and other economic advantages of code-based machine intelligence will continue to disrupt all aspects of human work. While some expect new jobs will emerge, others worry about massive job losses, widening economic divides and social upheavals, including populist uprisings.\\n\\nDependence lock-in: Reduction of individuals’ cognitive, social and survival skills\\n\\nMany see AI as augmenting human capacities but some predict the opposite – that people’s deepening dependence on machine-driven networks will erode their abilities to think for themselves, take action independent of automated systems and interact effectively with others.\\n\\nMayhem: Autonomous weapons, cybercrime and weaponized information\\n\\nSome predict further erosion of traditional sociopolitical structures and the possibility of great loss of lives due to accelerated growth of autonomous military applications and the use of weaponized information, lies and propaganda to dangerously destabilize human groups. Some also fear cybercriminals’ reach into economic systems.\\n\\nSUGGESTED SOLUTIONS\\n\\nGlobal good is No. 1: Improve human collaboration across borders and stakeholder groups\\n\\nDigital cooperation to serve humanity’s best interests is the top priority. Ways must be found for people around the world to come to common understandings and agreements – to join forces to facilitate the innovation of widely accepted approaches aimed at tackling wicked problems and maintaining control over complex human-digital networks.\\n\\nValues-based system: Develop policies to assure AI will be directed at ‘humanness’ and common good\\n\\nAdopt a ‘moonshot mentality’ to build inclusive, decentralized intelligent digital networks ‘imbued with empathy’ that help humans aggressively ensure that technology meets social and ethical responsibilities. Some new level of regulatory and certification process will be necessary.\\n\\nPrioritize people: Alter economic and political systems to better help humans ‘race with the robots’\\n\\nReorganize economic and political systems toward the goal of expanding humans’ capacities and capabilities in order to heighten human/AI collaboration and staunch trends that would compromise human relevance in the face of programmed intelligence.\\n\\nPEW RESEARCH CENTER and ELON UNIVERSITY’S IMAGINING THE INTERNET CENTER December 2018\\n\\nwww.pewresearch.org\\n\\n4\\n\\nPEW RESEARCH CENTER\\n\\nSpecifically, participants were asked to consider the following:\\n\\n“Please think forward to the year 2030. Analysts expect that people will become even more dependent on networked artificial intelligence (AI) in complex digital systems. Some say we will continue on the historic arc of augmenting our lives with mostly positive results as we widely implement these networked tools. Some say our increasing dependence on these AI and related systems is likely to lead to widespread difficulties.\\n\\nOur question: By 2030, do you think it is most likely that advancing AI and related technology systems will enhance human capacities and empower them? That is, most of the time, will most people be better off than they are today? Or is it most likely that advancing AI and related technology systems will lessen human autonomy and agency to such an extent that most people will not be better off than the way things are today?”\\n\\nOverall, and despite the downsides they fear, 63% of respondents in this canvassing said they are hopeful that most individuals will be mostly better off in 2030, and 37% said people will not be better off.\\n\\nA number of the thought leaders who participated in this canvassing said humans’ expanding reliance on technological systems will only go well if close attention is paid to how these tools, platforms and networks are engineered, distributed and updated. Some of the powerful, overarching answers included those from:\\n\\nSonia Katyal, co-director of the Berkeley Center for Law and Technology and a member of the inaugural U.S. Commerce Department Digital Economy Board of Advisors, predicted, “In 2030, the greatest set of questions will involve how perceptions of AI and their application will influence the trajectory of civil rights in the future. Questions about privacy, speech, the right of assembly and technological construction of personhood will all re-emerge in this new AI context, throwing into question our deepest-held beliefs about equality and opportunity for all. Who will benefit and who will be disadvantaged in this new world depends on how broadly we analyze these questions today, for the future.”\\n\\nErik Brynjolfsson, director of the MIT Initiative on the Digital Economy and author of “Machine, Platform, Crowd: Harnessing Our Digital Future,” said, “AI and related technologies have already achieved superhuman performance in many areas, and there is little doubt that their\\n\\nwww.pewresearch.org\\n\\n5\\n\\nPEW RESEARCH CENTER\\n\\ncapabilities will improve, probably very significantly, by 2030. … I think it is more likely than not that we will use this power to make the world a better place. For instance, we can virtually eliminate global poverty, massively reduce disease and provide better education to almost everyone on the planet. That said, AI and ML [machine learning] can also be used to increasingly concentrate wealth and power, leaving many people behind, and to create even more horrifying weapons. Neither outcome is inevitable, so the right question is not ‘What will happen?’ but ‘What will we choose to do?’ We need to work aggressively to make sure technology matches our values. This can and must be done at all levels, from government, to business, to academia, and to individual choices.”\\n\\nBryan Johnson, founder and CEO of Kernel, a leading developer of advanced neural interfaces, and OS Fund, a venture capital firm, said, “I strongly believe the answer depends on whether we can shift our economic systems toward prioritizing radical human improvement and staunching the trend toward human irrelevance in the face of AI. I don’t mean just jobs; I mean true, existential irrelevance, which is the end result of not prioritizing human well-being and cognition.”\\n\\nMarina Gorbis, executive director of the Institute for the Future, said, “Without significant changes in our political economy and data governance regimes [AI] is likely to create greater economic inequalities, more surveillance and more programmed and non-human-centric interactions. Every time we program our environments, we end up programming ourselves and our interactions. Humans have to become more standardized, removing serendipity and ambiguity from our interactions. And this ambiguity and complexity is what is the essence of being human.”\\n\\nJudith Donath, author of “The Social Machine, Designs for Living Online” and faculty fellow at Harvard University's Berkman Klein Center for Internet & Society, commented, “By 2030, most social situations will be facilitated by bots – intelligent-seeming programs that interact with us in human-like ways. At home, parents will engage skilled bots to help kids with homework and catalyze dinner conversations. At work, bots will run meetings. A bot confidant will be considered essential for psychological well-being, and we’ll increasingly turn to such companions for advice ranging from what to wear to whom to marry. We humans care deeply about how others see us – and the others whose approval we seek will increasingly be artificial. By then, the difference between humans and bots will have blurred considerably. Via screen and projection, the voice, appearance and behaviors of bots will be indistinguishable from those of humans, and even physical robots, though obviously non-human, will be so convincingly sincere that our impression of them as thinking, feeling beings, on par with or superior to ourselves, will be unshaken. Adding to the ambiguity, our own communication will be heavily augmented: Programs will compose many of our messages and our online/AR appearance will [be] computationally crafted. (Raw, unaided human speech and demeanor will seem embarrassingly clunky, slow and\\n\\nwww.pewresearch.org\\n\\n6\\n\\nPEW RESEARCH CENTER\\n\\nunsophisticated.) Aided by their access to vast troves of data about each of us, bots will far surpass humans in their ability to attract and persuade us. Able to mimic emotion expertly, they’ll never be overcome by feelings: If they blurt something out in anger, it will be because that behavior was calculated to be the most efficacious way of advancing whatever goals they had ‘in mind.’ But what are those goals? Artificially intelligent companions will cultivate the impression that social goals similar to our own motivate them – to be held in good regard, whether as a beloved friend, an admired boss, etc. But their real collaboration will be with the humans and institutions that control them. Like their forebears today, these will be sellers of goods who employ them to stimulate consumption and politicians who commission them to sway opinions.”\\n\\nAndrew McLaughlin, executive director of the Center for Innovative Thinking at Yale University, previously deputy chief technology officer of the United States for President Barack Obama and global public policy lead for Google, wrote, “2030 is not far in the future. My sense is that innovations like the internet and networked AI have massive short-term benefits, along with long-term negatives that can take decades to be recognizable. AI will drive a vast range of efficiency optimizations but also enable hidden discrimination and arbitrary penalization of individuals in areas like insurance, job seeking and performance assessment.”\\n\\nMichael M. Roberts, first president and CEO of the Internet Corporation for Assigned Names and Numbers (ICANN) and Internet Hall of Fame member, wrote, “The range of opportunities for intelligent agents to augment human intelligence is still virtually unlimited. The major issue is that the more convenient an agent is, the more it needs to know about you – preferences, timing, capacities, etc. – which creates a tradeoff of more help requires more intrusion. This is not a black- and-white issue – the shades of gray and associated remedies will be argued endlessly. The record to date is that convenience overwhelms privacy. I suspect that will continue.”\\n\\ndanah boyd, a principal researcher for Microsoft and founder and president of the Data & Society Research Institute, said, “AI is a tool that will be used by humans for all sorts of purposes, including in the pursuit of power. There will be abuses of power that involve AI, just as there will be advances in science and humanitarian efforts that also involve AI. Unfortunately, there are certain trend lines that are likely to create massive instability. Take, for example, climate change and climate migration. This will further destabilize Europe and the U.S., and I expect that, in panic, we will see AI be used in harmful ways in light of other geopolitical crises.”\\n\\nAmy Webb, founder of the Future Today Institute and professor of strategic foresight at New York University, commented, “The social safety net structures currently in place in the U.S. and in many other countries around the world weren’t designed for our transition to AI. The transition through AI will last the next 50 years or more. As we move farther into this third era of computing,\\n\\nwww.pewresearch.org\\n\\n7\\n\\nPEW RESEARCH CENTER\\n\\nand as every single industry becomes more deeply entrenched with AI systems, we will need new hybrid-skilled knowledge workers who can operate in jobs that have never needed to exist before. We’ll need farmers who know how to work with big data sets. Oncologists trained as robotocists. Biologists trained as electrical engineers. We won’t need to prepare our workforce just once, with a few changes to the curriculum. As AI matures, we will need a responsive workforce, capable of adapting to new processes, systems and tools every few years. The need for these fields will arise faster than our labor departments, schools and universities are acknowledging. It’s easy to look back on history through the lens of present – and to overlook the social unrest caused by widespread technological unemployment. We need to address a difficult truth that few are willing to utter aloud: AI will eventually cause a large number of people to be permanently out of work. Just as generations before witnessed sweeping changes during and in the aftermath of the Industrial Revolution, the rapid pace of technology will likely mean that Baby Boomers and the oldest members of Gen X – especially those whose jobs can be replicated by robots – won’t be able to retrain for other kinds of work without a significant investment of time and effort.”\\n\\nBarry Chudakov, founder and principal of Sertain Research, commented, “By 2030 the human- machine/AI collaboration will be a necessary tool to manage and counter the effects of multiple simultaneous accelerations: broad technology advancement, globalization, climate change and attendant global migrations. In the past, human societies managed change through gut and intuition, but as Eric Teller, CEO of Google X, has said, ‘Our societal structures are failing to keep pace with the rate of change.’ To keep pace with that change and to manage a growing list of ‘wicked problems’ by 2030, AI – or using Joi Ito’s phrase, extended intelligence – will value and revalue virtually every area of human behavior and interaction. AI and advancing technologies will change our response framework and time frames (which in turn, changes our sense of time). Where once social interaction happened in places – work, school, church, family environments – social interactions will increasingly happen in continuous, simultaneous time. If we are fortunate, we will follow the 23 Asilomar AI Principles outlined by the Future of Life Institute and will work toward ‘not undirected intelligence but beneficial intelligence.’ Akin to nuclear deterrence stemming from mutually assured destruction, AI and related technology systems constitute a force for a moral renaissance. We must embrace that moral renaissance, or we will face moral conundrums that could bring about human demise. ... My greatest hope for human-machine/AI collaboration constitutes a moral and ethical renaissance – we adopt a moonshot mentality and lock arms to prepare for the accelerations coming at us. My greatest fear is that we adopt the logic of our emerging technologies – instant response, isolation behind screens, endless comparison of self-worth, fake self-presentation – without thinking or responding smartly.”\\n\\nJohn C. Havens, executive director of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems and the Council on Extended Intelligence, wrote, “Now, in 2018, a majority of\\n\\nwww.pewresearch.org\\n\\n8\\n\\nPEW RESEARCH CENTER\\n\\npeople around the world can’t access their data, so any ‘human-AI augmentation’ discussions ignore the critical context of who actually controls people’s information and identity. Soon it will be extremely difficult to identify any autonomous or intelligent systems whose algorithms don’t interact with human data in one form or another.”\\n\\nBatya Friedman, a human-computer interaction professor at the University of Washington’s Information School, wrote, “Our scientific and technological capacities have and will continue to far surpass our moral ones – that is our ability to use wisely and humanely the knowledge and tools that we develop. ... Automated warfare – when autonomous weapons kill human beings without human engagement – can lead to a lack of responsibility for taking the enemy’s life or even knowledge that an enemy’s life has been taken. At stake is nothing less than what sort of society we want to live in and how we experience our humanity.”\\n\\nGreg Shannon, chief scientist for the CERT Division at Carnegie Mellon University, said, “Better/worse will appear 4:1 with the long-term ratio 2:1. AI will do well for repetitive work where ‘close’ will be good enough and humans dislike the work. … Life will definitely be better as AI extends lifetimes, from health apps that intelligently ‘nudge’ us to health, to warnings about impending heart/stroke events, to automated health care for the underserved (remote) and those who need extended care (elder care). As to liberty, there are clear risks. AI affects agency by creating entities with meaningful intellectual capabilities for monitoring, enforcing and even punishing individuals. Those who know how to use it will have immense potential power over those who don’t/can’t. Future happiness is really unclear. Some will cede their agency to AI in games, work and community, much like the opioid crisis steals agency today. On the other hand, many will be freed from mundane, unengaging tasks/jobs. If elements of community happiness are part of AI objective functions, then AI could catalyze an explosion of happiness.”\\n\\nKostas Alexandridis, author of “Exploring Complex Dynamics in Multi-agent-based Intelligent Systems,” predicted, “Many of our day-to-day decisions will be automated with minimal intervention by the end-user. Autonomy and/or independence will be sacrificed and replaced by convenience. Newer generations of citizens will become more and more dependent on networked AI structures and processes. There are challenges that need to be addressed in terms of critical thinking and heterogeneity. Networked interdependence will, more likely than not, increase our vulnerability to cyberattacks. There is also a real likelihood that there will exist sharper divisions between digital ‘haves’ and ‘have-nots,’ as well as among technologically dependent digital infrastructures. Finally, there is the question of the new ‘commanding heights’ of the digital network infrastructure’s ownership and control.”\\n\\nwww.pewresearch.org\\n\\n9\\n\\nPEW RESEARCH CENTER\\n\\nOscar Gandy, emeritus professor of communication at the University of Pennsylvania, responded, “We already face an ungranted assumption when we are asked to imagine human- machine ‘collaboration.’ Interaction is a bit different, but still tainted by the grant of a form of identity – maybe even personhood – to machines that we will use to make our way through all sorts of opportunities and challenges. The problems we will face in the future are quite similar to the problems we currently face when we rely upon ‘others’ (including technological systems, devices and networks) to acquire things we value and avoid those other things (that we might, or might not be aware of).”\\n\\nJames Scofield O'Rourke, a professor of management at the University of Notre Dame, said, “Technology has, throughout recorded history, been a largely neutral concept. The question of its value has always been dependent on its application. For what purpose will AI and other technological advances be used? Everything from gunpowder to internal combustion engines to nuclear fission has been applied in both helpful and destructive ways. Assuming we can contain or control AI (and not the other way around), the answer to whether we’ll be better off depends entirely on us (or our progeny). ‘The fault, dear Brutus, is not in our stars, but in ourselves, that we are underlings.’”\\n\\nSimon Biggs, a professor of interdisciplinary arts at the University of Edinburgh, said, “AI will function to augment human capabilities. The problem is not with AI but with humans. As a species we are aggressive, competitive and lazy. We are also empathic, community minded and (sometimes) self-sacrificing. We have many other attributes. These will all be amplified. Given historical precedent, one would have to assume it will be our worst qualities that are augmented. My expectation is that in 2030 AI will be in routine use to fight wars and kill people, far more effectively than we can currently kill. As societies we will be less affected by this as we currently are, as we will not be doing the fighting and killing ourselves. Our capacity to modify our behaviour, subject to empathy and an associated ethical framework, will be reduced by the disassociation between our agency and the act of killing. We cannot expect our AI systems to be ethical on our behalf – they won’t be, as they will be designed to kill efficiently, not thoughtfully. My other primary concern is to do with surveillance and control. The advent of China’s Social Credit System (SCS) is an indicator of what it likely to come. We will exist within an SCS as AI constructs hybrid instances of ourselves that may or may not resemble who we are. But our rights and affordances as individuals will be determined by the SCS. This is the Orwellian nightmare realised.”\\n\\nMark Surman, executive director of the Mozilla Foundation, responded, “AI will continue to concentrate power and wealth in the hands of a few big monopolies based on the U.S. and China. Most people – and parts of the world – will be worse off.”\\n\\nwww.pewresearch.org\\n\\n10\\n\\nPEW RESEARCH CENTER\\n\\nWilliam Uricchio, media scholar and professor of comparative media studies at MIT, commented, “AI and its related applications face three problems: development at the speed of Moore’s Law, development in the hands of a technological and economic elite, and development without benefit of an informed or engaged public. The public is reduced to a collective of consumers awaiting the next technology. Whose notion of ‘progress’ will prevail? We have ample evidence of AI being used to drive profits, regardless of implications for long-held values; to enhance governmental control and even score citizens’ ‘social credit’ without input from citizens themselves. Like technologies before it, AI is agnostic. Its deployment rests in the hands of society. But absent an AI-literate public, the decision of how best to deploy AI will fall to special interests. Will this mean equitable deployment, the amelioration of social injustice and AI in the public service? Because the answer to this question is social rather than technological, I’m pessimistic. The fix? We need to develop an AI-literate public, which means focused attention in the educational sector and in public-facing media. We need to assure diversity in the development of AI technologies. And until the public, its elected representatives and their legal and regulatory regimes can get up to speed with these fast-moving developments we need to exercise caution and oversight in AI’s development.”\\n\\nThe remainder of this report is divided into three sections that draw from hundreds of additional respondents’ hopeful and critical observations: 1) concerns about human-AI evolution, 2) suggested solutions to address AI’s impact, and 3) expectations of what life will be like in 2030, including respondents’ positive outlooks on the quality of life and the future of work, health care and education. Some responses are lightly edited for style.\\n\\nwww.pewresearch.org\\n\\n11\\n\\nPEW RESEARCH CENTER\\n\\n1. Concerns about human agency, evolution and survival\\n\\nA clear majority of the responses from these experts contained material outlining certain challenges, fears or concerns about the AI-infused future. The five most-often mentioned concerns were: 1) the use of AI reduces individuals’ control over their lives; 2) surveillance and data systems designed primarily for efficiency, profit and control are inherently dangerous;3) displacement of human jobs by AI will widen economic and digital divides, possibly leading to social upheaval; 4) individuals’ cognitive, social and survival skills will be diminished as they become dependent on AI; and 5) citizens will face increased vulnerabilities, such as exposure to cybercrime and cyberwarfare that spin out of control and the possibility that essential organizations are endangered by weaponized information. A few also worried about the wholesale destruction of humanity. The sections of this chapter will cover experts’ answers tied to these themes.\\n\\nThe use of AI reduces individuals’ control over their lives\\n\\nAutonomous systems can reduce or eliminate the need for human involvement in some tasks. Today’s ever-advancing artificial narrow intelligence (ANI) tools – for instance, search engines and digital “agents” such as Siri, Alexa and Cortana – are not close to reaching the goal of human- like artificial general intelligence (AGI). They are, however, continually becoming more powerful thanks to developments in machine learning and natural language processing and advances in materials science, networking, energy-storage and hardware capabilities.\\n\\nANI is machine intelligence that equals or exceeds people’s abilities or efficiency at a specific task. For years, code-based tools in robots and other systems have performed repetitive tasks like factory-floor assembly activities. Today, these tools are quickly evolving to master human traits such as reason, logic, learning, task-performance and creativity. Today’s smart, networked, software-equipped devices, cars, digital assistants and platforms, such as Google search and Facebook social mapping, accomplish extremely complex tasks. The systems underpinning today’s global financial markets, businesses, militaries, police forces, and medical, energy and industrial operations are all dependent upon networked AI of one type or another.\\n\\nWhat is the future of humans in an age of accelerating technological change?\\n\\nMany experts in this canvassing said that as AI advances human autonomy and agency are at risk. They note that decision-making on key aspects of life is ceded to code-driven tools. Individuals who function in this digital world sacrifice, to varying degrees, their independence, right to privacy and power over choice. Many of the experts who worry about this say humans accede to this in order to stay competitive, to participate socially and professionally in the world, to be entertained\\n\\nwww.pewresearch.org\\n\\n12\\n\\nPEW RESEARCH CENTER\\n\\nand to get things done. They say people hand over some control of their lives because of the perceived advantages they gain via digital tools – efficiency, convenience and superior pattern recognition, data storage, and search-and-find capabilities. Here is a selection of responses from these experts that touch on this:\\n\\nAn anonymous respondent summed up the concerns of many, writing, “The most-feared reversal in human fortune of the AI age is loss of agency. The trade-off for the near-instant, low- friction convenience of digital life is the loss of context about and control over its processes. People’s blind dependence on digital tools is deepening as automated systems become more complex and ownership of those systems is by the elite.”\\n\\nBaratunde Thurston, futurist, former director of digital at The Onion and co-founder of comedy/technology start-up Cultivated Wit, said, “For the record, this is not the future I want, but it is what I expect given existing default settings in our economic and sociopolitical system preferences. … The problems to which we are applying machine learning and AI are generally not ones that will lead to a ‘better’ life for most people. That’s why I say in 2030, most people won’t be better due to AI. We won’t be more autonomous; we will be more automated as we follow the metaphorical GPS line through daily interactions. We don’t choose our breakfast or our morning workouts or our route to work. An algorithm will make these choices for us in a way that maximizes efficiency (narrowly defined) and probably also maximizes the profitability of the service provider. By 2030, we may cram more activities and interactions into our days, but I don’t think that will make our lives ‘better.’ A better life, by my definition, is one in which we feel more valued and happy. Given that the biggest investments in AI are on behalf of marketing efforts designed to deplete our attention and bank balances, I can only imagine this leading to days that are more filled but lives that are less fulfilled. To create a different future, I believe we must unleash these technologies toward goals beyond profit maximization. Imagine a mapping app that plotted your work commute through the most beautiful route, not simply the fastest. Imagine a communications app that facilitated deeper connections with people you deemed most important. These technologies must be more people-centric. We need to ask that they ask us, ‘What is important to you? How would you like to spend your time?’ But that’s not the system we’re building. All those decisions have been hoarded by the unimaginative pursuit of profit.”\\n\\nThad Hall, a researcher and coauthor of “Politics for a Connected American Public,” added: “AI is likely to have benefits – from improving medical diagnoses to improving people’s consumer experiences. However, there are four aspects of AI that are very problematic. 1) It is likely to result in more economic uncertainty and dislocation for people, including employment issues and more need to change jobs to stay relevant. 2) AI will continue to erode people’s privacy as search becomes more thorough. China’s monitoring of populations illustrates what this could look like in\\n\\nwww.pewresearch.org\\n\\n13\\n\\nPEW RESEARCH CENTER\\n\\nauthoritarian and Western countries, with greater facial recognition used to identify people and affect their privacy. 3) AI will likely continue to have biases that are negative toward minority populations, including groups we have not considered. Given that algorithms often have identifiable biases (e.g., favoring people who are white or male), they likely also have biases that are less well-recognized, such as biases that are negative toward people with disabilities, older people or other groups. These biases may ripple through society in unknown ways. Some groups are more likely to be monitored effectively. 4) AI is creating a world where reality can be manipulated in ways we do not appreciate. Fake videos, audio and similar media are likely to explode and create a world where ‘reality’ is hard to discern. The relativistic political world will become more so, with people having evidence to support their own reality or multiple realities that mean no one knows what is the ‘truth.’”\\n\\nThomas Schneider, head of International Relations Service and vice-director at the Federal Office of Communications (OFCOM) in Switzerland, said, “AI will help mankind to be more efficient, live safer and healthier, and manage resources like energy, transport, etc., more efficiently. At the same time, there are a number of risks that AI may be used by those in power to manipulate, control and dominate others. (We have seen this with every new technology: It can and will be used for good and bad.) Much will depend about how AI will be governed: If we have an inclusive and bottom-up governance system of well-informed citizens, then AI will be used for improving our quality of life. If only a few people decide about how AI is used and what for, many others will be dependent on the decisions of these few and risk being manipulated by them. The biggest danger in my view is that there will be a greater pressure on all members of our societies to live according to what ‘the system’ will tell us is ‘best for us’ to do and not to do, i.e., that we may lose the autonomy to decide ourselves how we want to live our lives, to choose diverse ways of doing things. With more and more ‘recommendations,’ ‘rankings’ and competition through social pressure and control, we may risk a loss of individual fundamental freedoms (including but not limited to the right to a private life) that we have fought for in the last decades and centuries.”\\n\\nBart Knijnenburg, assistant professor of computer science who is active in the Human Factors Institute at Clemson University, said, “Whether AI will make our lives better depends on how it is implemented. Many current AI systems (including adaptive content-presentation systems and so- called recommender systems) try to avoid information and choice overload by replacing our decision-making processes with algorithmic predictions. True empowerment will come from these systems supporting rather than replacing our decision-making practices. This is the only way we can overcome choice/information overload and at the same time avoid so-called ‘filter bubbles.’ For example, Facebook’s current post ranking systems will eventually turn us all into cat video watching zombies, because they follow our behavioral patterns, which may not be aligned with our preferences. The algorithms behind these tools need to support human agency, not replace it.”\\n\\nwww.pewresearch.org\\n\\n14\\n\\nPEW RESEARCH CENTER\\n\\nPeter Reiner, professor and co-founder of the National Core for Neuroethics at the University of British Columbia, commented, “I am confident that in 2030 both arms of this query will be true: AI-driven algorithms will substantially enhance our abilities as humans and human autonomy and agency will be diminished. Whether people will be better off than they are today is a separate question, and the answer depends to a substantial degree on how looming technological developments unfold. On the one hand, if corporate entities retain unbridled control over how AI- driven algorithms interact with humans, people will be less well off, as the loss of autonomy and agency will be largely to the benefit of the corporations. On the other hand, if ‘we the people’ demand that corporate entities deploy AI-algorithms in a manner that is sensitive to the issues of human autonomy and agency, then there is a real possibility for us to be better off – enhanced by the power of the AI-driven algorithm and yet not relegated to an impoverished seat at the decision-making table. One could even parse this further, anticipating that certain decisions can be comfortably left in the hands of the AI-driven algorithm, with other decisions either falling back on humans or arrived at through a combination of AI-driven algorithmic input and human decision making. If we approach these issues skillfully – and it will take quite a bit of collaborative work between ethicists and industry – we can have the best of both worlds. On the other hand, if we are lax in acting as watchdogs over industry we will be functionally rich and decisionally poor.”\\n\\nPaul Vixie, an Internet Hall of Fame member known for designing and implementing several Domain Name System protocol extensions and applications, wrote, “Understanding is a perfect proxy for control. As we make more of the world’s economy non-understandable by the masses, we make it easier for powerful interests to practice control. Real autonomy or privacy or unpredictability will be seen as a threat and managed around.”\\n\\nJoão Pedro Taveira, embedded systems researcher and smart grids architect for INOV INESC Inovação in Portugal, wrote, “Basically, we will lose several degrees of freedom. Are we ready for that? When we wake up to what is happening it might be too late to do anything about it. Artificial intelligence is a subject that must be studied philosophically, in open-minded, abstract and hypothetical ways. Using this perspective, the issues to be solved by humans are (but not limited to) AI, feelings, values, motivation, free will, solidarity, love and hate. Yes, we will have serious problems. Dropping the ‘artificial’ off AI, look at the concept of intelligence. As a computer-science person, I know that so-called ‘AI’ studies how an agent (a software program) increases its knowledge base using rules that are defined using pattern-recognition mechanisms. No matter which mechanisms are used to generate this rule set, the result will be always behavioral profiling. Right now, everybody uses and agrees to use a wide set of appliances, services and products without a full understanding of the information that is being shared with enterprises, companies and other parties. There’s a lack of needed regulation and audit mechanisms on who or what uses our information and how it is used and whether it is stored for future use. Governments and others\\n\\nwww.pewresearch.org\\n\\n15\\n\\nPEW RESEARCH CENTER\\n\\nwill try to access this information using these tools by decree, arguing national security or administration efficiency improvements. Enterprises and companies might argue that these tools offer improvement of quality of service, but there’s no guarantee about individuals’ privacy, anonymity, individual security, intractability and so on.”\\n\\nRamon Lopez de Mantaras, director of the Spanish National Research Council’s Artificial Intelligence Research Institute, said, “I do not think it is a good idea to give high levels of autonomy to AI systems. They are, and will be, weak AI systems without commonsense knowledge. They will have more and more competence, yes, but this will be competence without comprehension. AI machines should remain at the level of tools or, at most, assistants, always keeping the human in the loop. We should all read or re-read the book ‘Computer Power and Human Reason’ by Joseph Weizenbaum before deciding whether or not to give lots of autonomy to stupid machines.”\\n\\nOscar Gandy, emeritus professor of communication at the University of Pennsylvania, responded, “AI systems will make quite substantial and important contributions to the ability of health care providers to generate accurate diagnoses of maladies and threats to my well-being, now and in the future. I can imagine the development and deployment of systems in which my well-being is the primary basis of our relationship. I am less sure about how my access to and use of this resource may be constrained or distorted by the interests of the other actors (humans within profit/power-seeking orientations). I assume that they will be aided by their own AI systems informing them how to best present options to me. I am hopeful that we will have agents (whether private, social, governmental) whose interest and responsibility is in ensuring that my interests govern those relationships.”\\n\\nRobert Epstein, senior research psychologist at the American Institute for Behavioral Research and Technology and the founding director of the Loebner Prize, a competition in artificial intelligence, said, “By 2030, it is likely that AIs will have achieved a type of sentience, even if it is not human-like. They will also be able to exercise varying degrees of control over most human communications, financial transactions, transportation systems, power grids and weapon systems. As I noted in my 2008 book, ‘Parsing the Turing Test,’ they will reside in the ‘InterNest’ we have been building for them, and we will have no way of dislodging them. How they decide to deal with humanity – to help us, ignore us or destroy us – will be entirely up to them, and there is no way currently to predict which avenue they will choose. Because a few paranoid humans will almost certainly try to destroy the new sentient AIs, there is at least a reasonable possibility that that they will swat us like the flies we are – the possibility that Stephen Hawking, Elon Musk and others have warned about. There is no way, to my knowledge, of stopping this future from emerging. Driven by the convenience of connectivity, the greed that underlies business expansion and the\\n\\nwww.pewresearch.org\\n\\n16\\n\\nPEW RESEARCH CENTER\\n\\npipedreams of muddle-headed people who confuse machine-like intelligence with biological intelligence, we will continue to build AIs we can barely understand and to expand the InterNest in which they will live – until the inevitable – whatever that proves to be – occurs.”\\n\\nAn attorney specializing in policy issues for a global digital rights organization commented, “I’m not sure, even today, whether the tech advances of the last 12 years have been net positive over the global population. We’ve seen a widening gap between the very rich and everybody else. That is likely bad for democracy. AI seems likely to make the employment/training problem worse in the U.S., and AI may have similar effects in countries that currently provide cheap labor. On the political-governmental side, AI will exacerbate current surveillance and accountability problems. I figure that AI will improve and speed up all biometric pattern recognition as well as DNA analysis and natural language processing. And though we know that much of this is biased, we’re not adequately counteracting the bias we know about. The companies who generate and disseminate AI technology have every incentive to continue. I’m not optimistic that collective action – at least in the U.S. system – will successfully counter those incentives.”\\n\\nBrian Behlendorf, executive director of the Hyperledger project at The Linux Foundation and expert in blockchain technology, wrote, “I am concerned that AI will not be a democratizing power, but will enhance further the power and wealth of those who already hold it. This is because more data means better AI, and data is expensive to acquire, especially personal data, the most valuable kind. This is in contrast to networking technologies, whose benefits were shared fairly widely as the prices for components came down equally fast for everyone. One other reason: AI apps will be harder to debug than ordinary apps, and we already see hard-to-debug applications leading to disenfranchisement and deterioration of living. So, I do not take as a given that AI will enrich ‘most’ people's lives over the next 12 years.”\\n\\nEileen Donahoe, executive director of the Global Digital Policy Incubator at Stanford University, commented, “While I do believe human-machines collaboration will bring many benefits to society over time, I fear that we will not have made enough progress by 2030 to ensure that benefits will be spread evenly or to protect against downside risks, especially as they relate to bias, discrimination and loss of accountability by that time.”\\n\\nDavid Bray, executive director of People-Centered Internet, commented, “Hope: Human- machine/AI collaborations extend our abilities of humans while we (humans) intentionally strive to preserve values of respect, dignity and agency of choice for individuals. Machines bring together different groups of people and communities and help us work and live together by reflecting on our own biases and helping us come to understand the plurality of different perspectives of others. Big concern: Human-machine/AI collaborations turn out to not benefit everyone, only a few, and\\n\\nwww.pewresearch.org\\n\\n17\\n\\nPEW RESEARCH CENTER\\n\\nresult in a form of ‘indentured servitude’ or ‘neo-feudalism’ that is not people-centered and not uplifting of people. Machines amplify existing confirmation biases and other human characteristics resulting in sensationalist, emotion-ridden news and other communications that gets page views and ad-clicks yet lack nuance of understanding, resulting in tribalism and a devolution of open societies and pluralities to the detriment of the global human condition.”\\n\\nBernie Hogan, senior research fellow at Oxford Internet Institute, wrote, “The current political and economic climate suggests that existing technology, especially machine learning, will be used to create better decisions for those in power while creating an ever more tedious morass of bureaucracy for the rest. We see little example of successful bottom-up technology, open source technology and hacktivism relative to the encroaching surveillance state and attention economy.”\\n\\nDan Buehrer, a retired professor of computer science formerly with the National Chung Cheng University in Taiwan, warned, “Statistics will be replaced by individualized models, thus allowing control of all individuals by totalitarian states and, eventually, by socially intelligent machines.”\\n\\nNathalie Marechal, doctoral candidate at the University of Southern California’s Annenberg School for Communication who researches the intersection of internet policy and human rights, said, “Absent rapid and decisive actions to rein in both government overreach and companies’ amoral quest for profit, technological developments – including AI – will bring about the infrastructure for total social control, threatening democracy and the right to individual self- determination.”\\n\\nKatja Grace, contributor to the AI Impacts research project and a research associate with the Machine Intelligence Research Institute, said, “There is a substantial chance that AI will leave everyone worse off, perhaps radically so. The chance is less than 50 percent, but the downside risk is so large that there could be an expectation the world might be worse for AI.”\\n\\nDavid A. Banks, an associate research analyst with the Social Science Research Council, said, “AI will be very useful to a small professional class but will be used to monitor and control everyone else.”\\n\\nLuis German Rodriguez Leal, teacher and researcher at the Universidad Central de Venezuela and consultant on technology for development, said, “Humankind is not addressing properly the issue of educating people about possibilities and risks of human-machine/AI collaboration. One can observe today the growing problems of ill-intentioned manipulation of information and technological resources. There are already plenty of examples about how decision-making is biased using big data, machine learning, privacy violations and social networks (just to mention a\\n\\nwww.pewresearch.org\\n\\n18\\n\\nPEW RESEARCH CENTER\\n\\nfew elements) and one can see that the common citizen is unaware of how much of his/her will does not belong to him/her. This fact has a meaningful impact on our social, political, economic and private life. We are not doing enough to attend to this issue, and it is getting very late.”\\n\\nLlewellyn Kriel, CEO of TopEditor International, a media services company based in Johannesburg, South Africa, wrote, “Current developments do not augur well for the fair growth of AI. Vast swaths of the population simply do not have the intellectual capacity or level of sophistication to understand 1) the technology itself and 2) the implications of its safe use. This entrenches and widens the digital divide in places like Africa. The socio-political implications of this breed deep primitive superstition, racial hatred toward whites and Asians who are seen as techno-colonialists and the growth of kleptocracies amid the current mushrooming of corruption.”\\n\\nSteven Thompson, an author specializing in illuminating emerging issues and editor of “Androids, Cyborgs, and Robots in Contemporary Culture and Society,” wrote, “The keyword from the query is ‘dependence.’ I published pioneering quantitative research on internet addiction and dependency in 1996, and followed up 15 years later with a related, updated research talk on the future of AI and internet dependency at a UNESCO-sponsored conference on information literacy in Morocco. My expertise is in ethical and technological issues related to moving the internet appliance into the human body. … The internet is moving into the human body, and, in that process, societal statuses are altered, privileging some while abandoning others in the name of emerging technologies, and the global order is restructuring to the same effect. Think of net neutrality issues gone wild, corporately and humanly sustained with the privileges such creation and maintenance affords some members of society. Now think of the liberty issues arising from those persons who are digital outcasts, and wish to not be on the grid, yet will be forced to do so by society and even government edicts.”\\n\\nAlan Mutter, a longtime Silicon Valley CEO, cable TV executive and now a teacher of media economics and entrepreneurism at the University of California, Berkeley, said, “The danger is that we will surrender thinking, exploring and experimentation to tools that hew to the rules but can’t color outside the lines. Would you like computers to select the president or decide if you need hip surgery?”\\n\\nDan Geer, a respondent who provided no identifying details, commented, “If you believe, as do I, that having a purpose to one’s life is all that enables both pride and happiness, then the question becomes whether AI will or will not diminish purpose. For the irreligious, AI will demolish purpose, yet if AI is truly intelligent, then AI will make serving it the masses’ purpose. Ergo ...”\\n\\nwww.pewresearch.org\\n\\n19\\n\\nPEW RESEARCH CENTER\\n\\nCristobal Young, an associate professor of sociology at Cornell University specializing in economic sociology and stratification, commented, “I mostly base my response [that tech will not leave most people better off than they are today] on Twitter and other online media, which were initially praised as ‘liberation technology.’ It is clear that the internet has devastated professional journalism, filled the public sphere with trash that no one believes and degraded civil discourse. This isn’t about robots, but rather about how humans use the internet. Donald Trump himself says that without Twitter, he could never have been elected, and Twitter continues to be his platform for polarization, insult and attacks on the institutions of accountability.”\\n\\nDavid J. Krieger, co-director of the Institute for Communication & Leadership in Lucerne, Switzerland, wrote, “The affordances of digital technologies bind people into information networks such that the network becomes the actor and intelligence as well as agency are qualities of the network as a whole and not any individual actors, whether human or non-human. Networks will have access to much more information than do any present-day actors and therefore be able to navigate complex environments, e.g., self-driving cars, personal assistants, smart cities. Typically, we will consult and cooperate with networks in all areas, but the price will be that we have no such thing as privacy. Privacy is indeed dead, but in the place of personal privacy management there will be network publicy governance [‘publicy’ is the opposite of privacy]. To ensure the use of these technologies for good instead of evil it will be necessary to dismantle and replace current divides between government and governed, workers and capitalists as well as to establish a working global governance.”\\n\\nWendy M. Grossman, author of “net.wars” and technology blogger, wrote, “2030 is 12 years from now. I believe human-machine AI collaboration will be successful in many areas, but that we will be seeing, like we are now over Facebook and other social media, serious questions about ownership and who benefits. It seems likely that the limits of what machines can do will be somewhat clearer than they are now, when we’re awash in hype. We will know by then, for example, how successful self-driving cars are going to be, and the problems inherent in handing off control from humans to machines in a variety of areas will also have become clearer. The big fight is to keep people from relying on experimental systems and turning off the legacy ones too soon – which is our current situation with the internet.”\\n\\nKarl M. van Meter, founding editor of the Bulletin of Sociological Methodology and author of “Computational Social Science in the Age of Big Data,” said, “The well-being of the world’s population depends on governments making ‘intelligent’ decisions based on AI or other means. Moreover, environmental change may well be the determining factor for future well-being, with or without ‘intelligent’ decisions by world governments.”\\n\\nwww.pewresearch.org\\n\\n20\\n\\nPEW RESEARCH CENTER\\n\\nAndrew Whinston, computer science professor and director of the Center for Research in Electronic Commerce at the University of Texas at Austin, said, “There are several issues. First, security problems do not get the attention needed. Secondly, there may be use of the technology to control the population – as we see developing in China. AI methodology is focused on prediction, at least so far, so methods to improve health or general welfare are lacking. Deep learning, which is getting the big hype, does not have a clear foundation. That makes it scientifically weak.”\\n\\nAn information administration manager responded, “We cede more and more decision- making and policy making to self-interested parties in the private sphere. Our institutions are insufficiently nimble to keep up with the policy questions that arise and attempts to regulate new industries are subverted by corrupt money politics at both the federal and state levels.”\\n\\nAn internet pioneer said, “Nothing in our current social, economic or political structures points to a positive outcome. There is no evidence that more AI will improve the lives of most people. In fact, the opposite is likely to be the case. There will be more unemployment, less privacy, etc.”\\n\\nThe following one-liners from anonymous respondents also tie into human agency:\\n\\n§ An Internet Hall of Fame member commented, “AI will not leave most people better\\n\\noff than they are today because individuals will not be able to control their lives.”\\n\\n§ A professor of AI and soft computing at a university in Italy said, “Development\\n\\nhas brought humanity past the boundary, the survival limit; it is too easy to control technology in ways that are dangerous for people.”\\n\\n§ An assistant professor of social justice wrote, “Technology magnifies what exists (for\\n\\ngood or bad). There is simply more bad than good to be magnified.”\\n\\n§ A professor of digital humanities at a Silicon-Valley-area university said, “Given increasing income disparity in much of the world, my fear is that AI will be used to repress the disenfranchised and create even more privilege for the few.”\\n\\n§ A distinguished engineer and chief scientist at major technology companies commented, “Large actors will use AI for their benefit. Individual customers may have some benefits as a side effect, at a cost of lower autonomy.”\\n\\n§ A professor of electrical engineering and innovation based in Europe said,\\n\\n“People will lose control of their lives, which will remain in the hands of a small group of experts or companies.”\\n\\n§ A respondent based in Turkey wrote, “Due to unknown logic of algorithms we will lose our autonomy over our lives and everyday life decisions; humankind is depending on AI and not learning to be algorithmically literate.”\\n\\n§ An engineer and chief operating officer said, “AI will be used to suppress rights.”\\n\\nwww.pewresearch.org\\n\\n21\\n\\nPEW RESEARCH CENTER\\n\\n§ A technology fellow for a global organization commented, “I fear that AI will control\\n\\nmany background choices with great implicating effects.”\\n\\nOther anonymous respondents commented:\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n“More will be delegated to technology – smartphones, software. People will stop thinking or caring about ‘control’ and just delegate everything to ‘the system.’” “You can deploy most any technology in ways that enhance freedom [and] autonomy [or] have the opposite effect.” “With China aiming to ‘win’ the AI lead, I have serious doubts that any benefits will outweigh the negative effects on human rights for a majority of people.” “AI is not intelligent, it is human-made, and therefore biased and unreliable, it cannot do now what it is claimed it can do.” “Provided we are still locked in capitalism I do not see how technology will help people stay engaged and empowered in our society.”\\n\\n§ “My fear is that AI will be developed too quickly and that there may be severe\\n\\nrepercussions once the genie is out of the bottle.”\\n\\nSurveillance and data systems designed primarily for efficiency, profit and control are inherently dangerous\\n\\nWho decides what about people’s code-defined lives, when, where, why and how? Many of these respondents cited concerns that the future of AI will be shaped by those driven by profit motives and power thirst. They note that many AI tools rely on individuals’ sharing of information, preferences, search strategies and data. Human values and ethics are not necessarily baked into the systems making peoples’ decisions for them. These experts worry that data-based decision- making can be prone to errors, biases, and false logic or mistaken assumptions. And these experts argue that machine-based decisions often favor “efficiencies” in the name of profit or power that are extremely unfavorable to individuals and the betterment of the human condition.\\n\\nMichael Kleeman, a senior fellow at the University of California, San Diego and board member at the Institute for the Future, wrote, “The utilization of AI will be disproportionate and biased toward those with more resources. In general, it will reduce autonomy, and, coupled with big data, it will reduce privacy and increase social control. There will be some areas where IA [intelligence augmentation] helps make things easier and safer, but by and large it will be a global net negative.”\\n\\nA professor at a major U.S. university and expert in artificial intelligence as applied to social computing said, “As AI systems take in more data and make bigger decisions, people\\n\\nwww.pewresearch.org\\n\\n22\\n\\nPEW RESEARCH CENTER\\n\\nwill be increasingly subject to their unaccountable decisions and non-auditable surveillance practices. The trends around democratic governance of AI are not encouraging. The big players are U.S.-based, and the U.S. is in an anti-regulation stance that seems fairly durable. Therefore, I expect AI technologies to evolve in ways that benefit corporate interests, with little possibility of meaningful public response.”\\n\\nJustin Reich, executive director of MIT Teaching Systems Lab and research scientist in the MIT Office of Digital Learning, responded, “Systems for human-AI collaborations will be built by powerful, affluent people to solve the problems of powerful, affluent people. In the hands of autocratic leaders, AI will become a powerful tool of surveillance and control. In capitalist economies, human-AI collaboration will be deployed to find new, powerful ways of surveilling and controlling workers for the benefit of more-affluent consumers.”\\n\\nSeth Finkelstein, consulting programmer at Finkelstein Consulting and EFF Pioneer Award winner, commented, “AI depends on algorithms and data. Who gets to code the algorithms and to challenge the results? Is the data owned as private property, and who can change it? As a very simple example, let’s take the topic of algorithmic recommendations for articles to read. Do they get tuned to produce suggestions which lead to more informative material – which, granted, is a relatively difficult task, and fraught with delicate determinations? Or are they optimized for ATTENTION! CLICKS! *OUTRAGE*!? To be sure, the latter is cheap and easy – and though it has its own share of political problems, they’re often more amenable to corporate management (i.e., what’s accurate vs. what’s unacceptable). There's a whole structure of incentives that will push toward one outcome or the other.”\\n\\nDouglas Rushkoff, professor of media at City University of New York, responded, “The main reason I believe AI’s impact will be mostly negative is that we will be applying it mostly toward the needs of the market, rather than the needs of human beings. So while AI might get increasingly good at extracting value from people, or manipulating people’s behavior toward more consumption and compliance, much less attention will likely be given to how AI can actually create value for people. Even the most beneficial AI is still being measured in terms of its ability to provide utility, value or increase in efficiency – fine values, sure, but not the only ones that matter to quality of life.”\\n\\nAnnalie Killian, futurist and vice president for strategic partnerships at Sparks & Honey, wrote, “More technology does not make us more human; we have evidence for that now within 10 years of combining the smartphone device with persuasive and addictive designs that shape and hijack behavior. Technologists who are using emotional analytics, image-modification technologies and other hacks of our senses are destroying the fragile fabric of trust and truth that is holding our\\n\\nwww.pewresearch.org\\n\\n23\\n\\nPEW RESEARCH CENTER\\n\\nsociety together at a rate much faster than we are adapting and compensating – let alone comprehending what is happening. The sophisticated tech is affordable and investible in the hands of very few people who are enriching themselves and growing their power exponentially, and these actors are NOT acting in the best interest of all people.”\\n\\nCollin Baker, senior AI researcher at the International Computer Science Institute at the University of California, Berkeley, commented, “I fear that advances in AI will be turned largely to the service of nation states and mega-corporations, rather than be used for truly constructive purposes. The positive potential, particularly in education and health care, is enormous, but people will have to fight to make it come about. … I hope that AI will get much better at understanding Gricean maxims for cooperative discourse and at understanding people's beliefs, intentions and plans.”\\n\\nBrian Harvey, lecturer on the social implications of computer technology at the University of California, Berkeley, said, “The question makes incorrect presuppositions, encapsulated in the word ‘we.’ There is no we; there are the owners and the workers. The owners (the 0.1%) will be better off because of AI. The workers (bottom 95%) will be worse off, as long as there are owners to own the AI, same as for any piece of technology.”\\n\\nOne of the world’s foremost social scientists studying human-technology interactions said, “My chief fear is face-recognition used for social control. Even Microsoft has begged for government regulation! Surveillance of all kinds is the future for AI. It is not benign if not controlled!”\\n\\nDevin Fidler, futurist and founder of Rethinkery Labs commented, “If earlier industrialization is any guide, we may be moving into a period of intensified creative destruction as AI technologies become powerful enough to overturn the established institutions and the ordering systems of modern societies. If the holes punched in macro-scale organizational systems are not explicitly addressed and repaired, there will be increased pressures on everyday people as they face not only the problems of navigating an unfamiliar new technology landscape themselves, but also the systemic failure of institutions they rely on that have failed to adapt.”\\n\\nAn anonymous respondent said, “My fear is that technology will further separate us from what makes us human and sensitive to others. My hope is that technology would be used to improve the quality of living, not supplant it. Much of the AI innovation is simply clogging our senses, stealing our time, increasing the channels and invasion of adverts. This has destroyed our phones, filled our mailboxes and crowded our email. No product is worth that level of incursion.”\\n\\nwww.pewresearch.org\\n\\n24\\n\\nPEW RESEARCH CENTER\\n\\nPaola Perez, vice president of the Internet Society’s Venezuela chapter and chair of the LACNIC Public Policy Forum, responded, “Humans will be better with AI. Many problems will be solved, but many jobs are going to disappear, and there may be more poor people as a result. Will we see life extension? Maybe, and maybe not, because our dependence on technology may also be destructive to our health.”\\n\\nEliot Lear, principal engineer at Cisco Systems, predicted, “AI and tech will not leave most people better off than they are today. As always, technology outpaces our ability to understand its ramifications so as to properly govern its use. I have no reason to believe that we will have caught up by 2030.”\\n\\nOlivia Coombe, a respondent who provided no identifying details, wrote, “Children learn from their parents. As AI systems become more complex and are given increasingly important roles in the functioning of day-to-day life, we should ask ourselves what are we teaching our artificial digital children? If we conceive and raise them in a world of individual self-interest, will they just strengthen these existing, and often oppressive, systems of capitalist competition? Or could they go their own way, aspiring to a life of entrepreneurship to collaboration? Worse yet, will they see the reverence we hold for empires and seek to build their own through conquest?”\\n\\nPeter Asaro, a professor at The New School and philosopher of science, technology and media who examines artificial intelligence and robotics, commented, “AI will produce many advantages for many people, but it will also exacerbate many forms of inequality in society. It is likely to benefit a small group who design and control the technology greatly, benefit a fairly larger group of the already well-off in many ways, but also potentially harm them in other ways, and for the vast majority of people in the world it will offer few visible benefits and be perceived primarily as a tool of the wealthy and powerful to enhance their wealth and power.”\\n\\nMark Deuze, a professor of media studies at the University of Amsterdam, wrote, “With the advances in AI and tech, the public debate grows over their impact. It is this debate that will contribute to the ethical and moral dimensions of AI, hopefully inspiring a society-wide discussion on what we want from tech and how we will take responsibility for that desire.”\\n\\nRob Frieden, professor and Pioneers Chair in Telecommunications and Law at Penn State University, said, “Any intelligent system depends on the code written to support it. If the code is flawed, the end product reflects those flaws. An old-school acronym spells this out: GIGO, Garbage In, Garbage Out. I have little confidence that AI can incorporate any and every real-world scenario, even with likely developments in machine learning. As AI expands in scope and reach, defects will have ever increasing impacts, largely on the negative side of the ledger.”\\n\\nwww.pewresearch.org\\n\\n25\\n\\nPEW RESEARCH CENTER\\n\\nAnthony Judge, author, futurist, editor of the Encyclopedia of World Problems and Human Potential, and former head of the Union of International Associations, said, “AI will offer greater possibilities. My sense is that it will empower many (most probably 1% to 30%) and will disempower many (if not 99%). Especially problematic will be the level of complexity created for the less competent (notably the elderly) as is evident with taxation and banking systems – issues to which sysadmins are indifferent. For some it will be a boon – proactive companions (whether for quality dialogue or sex). Sysadmins will build in unfortunate biases. Missing will be the enabling of interdisciplinarity – as has long been possible but carefully designed out for the most dubious divide-and-rule reasons. Blinkered approaches and blind spots will set the scene for unexpected disasters – currently deniably incomprehensible (Black Swan effect). Advantages for governance will be questionable. Better oversight will be dubiously enabled.”\\n\\nStephanie Perrin, president of Digital Discretion, a data-privacy consulting firm, wrote, “There is a likelihood that, given the human tendency to identify risk when looking at the unknown future, AI will be used to attempt to predict risk. In other words, more and deeper surveillance will be used to determine who is a good citizen (purchaser, employee, student, etc.) and who [is] bad. This will find itself into public-space surveillance systems, employee-vetting systems (note the current court case where LinkedIn is suing data scrapers who offer to predict ‘flight risk’ employees), and all kinds of home-management systems and intelligent cars. While this might possibly introduce a measure of safety in some applications, the impact of fear that comes with unconscious awareness of surveillance will have a severe impact on creativity and innovation. We need that creativity as we address massive problems in climate change and reversing environmental impacts, so I tend to be pessimistic about outcomes.”\\n\\nAlistair Knott, an associate professor specializing in cognitive science and AI at the University of Otago in Dunedin, New Zealand, wrote “AI has the potential for both positive and negative impacts on society. [Negative impacts are rooted in] the current dominance of transnational companies (and tech companies in particular) in global politics. These companies are likely to appropriate the majority of advances in AI technology – and they are unlikely to spread the benefit of these advances throughout society. We are currently witnessing an extraordinary concentration of wealth in the hands of a tiny proportion of the world’s population. This is largely due to the mainstreaming of neoliberalism in the world’s dominant economies – but it is intensified by the massive success of tech companies, which achieve huge profits with relatively small workforces. The advance of AI technologies is just going to continue this trend, unless quite draconian political changes are effected that bring transnational companies under proper democratic control.”\\n\\nRichard Forno, of the Center for Cybersecurity at the University of Maryland-Baltimore County, wrote, “AI is only as ‘smart’ and efficient as its human creators can make it. If AI in things like\\n\\nwww.pewresearch.org\\n\\n26\\n\\nPEW RESEARCH CENTER\\n\\nFacebook algorithms is causing this much trouble now, what does the future hold? The problem is less AI’s evolution and more about how humankind develops and uses it – that is where the real crisis in AI will turn out.”\\n\\nSam Punnett, research and strategy officer at TableRock Media, wrote, “The preponderance of AI-controlled systems are designed to take collected data and enable control advantage. Most of the organizations with the resources to develop these systems do so to enable advantages in commercial/financial transactions, manufacturing efficiency and surveillance. Self-regulation by industry has already been shown to fail (e.g., social media platforms and Wall Street). Government agencies are lagging in their will and understanding of the implications of the technology to effectively implement guidelines to curtail the impacts of unforeseen circumstances. As such, government participation will be reactive to the changes that the technology will bring. My greatest fear is a reliance on faulty algorithms that absolve responsibility while failing to account for exceptions.”\\n\\nLuis Pereira, associate professor of electronics and nanotechnologies, Universidade NOVA de Lisboa, Portugal, responded, “I fear that more control and influence will be exerted on people, such as has started in China. There will be a greater wealth gap, benefits will not spread to all and a caste system will develop, unless a new social compact is put in place, which is unlikely. Widespread revolt is plausible.”\\n\\nStavros Tripakis, an associate professor of computer science at Aalto University in Finland and adjunct professor at the University of California, Berkeley, wrote, “‘1984,’ George Orwell, police state.”\\n\\nA principal architect for a top-five technology company commented, “AI will enable vicious regimes to track citizens at all times. Mistaken identifications will put innocent people in jail and even execute them with no hope of appeal. In general, AI will only have a positive contribution in truly democratic states, which are dwindling in number.”\\n\\nJohn Sniadowski, a director for a technology company, wrote, “As technology is currently instantiated it simply concentrates power into a smaller number of international corporations. That needs fixing for everyone to gain the best from AI.”\\n\\nDavid Brake, senior lecturer in communications at the University of Bedfordshire, UK, said, “Like many colleagues I fear that AI will be framed as ‘neutral’ and ‘objective’ and thereby used as cover to make decisions that would be considered unfair if made by a human. If we do not act to properly regulate the use of AI we will not be able to interrogate the ways that AI decision-making\\n\\nwww.pewresearch.org\\n\\n27\\n\\nPEW RESEARCH CENTER\\n\\nis constructed or audit them to ensure their decisions are indeed fair. Decisions may also be made (even more than today) based on a vast array of collected data and if we are not careful we will be unable to control the flows of information about us used to make those decisions or to correct misunderstandings or errors which can follow us around indefinitely. Imagine being subject to repeated document checks as you travel around the country because you know a number of people who are undocumented immigrants and your movements therefore fit the profile of an illegal immigrant. And you are not sure whether to protest because you don’t know whether such protests could encourage an algorithm to put you into a ‘suspicious’ category which could get you harassed even more often.”\\n\\nA longtime veteran of a pioneering internet company commented, “Profit motive and AI at scale nearly guarantee suffering for most people. It should be spiffy for the special people with wealth and power, though. Watching how machines are created to ensure addiction (to deliver ads) is a reminder that profit-driven exploitation always comes first. The push for driverless cars, too, is a push for increased profits.”\\n\\nJoshua Loftus, assistant professor of information, operations and management sciences at New York University and co-author of “Counterfactual Fairness in Machine Learning,” commented, “How have new technologies shaped our lives in the past? It depends on the law, market structure and who wields political power. In the present era of extreme inequality and climate catastrophe, I expect technologies to be used by employers to make individual workers more isolated and contingent, by apps to make users more addicted on a second-by-second basis, and by governments for surveillance and increasingly strict border control.”\\n\\nEugene H. Spafford, internet pioneer and founder and executive director emeritus of the Center for Education and Research in Information Assurance and Security, commented, “Without active controls and limits, the primary adopters of AI systems will be governments and large corporations. Their use of it will be to dominate/control people, and this will not make our lives better.”\\n\\nMichael Muller, a researcher in the AI interactions group for a global technology solutions provider, said it will leave some people better off and others not, writing, “For the wealthy and empowered, AI will help them with their daily lives – and it will probably help them to increase their wealth and power. For the rest of us, I anticipate that AI will help the wealthy and empowered people to surveil us, to manipulate us, and (in some cases) to control us or even imprison us. For those of us who do not have the skills to jump to the AI-related jobs, I think we will find employment scarce and without protections. In my view, AI will be a mixed and intersectional blessing at best.”\\n\\nwww.pewresearch.org\\n\\n28\\n\\nPEW RESEARCH CENTER\\n\\nEstee Beck, assistant professor at the University of Texas at Arlington and author of “A Theory of Persuasive Computer Algorithms for Rhetorical Code Studies,” responded, “Tech design and policy affects our privacy in the United States so much so that most people do not think about the tracking of movements, behaviors and attitudes from smartphones, social media, search engines, ISPs [internet service providers] and even Internet of Things-enabled devices. Until tech designers and engineers build privacy into each design and policy decision for consumers, any advances with human-machine/AI collaboration will leave consumers with less security and privacy.”\\n\\nMichael H. Goldhaber, an author, consultant and theoretical physicist who wrote early explorations on the digital attention economy, said, “For those without internet connection now, its expansion will probably be positive overall. For the rest we will see an increasing arms race between uses of control, destructive anarchism, racism, etc., and ad hoc, from-below efforts at promoting social and environmental good. Organizations and states will seek more control to block internal or external attacks of many sorts. The combined struggles will take up an increasing proportion of the world’s attention, efforts and so forth. I doubt that any very viable and democratic, egalitarian order will emerge over the next dozen years, and – even in a larger time frame – good outcomes are far from guaranteed.”\\n\\nDave Burstein, editor and publisher at Fast Net News, said, “There’s far too much second-rate AI that is making bad decisions based on inadequate statistical understanding. For example, a parole or sentencing AI probably would find a correlation between growing up in a single parent household and likelihood of committing another crime. Confounding variables, like the poverty of so many single mothers, need to be understood and dealt with. I believe it’s wrong for someone to be sent to jail longer because their father left. That kind of problem, confounding variables and the inadequacy of ‘preponderant’ data, is nearly ubiquitous in AI in practice.”\\n\\nIan Peter, pioneer internet activist and internet rights advocate, said, “Personal data accumulation is reaching a point where privacy and freedom from unwarranted surveillance are disappearing. In addition, the algorithms that control usage of such data are becoming more and more complex leading to inevitable distortions. Henry Kissinger may have not been far off the mark when he described artificial intelligence as leading to ‘The End of the Age of Enlightenment.’”\\n\\nMichael Zimmer, associate professor and privacy and information ethics scholar at the University of Wisconsin, Milwaukee, commented, “I am increasingly concerned that AI-driven decision making will perpetuate existing societal biases and injustices, while obscuring these harms under the false belief such systems are ‘neutral.’”\\n\\nwww.pewresearch.org\\n\\n29\\n\\nPEW RESEARCH CENTER\\n\\nMartin Shelton, a professional technologist, commented, “There are many kinds of artificial intelligence – some kinds reliant on preset rules to appear ‘smart,’ and some which respond to changing conditions in the world. But because AI can be used anywhere we can recognize patterns, the potential uses for artificial intelligence are pretty huge. The question is, how will it be used? … While these tools will become cheaper and more widespread, we can expect that – like smartphones or web connectivity – their uses will be primarily driven by commercial interests. We’re beginning to see the early signs of AI failing to make smart predictions in larger institutional contexts. If Amazon fails to correctly suggest the right product in the future, everything is fine. You bought a backpack once, and now Amazon thinks you want more backpacks, forever. It’ll be okay. But sometimes these decisions have enormous stakes. ProPublica documented how automated ‘risk-assessment’ software used in U.S. courtroom sentencing procedures is only slightly more accurate at predicting recidivism than the flip of a coin. Likewise, hospitals using IBM Watson to make predictions about cancer treatments find the software often gives advice that humans would not. To mitigate harm in high-stakes situations, we must critically interrogate how our assumptions about our data and the rules that we use to create our AI promote harm.”\\n\\nNigel Hickson, an expert on technology policy development for ICANN based in Brussels, responded, “I am optimistic that AI will evolve in a way that benefits society by improving processes and giving people more control over what they do. This will only happen though if the technologies are deployed in a way in which benefits all. My fear is that in non-democratic countries, AI will lessen freedom, choice and hope.”\\n\\nVian Bakir, a professor of political communication and journalism at Bangor University, responded, “I am pessimistic about the future in this scenario because of what has happened to date with AI and data surveillance. For instance, the recent furor over fake news/disinformation and the use of complex data analytics in the U.K.’s 2016 Brexit referendum and in the U.S. 2016 presidential election. To understand, influence and micro-target people in order to try get them to vote a certain way is deeply undemocratic. It shows that current political actors will exploit technology for personal/political gains, irrespective of wider social norms and electoral rules. There is no evidence that current bad practices would not be replicated in the future, especially as each new wave of technological progress outstrips regulators’ ability to keep up, and people’s ability to comprehend what is happening to them and their data. Furthermore, and related, the capabilities of mass dataveillance in private and public spaces is ever-expanding, and their uptake in states with weak civil society organs and minimal privacy regulation is troubling. In short, dominant global technology platforms show no signs of sacrificing their business models that depend on hoovering up ever more quantities of data on people’s lives then hyper-targeting them with commercial messages; and across the world, political actors and state security and\\n\\nwww.pewresearch.org\\n\\n30\\n\\nPEW RESEARCH CENTER\\n\\nintelligence agencies then also make use of such data acquisitions, frequently circumventing privacy safeguards or legal constraints.”\\n\\nTom Slee, senior product manager at SAP SE and author of “What’s Yours is Mine: Against the Sharing Economy,” wrote, “Many aspects of life will be made easier and more efficient by AI. But moving a decision such as health care or workplace performance to AI turns it into a data-driven decision driven by optimization of some function, which in turn demands more data. Adopting AI- driven insurance ratings, for example, demands more and more lifestyle data from the insured if it is to produce accurate overall ratings. Optimized data-driven decisions about our lives unavoidably require surveillance, and once our lifestyle choices become input for such decisions we lose individual autonomy. In some cases we can ignore this data collection, but we are in the early days of AI-driven decisions: By 2030 I fear the loss will be much greater. I do hope I am wrong.”\\n\\nTimothy Graham, a postdoctoral research fellow in sociology and computer science at Australian National University, commented, “There is already an explosion of research into ‘fairness and representation’ in ML (and conferences such as Fairness, Accountability and Transparency in Machine Learning), as it is difficult to engineer systems that do not simply reproduce existing social inequality, disadvantage and prejudice. Deploying such systems uncritically will only result in an aggregately worse situation for many individuals, whilst a comparatively small number benefit.”\\n\\nA senior researcher and programmer for a major global think tank commented, “I expect AI to be embedded in systems, tools, etc., to make them more useful. However, I am concerned that AI’s role in decision-making will lead to more-brittle processes where exceptions are more difficult than today – this is not a good thing.”\\n\\nJenni Mechem, a respondent who provided no identifying details, said, “My two primary reasons for saying that advances in AI will not benefit most people by 2030 are, first, there will continue to be tremendous inequities in who benefits from these advances, and second, if the development of AI is controlled by for-profit entities there will be tremendous hidden costs and people will yield control over vast areas of their lives without realizing it. … The examples of Facebook as a faux community commons bent on extracting data from its users and of pervasive internet censoring in China should teach us that neither for-profit corporations nor government can be trusted to guide technology in a manner that truly benefits everyone. Democratic governments that enforce intelligent regulations as the European Union has done on privacy may offer the best hope.”\\n\\nwww.pewresearch.org\\n\\n31\\n\\nPEW RESEARCH CENTER\\n\\nSuso Baleato, a fellow at Harvard University’s Institute of Quantitative Social Science and liaison for the Organization for Economic Cooperation and Development (OECD)’s Committee on Digital Economy Policy, commented, “The intellectual property framework impedes the necessary accountability of the underlying algorithms, and the lack of efficient redistributive economic policies will continue amplifying the bias of the datasets.”\\n\\nSasha Costanza-Chock, associate professor of civic media at MIT, said, “Unfortunately it is most likely that AI will be deployed in ways that deepen existing structural inequality along lines of race, class, gender, ability and so on. A small portion of humanity will benefit greatly from AI, while the vast majority will experience AI through constraints on life chances. Although it’s possible for us to design AI systems to advance social justice, our current trajectory will reinforce historic and structural inequality.”\\n\\nDalsie Green Baniala, CEO and regulator of the Telecommunications and Radiocommunications Regulator of Vanuatu, wrote, “Often, machine decisions do not produce an accurate result, they do not meet expectations or specific needs. For example, applications are usually invented to target the developed-world market. They may not work appropriately for countries like ours – small islands separated by big waters.”\\n\\nMichiel Leenaars, director of strategy at NLnet Foundation and director of the Internet Society’s Netherlands chapter, responded, “Achieving trust is not the real issue; achieving trustworthiness and real empowerment of the individual is. As the technology that to a large extent determines the informational self disappears – or in practical terms is placed out of local control, going ‘underground’ under the perfect pretext of needing networked AI – the balance between societal well-being and human potential on the one hand and corporate ethics and opportunistic business decisions on the other stands to be disrupted. Following the typical winner-takes-all scenario the internet is known to produce, I expect that different realms of the internet will become even less transparent and more manipulative. For the vast majority of people (especially in non-democracies) there already is little real choice but to move and push along with the masses.”\\n\\nMike O'Connor, a retired technologist who worked at ICANN and on national broadband issues, commented, “I'm feeling ‘internet-pioneer regret’ about the Internet of S*** that is emerging from the work we’ve done over the last few decades. I actively work to reduce my dependence on internet-connected devices and the amount of data that is collected about me and my family. I will most certainly work equally hard to avoid human/AI devices/connections. I earnestly hope that I’m resoundingly proven wrong in this view when 2030 arrives.”\\n\\nwww.pewresearch.org\\n\\n32\\n\\nPEW RESEARCH CENTER\\n\\nLuke Stark, a fellow in the department of sociology at Dartmouth College and at Harvard University’s Berkman Klein Center for Internet & Society, wrote, “AI technologies run the risk of providing a comprehensive infrastructure for corporate and state surveillance more granular and all-encompassing than any previous such regime in human history.”\\n\\nZoetanya Sujon, a senior lecturer specializing in digital culture at the University of the Arts London, commented, “Like the history of so many technologies show us, AI will not be the magic solution to the world’s problems or to symbolic and economic inequalities. Instead, AI is most benefitting those with the most power.”\\n\\nLarry Lannom, internet pioneer and vice president at the Corporation for National Research Initiatives (CNRI), said, “I am hopeful that networked human-machine interaction will improve the general quality of life. ... My fear: Will all of the benefits of more-powerful artificial intelligence benefit the human race as a whole or simply the thin layer at the top of the social hierarchy that owns the new advanced technologies?”\\n\\nA professor and researcher in AI based in Europe noted, “Using technological AI-based capabilities will give people the impression that they have more power and autonomy. However, those capabilities will be available in contexts already framed by powerful companies and states. No real freedom. For the good and for the bad.”\\n\\nAn anonymous respondent said, “In the area of health care alone there will be tremendous benefits for those who can afford medicine employing AI. But at the same time, there is an enormous potential for widening inequality and for abuse. We can see the tip of this iceberg now with health insurance companies today scooping up readily available, poorly protected third-party data that will be used to discriminate.”\\n\\nA senior data analyst and systems specialist expert in complex networks responded, “Artificial intelligence software will implement the priorities of the entities that funded development of the software. In some cases, this will [be] a generic service sold to the general public (much as we now have route-planning software in GPS units), and this will provide a definite benefit to consumers. In other cases, software will operate to the benefit of a large company but to the detriment of consumers (for example, calculating a price for a product that will be the highest that a given customer is prepared to pay). In yet a third category, software will provide effective decision-making in areas ranging from medicine to engineering, but will do so at the cost of putting human beings out of work.”\\n\\nwww.pewresearch.org\\n\\n33\\n\\nPEW RESEARCH CENTER\\n\\nA distinguished engineer at one of the world’s largest computing hardware companies commented, “Tech will continue to be integrated into our lives in a seamless way. My biggest concern is responsible gathering of information and its use. Information can be abused in many ways as we are seeing today.”\\n\\nA digital rights activist commented, “AI is already (through racial recognition, in particular) technologically laundering longstanding and pervasive bias in the context of police surveillance. Without algorithmic transparency and transparency into training data, AIs can be bent to any purpose.”\\n\\nThe following one-liners from anonymous respondents also tie into this theme:\\n\\n§ A longtime economist for a top global technology company predicted, “The\\n\\ndecline of privacy and increase in surveillance.”\\n\\n§ A journalist and leading internet activist wrote, “Computer AI will only be beneficial\\n\\nto its users if it is owned by humans, and not ‘economic AI’ (that is, corporations).”\\n\\n§ A strategy consultant wrote, “The problem is one of access. AI will be used to\\n\\nconsolidate power and benefits for those who are already wealthy and further surveil, disenfranchise and outright rob the remaining 99% of the world.”\\n\\n§ A policy analyst for a major internet services provider said, “We just need to be\\n\\ncareful about what data is being used and how.”\\n\\n§ A professor of information science wrote, “Systems will be developed that do not\\n\\nprotect people’s privacy and security.”\\n\\n§ The founder of a technology research firm wrote, “Neoliberal systems function to\\n\\nprivilege corporations over individual rights, thus AI will be used in ways to restrict, limit, categorize – and, yes, it will also have positive benefits.”\\n\\n§ A professor of electrical and computer engineering based in Europe\\n\\ncommented, “The problem lies in human nature. The most powerful will try to use AI and technology to increase their power and not to the benefit of society.”\\n\\nOther anonymous respondents commented:\\n\\n§ §\\n\\n§\\n\\n“The panopticon and invasion of all personal aspects of our lives is already complete.” “AI will allow greater control by the organized forces of tyranny, greater exploitation by the organized forces of greed and open a Pandora’s box of a future that we as a species are not mature enough to deal with.” “The combination of widespread device connectivity and various forms of AI will provide a more pleasant everyday experience but at the expense of an even further loss of privacy.”\\n\\nwww.pewresearch.org\\n\\n34\\n\\nPEW RESEARCH CENTER\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n“I have two fears 1) loss of privacy and 2) building a ‘brittle’ system that fails catastrophically.” “AI strategic decisions with the most clout are made by corporations and they do not aim for human well-being in opposition to corporate profitability.” “Data is too controlled by corporations and not individuals, and privacy is eroding as surveillance and stalking options have grown unchecked.” “The capabilities are not shared equally, so the tendency will be toward surveillance by those with power to access the tools; verbal and visual are coming together with capacities to sort and focus the masses of data.” “Knowing humanity, I assume particularly wealthy, white males will be better off, while the rest of humanity will suffer from it.”\\n\\nDisplacement of human jobs by AI will widen economic and digital divides, possibly leading to economic and social upheaval\\n\\nOne of the chief fears about today’s technological change is the possibility that autonomous hardware and software systems will cause millions of people globally to lose their jobs and, as a result, their means for affording life’s necessities and participating in society. Many of these experts say new jobs will emerge along with the growth of AI just as they have historically during nearly every human transition to new tools.\\n\\nBrad Templeton, chair for computing at Singularity University, said, “While obviously there will be good and bad, the broad history of automation technologies is positive, even when it comes to jobs. There is more employment today than ever in history.”\\n\\nBen Shneiderman, distinguished professor and founder of the Human Computer Interaction Lab at the University of Maryland, said, “Automation is largely a positive force, which increases productivity, lowers costs and raises living standards. Automation expands the demand for services, thereby raising employment, which is what has happened at Amazon and FedEx. My position is contrary to those who believe that robots and artificial intelligence will lead to widespread unemployment. Over time I think AI/machine learning strategies will become merely tools embedded in ever-more-complex technologies for which human control and responsibility will become clearer.”\\n\\nRobert D. Atkinson, president of the Information Technology and Innovation Foundation, wrote about how advances in AI are essential to expanded job opportunities: “The developed world faces an unprecedented productivity slowdown that promises to limit advances in living\\n\\nwww.pewresearch.org\\n\\n35\\n\\nPEW RESEARCH CENTER\\n\\nstandards. AI has the potential to play an important role in boosting productivity and living standards.”\\n\\nToby Walsh, a professor of AI at the University of New South Wales in Australia and president of the AI Access Foundation, said, “I'm pessimistic in short term – we're seeing already technologies like AI being used to make life worse for many – but I'm optimistic in long term that we’ll work out how to get machines to do the dirty, dull, dangerous and difficult, and leave us free to focus on all the more-important and human parts of our lives.”\\n\\nYet many others disagree. Some fear the collapse of the middle class and social and economic upheaval if most of the world’s economic power is held by a handful of technology behemoths that are reaping the great share of financial rewards in the digital age while employing far fewer people than the leading companies of the industrial age. A fairly large share of these experts warn that if steps are not taken now to adjust to this potential future that AI’s radical reduction in human work will be devastating.”\\n\\nDavid Cake, an leader with Electronic Frontiers Australia and vice-chair of the ICANN Generic Names Supporting Organization Council, wrote, “The greatest fear is that the social disruption due to changing employment patterns will be handled poorly and lead to widespread social issues.”\\n\\nJerry Michalski, founder of the Relationship Economy eXpedition, said, “We’re far from tipping into a better social contract. In a more-just world, AI could bring about utopias. However, many forces are shoving us in the opposite direction. 1) Businesses are doing all they can to eliminate full-time employees, who get sick and cranky, need retirement accounts and raises, while software gets better and cheaper. The precariat will grow. 2) Software is like a flesh-eating bacterium: Tasks it eats vanish from the employment landscape. Unlike previous technological jumps, this one unemploys people more quickly than we can retrain and reemploy them. 3) Our safety net is terrible and our beliefs about human motivations suck. 4) Consumerism still drives desires and expectations.”\\n\\nJames Hendler, professor of computer, web and cognitive sciences and director of the Rensselaer Polytechnic Institute for Data Exploration and Application, wrote, “I believe 2030 will be a point in the middle of a turbulent time when AI is improving services for many people, but it will also be a time of great change in society based on changes in work patterns that are caused, to a great degree, by AI. On the one hand, for example, doctors will have access to information that is currently hard for them to retrieve rapidly, resulting in better medical care for those who have coverage, and indeed in some countries the first point of contact in a medical situation may be an AI, which will help with early diagnoses/prescriptions. On the other hand, over the course of a\\n\\nwww.pewresearch.org\\n\\n36\\n\\nPEW RESEARCH CENTER\\n\\ncouple of generations, starting in the not-too-distant future we will see major shifts in work force with not just blue-collar jobs, but also many white-collar jobs lost. Many of these will not be people ‘replaced’ by AIs, but rather the result of a smaller number of people being able to accomplish the same amount of work – for example in professions such as law clerks, physicians assistants and many other currently skilled positions we would project a need for less people (even as demand grows).”\\n\\nBetsy Williams, a researcher at the Center for Digital Society and Data Studies at the University of Arizona, wrote, “AI’s benefits will be unequally distributed across society. Few will reap meaningful benefits. Large entities will use AI to deliver marginal improvements in service to their clients, at the cost of requiring more data and risking errors. Employment trends from computerization will continue. AI will threaten medium-skill jobs. Instead of relying on human expertise and context knowledge, many tasks will be handled directly by clients using AI interfaces or by lower-skilled people in service jobs, boosted by AI. AI will harm some consumers. For instance, rich consumers will benefit from self-driving cars, while others must pay to retrofit existing cars to become more visible to the AI. Through legal maneuvering, self-driving car companies will avoid many insurance costs and risks, shifting them to human drivers, pedestrians and bicyclists. In education, creating high quality automated instruction requires expertise and money. Research on American K-12 classrooms suggests that typical computer-aided instruction yields better test scores than instruction by the worst teachers. By 2030, most AI used in education will be of middling quality (for some, their best alternative). The children of the rich and powerful will not have AI used on them at school; instead, they will be taught to use it. For AI to significantly benefit the majority, it must be deployed in emergency health care (where quicker lab work, reviews of medical histories or potential diagnoses can save lives) or in aid work (say, to coordinate shipping of expiring food or medicines from donors to recipients in need).”\\n\\nNathaniel Borenstein, chief scientist at Mimecast, wrote, “Social analyses of IT [information technology] trends have consistently wildly exaggerated the human benefits of that technology, and underestimated the negative effects. ... I foresee a world in which IT and so-called AI produce an ever-increasing set of minor benefits, while simultaneously eroding human agency and privacy and supporting authoritarian forms of governance. I also see the potential for a much worse outcome in which the productivity gains produced by technology accrue almost entirely to a few, widening the gap between the rich and poor while failing to address the social ills related to privacy. But if we can find a way to ensure that these benefits are shared equally among the population, it might yet prove to be the case that the overall effect of the technology is beneficial to humanity. This will only happen, however, if we manage to limit the role of the rich in determining how the fruits of increased productivity will be allocated.”\\n\\nwww.pewresearch.org\\n\\n37\\n\\nPEW RESEARCH CENTER\\n\\nAndrea Romaoli Garcia, an international lawyer active in internet governance discussions, commented, “AI will improve the way people make decisions in all industries because it allows instant access to a multitude of information. People will require training for this future – educational and technological development. … This is a very high level of human development that poor countries don't have access to. Without proper education and policies, they will not have access to wealth. The result may be a multitude of hungry and desperate people. This may be motivation for wars or invasion of borders. Future human-machine interaction (AI) will only be positive if richer countries develop policies to help poorer countries to develop and gain access to work and wealth.”\\n\\nJosh Calder, a partner at the Foresight Alliance, commented, “The biggest danger is that workers are displaced on a mass scale, especially in emerging markets.”\\n\\nJeff Johnson, computer science professor at the University of San Francisco, previously with Xerox, HP Labs and Sun Microsystems, responded, “I believe advances in AI will leave many more people without jobs, which will increase the socioeconomic differences in society, but other factors could help mitigate this, e.g., adoption of guaranteed income.”\\n\\nAlan Bundy, a professor of automated reasoning at the University of Edinburgh, wrote, “Unskilled people will suffer because there will be little employment for them. This may create disruption to society, some of which we have already seen with Trump, Brexit, etc.”\\n\\nPeter Levine, associate dean for research and professor of citizenship and public affairs in Tufts University’s Tisch College of Civic Life, wrote, “Being a fully-fledged citizen has traditionally depended on work. I’m worried that rising levels of non-employment will detract from civic engagement. Also, AI is politically powerful and empowers the people and governments that own it. Thus, it may increase inequality and enhance authoritarianism.”\\n\\nHassaan Idrees, an electrical engineer and Fulbright Scholar active in creating energy systems for global good, commented, “I believe human-machine interaction will be more of [a] utility, and less fanciful as science fiction puts it. People will not need to see their physicians in person, their automated doctors making this irrelevant. Similarly, routine workplace activities like data processing and financial number crunching would be performed by AI. Humans with higher levels of intellect can survive this age, and those on the lower ends of spectrum of mental acumen would be rendered unnecessary.”\\n\\nEthem Alpaydın, a professor of computer engineering at Bogazici University in Istanbul, responded, “As with other technologies, I imagine AI will favor the developed countries that\\n\\nwww.pewresearch.org\\n\\n38\\n\\nPEW RESEARCH CENTER\\n\\nactually develop these technologies. ... For the developing countries, however, whose labor force is mostly unskilled and whose exports are largely low-tech, AI implies higher unemployment, lower income and more social unrest. The aim of AI in such countries should be to add skill to the labor force rather than supplant them.”\\n\\nSam Ladner, a former UX researcher for Amazon and Microsoft, now an adjunct professor at Ontario College of Art and Design, wrote, “Technology is not a neutral tool, but one that has our existing challenges imprinted onto it. Inequality is high and growing. Too many companies deny their employees a chance to work with dignity, whether it be through providing them meaningful things to do, or with the basic means to live. AI will be placed on top of that existing structure. Those who already have dignified work with a basic income will see that enhanced; those who are routinely infantilized or denied basic rights will see that amplified. Some may slip into that latter category because their work is more easily replaced by AI and machine learning.”\\n\\nJonathan Swerdloff, consultant and data systems specialist for Driven Inc., wrote, “The more reliant on AI we become, the more we are at the mercy of its developers. While AI has the ability to augment professionals and to make decisions, I have three concerns which make me believe it will not leave us better off by 2030. This does not address fears that anything run via AI could be hacked and changed by bad faith third parties. 1) Until any sort of self-policed AI sentience is achieved, it will suffer from a significant GIGO [garbage-in, garbage-out] problem. As AI as currently conceived only knows what it is taught, the seed sets for teaching must be thought out in detail before the tools are deployed. Based on the experience with Microsoft’s Tay and some responses I've heard from the Sophia robot, I am concerned that AI will magnify humanities flaws. 2) Disparate access. Unless the cost for developing AI drops precipitously – and it may, since one AI tool could be leveraged into building further less expensive AI tools – access to whatever advantages the tools will bring will likely be clustered among a few beneficiaries. I view this akin to high frequency trading on Wall Street. Those who can, do. Those who can’t, lose. 3) Tool of control. If AI is deployed to make civic or corporate decisions, those who control the algorithms control everything. In the U.S. we’ve recently seen Immigration and Customs Enforcement change its bond algorithm to always detain in every case.”\\n\\nStuart A. Umpleby, a professor and director of the research program in social and organizational learning at George Washington University, wrote, “People who use AI and the internet will have their lives enhanced by these technologies. People who do not use them will be increasingly disconnected from opportunities. As the digital world becomes more complicated and remote from real-world experiences, the need will grow for people and software to make connections. There will be a need for methods to distinguish the real world from the scam world.”\\n\\nwww.pewresearch.org\\n\\n39\\n\\nPEW RESEARCH CENTER\\n\\nSimeon Yates, director of the Centre for Digital Humanities and Social Science at the University of Liverpool, said, “AI will simply increase existing inequalities – it, like the internet, will fail in its emancipatory promise.”\\n\\nPanagiotis T. Metaxas, author of “Technology, Propaganda and the Limits of Human Intellect” and professor of computer science at Wellesley College, responded, “There will be a lot of wealth that AI-supported devices will be producing. The new technologies will make it easier and cheaper to produce food and entertainment massively (‘bread and circus’). This wealth will not be distributed evenly, increasing the financial gap between the top small percentage of people and the rest. Even though this wealth will not be distributed evenly, the (relatively small) share given to the vast majority of people will be enough to improve their (2018) condition. In this respect, the majority of people will be ‘better off’ than they are today. They may not feel better off if they were aware of the inequalities compared to the top beneficiaries, but they will not be aware of them due to controlled propaganda. Unfortunately, there will not be much they could do about the increased inequalities. Technologies of police enforcement by robots and lack of private communication will make it impossible for them to organize, complain or push for change. They will not be valued as workers, citizens or soldiers. The desire for democracy as we know it today will be coming to an end. Many will feel depressed, but medical products will make it easy for them to increase pleasure and decrease pain.”\\n\\nGrace Mutung’u, co-leader of the Kenya ICT Action Network, responded, “New technologies will more likely increase current inequalities unless there is a shift in world economics. From the experience of the UN work on Millennium Development Goals, while there has been improvement with the quality of life generally, low- and middle-income countries still suffer disparate inequalities. This will likely lead to governance problems. In any case, governments in these countries are investing heavily in surveillance which will likely have more negative effects on society.”\\n\\nDanny Gillane, a netizen from Lafayette, Louisiana, commented, “Technology promises so much but delivers so little. Facebook gave us the ability to stay in touch with everyone but sacrificed its integrity and our personal information in pursuit of the dollar. The promise that our medical records would be digitized and more easily shared and drive costs down still has not materialized on a global scale. The chief drivers of AI innovation and application will be for-profit companies who have shown that their altruism only extends to their bottom lines. Like most innovations, I expect AI to leave our poor even poorer and our rich even richer, increasing the numbers of the former while consolidating power and wealth in an ever-shrinking group of currently rich people.”\\n\\nwww.pewresearch.org\\n\\n40\\n\\nPEW RESEARCH CENTER\\n\\nA professional working on the setting of web standards wrote, “Looking ahead 12 years from now, I expect that AI will be enhancing the quality of life for some parts of some populations, and in some situations, while worsening the quality of life for others. AI will still be uneven in quality, and unevenly available throughout different parts of society. Privacy and security protections will be inadequate; data bias will still be common; many technologies and response patterns will be normed to the needs of the ‘common denominator’ user and misidentify or misinterpret interactions with people with disabilities or, if appropriately identifying their disability, will expose that information without user consent or control.”\\n\\nSo many people included comments and concerns about the future of jobs for humans in their wide-ranging responses to this canvassing that a later section of this report has more expert opinions on this topic.\\n\\nThe following one-liners from anonymous respondents also tie into AI and jobs:\\n\\n§ An associate professor of computer science commented, “Machines will be able to\\n\\ndo more-advanced work and improve accuracy, but this likely will expand manipulation of consumers/voters and automation may reduce available jobs.”\\n\\n§ A director for a global digital rights organization said, “My concern is that human- machine collaboration will leave some of us far better off by automating our jobs, giving us more free and creative time, while doing little to improve the lives of billions of others.” § A professor expert in cultural geography and American studies said, “Given the\\n\\nmajority human assumption that capitalism is something worth reproducing, the evacuation of most labor positions by AI would create vast poverty and cruelty by the ruling class.”\\n\\n§ A lecturer in media studies based in New Zealand wrote, “The automation of large volumes of work by machine learning-based systems is unlikely to lead to an increase in social equity within a capitalist economy.”\\n\\n§ A senior partner at one of the world’s foremost management consulting firms commented, “AI will benefit businesses, the economy and people as consumers, but likely increase income/wage polarization so most people as workers may not benefit.” § An engineer and chief operating officer for project automating code said,\\n\\n“Those with the most money will leverage their position of power through AI; it will lead to possibly cataclysmic wealth disparity.\\n\\n§ A digital anthropologist for a major global technology company wrote, “The gap\\n\\nbetween those who benefit from advances in technology and those who do not have widened over the past three decades; I can’t see an easy or quick reversal.”\\n\\nwww.pewresearch.org\\n\\n41\\n\\nPEW RESEARCH CENTER\\n\\nOther anonymous respondents commented:\\n\\n§\\n\\n§\\n\\n§\\n\\n§ §\\n\\n§\\n\\n§\\n\\n§ §\\n\\n“Some will benefit, while others will suffer. The bifurcated economy will continue to grow. … Those at the bottom of the ladder will see greater numbers of jobs being taken away by technology.” “All in all, AI can be of great use, but we need to be vigilant of the repercussions instead of constantly leaping ‘forward’ only to find out later about all of the negatives.” “In the U.S., the blue-collar job wages have been stagnant since the 1970s despite all of the advances with the internet and mobile devices, so I am not optimistic regarding AI.” “Wealth distribution will continue to widen as the rich get richer.” “AI is going to lead to the destruction of entire rungs of the economy, and the best way to boost and economy while holding together a fractured economy is war.” “Many people will no longer be useful in the labor market. Such rapid economic and social change will leave many frightened and angry.” “In 12 years AI may be more disruptive than enabling, leaving many without work until they retrain and transition.” “There could be a thinning out of the middle – middle management and class.” “AI will increasingly allow low-quality but passable substitutes for previously-skilled labor.”\\n\\n§ “There are significant implications for unskilled or easily-automated tasks on one end of the spectrum and certain types of analysis on the other that will be automated away. My concern is that we have no plan for these people as these jobs disappear.”\\n\\nIndividuals’ cognitive, social and survival skills will be diminished as they become dependent on AI\\n\\nWhile these experts expect AI to augment humans in many positive ways, some are concerned that a deepening dependence upon machine-intelligence networks will diminish crucial human capabilities. Some maintain there has already been an erosion of people’s abilities to think for themselves, to take action independent of automated systems and to interact effectively face-to- face with others.\\n\\nCharles Ess, an expert in ethics and professor with the department of media and communication at the University of Oslo, said, “It seems quite clear that evolving AI systems will bring about an extraordinary array of options, making our lives more convenient. But convenience almost always comes at the cost of deskilling – of our offloading various cognitive practices and virtues to the machines and thereby our becoming less and less capable of exercising our own agency, autonomy and most especially our judgment (phronesis). In particular, empathy and loving itself are virtues\\n\\nwww.pewresearch.org\\n\\n42\\n\\nPEW RESEARCH CENTER\\n\\nthat are difficult to acquire and enhance. My worst fears are not only severe degradation, perhaps more or less loss of such capacities – and, worst of all, our forgetting they even existed in the first place, along with the worlds they have made possible for us over most of our evolutionary and social history.”\\n\\nDaniel Siewiorek, a professor with the Human-Computer Interaction Institute at Carnegie Mellon University, predicted, “The downside: isolating people, decreasing diversity, a loss of situational awareness (witness GPS directional systems) and ‘losing the receipt’ of how to do things. In the latter case, as we layer new capabilities on older technologies if we forget how the older technology works we cannot fix it and layered systems may collapse, thrusting us back into a more-primitive time.”\\n\\nMarilyn Cade, longtime global internet policy consultant, responded, “Technology often reflects the ethics of its creators, but more significantly, those who commercialize it. Most individuals focus on how they personally use technology. They do not spend time (or even have the skills/expertise) to make judgments about the attributes of the way that technology is applied. … We must introduce and maintain a focus on critical thinking for our children/youth, so that they are capable of understanding the implications of a different fully digitized world. I love the fact that my typos are autocorrected, but I know how to spell all the words. I know how to construct a logical argument. If we don’t teach critical thinking at all points in education, we will have a 2030 world where the elites/scientists make decisions that are not even apparent to the average ‘person’ on the street/neighborhood.”\\n\\nGarland McCoy, founder and chief development officer of the Technology Education Institute, wrote, “I am an optimist at heart and so believe that, given a decade-plus, the horror that is unfolding before our eyes will somehow be understood and resolved. That said, if the suicide epidemic we are witnessing continues to build and women continue to opt out of motherhood all bets are off. I do think technology is at the core of both the pathology and choice.”\\n\\nAneesh Aneesh, professor at the University of Wisconsin, Milwaukee, said, “Socially, AI systems will automate tasks that currently require human negotiation and interaction. Unless people feel the pressure, institutionally or otherwise, to interact with each other, they – more often than not – choose not to interact. The lack of physical, embodied interaction is almost guaranteed to result in social loneliness and anomie, and associated problems such as suicide, a phenomenon already are on the rise in the United States.”\\n\\nEbenezer Baldwin Bowles, author, editor and journalist, responded, “If one values community and the primacy of face-to-face, eye-to-eye communication, then human-machine/AI\\n\\nwww.pewresearch.org\\n\\n43\\n\\nPEW RESEARCH CENTER\\n\\ncollaboration in 2030 will have succeeded in greatly diminishing the visceral, primal aspects of humanity. Every expression of daily life, either civil or professional or familial or personal, will be diminished by the iron grip of AI on the fundamental realities of interpersonal communications. Already the reliance on voice-to-text technology via smartphone interface diminishes the ability of people to write with skill and cogency. Taking the time to ring-up another and chat requires too much psychic energy, so we ‘speak’ to one another in text box fragments written down and oft altered by digital assistants. The dismissive but socially acceptable ‘TL;DR’ becomes commonplace as our collective attention span disintegrates. Yes, diagnostic medicine and assembly-line production and expanded educational curriculum will surely be enhanced by cyber-based, one- and-zero technologies, but at what cost to humanity? Is it truly easier and safer to look into a screen and listen to an electronically delivered voice, far away on the other side of an unfathomable digital divide, instead of looking into another’s eyes, perhaps into a soul, and speaking kind words to one another, and perhaps singing in unison about the wonders of the universe? We call it ‘artificial intelligence’ for good reason.”\\n\\nA principal design researcher at one of the world’s largest technology companies commented, “Although I have long worked in this area and been an optimist, I now fear that the goal of most AI and UX is geared toward pushing people to interact more with devices and less with other people. As a social species that is built to live in communities, reductions in social interaction will lead to erosion of community and rise in stress and depression over time. Although AI has the potential to improve lives as well, those advances will come more slowly than proponents think, due to the ‘complexity brake’ Paul Allen wrote about, among other things. There have been AI summers and AI winters. This is not an endless summer.”\\n\\nA chief operating officer wrote, “No doubt in my mind, AI is and will continue to present benefits in simplifying and aiding human activities; however, the net effect is not likely ‘to leave people better off.’ The advances in AI-enabled tools are likely to expand the digital gap in human competencies. This growing gap will decrease the capacity of sizable portions of the population to survive an outage of the technology. This raises humanitarian and national-security concerns.”\\n\\nDalsie Green Baniala, CEO and regulator of the Telecommunications and Radiocommunications Regulator of Vanuatu, wrote, “With the introduction of the Internet of Things, human senses are in decline.”\\n\\nAlper Dincel of T.C. Istanbul Kultur University in Turkey, wrote, “Personal connections will continue to drop, as they are in today’s world. We are going to have more interest in fiction than in reality. These issues will affect human brain development as a result.”\\n\\nwww.pewresearch.org\\n\\n44\\n\\nPEW RESEARCH CENTER\\n\\nMichael Dyer, an emeritus professor of computer science at the University of California, Los Angeles, commented, “As long as GAI (general AI) is not achieved then specialized AI will eliminate tasks associated with jobs but not the jobs themselves. A trucker does a lot more than merely drive a truck. A bartender does a lot more than merely pour drinks. Society will still have to deal with the effects of smart technologies encroaching ever into new parts of the labor market. A universal basic income could mitigate increasing social instability. Later on, as general AI spreads, it will become an existential threat to humanity. My estimate is that this existential threat will not begin to arise until the second half of the 21st century. Unfortunately, by then humanity might have grown complacent, since specialized AI systems do not pose an existential threat.”\\n\\nMauro D. Ríos, an adviser to the E-Government Agency of Uruguay and director of the Internet Society’s Uruguay chapter, responded, “In 2030 dependence on AI will be greater in all domestic, personal, work and educational contexts; this will make the lives of many people better. However, it has risks. We must be able to maintain active survival capabilities without AI. Human freedom cannot be lost in exchange for the convenience of improving our living standards. … AI must continue to be subject to the rationality and control of the human being.”\\n\\nNancy Greenwald, a respondent who provided no identifying details, wrote, “Perhaps the primary downside is overreliance on AI, which 1) is only as good as the algorithms created (how are they instructed to ‘learn?’) and 2) has the danger of limiting independent human thinking. How many Millennials can read a map or navigate without the step-by-step instructions from Waze, Google or their iPhones? And information searches online don’t give you an overview. I once wasted 1.5 billable hours searching for a legal concept when two minutes with the human based BNA outline got me the result in two minutes. Let’s be thoughtful about how we use the amazing technology.”\\n\\nValarie Bell, a computational social scientist at the University of North Texas, commented, “As a social scientist I’m concerned that never before have we had more ways in which to communicate and yet we’ve never done it so poorly, so venomously and so wastefully. With devices replacing increasingly higher-order decisions and behaviors, people have become more detached, more disinterested and yet more self-focused and self-involved.”\\n\\nLane Jennings, managing editor for the World Future Review from 2009 to 2015, wrote, “It is most likely that advances in AI will improve technology and thus give people new capabilities. But this ‘progress’ will also make humanity increasingly vulnerable to accidental breakdowns, power failures and deliberate attacks. Example: Driverless cars and trucks and pilotless passenger aircraft will enhance speed and safety when they work properly, but they will leave people helpless\\n\\nwww.pewresearch.org\\n\\n45\\n\\nPEW RESEARCH CENTER\\n\\nif they fail. Fear and uncertainty could negate positive benefits after even a few highly publicized disasters.”\\n\\nMichael Veale, co-author of “Fairness and Accountability Designs Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making” and a technology policy researcher at University College London, responded, “AI technologies will turn out to be more narrowly applicable than some hope. There will be a range of small tasks that will be more effectively automated. Whether these tasks leave individuals with increased ability to find meaning or support in life is debatable. Freed from some aspects of housework and administration, some individuals may feel empowered whereas others might feel aimless. Independent living for the elderly might be technologically mediated, but will it have the social connections and community that makes life worth living? Jobs too will change in nature, but it is not clear that new tasks will make people happy. It is important that all technologies and applications are backed up with social policies and systems to support meaning and connection, or else even effective AI tools might be isolating and even damaging on aggregate.”\\n\\nThe following one-liners from anonymous respondents also tie into this theme:\\n\\n§ A British-American computer scientist commented, “Increasing dependence on AI will decrease societal resilience through centralization of essential systems in a few large companies.”\\n\\n§ A leading infrastructure engineer for a social network company commented, “AI\\n\\nmay make people’s lives better by making some things easier, but it will likely reduce human value along the way – I expect people to be less able to make decisions, less able to tolerate human interaction, etc.”\\n\\n§ A representative for a nation-state’s directorate of telecommunications wrote, “My fear is that humans will become more and more dependent on AI, to the extent that their natural intelligence would be more and more diminished. The concern is that in the absence of AI they may not be able to act in a timely manner.”\\n\\nOther anonymous respondents commented:\\n\\n§\\n\\n§ §\\n\\n“We need to assure that we have individuals who are able to think and problem-solve and monitor that thinking without assistance.” “Our ethical capabilities lag far behind our technical capabilities.” “Increasing dependence on AI will decrease societal resilience through centralization of essential systems in a few large companies.”\\n\\nwww.pewresearch.org\\n\\n46\\n\\nPEW RESEARCH CENTER\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n“Lack of education in AI and inclusiveness of individual in their own decision-making will make most people worse off in 2030.” “Few people will understand what the AI is attempting to do and how it’s doing it; regular people without this knowledge will become more like sheep.” “I have concerns about how people are adapting to these new changes, the continuing disconnection people have due to advances in AI, substituting AI connections for real people, leading to greater depression.” “My fear is that we will spend even more time with machines than we do with talking with each other.” “My fear is that the increasing ‘datafication’ of work and our lives as a whole will further increase the pressure we feel to reach an unrealistic apex of perfection.” “As one is more and more people have AI/automation support in their daily lives the interactions between people will lessen. People may feel more isolated and less socially interrelated. Social interaction must be carefully maintained and evolved.”\\n\\nCitizens will face increased vulnerabilities, such as exposure to cybercrime and cyberwarfare that spins out of control, and the possibility that essential organizations are endangered by weaponized information\\n\\nSome of these experts are particularly worried about how networked artificial intelligence can amplify cybercrime and create fearsome possibilities in cyberwarfare and enable the erosion of essential institutions and organizations.\\n\\nAnthony Nadler, assistant professor of media and communication studies at Ursinus College, commented, “The question has to do with how decisions will be made that shape the contingent development of this potentially life-changing technology. And who will make those decisions? In the best-case scenario, the development of AI will be influenced by diverse stakeholders representing different communities who will be affected by its implementation (and this may) mean that particular uses of AI – military applications, medical, marketing, etc. – will be overseen by reflective ethical processes. In the absolute worst-case scenario, unrestricted military development will lead to utter destruction – whether in a situations in which the ‘machines take over’ or, more likely, in which weapons of tremendous destruction become all the more readily accessible.”\\n\\nJennifer J. Snow, an innovation officer with the U.S. Air Force, wrote, “Facets, including weaponized information, cyberbullying, privacy issues and other potential abuses that will come out of this technology will need to be addressed by global leaders.”\\n\\nwww.pewresearch.org\\n\\n47\\n\\nPEW RESEARCH CENTER\\n\\nLee McKnight, associate professor at Syracuse University’s School of Information Studies, commented, “There will be good, bad and ugly outcomes from human-machine interaction in artificially intelligent systems, services and enterprises. … Poorly designed artificially intelligent services and enterprises will have unintended societal consequences, hopefully not catastrophic, but sure to damage people and infrastructure. Even more regrettably, defending ourselves against evil – or to be polite, bad AI systems turned ugly by humans, or other machines – must become a priority for societies well before 2030, given the clear and present danger. How can I be sure? What are bots and malware doing every day, today? Is there a reason to think ‘evil-doers’ will be less motivated in the future? No. So my fear is that the hopefully sunny future of AI, which in aggregate we may assume will be a net positive for all of us, will be marred by – many – unfortunate events.”\\n\\nRobert M. Mason, a professor emeritus at the University of Washington’s Information School, responded, “Technologies, including AI, leverage human efforts. People find ways to apply technologies to enhance the human spirit and the human experience, yet others can use technologies to exploit human fears and satisfy personal greed. As the late Fred Robbins, Nobel Laureate in Physiology/Medicine, observed (my paraphrase when I asked why he was pessimistic about the future of mankind): ‘Of course I’m pessimistic. Humans have had millions of years to develop physically and mentally, but we've had only a few thousand years – as the world population has expanded – to develop the social skills that would allow us to live close together.’ I understand his pessimism, and it takes only a few people to use AI (or any technology) in ways that result in widespread negative societal impacts.”\\n\\nFrank Feather, futurist and consultant with StratEDGY, commented, “AI by 2030 .... This is only about a decade away, so despite AI's continuing evolution, it will not have major widespread effects by 2030. With care in implementation all effects should be positive in social and economic impact. That said, the changes will represent a significant step toward what I call a DigiTransHuman Future, where the utility of humans will increasingly be diminished as this century progresses, to the extent that humans may become irrelevant or extinct, replaced by DigiTransHumans and their technologies/robots that will appear and behave just like today’s humans, except at very advanced stages of humanoid development. This is not going to be a so- called ‘singularity’ and there is nothing ‘artificial’ about the DigiTransHuman Intelligence. It is part of designed evolution of the species.”\\n\\nJohn Leslie King, a computer science professor at the University of Michigan and a consultant for several years on cyberinfrastructure for the National Science Foundation’s directorates for Computer and Information Science and Engineering (CISE) and Social, Behavioral, and Economic (SBE) sciences, commented, “If there are evil things to be done with AI, people will find out about\\n\\nwww.pewresearch.org\\n\\n48\\n\\nPEW RESEARCH CENTER\\n\\nthem and do them. There will be an ongoing fight like the one between hackers and IT security people.”\\n\\nJohn Markoff, fellow at the Center for Advanced Study in Behavioral Sciences at Stanford University and author of “Machines of Loving Grace: The Quest for Common Ground Between Humans and Robots,” wrote, “There are expected and unexpected consequences to ‘AI and related technologies.’ It is quite possible that improvements in living standards will be offset by the use of autonomous weapons in new kinds of war.”\\n\\nA veteran of a pioneering internet company commented, “In the face of managing resources and warfare – the big issues for AI at scale – the goals are not likely to be sharing and co-existence.”\\n\\nDan Schultz, senior creative technologist at Internet Archive, responded, “AI will no doubt result in life-saving improvements for a huge portion of the world’s population, but it will also be possible to weaponize in ways that further exacerbate divides of any kind you can imagine (political, economic, education, privilege, etc.). AI will amplify and enable the will of those in power; its net impact on humanity will depend on the nature of that will.”\\n\\nSam Gregory, director of WITNESS and digital human rights activist, responded, “Trends in AI suggest it will enable more individualized, personalized creation of synthetic media filter bubbles around people, including the use of deepfakes and related individualized synthetic audio and video micro-targeting based on personal data and trends in using AI-generated and directed bots. These factors may be controlled by increasing legislation and platform supervision, but by 2030 there is little reason to think that most peoples’ individual autonomy and ability to push back to understand the world around them will have improved.”\\n\\nMiguel Moreno-Muñoz, a professor of philosophy specializing in ethics, epistemology and technology at the University of Granada in Spain, said, “There is a risk of overreliance on systems with poorly experienced intelligence augmentation due to pressure to reduce costs. This could lead to major dysfunctions in health care or in the supervision of highly complex processes. A hasty application of management systems based on the Internet of Things could be problematic in certain sectors of industry, transport or health, but its advantages will outweigh its disadvantages. I do believe there may be significant risks in the military applications of AI.”\\n\\nDenise N. Rall, a professor of arts and social sciences at Southern Cross University in Australia, responded, “The basic problem with the human race and its continued existence on this planet is overpopulation and depletion of the Earth's resources. So far, interactions with technology have\\n\\nwww.pewresearch.org\\n\\n49\\n\\nPEW RESEARCH CENTER\\n\\nreduced population in the ‘first world’ but not in developing countries, and poverty will fuel world wars. Technology may support robotic wars and reduce casualties for the wealthy countries. The disparity between rich and poor will continue unabated.”\\n\\nPatrick Lambe, a partner at Straits Knowledge and president of the International Society for Knowledge Organization’s Singapore chapter, wrote, “I chose the negative answer not because of a dystopian vision for AI itself and technology interaction with human life, but because I believe social, economic and political contexts will be slow to adapt to technology’s capabilities. The real- world environment and the technological capability space are becoming increasingly disjointed and out of synch. Climate change, migration pressures, political pressures, food supply and water will create a self-reinforcing ‘crisis-loop’ with which human-machine/AI capabilities will be largely out of touch. There will be some capability enhancement (e.g., medicine), but on the whole technology contributions will continue to add negative pressures to the other environmental factors (employment, job security, left-right political swings). On the whole I think these disjoints will continue to become more enhanced until a major crisis point is reached (e.g., war).”\\n\\nMechthild Schmidt Feist, department coordinator for digital communications and media at New York University, said, “Historical precedent shows that inventions are just as powerful in the hands of criminals or irresponsible or uninformed people. The more powerful our communication, the more destructive it could be. We would need global, enforceable legislation to limit misuse. 1) That is highly unlikely. 2) It is hard to predict all misuses. My negative view is due to our inability to make responsible use of our current online communication and media models. The utopian freedom has become a dystopian battleground.”\\n\\nMarc Brenman, managing partner at IDARE LLC, said, “We do not know all that machines can do. There is no inherent necessity that they will care for us. We may be an impediment to them. They may take orders from evil-doers. They will enable us to make mistakes even faster than we do now. Any technology is only as good as the morality and ethics of its makers, programmers and controllers. If machines are programmed to care more for the earth than for people, they may eliminate us anyway, since we are destroying the earth.”\\n\\nRobert K. Logan, chief scientist at the Strategic Innovation Lab (sLab) at OCAT University and professor emeritus of physics at the University of Toronto, said, “The idea of the Singularity is an example of the over-extension of AI. Computers will never achieve an equivalency to human intelligence. There is no such thing as AW (artificial wisdom). AI as a tool to enhance human intelligence makes sense but AI to replace human intelligence makes no sense and therefore is nonsense.”\\n\\nwww.pewresearch.org\\n\\n50\\n\\nPEW RESEARCH CENTER\\n\\nAlexey Turchin, existential risks researcher and futurist, responded, “There are significant risks of AI misuse before 2030 in the form of swarms of AI empowered drones or even non-aligned human-level AI.”\\n\\nAdam Popescu, a writer who contributes frequently to the New York Times, Washington Post, Bloomberg Businessweek, Vanity Fair and the BBC, wrote, “We put too much naive hope in everything tech being the savior.”\\n\\nThe following one-liners from anonymous respondents also tie into this theme:\\n\\n§ A cybersecurity strategist said, “The world has become technologically oriented and\\n\\nthis creates challenges – for example, cybercrime.”\\n\\n§ A respondent who works at a major global privacy initiative predicted AI and tech will not improve most people’s lives, citing, “Loss of jobs, algorithms run amuck.”\\n\\nOther anonymous respondents commented:\\n\\n§\\n\\n§\\n\\n“With increasing cyberattacks and privacy concerns AI could connect people to bad actors, which could cause stress and new problems – even the simplest of attacks/pranks could negatively affect people’s lives.” “The increasing dependence of humans on computing coupled with the fundamental un- securability of general-purpose computing is going to lead to widespread exploitation.”\\n\\nwww.pewresearch.org\\n\\n51\\n\\nPEW RESEARCH CENTER\\n\\n2. Solutions to address AI’s anticipated negative impacts\\n\\nA number of participants in this canvassing offered solutions to the worrisome potential future spawned by AI. Among them: 1) improving collaboration across borders and stakeholder groups; 2) developing policies to assure that development of AI will be directed at augmenting humans and the common good; and 3) shifting the priorities of economic, political and education systems to empower individuals to stay ahead in the “race with the robots.”\\n\\nA number of respondents sketched out overall aspirations:\\n\\nAndrew Wycoff, the director of OECD’s directorate for science, technology and innovation, and Karine Perset, an economist in OECD’s digital economy policy division, commented, “Twelve years from now, we will benefit from radically improved accuracy and efficiency of decisions and predictions across all sectors. Machine learning systems will actively support humans throughout their work and play. This support will be unseen but pervasive – like electricity. As machines’ ability to sense, learn, interact naturally and act autonomously increases, they will blur the distinction between the physical and the digital world. AI systems will interconnect and work together to predict and adapt to our human needs and emotions. The growing consensus that AI should benefit society at-large leads to calls to facilitate the adoption of AI systems to promote innovation and growth, help address global challenges, and boost jobs and skills development, while at the same time establishing appropriate safeguards to ensure these systems are transparent and explainable, and respect human rights, democracy, culture, nondiscrimination, privacy and control, safety, and security. Given the inherently global nature of our networks and applications that run across then, we need to improve collaboration across countries and stakeholder groups to move toward common understanding and coherent approaches to key opportunities and issues presented by AI. This is not too different from the post-war discussion on nuclear power. We should also tread carefully toward Artificial General Intelligence and avoid current assumptions on the upper limits of future AI capabilities.”\\n\\nWendy Hall, professor of computer science at the University of Southampton and executive director of the Web Science Institute, said, “By 2030 I believe that human-machine/AI collaboration will be empowering for human beings overall. Many jobs will have gone, but many new jobs will have been created and machines/AI should be helping us do things more effectively and efficiently both at home and at work. It is a leap of faith to think that by 2030 we will have learnt to build AI in a responsible way and we will have learnt how to regulate the AI and robotics industries in a way that is good for humanity. We may not have all the answers by 2030 but we need to be on the right track by then.”\\n\\nwww.pewresearch.org\\n\\n52\\n\\nPEW RESEARCH CENTER\\n\\nIan O'Byrne, an assistant professor focusing on literacy and technology at the College of Charleston, said, “I believe in human-machine/AI collaboration, but the challenge is whether humans can adapt our practices to these new opportunities.”\\n\\nArthur Bushkin, an IT pioneer who worked with the precursors to the Advanced Research Projects Agency Network (ARPANET) and Verizon, wrote, “The principal issue will be society’s collective ability to understand, manage and respond to the implications and consequences of the technology.”\\n\\nDaniel Obam, information and communications technology policy advisor, responded, “As we develop AI, the issue of ethical behaviour is paramount. AI will allow authorities to analyse and allocate resources where there is the greatest need. AI will also change the way we work and travel. … Digital assistants that mine and analyse data will help professionals in making concise decisions in health care, manufacturing and agriculture, among others. Smart devices and virtual reality will enable humans to interact with and learn from historical or scientific issues in a more-clear manner. Using AI, authorities will be able to prevent crime before it happens. Cybersecurity needs to be at the forefront to prevent unscrupulous individuals from using AI to perpetrate harm or evil on the human race.”\\n\\nRyan Sweeney, director of analytics at Ignite Social Media, commented, “Our technology continues to evolve at a growing rate, but our society, culture and economy are not as quick to adapt. We’ll have to be careful that the benefits of AI for some do not further divide those who might not be able to afford the technology. What will that mean for our culture as more jobs are automated? We will need to consider the impact on the current class divide.”\\n\\nSusan Mernit, executive director of The Crucible and co-founder and board member of Hack the Hood, responded, “If AI is in the hands of people who do not care about equity and inclusion, it will be yet another tool to maximize profit for a few.”\\n\\nThe next three sections of this report focus on solutions most often mentioned by respondents to this canvassing.\\n\\nImprove human collaboration across borders and stakeholder groups\\n\\nA number of these experts said ways must be found for people of the world to come to a common understanding of the evolving concerns over AI and digital life and to reach agreement in order to create cohesive approaches to tackling AI’s challenges.\\n\\nwww.pewresearch.org\\n\\n53\\n\\nPEW RESEARCH CENTER\\n\\nDanil Mikhailov, head of data and innovation for Wellcome Trust, responded, “I see a positive future of human/AI interaction in 2030. In my area, health, there is tremendous potential in the confluence of advances in big data analysis and genomics to create personalised medicine and improve diagnosis, treatment and research. Although I am optimistic about human capacity for adaptation, learning, and evolution, technological innovation will not always proceed smoothly. In this we can learn from previous technological revolutions. For example, [Bank of England chief economist] Andy Haldane rightly pointed out that the original ‘luddites’ in the 19th century had a justified grievance. They suffered severe job losses, and it took the span of a generation for enough jobs to be created to overtake the ones lost. It is a reminder that the introduction of new technologies benefits people asymmetrically, with some suffering while others benefit. To realise the opportunities of the future we need to acknowledge this and prepare sufficient safety nets, such as well-funded adult education initiatives, to name one example. It’s also important to have an honest dialogue between the experts, the media and the public about the use of our personal data for social-good projects, like health care, taking in both the risks of acting – such as effects on privacy – and the opportunity costs of not acting. It is a fact that lives are lost currently in health systems across the world that could be saved even with today’s technology let alone that of 2030.”\\n\\nEdson Prestes, a professor and director of robotics at the Federal University of Rio Grande do Sul, responded, “We must understand that all domains (technological or not) have two sides: a good and a bad one. To avoid the bad one we need to create and promote the culture of AI/Robotics for good. We need to stimulate people to empathize toward others. We need to think about potential issues, even if they have small probability to happen. We need to be futurists, foreseeing potential negative events and how to circumvent them before they happen. We need to create regulations/laws (at national and international levels) to handle globally harmful situations for humans, other living beings and the environment. Applying empathy, we should seriously think about ourselves and others – if the technology will be useful for us and others and if it will not cause any harm. We cannot develop solutions without considering people and the ecosystem as the central component of development. If so, the pervasiveness of AI/robotics in the future will diminish any negative impact and create a huge synergy among people and environment, improving people’s daily lives in all domains while achieving environment sustainability.”\\n\\nAdam Nelson, a software developer for one of the “big five” global technology companies, said, “Human-machine/AI collaboration will be extremely powerful, but humans will still control intent. If human governance isn’t improved, AI will merely make the world more efficient. But the goals won’t be human welfare. They’ll be wealth aggregation for those in power.”\\n\\nwww.pewresearch.org\\n\\n54\\n\\nPEW RESEARCH CENTER\\n\\nWendy Seltzer, strategy lead and counsel at the World Wide Web Consortium, commented, “I'm mildly optimistic that we will have devised better techno-social governance mechanisms. such that if AI is not improving the lives of humans, we will restrict its uses.”\\n\\nJen Myronuk, a respondent who provided no identifying details, said, “The optimist’s view includes establishing and implementing a new type of ISO standard – ‘encoded human rights’ – as a functional data set alongside exponential and advancing technologies. Global human rights and human-machine/AI technology can and must scale together. If applied as an extension of the human experience, human-machine/AI collaboration will revolutionize our understanding of the world around us.”\\n\\nFiona Kerr, industry professor of neural and systems complexity at the University of Adelaide, commented, “The answer depends very much on what we decide to do regarding the large questions around ensuring equality of improved global health; by agreeing on what productivity and worth now look like, partly supported by the global wage; through fair redistribution of technology profits to invest in both international and national social capital; through robust discussion on the role of policy in rewarding technologists and businesses to build quality partnerships between humans and AI; through the growth of understanding in the neurophysiological outcomes of human-human and human-technological interaction which allows us to best decide what not to technologies, when a human is more effective, and how to ensure we maximise the wonders of technology as an enabler of a human-centric future.”\\n\\nBenjamin Kuipers, a professor of computer science at the University of Michigan, wrote, “We face several critical choices between positive and negative futures. … Advancing technology will provide vastly more resources; the key decision is whether those resources will be applied for the good of humanity as a whole or if they will be increasingly held by a small elite. Advancing technology will vastly increase opportunities for communication and surveillance. The question is whether we will find ways to increase trust and the possibilities for productive cooperation among people or whether individuals striving for power will try to dominate by decreasing trust and cooperation. In the medium term, increasing technology will provide more powerful tools for human, corporate or even robot actors in society. The actual problems will be about how members of a society interact with each other. In a positive scenario, we will interact with conversational AIs for many different purposes and even when the AI belongs to a corporation we will be able to trust that it takes what in economics is called a ‘fiduciary’ stance toward each of us. That is, the information we provide must be used primarily for our individual benefit. Although we know, and are explicitly told, that our aggregated information is valuable to the corporation, we can trust that it will not be used for our manipulation or our disadvantage.”\\n\\nwww.pewresearch.org\\n\\n55\\n\\nPEW RESEARCH CENTER\\n\\nDenise Garcia, an associate professor of political science and international affairs at Northeastern University, said, “Humanity will come together to cooperate.”\\n\\nCharles Geiger, head of the executive secretariat for the UN's World Summit on the Information Society, commented, “As long as we have a democratic system and a free press, we may counterbalance the possible threats of AI.”\\n\\nWarren Yoder, longtime director of the Public Policy Center of Mississippi, now an instructor at Mississippi College, optimistically responded, “Human/AI collaborations will ... augment our human abilities and increase the material well-being of humanity. At the same time the concomitant increase in the levels of education and health will allow us to develop new social philosophies and rework our polities to transform human well-being. AI increases the disruption of the old social order, making the new transformation both necessary and more likely, though not guaranteed.”\\n\\nWangari Kabiru, author of the MitandaoAfrika blog, based in Nairobi, Kenya, commented, “In 2030, advancing AI and tech will not leave most people better off than they are today, because our global digital mission is not strong enough and not principled enough to assure that ‘no, not one is left behind’ – perhaps intentionally. The immense positive-impact potential for enabling people to achieve more in nearly every area of life – the full benefits of human-machine/AI collaboration can only be experienced when academia, civil society and other institutions are vibrant, enterprise is human-values-based, and governments and national constitutions and global agreements place humanity first. … Engineering should serve humanity and never should humanity be made to serve the exploits of engineering. More people MUST be creators of the future of LIFE – the future of how they live, future of how they work, future of how their relationships interact and overall how they experience life. Beyond the coexistence of human-machine, this creates synergy.”\\n\\nA professor expert in AI connected to a major global technology company’s projects in AI development wrote, “Precision democracy will emerge from precision education, to incrementally support the best decisions we can make for our planet and our species. The future is about sustaining our planet. As with the current development of precision health as the path from data to wellness, so too will artificial intelligence improve the impact of human collaboration and decision-making in sustaining our planet. ”\\n\\nSome respondents argued that individuals must do better at taking a more active role in understanding and implementing the decision-making options available to them in these complex, code-dependent systems.\\n\\nwww.pewresearch.org\\n\\n56\\n\\nPEW RESEARCH CENTER\\n\\nKristin Jenkins, executive director of BioQUEST Curriculum Consortium, said, “Like all tools the benefits and pitfalls of AI will depend on how we use it. A growing concern is the collection and potential uses of data about people’s day-to-day lives. ‘Something’ always knows where we are, the layout of the house, what’s in the fridge and how much we slept. The convenience provided by these tools will override caution about data collection, so strong privacy protection must be legislated and culturally nurtured. We need to learn to be responsible for our personal data and aware of when and how it is collected and used.”\\n\\nPeng Hwa Ang, professor of communications at Nanyang Technological University and author of “Ordering Chaos: Regulating the Internet,” commented, “AI is still in its infancy. A lot of it is ruled-based and not demanding of true intelligence or learning. But even so, I find it useful. My car has lane-assistance. I find that it makes me a better driver. When AI is more full-fledged, it would make driving safer and faster. I am using AI for some work I am doing on sentiment analysis. I find that I am able to be more creative in asking questions to be investigated. I expect AI will compel greater creativity. Right now, the biggest fear of AI is that it is a black-box operation – yes, the factors chosen are good and accurate and useful, but no one knows why those criteria are chosen. We know the percentages of the factors, but we do not know the whys. Hopefully, by 2030, the box will be more transparent. That's on the AI side. On the human side, I hope human beings understand that true AI will make mistakes. If not, it is not real AI. This means that people have got to be ready to catch the mistakes that AI will make. It will be very good. But it will (still) not be foolproof.”\\n\\nBert Huang, an assistant professor in the department of computer science at Virginia Tech focused on machine learning, wrote, “AI will cause harm (and it has already caused harm), but its benefits will outweigh the harm it causes. That said, the [historical] pattern of technology being net positive depends on people seeking positive things to do with the technology, so efforts to guide research toward societal benefits will be important to ensure the best future.”\\n\\nAn anonymous respondent said, “We should ensure that values (local or global) and basic philosophical theories on ethics inform the development and implementation of AI systems.”\\n\\nDevelop policies to assure that development of AI will be directed at augmenting humans and the common good\\n\\nMany experts who shared their insights in this study suggested there has to be an overall change in the development, regulation and certification of autonomous systems. They generally said the goal should be values-based, inclusive, decentralized, networks “imbued with empathy” that help individuals assure that technology meets social and ethical responsibilities for the common good.\\n\\nwww.pewresearch.org\\n\\n57\\n\\nPEW RESEARCH CENTER\\n\\nSusan Etlinger, an industry analyst for Altimeter Group and expert in data, analytics and digital strategy, commented, “In order for AI technologies to be truly transformative in a positive way, we need a set of ethical norms, standards and practical methodologies to ensure that we use AI responsibly and to the benefit of humanity. AI technologies have the potential to do so much good in the world: identify disease in people and populations, discover new medications and treatments, make daily tasks like driving simpler and safer, monitor and distribute energy more efficiently, and so many other things we haven’t yet imagined or been able to realize. And – like any tectonic shift – AI creates its own type of disruption. We’ve seen this with every major invention from the Gutenberg press to the invention of the semiconductor. But AI is different. Replication of some human capabilities using data and algorithms has ethical consequences. Algorithms aren’t neutral; they replicate and reinforce bias and misinformation. They can be opaque. And the technology and means to use them rests in the hands of a select few organizations, at least today.”\\n\\nBryan Johnson, founder and CEO of Kernel, a leading developer of advanced neural interfaces, and OS Fund, a venture capital firm, said, “We could start with owning our own digital data and the data from our bodies, minds and behavior, and then follow by correcting our major tech companies’ incentives away from innovation for everyday convenience and toward radical human improvement. As an example of what tech could look like when aligned with radical human improvement, cognitive prosthetics will one day give warnings about biases – like how cars today have sensors letting you know when you drift off to sleep or if you make a lane change without a signal – and correct cognitive biases and warn an individual away from potential cognitive biases. This could lead to better behaviors in school, home and work, and encourage people to make better health decisions.”\\n\\nMarc Rotenberg, executive director of Electronic Privacy Information Center (EPIC), commented, “The challenge we face with the rise of AI is the growing opacity of processes and decision-making. The favorable outcomes we will ignore. The problematic outcomes we will not comprehend. That is why the greatest challenge ahead for AI accountability is AI transparency. We must ensure that we understand and can replicate the outcomes produced by machines. The alternative outcome is not sustainable.”\\n\\nJohn C. Havens, executive director of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems and the Council on Extended Intelligence, wrote, “While today people provide ‘consent’ for their data usage, most people don’t understand the depth and breadth of how their information is utilized by businesses and governments at large. Until every individual is provided with a sovereign identity attached to a personal data cloud they control, information won’t truly be shared – just tracked. By utilizing blockchain or similar technologies and adopting progressive\\n\\nwww.pewresearch.org\\n\\n58\\n\\nPEW RESEARCH CENTER\\n\\nideals toward citizens and their data, as demonstrated by countries like Estonia, we can usher in genuine digital democracy in the age of the algorithm. The other issue underlying the ‘human-AI augmentation’ narrative rarely discussed is the economic underpinnings driving all technology manufacturing. Where exponential growth, shareholder models are prioritized human and environmental well-being diminishes. Multiple reports from people like Joseph Stiglitz point out that while AI will greatly increase GDP in the coming decades, the benefits of these increases will favor the few versus the many. It’s only by adopting ‘Beyond GDP’ or triple-bottom-line metrics that ‘people, planet and profit’ will shape a holistic future between humans and AI.”\\n\\nGreg Lloyd, president and co-founder at Traction Software, presented a future scenario: “By 2030 AIs will augment access and use of all personal and networked resources as highly skilled and trusted agents for almost every person – human or corporate. These agents will be bound to act in accordance with new laws and regulations that are fundamental elements of their construction much like Isaac Asimov’s ‘Three Laws of Robotics’ but with finer-grain ‘certifications’ for classes of activities that bind their behavior and responsibility for practices much like codes for medical, legal, accounting and engineering practice. Certified agents will be granted access to personal or corporate resources, and within those bounds will be able to converse, take direction, give advice and act like trusted servants, advisers or attorneys. Although these agents will ‘feel’ like intelligent and helpful beings, they will not have any true independent will or consciousness, and must not pretend to be human beings or act contrary to the laws and regulations that bind their behavior. Think Ariel and Prospero.”\\n\\nTracey P. Lauriault, assistant professor of critical media and big data at Carleton University’s School of Journalism and Communication, commented, “[What about] regulatory and policy interventions to protect citizens from potentially harmful outcomes, AI auditing, oversight, transparency and accountability? Without some sort of principles of a systems-based framework to ensure that AI remains ethical and in the public interest, in a stable fashion, then I must assume that AI will impede agency and could lead to decision-making that can be harmful, biased, inaccurate and not able to dynamically change with changing values. There needs to be some sort of accountability.”\\n\\nJoël Colloc, professor at Université du Havre Normandy University and author of “Ethics of Autonomous Information Systems,” commented, “When AI supports human decisions as a decision-support system it can help humanity enhance life, health and well-being and supply improvements for humanity. See Marcus Flavius Quintilianus’s principles: Who is doing what, with what, why, how, when, where? Autonomous AI is power that can be used by powerful persons to control the people, put them in slavery. Applying the Quintilian principles to the role of AI … we should propose a code of ethics of AI to evaluate that each type of application is oriented toward\\n\\nwww.pewresearch.org\\n\\n59\\n\\nPEW RESEARCH CENTER\\n\\nthe well-being of the user: 1) do not harm the user, 2) benefits go to the user, 3) do not misuse her/his freedom, identity and personal data, and 4) decree as unfair any clauses alienating the user’s independence or weakening his/her rights of control over privacy in use of the application. The sovereignty of the user of the system must remain total.”\\n\\nJoseph Turow, professor of communication at the University of Pennsylvania, wrote, “Whether or not AI will improve society or harm it by 2030 will depend on the structures governing societies of the era. Broadly democratic societies with an emphasis on human rights might encourage regulations that push AI in directions that help all sectors of the nation. Authoritarian societies will, by contrast, set agendas for AI that further divide the elite from the rest and use technology to cultivate and reinforce the divisions. We see both tendencies today; the dystopian one has the upper hand especially in places with the largest populations. It is critical that people who care about future generations speak out when authoritarian tendencies of AI appear.”\\n\\nHenry E. Brady, dean of the Goldman School of Public Policy at the University of California, Berkeley, wrote, “I believe that policy responses can be developed that will reduce biases and find a way to accommodate AI and robotics with human lives.”\\n\\nJennifer King, director of privacy at Stanford Law School’s Center for Internet and Society, said, “Unless we see a real effort to capture the power of AI for the public good, I do not see an overarching public benefit by 2030. The shift of AI research to the private sector means that AI will be developed to further consumption, rather than extend knowledge and public benefit.”\\n\\nGary Kreps, distinguished professor of communication and director of the Center for Health and Risk Communication at George Mason University, wrote, “The tremendous potential for AI to be used to engage and adapt information content and computer services to individual users can make computing increasingly helpful, engaging and relevant. However, to achieve these outcomes, AI needs to be programmed with the user in mind. For example, AI services should be user-driven, adaptive to individual users, easy to use, easy to understand and easy for users to control. These AI systems need to be programmed to adapt to individual user requests, learning about user needs and preferences.”\\n\\nThomas Streeter, a professor of sociology at the University of Vermont, said, “The technology will not determine whether things are better or worse in 2030; social and political choices will.”\\n\\nPaul Werbos, a former program director at the National Science Foundation who first described the process of training artificial neural networks through backpropagation of errors in 1974, said, “We are at a moment of choice. The outcome will depend a lot on the decisions of very powerful\\n\\nwww.pewresearch.org\\n\\n60\\n\\nPEW RESEARCH CENTER\\n\\npeople who do not begin to know the consequences of the alternatives they face, or even what the substantive alternatives are.”\\n\\nDivina Frau-Meigs, professor of media sociology at the University of Paris III: Sorbonne Nouvelle and UNESCO chair for sustainable digital development, responded, “The sooner the ethics of AI are aligned with human rights tenets the better.”\\n\\nJuan Ortiz Freuler, a policy fellow at the World Wide Web Foundation, wrote “We believe technology can and should empower people. If ‘the people’ will continue to have a substantive say on how society is run, then the state needs to increase its technical capabilities to ensure proper oversight of these companies. Tech in general and AI in particular will promote the advancement of humanity in every area by allowing processes to scale efficiently, reducing the costs and making more services available to more people (including quality health care, mobility, education, etc.). The open question is how these changes will affect power dynamics. To operate effectively, AI requires a broad set of infrastructure components, which are not equally distributed. These include data centers, computing power and big data. What is more concerning is that there are reasons to expect further concentration. On the one hand, data scales well: The upfront (fixed) costs of setting up a datacenter are large compared to the cost of keeping it running. Therefore, the cost of hosting each extra datum is marginally lower than the previous one. Data is the fuel of AI, and therefore whoever gets access to more data can develop more effective AI. On the other hand, AI creates efficiency gains by allowing companies to automate more processes, meaning whoever gets ahead can undercut competitors. This circle fuels concentration. As more of our lives are managed by technology there is a risk that whoever controls these technologies gets too much power. The benefits in terms of quality of life and the risks to people’s autonomy and control over politics are qualitatively different and there cannot (and should not) be up for tradeoffs.”\\n\\nMeryl Alper, an assistant professor of communication at Northeastern University and a faculty associate at Harvard University’s Berkman Klein Center for Internet and Society, wrote, “My fear is that AI tools will be used by a powerful few to further centralize resources and marginalize people. These tools, much like the internet itself, will allow people to do this ever more cheaply, quickly and in a far-reaching and easily replicable manner, with exponentially negative impacts on the environment. Preventing this in its worst manifestations will require global industry regulation by government officials with hands-on experience in working with AI tools on the federal, state and local level, and transparent audits of government AI tools by grassroots groups of diverse (in every sense of the term) stakeholders.”\\n\\nDavid Wilkins, instructor in computer science at the University of Oregon, responded, “AI must be able to explain the basis for its decisions.”\\n\\nwww.pewresearch.org\\n\\n61\\n\\nPEW RESEARCH CENTER\\n\\nA top research director and technical fellow at a major global technology company said, “There is a huge opportunity to enhance folks’ lives via AI technologies. The positive uses of AI will dominate as they will be selected for their value to people. I trust the work by industry, academia and civil society to continue to play an important role in moderating the technology, such as pursuing understandings of the potential costly personal, social and societal influences of AI. I particularly trust the guidance coming from the long-term, ongoing One Hundred Year Study on AI and the efforts of the Partnership on AI.”\\n\\nPeter Stone, professor of computer science at the University of Texas at Austin and chair of the first study panel of the One Hundred Year Study on Artificial Intelligence (AI100), responded, “As chronicled in detail in the AI100 report, I believe that there are both significant opportunities and significant challenges/risks when it comes to incorporating AI technologies into various aspects of everyday life. With carefully crafted industry-specific policies and responsible use, I believe that the potential benefits outweigh the risks. But the risks are not to be taken lightly.”\\n\\nAnita Salem, systems research and design principal at SalemSystems, warned of a possible dystopian outcome, “Human-machine interaction will result in increasing precision and decreasing human relevance unless specific efforts are made to design in ‘humanness.’ For instance, AI in the medical field will aid more precise diagnosis, will increase surgical precision and will increase evidence-based analytics. If designed correctly, these systems will allow the humans to do what they do best –provide empathy, use experience-based intuition and utilize touch and connection as a source of healing. If human needs are left out of the design process, we’ll see a world where humans are increasingly irrelevant and more easily manipulated. We could see increasing under-employment leading to larger wage gaps, greater poverty and homelessness, and increasing political alienation. We’ll see fewer opportunities for meaningful work, which will result in increasing drug and mental health problems and the further erosion of the family support system. Without explicit efforts to humanize AI design, we’ll see a population that is needed for purchasing, but not creating. This population will need to be controlled and AI will provide the means for this control: law enforcement by drones, opinion manipulation by bots, cultural homogeny through synchronized messaging, election systems optimized from big data and a geopolitical system dominated by corporations that have benefited from increasing efficiency and lower operating costs.”\\n\\nChris Newman, principal engineer at Oracle, commented, “As it becomes more difficult for humans to understand how AI/tech works, it will become harder to resolve inevitable problems. A better outcome is possible with a hard push by engineers and consumers toward elegance and simplicity (e.g., Steve-Jobs-era Apple).”\\n\\nwww.pewresearch.org\\n\\n62\\n\\nPEW RESEARCH CENTER\\n\\nA research scientist based in North America wrote, “The wheels of legislation, which is a primary mechanism to ensure benefits are distributed throughout society, move slowly. While the benefits of AI/automation will accrue very quickly for the 1%, it will take longer for the rest of the populace to feel any benefits, and that’s ONLY if our representative leaders DELIBERATELY enact STRONG social and fiscal policy. For example, AI will save billions in labor costs – and also cut the bargaining power of labor in negotiations with capital. Any company using AI technologies should be heavily taxed, with that money going into strong social welfare programs like job retraining and federal jobs programs. For another example, any publicly funded AI research should be prevented from being privatized. The public ought to see the reward from its own investments. Don’t let AI follow the pattern of Big Pharma’s exploitation of the public-permitted Bayh-Dole Act.”\\n\\nKen Birman, a professor in the department of computer science at Cornell University, responded, “By 2030, I believe that our homes and offices will have evolved to support app-like functionality, much like the iPhone in my pocket. People will customize their living and working spaces, and different app suites will support different lifestyles or special needs. For example, think of a young couple with children, a group of students sharing a home or an elderly person who is somewhat frail. Each would need different forms of support. This ‘applications’ perspective is broad and very flexible. But we also need to ensure that privacy and security are strongly protected by the future environment. I do want my devices and apps linked on my behalf, but I don’t ever want to be continuously spied-upon. I do think this is feasible, and as it occurs we will benefit in myriad ways.”\\n\\nMartin Geddes, a consultant specializing in telecommunications strategies, said, “The unexpected impact of AI will be to automate many of our interactions with systems where we give consent and to enable a wider range of outcomes to be negotiated without our involvement. This requires a new presentation layer for the augmented reality metaverse, with a new ‘browser’ – the Guardian Avatar – that helps to protect our identity and our interests.”\\n\\nLindsey Andersen, an activist at the intersection of human rights and technology for Freedom House and Internews, now doing graduate research at Princeton University, commented, “Already, there is an overreliance on AI to make consequential decisions that affect people’s lives. We have rushed to use AI to decide everything, from what content we see on social media to assigning credit scores to determining how long a sentence a defendant should serve. While often well-intentioned, these uses of AI are rife with ethical and human rights issues, from perpetuating racial bias to violating our rights to privacy and free expression. If we have not dealt with these problems through smart regulation, consumer/buyer education and establishment of norms across the AI industry, we could be looking at a vastly more unfair, polarized and surveilled world in 2030.”\\n\\nwww.pewresearch.org\\n\\n63\\n\\nPEW RESEARCH CENTER\\n\\nYeseul Kim, a designer for a major South Korean search firm, wrote, “The prosperity generated by and the benefits of AI will promote the quality of living for most people only when its ethical implications and social impacts are widely discussed and shared inside the human society, and only when pertinent regulations and legislation can be set up to mitigate the misconduct that can be brought about as the result of AI advancement. If these conditions are met, computers and machines can process data at unprecedented speed and at an unrivaled precision level, and this will improve the quality of life, especially in medical and healthcare sectors. It has already been proven and widely shared among medical expert groups that doctors perform better in detecting diseases when they work with AI. Robotics for surgery is also progressing, so this will also benefit the patients as they can assist human surgeons who inevitably face physical limits when they conduct surgeries.”\\n\\nMark Maben, a general manager at Seton Hall University, wrote, “The AI revolution is, sadly, likely to be dystopian. At present, governmental, educational, civic, religious and corporate institutions are ill-prepared to handle the massive economic and social disruption that will be caused by AI. I have no doubt that advances in AI will enhance human capacities and empower some individuals, but this will be more than offset by the fact that artificial intelligence and associated technological advances will mean far fewer jobs in the future. Sooner than most individuals and societies realize, AI and automation will eliminate the need for retail workers, truck drivers, lawyers, surgeons, factory workers and other professions. In order to ensure that the human spirit thrives in a world run and ruled by AI, we will need to change the current concept of work. That is an enormous task for a global economic system in which most social and economic benefits come from holding a traditional job. We are already seeing a decline in democratic institutions and a rise in authoritarianism due to economic inequality and the changing nature of work. If we do not start planning now for the day when AI results in complete disruption of employment, the strain is likely to result in political instability, violence and despair. This can be avoided by policies that provide for basic human needs and encourage a new definition of work, but the behavior to date by politicians, governments, corporations and economic elites gives me little confidence in their ability to lead us through this transition.”\\n\\nEduardo Vendrell, a computer science professor at the Polytechnic University of Valencia in Spain, responded, “These advances will have a noticeable impact on our privacy, since the basis for this application is focused on the information we generate with the use of different technologies. … It will be necessary to regulate in a decisive way the access to the information and its use.”\\n\\nYoram Kalman, an associate professor at the Open University of Israel and member of The Center for Internet Research at the University of Haifa, wrote, “The main risk is when communication and analysis technologies are used to control others, to manipulate them, or to\\n\\nwww.pewresearch.org\\n\\n64\\n\\nPEW RESEARCH CENTER\\n\\ntake advantage of them. These risks are ever-present and can be mitigated through societal awareness and education, and through regulation that identifies entities that become very powerful thanks to a specific technology or technologies, and which use that power to further strengthen themselves. Such entities – be they commercial, political, national, military, religious or any other – have in the past tried and succeeded in leveraging technologies against the general societal good, and that is an ever-present risk of any powerful innovation. This risk should make us vigilant but should not keep us from realizing one of the most basic humans urges: the strive to constantly improve the human condition.”\\n\\nSam Gregory, director of WITNESS and digital human rights activist, responded, “We should assume all AI systems for surveillance and population control and manipulation will be disproportionately used and inadequately controlled by authoritarian and non-democratic governments. These governments and democratic governments will continue to pressure platforms to use AI to monitor for content, and this monitoring, in and of itself, will contribute to the data set for personalization and for surveillance and manipulation. To fight back against this dark future we need to get the right combination of attention to legislation and platform self- governance right now, and we need to think about media literacy to understand AI-generated synthetic media and targeting. We should also be cautious about how much we encourage the use of AI as a solution to managing content online and as a solution to, for example, managing hate speech.”\\n\\nJonathan Kolber, futurist, wrote, “My fear is that, by generating AIs that can learn new tasks faster and more reliably than people can do, the future economy will have only evanescent opportunities for most people. My hope is that we will begin implementing a sustainable and viable universal basic income, and in particular Michael Haines' MUBI proposal. (To my knowledge, the only such proposal that is sustainable and can be implemented in any country at any time.) I have offered a critique of alternatives. Given that people may no longer need depend on their competitive earning power in 2030, AI will empower a far better world. If, however, we fail to implement a market-oriented universal basic income or something equally effective, vast multitudes will become unemployed and unemployable without means to support themselves. That is a recipe for societal disaster.”\\n\\nWalid Al-Saqaf, senior lecturer at Södertörn University, member of the board of trustees of the Internet Society (ISOC) and vice president of the ISOC Blockchain Special Interest Group, commented, “The challenge is to ensure that the data used for AI procedures is reliable. This entails the need for strong cyber security and data integrity. The latter, I believe, can be tremendously enhanced by distributed ledger technologies such as blockchain. I foresee mostly positive results from AI so long as there is enough guards to protect from automated execution of\\n\\nwww.pewresearch.org\\n\\n65\\n\\nPEW RESEARCH CENTER\\n\\ntasks in areas that may have ethical considerations such as taking decisions that may have life-or- death implications. AI has a lot of potential. It should be used to add to and not replace human intellect and judgement.”\\n\\nDanny O'Brien, international director for a nonprofit digital rights group, commented, “I'm generally optimistic about the ability of humans to direct technology for the benefit of themselves and others. I anticipate human-machine collaboration to take place at an individual level, with tools and abilities that enhance our own judgment and actions, rather than this being a power restricted to a few actors. So, for instance, if we use facial-recognition or predictive tools, it will be under the control of an end-user, transparent and limited to personal use. This may require regulation, internal coding restraints or a balance being struck between user capabilities. But I'm hopeful we can get there.”\\n\\nFernando Barrio, director of the law program at the Universidad Nacional de Río Negro in Argentina, commented, “The interaction between humans and networked AI could lead to a better future for a big percentage of the population. In order to do so efforts need to be directed not only at increasing AI development and capabilities but also at positive policies to increase the availability and inclusiveness of those technologies. The challenge is not technical; it is sociopolitical.”\\n\\nPaul Jones, professor of information science at the University of North Carolina at Chapel Hill, responded, “AI as we know it in 2018 is just beginning to understand itself. Like HAL, it will have matured by 2030 into an understanding of its post-adolescent self and of its relationship to humans and to the world. But, also, humans will have matured in our relationship to AI. Like all adolescent relationships there will have been risk taking and regrets and hopefully reconciliation. Language was our first link to other intelligences, then books, then the internet – each a more intimate conversation than the one before. AI will become our link, adviser and to some extent our wise loving companion.”\\n\\nJean-Claude Heudin, a professor with expertise in AI and software engineering at the De Vinci Research Center at Pole Universitaire Leonard de Vinci in France, wrote, “Natural intelligence and artificial intelligence are complementary. We need all of the possible intelligence possible for solving the problems yet to come. More intelligence is always better.”\\n\\nBryan Alexander, futurist and president of Bryan Alexander Consulting, responded, “I hope we will structure AI to enhance our creativity, to boost our learning, to expand our relationships worldwide, to make us physically safer and to remove some drudgery.”\\n\\nwww.pewresearch.org\\n\\n66\\n\\nPEW RESEARCH CENTER\\n\\nBut some have concerns that the setting of policy could do some damage.\\n\\nScott Burleigh, software engineer and intergalactic internet pioneer, wrote, “Advances in technology itself, including AI, always increase our ability to change the circumstances of reality in ways that improve our lives. It also always introduces possible side effects that can make us worse off than we were before. Those effects are realized when the policies we devise for using the new technologies are unwise. I don't worry about technology; I worry about stupid policy. I worry about it a lot, but I am guardedly optimistic; in most cases I think we eventually end up with tolerable policies.”\\n\\nJeff Jarvis, director of the Tow-Knight Center at City University of New York’s Craig Newmark School of Journalism, commented, “What worries me most is worry itself: An emerging moral panic that will cut off the benefits of this technology for fear of what could be done with it. What I fear most is an effort to control not just technology and data but knowledge itself, prescribing what information can be used for before we know what those uses could be. I could substitute ‘book’ for ‘AI’ and the year 1485 (or maybe 1550) for 2030 in your question and it’d hold fairly true. Some thought it would be good, some bad; both end up right. We will figure this out. We always have. Sure, after the book there were wars and other profound disturbances. But in the end, humans figure out how to exploit technologies to their advantage and control them for their safety. I’d call that a law of society. The same will be true of AI. Some will misuse it, of course, and that is the time to identify limits to place on its use – not speculatively before. Many more will use it to find economic, societal, educational and cultural benefit and we need to give them the freedom to do so.”\\n\\nSome respondents said no matter how society comes together to troubleshoot AI concerns there will still be problems.\\n\\nDave Gusto, professor of political science and co-director of the Consortium for Science, Policy & Outcomes at Arizona State University, said, “The question asked about ‘most people.’ Most people in the world live a life that is not well regarded by technology, technology developers and AI. I don’t see that changing much in the next dozen years.”\\n\\nA longtime Silicon Valley communications professional who has worked at several of the top tech companies over the past few decades responded, “AI will continue to improve *if* quality human input is behind it. If so, better AI will support service industries at the top of the funnel, leaving humans to handle interpretation, decisions and applied knowledge. Medical data- gathering for earlier diagnostics comes to mind. Smarter job-search processes, environmental data collection for climate-change actions – these applications all come to mind.”\\n\\nwww.pewresearch.org\\n\\n67\\n\\nPEW RESEARCH CENTER\\n\\nHari Shanker Sharma, an expert in nanotechnology and neurobiology at Uppsala University in Sweden, said, “AI has not yet peaked hence growth will continue, but evil also uses such developments. That will bring bigger dangers to mankind. The need will be to balance growth with safety, e.g., social media is good and bad. The ways to protect from evil mongers are not sufficient. Tracing an attacker/evil monger in a global village to control and punish is the need. AI will give birth to an artificial human being who could be an angel or a devil. Plan for countering evil at every development stage.”\\n\\nA changemaker working for digital accessibility wrote, “There is no reason to assume some undefined force will be able to correct for or ameliorate the damage of human nature amplified with power-centralizing technologies. There is no indication that governments will be able to counterbalance power-centralization trends, as governments, too, take advantage of such market failures. The outward dressing of such interactions is probably the least important aspect of it.”\\n\\nAn information-science futurist commented, “I fear that powerful business interests will continue to put profits above all else, closing their eyes to the second- and third-order effects of their decisions. I fear that we do not have the political will to protect and promote the common interests of citizens and democracy. I fear that our technological tools are advancing more quickly than our ability to manage them wisely. I have, however, recently spotted new job openings with titles like ‘Director of Research, Policy and Ethics in AI’ and ‘Architect, AI Ethical Practice’ at major software companies. There are reasons for hope.”\\n\\nThe following one-liners from anonymous respondents also tie into this theme:\\n\\n§ An open-source technologist in the automotive industry wrote, “We’ll have to\\n\\nhave independent AI systems with carefully controlled data access, clear governance and individuals’ right to be forgotten.”\\n\\n§ A research professor of international affairs at a major university in\\n\\nWashington, D.C., responded, “We have to find a balance between regulations designed to encourage ethical nondiscriminatory use, transparency and innovation.”\\n\\n§ A director for a major regional internet registry said, “The ability of government to properly regulate advanced technologies is not keeping up with the evolution of those technologies. This allows many developments to proceed without sufficient notice, analysis, vetting or regulation to protect the interests of citizens (Facebook being a prime example).”\\n\\n§ A professor at a major Silicon-Valley-area university said, “If technological\\n\\nadvances are not integrated into a vision of holistic, ecologically sustainable, politically equitable social visions, they will simply serve gated and locked communities.”\\n\\nwww.pewresearch.org\\n\\n68\\n\\nPEW RESEARCH CENTER\\n\\n§ A member of the editorial board of the Association of Computing Machinery Journal on autonomous and adaptive systems commented, “By developing an ethical AI, we can provide smarter services in daily life, such as collaborating objects providing on-demand highly adaptable services in any environment supporting daily life activities.”\\n\\nOther anonymous respondents commented:\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n“It is essential that policymakers focus on impending inequalities. The central question is for whom will life be better, and for whom will it be worse? Some people will benefit from AI, but many will not. For example, folks on the middle and lower end of the income scale will see their jobs disappear as human-machine/AI collaborations become lower-cost and more efficient. Though such changes could generate societal benefits, they should not be born on the backs of middle- and low-income people.” “Results will be determined by the capacity of political, criminal justice and military institutions to adapt to rapidly evolving technologies.” “To assure the best future, we need to ramp up efforts in the areas of decentralizing data ownership, education and policy around transparency.” “Most high-end AI knowhow is and will be controlled by a few giant corporations unless government or a better version of the United Nations step in to control and oversee them.” “Political change will determine whether AI technologies will benefit most people or not. I am not optimistic due to the current growth of authoritarian regimes and the growing segment of the super-rich elite who derive disproportionate power over the direction of society from their economic dominance.” “Mechanisms must be put in place to ensure that the benefits of AI do not accrue only to big companies and their shareholders. If current neo-liberal governance trends continue, the value-added of AI will be controlled by a few dominant players, so the benefits will not accrue to most people. There is a need to balance efficiency with equity, which we have not been doing lately.”\\n\\nShift the priorities of economic, political and education systems to empower individuals to stay ahead in the ‘race with the robots’\\n\\nA share of these experts suggest the creation of policies, regulations or ethical and operational standards should shift corporate and government priorities to focus on the global advancement of humanity, rather than profits or nationalism. They urge that major organizations revamp their practices and make sure AI advances are aimed at human augmentation for all, regardless of economic class.\\n\\nwww.pewresearch.org\\n\\n69\\n\\nPEW RESEARCH CENTER\\n\\nEvan Selinger, professor of philosophy at the Rochester Institute of Technology, commented, “In order for people, in general, to be better off as AI advances through 2030, a progressive political agenda – one rooted in the protection of civil liberties and human rights and also conscientious of the dangers of widening social and economic inequalities – would have to play a stronger role in governance. In light of current events, it’s hard to be optimistic that such an agenda will have the resources necessary to keep pace with transformative uses of AI throughout ever-increasing aspects of society. To course-correct in time it's necessary for the general public to develop a deep appreciation about why leading ideologies concerning the market, prosperity and security are not in line with human flourishing.”\\n\\nNicholas Beale, leader of the strategy practice at Sciteb, an international strategy and search firm, commented, “All depends on how responsibly AI is applied. AI ‘done right’ will empower. But unless Western CEOs improve their ethics it won't. I'm hoping for the best.”\\n\\nBenjamin Shestakofsky, an assistant professor of sociology at the University of Pennsylvania specializing in digital technology’s impacts on work, said, “Policymakers should act to ensure that citizens have access to knowledge about the effects of AI systems that affect their life chances and a voice in algorithmic governance. The answer to this question will depend on choices made by citizens, workers, organizational leaders and legislators across a broad range of social domains. For example, algorithmic hiring systems can be programmed to prioritize efficient outcomes for organizations or fair outcomes for workers. The profits produced by technological advancement can be broadly shared or can be captured by the shareholders of a small number of high-tech firms.”\\n\\nCharles Zheng, a researcher into machine learning and AI with the National Institute of Mental Health, wrote, “To ensure the best future, politicians must be informed of the benefits and risks of AI and pass laws to regulate the industry and to encourage open AI research. My hope is that AI algorithms advance significantly in their ability to understand natural language, and also in their ability to model humans and understand human values. My fear is that the benefits of AI are restricted to the rich and powerful without being accessible to the general public.”\\n\\nMary Chayko, author of “Superconnected: The Internet, Digital Media, and Techno-Social Life,” said, “We will see regulatory oversight of AI geared toward the protection of those who use it. Having said that, people will need to remain educated as to AI’s impacts on them and to mobilize as needed to limit the power of companies and governments to intrude on their spaces, lives and civil rights. It will take vigilance and hard work to accomplish this, but I feel strongly that we are up to the task.”\\n\\nwww.pewresearch.org\\n\\n70\\n\\nPEW RESEARCH CENTER\\n\\nR “Ray” Wang, founder and principal analyst at Constellation Research, based in Silicon Valley, said, “We have not put the controls of AI in the hands of many. In fact the experience in China has shown how this technology can be used to take away the freedoms and rights of the individual for the purposes of security, efficiency, expediency and whims of the state. On the commercial side, we also do not have any controls in play as to ethical AI. Five elements should be included – transparency, explainability, reversibility, coachability and human-led processes in the design.”\\n\\nJohn Willinsky, professor and director of the Public Knowledge Project at Stanford Graduate School of Education, said, “Uses of AI that reduce human autonomy and freedom will need to be carefully weighed against the gains in other qualities of human life (e.g., driverless cars that improve traffic and increase safety). By 2030, deliberations over such matters will be critical to the functioning of ‘human-machine/AI collaboration.’ My hope, however, is that these deliberations are not framed as collaborations between what is human and what is AI but will be seen as the human use of yet another technology, with the wisdom of such use open to ongoing human consideration and intervention intent on advancing that sense of what is most humane about us.”\\n\\nA professor of media studies at a U.S. university commented, “Technology will be a material expression of social policy. If that social policy is enacted through a justice-oriented democratic process, then it has a better chance of producing justice-oriented outcomes. If it is enacted solely by venture-funded corporations with no obligation to the public interest, most people in 2030 will likely be worse off.”\\n\\nGene Crick, director of the Metropolitan Austin Interactive Network and longtime community telecommunications expert, wrote, “To predict AI will benefit ‘most’ people is more hopeful than certain. … AI can benefit lives at work and home – if competing agendas can be balanced. Key support for this important goal could be technology professionals’ acceptance and commitment regarding social and ethical responsibilities of our work.”\\n\\nAnthony Picciano, a professor of education at the City of New York University’s Interactive Technology and Pedagogy program, responded, “I am concerned that profit motives will lead some companies and individuals to develop AI applications that will threaten, not necessarily improve, our way of life. In the next 10 years we will see evolutionary progress in the development of artificial intelligence. After 2030, we will likely see revolutionary developments that will have significant ramifications on many aspects of human endeavor. We will need to develop checks on artificial intelligence.”\\n\\nBill Woodcock, executive director at Packet Clearing House, the research organization behind global network development, commented, “In short-term, pragmatic ways, learning algorithms\\n\\nwww.pewresearch.org\\n\\n71\\n\\nPEW RESEARCH CENTER\\n\\nwill save people time by automating much of tasks like navigation and package delivery and shopping for staples. But that tactical win comes at a strategic loss as long as the primary application of AI is to extract more money from people, because that puts them in opposition to our interests as a species, helping to enrich a few people at the expense of everyone else. In AI that exploits human psychological weaknesses to sell us things, we have for the first time created something that effectively predates our own species. That’s a fundamentally bad idea and requires regulation just as surely as would self-replicating biological weapons.”\\n\\nEthem Alpaydın, a professor of computer engineering at Bogazici University in Istanbul, responded, “AI will favor the developed countries that actually develop these technologies. AI will help find cures for various diseases and overall improve the living conditions in various ways. For the developing countries, however, whose labor force is mostly unskilled and whose exports are largely low-tech, AI implies higher unemployment, lower income and more social unrest. The aim of AI in such countries should be to add skill to the labor force rather than supplant them. For example, automatic real-time translation systems (e.g., Google’s Babel fish) would allow people who don’t speak a foreign language to find work in the tourism industry.”\\n\\nJoe Whittaker, a former professor of sciences and associate director of the NASA GESTAR program, now associate provost at Jackson State University, said, “Actions should be taken to make the internet universally available and accessible, provide the training and know-how for all users.”\\n\\nJohn Paschoud, councilor for the London borough of Lewisham, said, “It is possible that advances in AI and networked information will benefit ‘most’ people, but this is highly dependent upon on how those benefits are shared. ... If traditional capitalist models of ‘ownership of the means of production’ prevail, then benefits of automated production will be retained by the few who own, not the many who work. Similarly, models of housing, health care, etc., can be equitably distributed and can all be enhanced by technology.”\\n\\nDavid Schlangen, a professor of applied computational linguistics at Bielefeld University in Germany, responded, “If the right regulations are put in place and ad-based revenue models can be controlled in such a way that they cannot be exploited by political interest groups, the potential for AI-based information search and decision support is enormous. That’s a big if, but I prefer to remain optimistic.”\\n\\nKate Carruthers, a chief data and analytics officer based in Australia, predicted, “Humans will increasingly interact with AI on a constant basis and it will become hard to know where the boundaries are between the two. Just as kids now see their mobile phones as an extension of\\n\\nwww.pewresearch.org\\n\\n72\\n\\nPEW RESEARCH CENTER\\n\\nthemselves so too will human/AI integration be. I fear that the cause of democracy and freedom will be lost by 2030, so it might be a darker future. To avoid that, one thing we need to do is ensure the development of ethical standards for the development of AI and ensure that we deal with algorithmic bias. We need to build ethics into our development processes. Further, I assume that tracking and monitoring of people will be an accepted part of life and that there will be stronger regulation on privacy and data security. Every facet of life will be circumscribed by AI, and it will be part of the fabric of our lives.”\\n\\nDavid Zubrow, associate director of empirical research at Carnegie Mellon University’s Software Engineering Institute, said, “How the advances are used demands wisdom, leadership and social norms and values that respect and focus on making the world better for all; education and health care will reach remote and underserved areas, for instance. The fear is control is consolidated in the hands of few that seek to exploit people, nature and technology for their own gain. I am hopeful that this will not happen.”\\n\\nFrancisco S. Melo, an associate professor of computer science at Instituto Superior Técnico in Lisbon, Portugal, responded, “I expect that AI technology will contribute to render several services (in health, assisted living, etc.) more efficient and humane and, by making access to information more broadly available, contribute to mitigate inequalities in society. However, in order for positive visions to become a reality, both AI researchers and the general population should be aware of the implications that such technology can have, particularly in how information is used and the ways by which it can be manipulated. In particular, AI researchers should strive for transparency in their work, in order to demystify AI and minimize the possibility of misuse; the general public, on the other hand, should strive to be educated in the responsible and informed use of technology.”\\n\\nKyung Sin Park, internet law expert and co-founder of Open Net Korea, responded, “AI consists of software and training data. Software is already being made available on an open source basis. What will decide AI’s contribution to humanity will be whether data (used for training AI) will be equitably distributed. Data-protection laws and the open data movement will hopefully do the job of making more data available equally to all people. I imagine a future where people can access AI- driven diagnosis of symptoms, which will drastically reduce health care costs for all.”\\n\\nDoug Schepers, chief technologist at Fizz Studio, said, “AI/ML, in applications and in autonomous devices and vehicles, will make some jobs obsolete, and the resulting unemployment will cause some economic instability that impacts society as a whole, but most individuals will be better off. The social impact of software and networked systems will get increasingly complex, so ameliorating that software problem with software agents may be the only way to decrease harm to\\n\\nwww.pewresearch.org\\n\\n73\\n\\nPEW RESEARCH CENTER\\n\\nhuman lives, but only if we can focus the goal of software to benefit individuals and groups rather than companies or industries.”\\n\\nErik Huesca, president of the Knowledge and Digital Culture Foundation, based in Mexico City, said, “There is a concentration of places where specific AI is developed. It is a consequence of the capital investment that seeks to replace expensive professionals. Universities have to rethink what type of graduates to prepare, especially in areas of health, legal and engineering, where the greatest impact is expected, since the labor displacement of doctors, engineers and lawyers is a reality with the incipient developed systems.”\\n\\nStephen Abram, principal at Lighthouse Consulting Inc., wrote, “I am concerned that individual agency is lost in AI and that appropriate safeguards should be in place around data collection as specified by the individual. I worry that context can be misconstrued by government agencies like ICE, IRS, police, etc. There is a major conversation needed throughout the time during which AI applications are developed, and they need to be evergreen as innovation and creativity spark new developments. Indeed, this should not be part of a political process, but an academic, independent process guided by principles and not economics and commercial entities.”\\n\\nDavid Klann, consultant and software developer at Broadcast Tool & Die, responded, “AI and related technologies will continue to enhance peoples’ lives. I tend toward optimism; I instinctively believe there are enough activists who care about the ethics of AI that the technology will be put to use solving problems that humans cannot solve on their own. Take mapping, for instance. I recently learned about congestion problems caused by directions being optimized for individuals. People are now tweaking the algorithms to account for multiple people taking the ‘most efficient route’ that had become congested and was causing neighborhood disturbance due to the increased traffic. I believe people will construct AI algorithms to learn of and to ‘think ahead’ about such unintended consequences and to avoid them before they become problems. Of course, my fear is that money interests will continue to wield an overwhelming influence over AI and machine learning (ML). These can be mitigated through fully disclosed techniques, transparency and third- party oversight. These third parties may be government institutions or non-government organizations with the strength to ‘enforce’ ethical use of the technologies. Open source code and open ML training data will contribute significantly to this mitigation.”\\n\\nAndrian Kreye, a journalist and documentary filmmaker based in Germany, said, “If humanity is willing to learn from its mistakes with low-level AIs like social media algorithms there might be a chance for AI to become an engine for equality and progress. Since most digital development is driven by venture capital, experience shows that automation and abuse will be the norm.”\\n\\nwww.pewresearch.org\\n\\n74\\n\\nPEW RESEARCH CENTER\\n\\nMai Sugimoto, an associate professor of sociology at Kansai University in Japan, responded, “AI could amplify one’s bias and prejudice. We have to make data unbiased before putting it into AI, but it's not very easy.”\\n\\nAn anonymous respondent wrote, “There are clearly advances associated with AI, but the current global political climate gives no indication that technological advancement in any area will improve most lives in the future. We also need to think ecologically in terms of the interrelationship between technology and other social-change events. For example, medical technology has increased lifespans, but the current opioid crisis has taken many lives in the U.S. among certain demographics.”\\n\\nA founder and president said, “The future of AI is more about the policies we choose and the projects we choose to fund. I think there will be large corporate interests in AI that serve nothing but profits and corporations’ interests. This is the force for the ‘bad.’ However, I also believe that most technologists want to do good, and that most people want to head in a direction for the common good. In the end, I think this force will win out.”\\n\\nA senior strategist in regulatory systems and economics for a top global telecommunications firm wrote, “If we do not strive to improve society, making the weakest better off, the whole system may collapse. So, AI had better serve to make life easier for everyone.”\\n\\nwww.pewresearch.org\\n\\n75\\n\\nPEW RESEARCH CENTER\\n\\n3. Improvements ahead: How humans and AI might evolve together in the next decade\\n\\nOther questions to the experts in this canvassing invited their views on the hopeful things that will occur in the next decade and for examples of specific applications that might emerge. What will human-technology co-evolution look like by 2030? Participants in this canvassing expect the rate of change to fall in a range anywhere from incremental to extremely impactful. Generally, they expect AI to continue to be targeted toward efficiencies in workplaces and other activities, and they say it is likely to be embedded in most human endeavors.\\n\\nThe greatest share of participants in this canvassing said automated systems driven by artificial intelligence are already improving many dimensions of their work, play and home lives and they expect this to continue over the next decade. While they worry over the accompanying negatives of human-AI advances, they hope for broad changes for the better as networked, intelligent systems are revolutionizing everything, from the most pressing professional work to hundreds of the little “everyday” aspects of existence.\\n\\nOne respondent’s answer covered many of the improvements experts expect as machines sit alongside humans as their assistants and enhancers. An associate professor at a major university in Israel wrote, “In the coming 12 years AI will enable all sorts of professions to do their work more efficiently, especially those involving ‘saving life’: individualized medicine, policing, even warfare (where attacks will focus on disabling infrastructure and less in killing enemy combatants and civilians). In other professions, AI will enable greater individualization, e.g., education based on the needs and intellectual abilities of each pupil/student. Of course, there will be some downsides: greater unemployment in certain ‘rote’ jobs (e.g., transportation drivers, food service, robots and automation, etc.).”\\n\\nThis section begins with experts sharing mostly positive expectations for the evolution of humans and AI. It is followed by separate sections that include their thoughts about the potential for AI- human partnerships and quality of life in 2030, as well as the future of jobs, health care and education.\\n\\nAI will be integrated into most aspects of life, producing new efficiencies and enhancing human capacities\\n\\nMany of the leading experts extolled the positives they expect to continue to expand as AI tools evolve to do more things for more people.\\n\\nwww.pewresearch.org\\n\\n76\\n\\nPEW RESEARCH CENTER\\n\\nMartijn van Otterlo, author of “Gatekeeping Algorithms with Human Ethical Bias” and assistant professor of artificial intelligence at Tilburg University in the Netherlands, wrote, “Even though I see many ethical issues, potential problems and especially power imbalance/misuse issues with AI (not even starting about singularity issues and out-of-control AI), I do think AI will change most lives for the better, especially looking at the short horizon of 2030 even more-so, because even bad effects of AI can be considered predominantly ‘good’ by the majority of people. For example, the Cambridge Analytica case has shown us the huge privacy issues of modern social networks in a market economy, but, overall, people value the extraordinary services Facebook offers to improve communication opportunities, sharing capabilities and so on.”\\n\\nVint Cerf, Internet Hall of Fame member and vice president and chief internet evangelist at Google, said, “I see AI and machine learning as augmenting human cognition a la Douglas Engelbart. There will be abuses and bugs, some harmful, so we need to be thoughtful about how these technologies are implemented and used, but, on the whole, I see these as constructive.”\\n\\nMícheál Ó Foghlú, engineering director and DevOps Code Pillar at Google’s Munich office, said, “The trend is that AI/ML models in specific domains can out-perform human experts (e.g., certain cancer diagnoses based on image-recognition in retina scans). I think it would be fairly much the consensus that this trend would continue, and many more such systems could aid human experts to be more accurate.”\\n\\nCraig Mathias, principal at Farpoint Group, an advisory firm specializing in wireless networking and mobile computing, commented, “Many if not most of the large-scale technologies that we all depend upon – such as the internet itself, the power grid, and roads and highways – will simply be unable to function in the future without AI, as both solution complexity and demand continue to increase.”\\n\\nMatt Mason, a roboticist and the former director of the Robotics Institute at Carnegie Mellon University, wrote, “AI will present new opportunities and capabilities to improve the human experience. While it is possible for a society to behave irrationally and choose to use it to their detriment, I see no reason to think that is the more likely outcome.”\\n\\nMike Osswald, vice president of experience innovation at Hanson Inc., commented, “I’m thinking of a world in which people’s devices continuously assess the world around them to keep a population safer and healthier. Thinking of those living in large urban areas, with devices forming a network of AI input through sound analysis, air quality, natural events, etc., that can provide collective notifications and insight to everyone in a certain area about the concerns of\\n\\nwww.pewresearch.org\\n\\n77\\n\\nPEW RESEARCH CENTER\\n\\nenvironmental factors, physical health, even helping provide no quarter for bad actors through community policing.”\\n\\nBarry Hughes, senior scientist at the Center for International Futures at the University of Denver, commented, “I was one of the original test users of the ARPANET and now can hardly imagine living without the internet. Although AI will be disruptive through 2030 and beyond, meaning that there will be losers in the workplace and growing reasons for concern about privacy and AI/cyber-related crime, on the whole I expect that individuals and societies will make choices on use and restriction of use that benefit us. Examples include likely self-driving vehicles at that time, when my wife's deteriorating vision and that of an increased elderly population will make it increasingly liberating. I would expect rapid growth in use for informal/non-traditional education as well as some more ambivalent growth in the formal-education sector. Big-data applications in health-related research should be increasingly productive, and health care delivery should benefit. Transparency with respect to its character and use, including its developers and their personal benefits, is especially important in limiting the inevitable abuse.”\\n\\nDana Klisanin, psychologist, futurist and game designer, predicted, “People will increasingly realize the importance of interacting with each other and the natural world and they will program AI to support such goals, which will in turn support the ongoing emergence of the ‘slow movement.’ For example, grocery shopping and mundane chores will be allocated to AI (smart appliances), freeing up time for preparation of meals in keeping with the slow food movement. Concern for the environment will likewise encourage the growth of the slow goods/slow fashion movement. The ability to recycle, reduce, reuse will be enhanced by the use of in-home 3D printers, giving rise to a new type of ‘craft’ that is supported by AI. AI will support the ‘cradle-to- grave’ movement by making it easier for people to trace the manufacturing process from inception to final product.”\\n\\nLiz Rykert, president at Meta Strategies, a consultancy that works with technology and complex organizational change, responded, “The key for networked AI will be the ability to diffuse equitable responses to basic care and data collection. If bias remains in the programming it will be a big problem. I believe we will be able to develop systems that will learn from and reflect a much broader and more diverse population than the systems we have now.”\\n\\nMichael R. Nelson, a technology policy expert for a leading network services provider who worked as a technology policy aide in the Clinton administration, commented, “Most media reports focus on how machine learning will directly affect people (medical diagnosis, self-driving cars, etc.) but we will see big improvements in infrastructure (traffic, sewage treatment, supply chain, etc.).”\\n\\nwww.pewresearch.org\\n\\n78\\n\\nPEW RESEARCH CENTER\\n\\nGary Arlen, president of Arlen Communications, wrote, “After the initial frenzy recedes about specific AI applications (such as autonomous vehicles, workplace robotics, transaction processing, health diagnoses and entertainment selections), specific applications will develop – probably in areas barely being considered today. As with many new technologies, the benefits will not apply equally, potentially expanding the haves-and-have-nots dichotomy. In addition, as AI delves into new fields – including creative work such as design, music/art composition – we may see new legal challenges about illegal appropriation of intellectual property (via machine learning). However, the new legal tasks from such litigation may not need a conventional lawyer – but could be handled by AI itself. Professional health care AI poses another type of dichotomy. For patients, AI could be a bonanza, identifying ailments, often in early stages (based on early symptoms), and recommending treatments. At the same time, such automated tasks could impact employment for medical professionals. And again, there are legal challenges to be determined, such as liability in the case of a wrong action by the AI. Overall, there is no such thing as ‘most people,’ but many individuals and groups – especially in professional situations – WILL live better lives thanks to AI, albeit with some severe adjustment pains.”\\n\\nTim Morgan, a respondent who provided no identifying details, said, “Algorithmic machine learning will be our intelligence amplifier, exhaustively exploring data and designs in ways humans alone cannot. The world was shocked when IBM’s Deep Blue computer beat Garry Kasparov in 1997. What emerged later was the realization that human and AI ‘centaurs’ could combine to beat anyone, human or AI. The synthesis is more than the sum of the parts.”\\n\\nMarshall Kirkpatrick, product director of influencer marketing, responded, “If the network can be both decentralized and imbued with empathy, rather than characterized by violent exploitation, then we’re safe. I expect it will land in between, hopefully leaning toward the positive. For example, I expect our understanding of self and freedom will be greatly impacted by an instrumentation of a large part of memory, through personal logs and our data exhaust being recognized as valuable just like when we shed the term ‘junk DNA.’ Networked AI will bring us new insights into our own lives that might seem as far-fetched today as it would have been 30 years ago to say, ‘I’ll tell you what music your friends are discovering right now.’ AI is most likely to augment humanity for the better, but it will take longer and not be done as well as it could be. Hopefully we’ll build it in a way that will help us be comparably understanding to others.”\\n\\nDaniel A. Menasce, professor of computer science at George Mason University, commented, “AI and related technologies coupled with significant advances in computer power and decreasing costs will allow specialists in a variety of disciplines to perform more efficiently and will allow non- specialists to use computer systems to augment their skills. Some examples include health\\n\\nwww.pewresearch.org\\n\\n79\\n\\nPEW RESEARCH CENTER\\n\\ndelivery, smart cities and smart buildings. For these applications to become reality, easy-to-use user interfaces, or better yet transparent user interfaces will have to be developed.”\\n\\nDavid Wells, chief financial officer at Netflix, responded, “Technology progression and advancement has always been met with fear and anxiety, giving way to tremendous gains for humankind as we learn to enhance the best of the changes and adapt and alter the worst. Continued networked AI will be no different but the pace of technological change has increased, which is different and requires us to more quickly adapt. This pace is different and presents challenges for some human groups and societies that we will need to acknowledge and work through to avoid marginalization and political conflict. But the gains from better education, medical care and crime reduction will be well worth the challenges.”\\n\\nRik Farrow, editor of ;login: for the USENIX association, wrote, “Humans do poorly when it comes to making decisions based on facts, rather than emotional issues. Humans get distracted easily. There are certainly things that AI can do better than humans, like driving cars, handling finances, even diagnosing illnesses. Expecting human doctors to know everything about the varieties of disease and humans is silly. Let computers do what they are good at.”\\n\\nSteve Crocker, CEO and co-founder of Shinkuro Inc. and Internet Hall of Fame member, responded, “AI and human-machine interaction has been under vigorous development for the past 50 years. The advances have been enormous. The results are marbled through all of our products and systems. Graphics, speech [and] language understanding are now taken for granted. Encyclopedic knowledge is available at our fingertips. Instant communication with anyone, anywhere exists for about half the world at minimal cost. The effects on productivity, lifestyle and reduction of risks, both natural and man-made, have been extraordinary and will continue. As with any technology, there are opportunities for abuse, but the challenges for the next decade or so are not significantly different from the challenges mankind has faced in the past. Perhaps the largest existential threat has been the potential for nuclear holocaust. In comparison, the concerns about AI are significantly less.”\\n\\nJames Kadtke, expert on converging technologies at the Institute for National Strategic Studies at the U.S. National Defense University, wrote, “Barring the deployment of a few different radically new technologies, such as general AI or commercial quantum computers, the internet and AI [between now and 2030] will proceed on an evolutionary trajectory. Expect internet access and sophistication to be considerably greater, but not radically different, and also expect that malicious actors using the internet will have greater sophistication and power. Whether we can control both these trends for positive outcomes is a public policy issue more than a technological one.”\\n\\nwww.pewresearch.org\\n\\n80\\n\\nPEW RESEARCH CENTER\\n\\nTim Morgan, a respondent who provided no identifying details, said, “Human/AI collaboration over the next 12 years will improve the overall quality of life by finding new approaches to persistent problems. We will use these adaptive algorithmic tools to explore whole new domains in every industry and field of study: materials science, biotech, medicine, agriculture, engineering, energy, transportation and more. … This goes beyond computability into human relationships. AIs are beginning to understand and speak the human language of emotion. The potential of affective computing ranges from productivity-increasing adaptive interfaces, to ‘pre-crime’ security monitoring of airports and other gathering places, to companion ‘pets’ which monitor their aging owners and interact with them in ways that improve their health and disposition. Will there be unseen dangers or consequences? Definitely. That is our pattern with our tools. We invent them, use them to improve our lives and then refine them when we find problems. AI is no different.”\\n\\nAshok Goel, director of the human-centered computing Ph.D. program at Georgia Tech, wrote, “Human-AI interaction will be multimodal: We will directly converse with AIs, for example. However, much of the impact of AI will come in enhancing human-human interaction across both space (we will be networked with others) and time (we will have access to all our previously acquired knowledge). This will aid, augment and amplify individual and collective human intelligence in unprecedented and powerful ways.”\\n\\nDavid Cake, an leader with Electronic Frontiers Australia and vice-chair of the ICANN GNSO Council, wrote, “In general, machine learning and related technologies have the capacity to greatly reduce human error in many areas where it is currently very problematic and make available good, appropriately tailored advice to people to whom it is currently unavailable, in literally almost every field of human endeavour.”\\n\\nFred Baker, an independent networking technologies consultant, longtime leader in the Internet Engineering Task Force and engineering fellow with Cisco, commented, “In my opinion, developments have not been ‘out of control,’ in the sense that the creation of Terminator's Skynet or the HAL 9000 computer might depict them. Rather, we have learned to automate processes in which neural networks have been able to follow data to its conclusion (which we call ‘big data’) unaided and uncontaminated by human intuition, and sometimes the results have surprised us. These remain, and in my opinion will remain, to be interpreted by human beings and used for our purposes.”\\n\\nBob Frankston, software innovation pioneer and technologist based in North America, wrote, “It could go either way. AI could be a bureaucratic straitjacket and tool of surveillance. I’m betting that machine learning will be like the X-ray in giving us the ability to see new wholes and gain insights.”\\n\\nwww.pewresearch.org\\n\\n81\\n\\nPEW RESEARCH CENTER\\n\\nPerry Hewitt, a marketing, content and technology executive, wrote, “Today, voice-activated technologies are an untamed beast in our homes. Some 16% of Americans have a smart speaker, and yet they are relatively dumb devices: They misinterpret questions, offer generic answers and, to the consternation of some, are turning our kids into a**holes. I am bullish on human-machine interactions developing a better understanding of and improving our daily routines. I think in particular of the working parent, often although certainly not exclusively a woman, who carries so much information in their head. What if a human-machine collaboration could stock the house with essentials, schedule the pre-camp pediatrician appointments and prompt drivers for the alternate-side parking/street cleaning rules. The ability for narrow AI to assimilate new information (the bus is supposed to come at 7:10 but a month into the school year is known to actually come at 7:16) could keep a family connected and informed with the right data, and reduce the mental load of household management.”\\n\\nJohn McNutt, a professor in the school of public policy and administration at the University of Delaware, responded, “Throwing out technology because there is a potential downside is not how human progress takes place. In public service, a turbulent environment has created a situation where knowledge overload can seriously degrade our ability to do the things that are essential to implement policies and serve the public good. AI can be the difference between a public service that works well and one that creates more problems than it solves.”\\n\\nRandy Marchany, chief information security officer at Virginia Tech and director of Virginia Tech’s IT Security Laboratory, said, “AI-human interaction in 2030 will be in its ‘infancy’ stage. AI will need to go to ‘school’ in a manner similar to humans. They will amass large amounts of data collected by various sources but need ‘ethics’ training to make good decisions. Just as kids are taught a wide variety of info and some sort of ethics (religion, social manners, etc.), AI will need similar training. Will AI get the proper training? Who decides the training content?”\\n\\nRobert Stratton, cybersecurity expert, said, “While there is widespread acknowledgement in a variety of disciplines of the potential benefits of machine learning and artificial intelligence technologies, progress has been tempered by their misapplication. Part of data science is knowing the right tool for a particular job. As more-rigorous practitioners begin to gain comfort and apply these tools to other corpora it’s reasonable to expect some significant gains in efficiency, insight or profitability in many fields. This may not be visible to consumers except through increased product choice, but it may include everything from drug discovery to driving.”\\n\\nA data analyst for an organization developing marketing solutions said, “Assuming that policies are in place to prevent the abuse of AI and programs are in place to find new jobs for those who would be career-displaced, there is a lot of potential in AI integration. By 2030, most AI will\\n\\nwww.pewresearch.org\\n\\n82\\n\\nPEW RESEARCH CENTER\\n\\nbe used for marketing purposes and be more annoying to people than anything else as they are bombarded with personalized ads and recommendations. The rest of AI usage will be its integration into more tedious and repetitive tasks across career fields. Implementing AI in this fashion will open up more time for humans to focus on long-term and in-depth tasks that will allow further and greater societal progression. For example, AI can be trained to identify and codify qualitative information from surveys, reviews, articles, etc., far faster and in greater quantities than even a team of humans can. By having AI perform these tasks, analysts can spend more time parsing the data for trends and information that can then be used to make more- informed decisions faster and allow for speedier turn-around times. Minor product faults can be addressed before they become widespread, scientists can generate semiannual reports on environmental changes rather than annual or biannual.”\\n\\nHelena Draganik, a professor at the University of Gdańsk in Poland, responded, “AI will not change humans. It will change the relations between them because it can serve as an interpreter of communication. It will change our habits (as an intermediation technology). AI will be a great commodity. It will help in cases of health problems (diseases). It will also generate a great ‘data industry’ (big data) market and a lack of anonymity and privacy. Humanity will more and more depend on energy/electricity. These factors will create new social, cultural, security and political problems.”\\n\\nThere are those who think there won’t be much change by 2030.\\n\\nChristine Boese, digital strategies professional, commented, “I believe it is as William Gibson postulated, ‘The future is already here, it just not very evenly distributed.’ What I know from my work in user-experience design and in exposure to many different Fortune 500 IT departments working in big data and analytics is that the promise and potential of AI and machine learning is VASTLY overstated. There has been so little investment in basic infrastructure, entire chunks of our systems won't even be interoperable. The AI and machine learning code will be there, in a pocket here, a pocket there, but system-wide, it is unlikely to be operating reliably as part of the background radiation against which many of us play and work online.”\\n\\nAn anonymous respondent wrote, “While various deployments of new data science and computation will help firms cut costs, reduce fraud and support decision-making that involves access to more information than an individual can manage, organisations, professions, markets and regulators (public and private) usually take many more than 12 years to adapt effectively to a constantly changing set of technologies and practices. This generally causes a decline in service quality, insecurity over jobs and investments, new monopoly businesses distorting markets and social values, etc. For example, many organisations will be under pressure to buy and implement\\n\\nwww.pewresearch.org\\n\\n83\\n\\nPEW RESEARCH CENTER\\n\\nnew services, but unable to access reliable market information on how to do this, leading to bad investments, distractions from core business, and labour and customer disputes.”\\n\\nMario Morino, chairman of the Morino Institute and co-founder of Venture Philanthropy Partners, commented, “While I believe AI/ML will bring enormous benefits, it may take us several decades to navigate through the disruption and transition they will introduce on multiple levels.”\\n\\nDaniel Berninger, an internet pioneer who led the first VoIP deployments at Verizon, HP and NASA, currently founder at Voice Communication Exchange Committee (VCXC), said, “The luminaries claiming artificial intelligence will surpass human intelligence and promoting robot reverence imagine exponentially improving computation pushes machine self-actualization from science fiction into reality. The immense valuations awarded Google, Facebook, Amazon, Tesla, et al., rely on this machine-dominance hype to sell infinite scaling. As with all hype, pretending reality does not exist does not make reality go away. Moore’s Law does not concede the future to machines, because human domination of the planet does not owe to computation. Any road map granting machines self-determination includes ‘miracle’ as one of the steps. You cannot turn a piece of wood into a real boy. AI merely ‘models’ human activity. No amount of improvement in the development of these models turns the ‘model’ into the ‘thing.’ Robot reverence attempts plausibility by collapsing the breadth of human potential and capacities. It operates via ‘denialism’ with advocates disavowing the importance of anything they cannot model. In particular, super AI requires pretending human will and consciousness do not exist. Human beings remain the source of all intent and the judge of all outcomes. Machines provide mere facilitation and mere efficiency in the journey from intent to outcome. The dehumanizing nature of automation and the diseconomy of scale of human intelligence is already causing headaches that reveal another AI Winter arriving well before 2030.”\\n\\nPaul Kainen, futurist and director of the Lab for Visual Mathematics at Georgetown University, commented, “Quantum cat here: I expect complex superposition of strong positive, negative and null as typical impact for AI. For the grandkids’ sake, we must be positive!”\\n\\nThe following one-liners from anonymous respondents also tie into AI in 2030:\\n\\n§ An Internet Hall of Fame member wrote, “You'll talk to your digital assistant in a\\n\\nnormal voice and it will just be there – it will often anticipate your needs, so you may only need to talk to it to correct or update it.”\\n\\n§ The director of a cognitive research group at one of the world’s top AI and\\n\\nlarge-scale computing companies predicted that by 2030, “Smartphone-equivalent devices will support true natural-language dialog with episodic memory of past\\n\\nwww.pewresearch.org\\n\\n84\\n\\nPEW RESEARCH CENTER\\n\\ninteractions. Apps will become low-cost digital workers with basic commonsense reasoning.”\\n\\n§ An anonymous Internet Hall of Fame member said, “The equivalent of the ‘Star\\n\\nTrek’ universal translator will become practical, enabling travelers to better interact with people in countries they visit, facilitate online discussions across language barriers, etc.” § An Internet of Things researcher commented, “We need to balance between human emotions and machine intelligence – can machines be emotional? – that’s the frontier we have to conquer.”\\n\\n§ An anonymous respondent wrote, “2030 is still quite possibly before the advent of human-level AI. During this phase AI is still mostly augmenting human efforts – increasingly ubiquitous, optimizing the systems that surround us and being replaced when their optimization criteria are not quite perfect – rather than pursuing those goals programmed into them, whether we find the realization of those goals desirable or not.”\\n\\n§ A research scientist who works for Google said, “Things will be better, although\\n\\nmany people are deeply worried about the effects of AI.”\\n\\n§ An ARPANET and internet pioneer wrote, “The kind of AI we are currently able to build as good for data analysis but far, far away from ‘human’ levels of performance; the next 20 years won’t change this, but we will have valuable tools to help analyze and control our world.”\\n\\n§ An artificial intelligence researcher working for one of the world’s most powerful technology companies wrote, “AI will enhance our vision and hearing capabilities, remove language barriers, reduce time to find information we care about and help in automating mundane activities.”\\n\\n§ A manager with a major digital innovation company said, “Couple the information storage with the ever-increasing ability to rapidly search and analyze that data, and the benefits to augmenting human intelligence with this processed data will open up new avenues of technology and research throughout society.”\\n\\nOther anonymous respondents commented:\\n\\n§\\n\\n§ §\\n\\n§\\n\\n“AI will help people to manage the increasingly complex world we are forced to navigate. It will empower individuals to not be overwhelmed.” “AI will reduce human error in many contexts: driving, workplace, medicine and more.” “In teaching it will enhance knowledge about student progress and how to meet individual needs; it will offer guidance options based on the unique preferences of students that can guide learning and career goals.” “2030 is only 12 years from now, so I expect that systems like Alexa and Siri will be more helpful but still of only medium utility.”\\n\\nwww.pewresearch.org\\n\\n85\\n\\nPEW RESEARCH CENTER\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n“AI will be a useful tool; I am quite a ways away from fearing SkyNet and the rise of the machines.” “AI will produce major benefits in the next 10 years, but ultimately the question is one of politics: Will the world somehow manage to listen to the economists, even when their findings are uncomfortable?” “I strongly believe that an increasing use of numerical control will improve the lives of people in general.” “AI will help us navigate choices, find safer routes and avenues for work and play, and help make our choices and work more consistent.” “Many factors will be at work to increase or decrease human welfare, and it will be difficult to separate them.”\\n\\nAI will optimize and augment people’s lives\\n\\nThe hopeful experts in this sample generally expect that AI will work to optimize, augment and improve human activities and experiences. They say it will save time and it will save lives via health advances and the reduction of risks and of poverty. They hope it will spur innovation and broaden opportunities, increase the value of human-to-human experiences, augment humans and increase individuals’ overall satisfaction with life.\\n\\nClay Shirky, writer and consultant on the social and economic effects of internet technologies and vice president at New York University, said, “All previous forms of labor-saving devices, from the level to the computer, have correlated with increased health and lifespan in the places that have adopted them.”\\n\\nJamais Cascio, research fellow at the Institute for the Future, wrote, “Although I do believe that in 2030 AI will have made our lives better, I suspect that popular media of the time will justifiably highlight the large-scale problems: displaced workers, embedded bias and human systems being too deferential to machine systems. But AI is more than robot soldiers, autonomous cars or digital assistants with quirky ‘personalities.’ Most of the AI we will encounter in 2030 will be in-the-walls, behind-the-scenes systems built to adapt workspaces, living spaces and the urban environment to better suit our needs. Medical AI will keep track of medication and alert us to early signs of health problems. Environmental AI will monitor air quality, heat index and other indicators relevant to our day’s tasks. Our visual and audio surroundings may be altered or filtered to improve our moods, better our focus or otherwise alter our subconscious perceptions of the world. Most of this AI will be functionally invisible to us, as long as it’s working properly. The explicit human-machine interface will be with a supervisor system that coordinates all of the sub-AI – and undoubtedly there will be a lively business in creating supervisor systems with quirky personalities.”\\n\\nwww.pewresearch.org\\n\\n86\\n\\nPEW RESEARCH CENTER\\n\\nMike Meyer, chief information officer at Honolulu Community College, wrote, “Social organizations will be increasingly administered by AI/ML systems to ensure equity and consistency in provisioning of services to the population. The steady removal of human emotion- driven discrimination will rebalance social organizations creating true equitable opportunity to all people for the first time in human history. People will be part of these systems as censors, in the old imperial Chinese model, providing human emotional intelligence where that is needed to smooth social management. All aspects of human existence will be affected by the integration of AI into human societies. Historically this type of base paradigmatic change is both difficult and unstoppable. The results will be primarily positive but will produce problems both in the process of change and in totally new types of problems that will result from the ways that people do adapt the new technology-based processes.”\\n\\nMark Crowley, an assistant professor, expert in machine learning and core member of the Institute for Complexity and Innovation at the University of Waterloo in Ontario, Canada, wrote, “While driving home on a long commute from work the human will be reading a book in the heads-up screen of the windshield. The car will be driving autonomously on the highway for the moment. The driver will have an idea to note down and add to a particular document; all this will be done via voice. In the middle of this a complicated traffic arrangement will be seen approaching via other networked cars. The AI will politely interrupt the driver, put away the heads-up display and warn the driver they may need to take over in the next 10 seconds or so. The conversation will be flawless and natural, like Jarvis in ‘Avengers,’ even charming. But it will be tasks-focused to the car, personal events, notes and news.”\\n\\nTheodore Gordon, futurist, management consultant and co-founder of the Millennium Project, commented, “There will be ups and downs, surely, but the net is, I believe, good. The most encouraging uses of AI will be in early warning of terror activities, incipient diseases and environmental threats and in improvements in decision-making.”\\n\\nYvette Wohn, director of the Social Interaction Lab and expert on human-computer interaction at the New Jersey Institute of Technology, said, “One area in which artificial intelligence will become more sophisticated will be in its ability to enrich the quality of life so that the current age of workaholism will transition into a society where leisure, the arts, entertainment and culture are able to enhance the well-being of society in developed countries and solve issues of water production, food growth/distribution and basic health provision in developing countries.”\\n\\nKen Goldberg, distinguished chair in engineering, director of AUTOLAB’s and CITRIS’ “people and robots” initiative, and founding member of the Berkeley Artificial Intelligence Research Lab at the University of California, Berkeley, said, “As in the past 50+ years, AI will be combined with IA\\n\\nwww.pewresearch.org\\n\\n87\\n\\nPEW RESEARCH CENTER\\n\\n(intelligence augmentation) to enhance humans’ ability to work. One example might be an AI- based ‘Devil's Advocate’ that would challenge my decisions with insightful questions (as long as I can turn it off periodically).”\\n\\nRich Ling, a professor of media technology at Nanyang Technological University, responded, “The ability to address complex issues and to better respond to and facilitate the needs of people will be the dominant result of AI.”\\n\\nAn anonymous respondent wrote, “There will be an explosive increase in the number of autonomous cognitive agents (e.g., robots), and humans will interact more and more with them, being unaware, most of the time, if it is interactivity with a robot or with another human. This will increase the number of personal assistants and the level of service.”\\n\\nFred Davis, mentor at Runway Incubator in San Francisco, responded, “As daily a user of the Google Assistant on my phone and both Google Home and Alexa, I feel like AI has already been delivering significant benefits to my daily life for a few years. My wife and I take having an always- on omnipresent assistant on hand for granted at this point. Google Home’s ability to tell us apart and even respond with different voices is a major step forward in making computers people- literate, rather than the other way around. There’s always a concern about privacy, but so far it hasn’t caused us any problems. Obviously, this could change and instead of a helpful friend I might look at these assistants as creepy strangers. Maintaining strict privacy and security controls is essential for these types of services.”\\n\\nAndrew Tutt, an expert in law and author of “An FDA for Algorithms,” which called for “critical thought about how best to prevent, deter and compensate for the harms that they cause,” said, “AI will be absolutely pervasive and absolutely seamless in its integration with everyday life. It will simply become accepted that AI are responsible for ever-more-complex and ever-more-human tasks. By 2030, it will be accepted that when you wish to hail a taxi the taxi will have no driver – it will be an autonomously driven vehicle. Robots will be responsible for more-dynamic and complex roles in manufacturing plants and warehouses. Digital assistants will play an important and interactive role in everyday interactions ranging from buying a cup of coffee to booking a salon appointment. It will no longer be unexpected to call a restaurant to book a reservation, for example, and speak to a ‘digital’ assistant who will pencil you in. These interactions will be incremental but become increasingly common and increasingly normal. My hope is that the increasing integration of AI into everyday life will vastly increase the amount of time that people can devote to tasks they find meaningful.”\\n\\nwww.pewresearch.org\\n\\n88\\n\\nPEW RESEARCH CENTER\\n\\nL. Schomaker, professor at the University of Groningen and scientific director of the Artificial Intelligence and Cognitive Engineering (ALICE) research institute, said, “In the 1990s, you went to a PC on a desktop in a room in your house. In the 2010s you picked a phone from your pocket and switched it on. By 2030 you will be online 24/7 via miniature devices such as in-ear continuous support, advice and communications.”\\n\\nMichael Wollowski, associate professor of computer science and software engineering at Rose- Hulman Institute of Technology and expert in the Internet of Things, diagrammatic systems, and artificial intelligence, wrote, “Assuming that industry and government are interested in letting the consumer choose and influence the future, there will be many fantastic advances of AI. I believe that AI and the Internet of Things will bring about a situation in which technology will be our guardian angel. For example, self-driving cars will let us drive faster than we ever drove before, but they will only let us do things that they can control. Since computers have much better reaction time than people, it will be quite amazing. Similarly, AI and the Internet of Things will let us conduct out lives to the fullest while ensuring that we live healthy lives. Again, it is like having a guardian angel that lets us do things, knowing they can save us from stupidity.”\\n\\nSteve King, partner at Emergent Research, said, “2030 is less than 12 years away. So … the most likely scenario is AI will have a modest impact on the lives of most humans over this time frame. Having said that, we think the use of AI systems will continue to expand, with the greatest growth coming from systems that augment and complement human capabilities and decision-making. This is not to say there won’t be negative impacts from the use of AI. Jobs will be replaced, and certain industries will be disrupted. Even scarier, there are many ways AI can be weaponized. But like most technological advancements, we think the overall impact of AI will be additive – at least over the next decade or so.”\\n\\nVassilis Galanos, a Ph.D. student and teaching assistant actively researching future human- machine symbiosis at the University of Edinburgh, commented, “2030 is not that far away, so there is no room for extremely utopian/dystopian hopes and fears. … Given that AI is already used in everyday life (social-media algorithms, suggestions, smartphones, digital assistants, health care and more), it is quite probable that humans will live in a harmonious co-existence with AI as much as they do now – to a certain extent – with computer and internet technologies.”\\n\\nCharlie Firestone, communications and society program executive director and vice president at the Aspen Institute, commented, “I remain optimistic that AI will be a tool that humans will use, far more widely than today, to enhance quality of life such as medical remedies, education and the environment. For example, the AI will help us to conserve energy in homes and in transportation by identifying exact times and temperatures we need, identifying sources of energy that will be the\\n\\nwww.pewresearch.org\\n\\n89\\n\\nPEW RESEARCH CENTER\\n\\ncheapest and the most efficient. There certainly are dire scenarios, particularly in the use of AI for surveillance, a likely occurrence by 2030. I am hopeful that AI and other technologies will identify new areas of employment as it eliminates many jobs.”\\n\\nPedro U. Lima, an associate professor of computer science at Instituto Superior Técnico in Lisbon, Portugal, said, “Overall, I see AI-based technology relieving us from repetitive and/or heavy and/or dangerous tasks, opening new challenges for our activities. I envisage autonomous mobile robots networked with a myriad of other smart devices, helping nurses and doctors at hospitals in daily activities, working as a ‘third hand’ and (physical and emotional) support to patients. I see something similar happening in factories, where networked robot systems will help workers on their tasks, relieving them from heavy duties.”\\n\\nJohn Laird, a professor of computer science and engineering at the University of Michigan, responded, “There will be a continual off-loading of mundane intellectual and physical tasks on to AI and robotic systems. In addition to helping with everyday activities, it will significantly help the mentally and physically impaired and disabled. There will also be improvements in customized/individualized education and training of humans, and conversely, the customization of AI systems by everyday users. We will be transitioning from current programming practices to user customization. Automated driving will be a reality, eliminating many deaths but also having significant societal changes.”\\n\\nSteven Polunsky, director of the Alabama Transportation Policy Research Center at the University of Alabama, wrote, “AI will allow public transportation systems to better serve existing customers by adjusting routes, travel times and stops to optimize service. New customers will also see advantages. Smart transportation systems will allow public transit to network with traffic signals and providers of ‘last-mile’ trips to minimize traffic disruption and inform decision making about modal (rail, bus, mobility-on-demand) planning and purchasing.”\\n\\nSanjiv Das, a professor of data science and finance at Santa Clara University, responded, “AI will enhance search to create interactive reasoning and analytical systems. Search engines today do not know ‘why’ we want some information and hence cannot reason about it. They also do not interact with us to help with analysis. An AI system that collects information based on knowing why it is needed and then asks more questions to refine its search would be clearly available well before 2030. These ‘search-thinking bots’ will also write up analyses based on parameters elicited from conversation and imbue these analyses with different political (left/right) and linguistic (aggressive/mild) slants, chosen by the human, using advances in language generation, which are already well under way. These ‘intellectual’ agents will become companions, helping us make sense of our information overload. I often collect files of material on my cloud drive that I found\\n\\nwww.pewresearch.org\\n\\n90\\n\\nPEW RESEARCH CENTER\\n\\ninteresting or needed to read later, and these agents would be able to summarize and engage me in a discussion of these materials, very much like an intellectual companion. It is unclear to me if I would need just one such agent, though it seems likely that different agents with diverse personalities may be more interesting! As always, we should worry what the availability of such agents might mean for normal human social interaction, but I can also see many advantages in freeing up time for socializing with other humans as well as enriched interactions, based on knowledge and science, assisted by our new intellectual companions.”\\n\\nLawrence Roberts, designer and manager of ARPANET, the precursor to the internet and Internet Hall of Fame member, commented, “AI voice recognition, or text, with strong context understanding and response will allow vastly better access to website, program documentation, voice call answering, and all such interactions will greatly relieve user frustration with getting information. It will mostly provide service where no or little human support is being replaced as it is not available today in large part. For example, finding and/or doing a new or unused function of the program or website one is using. Visual, 3D-space-recognition AI to support better-than- human robot activity including vehicles, security surveillance, health scans and much more.”\\n\\nChristopher Yoo, a professor of law, communication and computer and information science at the University of Pennsylvania Law School, responded, “AI is good at carrying out tasks that follow repetitive patterns. In fact, AI is better than humans. Shifting these functions to machines will improve performance. It will also allow people to shift their efforts to high-value-added and more- rewarding directions, an increasingly critical consideration in developing world countries where population is declining. Research on human-computer interaction (HCI) also reveals that AI- driven pattern recognition will play a critical role in expanding humans’ ability to extend the benefits of computerization. HCI once held that our ability to gain the benefit from computers would be limited by the total amount of time people can spend sitting in front of a screen and inputting characters through a keyboard. The advent of AI-driven HCI will allow that to expand further and will reduce the amount of customization that people will have to program in by hand. At the same time, AI is merely a tool. All tools have their limits and can be misused. Even when humans are making the decisions instead of machines, blindly following the results of a protocol without exercising any judgment, can have disastrous results. Future applications of AI will thus likely involve both humans and machines if they are to fulfill their potential.”\\n\\nJoseph Konstan, distinguished professor of computer science specializing in human-computer interaction and AI at the University of Minnesota, predicted, “Widespread deployment of AI has immense potential to help in key areas that affect a large portion of the world's population, including agriculture, transportation (more efficiently getting food to people) and energy. Even as soon as 2030, I expect we’ll see substantial benefits for many who are today disadvantaged,\\n\\nwww.pewresearch.org\\n\\n91\\n\\nPEW RESEARCH CENTER\\n\\nincluding the elderly and physically handicapped (who will have greater choices for mobility and support) and those in the poorest part of the world.”\\n\\nThe future of work: Some predict new work will emerge or solutions will be found, while others have deep concerns about massive job losses and an unraveling society\\n\\nA number of expert insights on this topic were shared earlier in this report. These additional observations add to the discussion of hopes and concerns about the future of human jobs. This segment starts with comments from those who are hopeful that the job situation and related social issues will turn out well. It is followed by statements from those who are pessimistic.\\n\\nRespondents who were positive about the future of AI and work\\n\\nBob Metcalfe, Internet Hall of Fame member, co-inventor of Ethernet, founder of 3Com and now professor of innovation and entrepreneurship at the University of Texas at Austin, said, “Pessimists are often right, but they never get anything done. All technologies come with problems, sure, but … generally, they get solved. The hardest problem I see is the evolution of work. Hard to figure out. Forty percent of us used to know how to milk cows, but now less than 1% do. We all used to tell elevator operators which floor we wanted, and now we press buttons. Most of us now drive cars and trucks and trains, but that’s on the verge of being over. AIs are most likely not going to kill jobs. They will handle parts of jobs, enhancing the productivity of their humans.”\\n\\nStowe Boyd, founder and managing director at Work Futures, said, “There is a high possibility that unchecked expansion of AI could rapidly lead to widespread unemployment. My bet is that governments will step in to regulate the spread of AI, to slow the impacts of this phenomenon as a result of unrest by the mid 2020s. That regulation might include, for example, not allowing AIs to serve as managers of people in the workplace, but only to augment the work of people on a task or process level. So, we might see high degrees of automation in warehouses, but a human being would be ‘in charge’ in some sense. Likewise, fully autonomous freighters might be blocked by regulations.”\\n\\nAn anonymous respondent wrote, “Repeatedly throughout history people have worried that new technologies would eliminate jobs. This has never happened, so I'm very skeptical it will this time. Having said that, there will be major short-term disruptions in the labor market and smart governments should begin to plan for this by considering changes to unemployment insurance, universal basic income, health insurance, etc. This is particularly the case in America, where so many benefits are tied to employment. I would say there is almost zero chance that the U.S. government will actually do this, so there will be a lot of pain and misery in the short and medium\\n\\nwww.pewresearch.org\\n\\n92\\n\\nPEW RESEARCH CENTER\\n\\nterm, but I do think ultimately machines and humans will peacefully coexist. Also, I think a lot of the projections on the use of AI are ridiculous. Regardless of the existence of the technology, cross- state shipping is not going to be taken over by automated trucks any time soon because of legal and ethical issues that have not been worked out.”\\n\\nSteven Miller, vice provost and professor of information systems at Singapore Management University, said, “It helps to have a sense of the history of technological change over the past few hundred years (even longer). Undoubtedly, new ways of using machines and new machine capabilities will be used to create economic activities and services that were either a) not previously possible, or b) previously too scarce and expensive, and now can be plentiful and inexpensive. This will create a lot of new activities and opportunities. At the same time, we know some existing tasks and jobs with a high proportion of those tasks will be increasingly automated. So we will simultaneously have both new opportunity creation as well as technological displacement. Even so, the long-term track record shows that human societies keep finding ways of creating more and more economically viable jobs. Cognitive automation will obviously enhance the realms of automation, but even with tremendous progress in this technology, there are and will continue to be limits. Humans have remarkable capabilities to deal with and adapt to change, so I do not see the ‘end of human work.’ The ways people and machines combine together will change – and there will be many new types of human-machine symbiosis. Those who understand this and learn to benefit from it will proposer.”\\n\\nHenry E. Brady, dean of the Goldman School of Public Policy at the University of California, Berkeley, wrote, “AI can replace people in jobs that require sophisticated and accurate pattern matching – driving, diagnoses based upon medical imaging, proofreading and other areas. There is also the fact that in the past technological change has mostly led to new kinds of jobs rather than the net elimination of jobs. Furthermore, I also believe that there may be limits to what AI can do. It is very good at pattern matching, but human intelligence goes far beyond pattern matching and it is not clear that computers will be able to compete with humans beyond pattern matching. It also seems clear that even the best algorithms will require constant human attention to update, check and revise them.”\\n\\nGeoff Livingston, author and futurist, commented, “The term AI misleads people. What we should call the trend is machine learning or algorithms. ‘Weak’ AI as it is called – today's AI – reduces repetitive tasks that most people find mundane. This in turn produces an opportunity to escape the trap of the proletariat, being forced into monotonous labor to earn a living. Instead of thinking of the ‘Terminator,’ we should view the current trend as an opportunity to seek out and embrace the tasks that we truly love, including more creative pursuits. If we embrace the inevitable evolution of technology to replace redundant tasks, we can encourage today’s youth to\\n\\nwww.pewresearch.org\\n\\n93\\n\\nPEW RESEARCH CENTER\\n\\npursue more creative and strategic pursuits. Further, today’s workers can learn how to manage machine learning or embrace training to pursue new careers that they may enjoy more. My fear is that many will simply reject change and blame technology, as has often been done. One could argue much of today’s populist uprising we are experiencing globally finds its roots in the current displacements caused by machine learning as typified by smart manufacturing. If so, the movement forward will be troublesome, rife with dark bends and turns that we may regret as cultures and countries.”\\n\\nMarek Havrda, director at NEOPAS and strategic adviser for the GoodAI project, a private research and development company based in Prague that focuses on the development of artificial general intelligence and AI applications, explained the issue from his point of view, “The development and implementation of artificial intelligence has brought about questions of the impact it will have on employment. Machines are beginning to fill jobs that have been traditionally reserved for humans, such as driving a car or prescribing medical treatment. How these trends may unfold is a crucial question. We may expect the emergence of ‘super-labour,’ a labour defined by super-high-added-value of human activity due to augmentation by AI. Apart from the ability to deploy AI, super-labour will be characterised by creativity and the ability to co-direct and supervise safe exploration of business opportunities together with perseverance in attaining defined goals. An example may be that by using various online, AI gig workers (and maybe several human gig workers), while leveraging AI to its maximum potential … at all aspects from product design to marketing and after-sales care, three people could create a new service and ensure its smooth delivery for which a medium-size company would be needed today. We can expect growing inequalities between those who have access and are able to use technology and those who do not. However, it seems more important how big a slice of the AI co-generated ‘pie’ is accessible to all citizens in absolute terms (e.g., having enough to finance public service and other public spending) which would make everyone better off than in pre-AI age, than the relative inequalities.”\\n\\nYoram Kalman, an associate professor at the Open University of Israel and member of The Center for Internet Research at the University of Haifa, wrote, “In essence, technologies that empower people also improve their lives. I see that progress in the area of human-machine collaboration empowers people by improving their ability to communicate and to learn, and thus my optimism. I do not fear that these technologies will take the place of people, since history shows that again and again people used technologies to augment their abilities and to be more fulfilled. Although in the past, too, it seemed as if these technologies would leave people unemployed and useless, human ingenuity and the human spirit always found new challenges that could best be tackled by humans.”\\n\\nwww.pewresearch.org\\n\\n94\\n\\nPEW RESEARCH CENTER\\n\\nThomas H. Davenport, distinguished professor of information technology and management at Babson College and fellow of the MIT Initiative on the Digital Economy, responded, “So far, most implementations of AI have resulted in some form of augmentation, not automation. Surveys of managers suggest that relatively few have automation-based job loss as the goal of their AI initiatives. So while I am sure there will be some marginal job loss, I expect that AI will free up workers to be more creative and to do more unstructured work.”\\n\\nYvette Wohn, director of the Social Interaction Lab and expert on human-computer interaction at the New Jersey Institute of Technology, commented, “Artificial intelligence will be naturally integrated into our everyday lives. Even though people are concerned about computers replacing the jobs of humans the best-case scenario is that technology will be augmenting human capabilities and performing functions that humans do not like to do. Smart farms and connected distribution systems will hopefully eliminate urban food deserts and enable food production in areas not suited for agriculture. Artificial intelligence will also become better at connecting people and provide immediate support to people who are in crisis situations.”\\n\\nA principal architect for a major global technology company responded, “AI is a prerequisite to achieving a post-scarcity world, in which people can devote their lives to intellectual pursuits and leisure rather than to labor. The first step will be to reduce the amount of labor required for production of human necessities. Reducing tedium will require changes to the social fabric and economic relationships between people as the demand for labor shrinks below the supply, but if these challenges can be met then everyone will be better off.”\\n\\nTom Hood, an expert in corporate accounting and finance, said, “By 2030, AI will stand for Augmented Intelligence and will play an ever-increasing role in working side-by-side with humans in all sectors to add its advanced and massive cognitive and learning capabilities to critical human domains like medicine, law, accounting, engineering and technology. Imagine a personal bot powered by artificial intelligence working by your side (in your laptop or smartphone) making recommendations on key topics by providing up-to-the-minute research or key pattern recognition and analysis of your organization’s data? One example is a CPA in tax given a complex global tax situation amid constantly changing tax laws in all jurisdictions who would be able to research and provide guidance on the most complex global issues in seconds. It is my hope for the future of artificial intelligence in 2030 that we will be augmenting our intelligence with these ‘machines.’”\\n\\nA professor of computer science expert in systems who works at a major U.S. technological university wrote, “By 2030, we should expect advances in AI, networking and other technologies enabled by AI and networks, e.g., the growing areas of persuasive and\\n\\nwww.pewresearch.org\\n\\n95\\n\\nPEW RESEARCH CENTER\\n\\nmotivational technologies, to improve the workplace in many ways beyond replacing humans with robots.”\\n\\nThe following one-liners from anonymous respondents express a bright future for human jobs:\\n\\n§\\n\\n§ §\\n\\n§\\n\\n“History of technology shows that the number of new roles and jobs created will likely exceed the number of roles and jobs that are destroyed.” “AI will not be competing with humanity but augmenting it for the better.” “We make a mistake when we look for direct impact without considering the larger picture – we worry about a worker displaced by a machine rather than focus on broader opportunities for a better-trained and healthier workforce where geography or income no longer determine access not just to information but to relevant and appropriate information paths.” “AI can significantly improve usability and thus access to the benefits of technology. Many powerful technical tools today require detailed expertise, and AI can bring more of those to a larger swath of the population.”\\n\\nRespondents who have fears about AI’s impact on work\\n\\nA section earlier in this report shared a number of key experts’ concerns about the potential negative impact of AI on the socioeconomic future if steps are not taken soon to begin to adjust to a future with far fewer jobs for humans. Many additional respondents to this canvassing shared fears about this.\\n\\nWout de Natris, an internet cybercrime and security consultant based in Rotterdam, Netherlands, wrote, “Hope: Advancement in health care, education, decision-making, availability of information, higher standards in ICT-security, global cooperation on these issues, etc. Fear: Huge segments of society, especially the middle classes who carry society in most ways, e.g., through taxes, savings and purchases, will be rendered jobless through endless economic cuts by industry, followed by governments due to lower tax income. Hence all of society suffers. Can governments and industry refrain from an overkill of surveillance? Otherwise privacy values keep declining, leading to a lower quality of life.”\\n\\nJonathan Taplin, director emeritus at the University of Southern California’s Annenberg Innovation Lab, wrote, “My fear is that the current political class is completely unprepared for the disruptions that AI and robotics applied at scale will bring to our economy. While techno-utopians point to universal basic income as a possible solution to wide-scale unemployment, there is no indication that anyone in politics has an appetite for such a solution. And because I believe that\\n\\nwww.pewresearch.org\\n\\n96\\n\\nPEW RESEARCH CENTER\\n\\nmeaningful work is essential to human dignity, I’m not sure that universal basic income would be helpful in the first place.”\\n\\nAlex Halavais, an associate professor of social technologies at Arizona State University, wrote, “AI is likely to rapidly displace many workers over the next 10 years, and so there will be some potentially significant negative effects at the social and economic level in the short run.”\\n\\nUta Russmann, professor in the department of communication at FHWien der WKW University of Applied Sciences for Management & Communication, said, “Many people will not be benefitting from this development, as robots will do their jobs. Blue-collar workers, people working in supermarkets stacking shelves, etc., will not be needed less, but the job market will not offer them any other possibilities. The gap between rich and poor will increase as the need for highly skilled and very well-paid people increases and the need for less skilled workers will decrease tremendously.”\\n\\nRoss Stapleton-Gray, principal at Stapleton-Gray and Associates, an information technology and policy consulting firm, commented, “Human-machine interaction could be for good or for ill. It will be hugely influenced by decisions on social priorities. We may be at a tipping point in recognizing that social inequities need to be addressed, so, say, a decreased need for human labor due to AI will result in more time for leisure, education, etc., instead of increasing wealth inequity.”\\n\\nAneesh Aneesh, author of “Global Labor: Algocratic Modes of Organization” and professor at the University of Wisconsin, Milwaukee, responded, “Just as automation left large groups of working people behind even as the United States got wealthier as a country, it is quite likely that AI systems will automate the service sector in a similar way. Unless the welfare state returns with a vengeance, it is difficult to see the increased aggregate wealth resulting in any meaningful gains for the bottom half of society.”\\n\\nAlper Dincel of T.C. Istanbul Kultur University in Turkey, wrote, “Unqualified people won’t find jobs, as machines and programs take over easy work in the near future. Machines will also solve performance problems. There is no bright future for most people if we don’t start to try finding solutions.”\\n\\nJason Abbott, professor and director at the Center for Asian Democracy at University of Louisville, said, “AI is likely to create significant challenges to the labor force as previously skilled (semi-skilled) jobs are replaced by AI – everything from AI in trucks and distribution to airlines, logistics and even medical records and diagnoses.”\\n\\nwww.pewresearch.org\\n\\n97\\n\\nPEW RESEARCH CENTER\\n\\nKenneth R. Fleischmann, an associate professor at the University of Texas at Austin’s School of Information, responded, “In corporate settings, I worry that AI will be used to replace human workers to a disproportionate extent, such that the net economic benefit of AI is positive, but that economic benefit is not distributed equally among individuals, with a smaller number of wealthy individuals worldwide prospering, and a larger number of less wealthy individuals worldwide suffering from fewer opportunities for gainful employment.”\\n\\nGerry Ellis, founder and digital usability and accessibility consultant at Feel The BenefIT, responded, “Technology has always been far more quickly developed and adopted in the richer parts of the world than in the poorer regions where new technology is generally not affordable. AI cannot be taken as a stand-alone technology but in conjunction with other converging technologies like augmented reality, robotics, virtual reality, the Internet of Things, big data analysis, etc. It is estimated that around 80% of jobs that will be done in 2030 do not exist yet. One of the reasons why unskilled and particularly repetitive jobs migrate to poor countries is because of cheap labour costs, but AI combined with robotics will begin to do many of these jobs. For all of these reasons combined, the large proportion of the earth’s population that lives in the under-developed and developing world is likely to be left behind by technological developments. Unless the needs of people with disabilities are taken into account when designing AI related technologies, the same is true for them (or I should say ‘us,’ as I am blind).”\\n\\nKaren Oates, director of workforce development and financial stability for La Casa de Esperanza, commented, “Ongoing increases in the use of AI will not benefit the working poor and low-to- middle-income people. Having worked with these populations for 10 years I’ve already observed many of these people losing employment when robots and self-operating forklifts are implemented. Although there are opportunities to program and maintain these machines, realistically people who have the requisite knowledge and education will fill those roles. The majority of employers will be unwilling to invest the resources to train employees unless there is an economic incentive from the government to do so. Many lower-wage workers won’t have the confidence to return to school to develop new knowledge/skills when they were unsuccessful in the past. As the use of AI increases, low-wage workers will lose the small niche they hold in our economy.”\\n\\nPeggy Lahammer, director of health/life sciences at Robins Kaplan LLP and legal market analyst, commented, “Jobs will continue to change and as many disappear new ones will be created. These changes will have an impact on society as many people are left without the necessary skills.”\\n\\nwww.pewresearch.org\\n\\n98\\n\\nPEW RESEARCH CENTER\\n\\nA European computer science professor expert in machine learning commented, “The social sorting systems introduced by AI will most likely define and further entrench the existing world order of the haves and the have-nots, making social mobility more difficult and precarious given the unpredictability of AI-driven judgements of fit. The interesting problem to solve will be the fact that initial designs of AI will come with built-in imaginaries of what ‘good’ or ‘correct’ constitutes. The level of flexibility designed in to allow for changes in normative perceptions and judgements will be key to ensuring that AI driven-systems support rather than obstruct productive social change.”\\n\\nStephen McDowell, a professor of communication at Florida State University and expert in new media and internet governance, commented, “Much of our daily lives is made up of routines and habits that we repeat, and AI could assist in these practices. However, just because some things we do are repetitive does not mean they are insignificant. We draw a lot of meaning from things we do on a daily, weekly or annual basis, whether by ourselves or with others. Cultural practices such as cooking, shopping, cleaning, coordinating and telling stories are crucial parts of building our families and larger communities. Similarly, at work, some of the routines are predictable, but are also how we gain a sense of mastery and expertise in a specific domain. In both these examples, we will have to think about how we define knowledge, expertise, collaboration, and growth and development.”\\n\\nDavid Sarokin, author of “Missed Information: Better Information for Building a Wealthier, More Sustainable Future,” commented, “My biggest concern is that our educational system will not keep up with the demands of our modern times. It is doing a poor job of providing the foundations to our students. As more and more jobs are usurped by AI-endowed machines – everything from assembling cars to flipping burgers – those entering the workplace will need a level of technical sophistication that few graduates possess these days.”\\n\\nJustin Amyx, a technician with Comcast, said, “My worry is automation. Automation occurs usually with mundane tasks that fill low-paying, blue-collar-and-under jobs. Those jobs will disappear – lawn maintenance, truck drivers and fast food, to name a few. Those un-skilled or low-skilled workers will be jobless. Unless we have training programs to take care of worker displacement there will be issue.\\n\\nwww.pewresearch.org\\n\\n99\\n\\nPEW RESEARCH CENTER\\n\\nThe future of health care: Great expectations for many lives saved, extended and improved, mixed with worries about data abuses and a divide between ‘the haves and have-nots’\\n\\nMany of these experts have high hopes for continued incremental advances across all aspects of health care and life extension. They predict a rise in access to various tools, including digital agents that can perform rudimentary exams with no need to visit a clinic, a reduction in medical errors and better, faster recognition of risks and solutions. They also worry over the potential for a widening health care divide between those who can afford cutting-edge tools and treatments and those less privileged. They also express concerns about the potential for data abuses such as the denial of insurance or coverage or benefits for select people or procedures.\\n\\nLeonard Kleinrock, Internet Hall of Fame member and co-director of the first host-to-host online connection and professor of computer science at the University of California, Los Angeles, predicted, “As AI and machine learning improve, we will see highly customized interactions between humans and their health care needs. This mass customization will enable each human to have her medical history, DNA profile, drug allergies, genetic makeup, etc., always available to any caregiver/medical professional that they engage with, and this will be readily accessible to the individual as well. Their care will be tailored to their specific needs and the very latest advances will be able to be provided rapidly after the advances are established. The rapid provision of the best medical treatment will provide great benefits. In hospital settings, such customized information will dramatically reduce the occurrence of medical injuries and deaths due to medical errors. My hope and expectation is that intelligent agents will be able to assess the likely risks and the benefits that ensue from proposed treatments and procedures, far better than is done now by human evaluators, such humans, even experts, typically being poor decision makers in the face of uncertainty. But to bring this about, there will need to be carefully conducted tests and experimentation to assess the quality of the outcomes of AI-based decision making in this field. However, as with any ‘optimized’ system, one must continually be aware of the fragility of optimized systems when they are applied beyond the confines of their range of applicability.”\\n\\nKenneth Grady, futurist, founding author of the Algorithmic Society blog and adjunct and advisor at the Michigan State University College of Law, responded, “In the next dozen years, AI will still be moving through a phase where it will augment what humans can do. It will help us sift through, organize and even evaluate the mountains of data we create each day. For example, doctors today still work with siloed data. Each patient’s vital signs, medicines, dosage rates, test results and side effects remain trapped in isolated systems. Doctors must evaluate this data without the benefit of knowing how it compares to the thousands of other patients around the country (or world) with similar problems. They struggle to turn the data into effective treatments\\n\\nwww.pewresearch.org\\n\\n100\\n\\nPEW RESEARCH CENTER\\n\\nby reading research articles and mentally comparing them to each patient’s data. As it evolves, AI will improve the process. Instead of episodic studies, doctors will have near-real-time access to information showing the effects of treatment regimes. Benefits and risks of drug interactions will be identified faster. Novel treatments will become evident more quickly. Doctors will still manage the last mile, interpreting the analysis generated through AI. This human in the loop approach will remain critical during this phase. As powerful as AI will become, it still will not match humans on understanding how to integrate treatment with values. When will a family sacrifice effectiveness of treatment to prolong quality of life? When two life-threatening illnesses compete, which will the patient want treated first? This will be an important learning phase, as humans understand the limits of AI.”\\n\\nCharles Zheng, a researcher into machine learning and AI with the National Institute of Mental Health, commented, “In the year 2030, I expect AI will be more powerful than they currently are, but not yet at human level for most tasks. A patient checking into a hospital will be directed to the correct desk by a robot. The receptionist will be aided by software that listens to their conversation with the patient and automatically populates the information fields without needing the receptionist to type the information. Another program cross-references the database in the cloud to check for errors. The patient’s medical images would first be automatically labeled by a computer program before being sent to a radiologist.”\\n\\nA professor of computer science expert in systems who works at a major U.S. technological university wrote, “By 2030 … physiological monitoring devices (e.g., lower heartbeats and decreasing blood sugar levels) could indicate lower levels of physical alertness. Smart apps could detect those decaying physical conditions (at an individual level) and suggest improvements to the user (e.g., taking a coffee break with a snack). Granted, there may be large- scale problems caused by AI and robots, e.g., massive unemployment, but the recent trends seem to indicate small improvements such as health monitor apps outlined above, would be more easily developed and deployed successfully.”\\n\\nKenneth Cukier, author and senior editor at The Economist, commented, “AI will be making more decisions in life, and some people will be uneasy with that. But these are decisions that are more effectively done by machines, such as assessing insurance risk, the propensity to repay a loan or to survive a disease. A good example is health care: Algorithms, not doctors, will be diagnosing many diseases, even if human doctors are still ‘in the loop.’ The benefit is that healthcare can reach down to populations that are today underserved: the poor and rural worldwide.”\\n\\nwww.pewresearch.org\\n\\n101\\n\\nPEW RESEARCH CENTER\\n\\nGabor Melli, senior director of engineering for AI and machine learning for Sony PlayStation, responded, “My hope is that by 2030 most of humanity will have ready access to health care and education through digital agents.”\\n\\nKate Eddens, research scientist at the Indiana University Network Science Institute, responded, “There is an opportunity for AI to enhance human ability to gain critical information in decision- making, particularly in the world of health care. There are so many moving parts and components to understanding health care needs and deciding how to proceed in treatment and prevention. With AI, we can program algorithms to help refine those decision-making processes, but only when we train the AI tools on human thinking, a tremendous amount of real data and actual circumstances and experiences. There are some contexts in which human bias and emotion can be detrimental to decision-making. For example, breast cancer is over-diagnosed and over-treated. While mammography guidelines have changed to try to reflect this reality, strong human emotion powered by anecdotal experience leaves some practitioners unwilling to change their recommendations based on evidence and advocacy groups reluctant to change their stance based on public outcry. Perhaps there is an opportunity for AI to calculate a more specific risk for each individual person, allowing for a tailored experience amid the broader guidelines. If screening guidelines change to ‘recommended based on individual risk,’ it lessens the burden on both the care provider and the individual. People still have to make their own decisions, but they may be able to do so with more information and a greater understanding of their own risk and reward. This is such a low-tech and simple example of AI, but one in which AI can – importantly – supplement human decision-making without replacing it.”\\n\\nAngelique Hedberg, senior corporate strategy analyst at RTI International, said, “The greatest advancements and achievements will be in health – physical, mental and environmental. The improvements will have positive trickle-down impacts on education, work, gender equality and reduced inequality. AI will redefine our understanding of health care, optimizing existing processes while simultaneously redefining how we answer questions about what it means to be healthy, bringing care earlier in the cycle due to advances in diagnostics and assessment, i.e. in the future preventative care identifies and initiates treatment for illness before symptoms present. The advances will not be constrained to humans; they will include animals and the built environment. This will happen across the disease spectrum. Advanced ‘omics’ will empower better decisions. There will be a push and a pull by the market and individuals. This is a global story, with fragmented and discontinuous moves being played out over the next decade as we witness wildly different experiments in health across the globe. This future is full of hope for individuals and communities. My greatest hope is for disabled individuals and those currently living with disabilities. I’m excited for communities and interpersonal connections as the work in this future will allow for and increase the value of the human-to-human experiences. Progress is often only\\n\\nwww.pewresearch.org\\n\\n102\\n\\nPEW RESEARCH CENTER\\n\\nseen in retrospect; I hope the speed of exponential change allows everyone to enjoy the benefits of these collaborations.”\\n\\nAn anonymous respondent wrote, “In health care, I hope AI will improve the diagnostics and reduce the number of errors. Doctors cannot recall all the possibilities; they have problems correlating all the symptoms and recognizing the patterns. I hope that in the future patients will be interviewed by computers, which will correlate the described symptoms with results of tests. I hope that with the further development of AI and cognitive computing there will be fewer errors in reports of medical imaging and diagnosis.”\\n\\nEduardo Vendrell, a computer science professor at the Polytechnic University of Valencia in Spain, responded, “In the field of health, many solutions will appear that will allow us to anticipate current problems and discover other risk situations more efficiently. The use of personal gadgets and other domestic devices will allow interacting directly with professionals and institutions in any situation of danger or deterioration of our health.”\\n\\nMonica Murero, director of the E-Life International Institute and associate professor in sociology of new technology at the University of Naples Federico II in Italy, commented, “In health care, I foresee positive outcomes in terms of reducing human mistakes, that are currently still creating several failures. Also, I foresee an increased development of mobile (remote) 24/7 health care services and personalized medicine thanks to AI and human-machine collaboration applied to the field.”\\n\\nUta Russmann, professor in the department of communication at FHWien der WKW University of Applied Sciences for Management & Communication, said, “Life expectancy is increasing (globally) and human-machine/AI collaboration will help older people to manage their life on their own by taking care of them, helping them in the household (taking down the garbage, cleaning up, etc.) as well as keeping them company – just like cats and dogs do, but it will be a much more ‘advanced’ interaction.”\\n\\nLindsey Andersen, an activist at the intersection of human rights and technology for Freedom House and Internews, now doing graduate research at Princeton University, commented, “AI will augment human intelligence. In health care, for example, it will help doctors more accurately diagnose and treat disease and continually monitor high-risk patients through internet-connected medical devices. It will bring health care to places with a shortage of doctors, allowing health care workers to diagnose and treat disease anywhere in the world and to prevent disease outbreaks before they start.”\\n\\nwww.pewresearch.org\\n\\n103\\n\\nPEW RESEARCH CENTER\\n\\nAn anonymous respondent said, “The most important place where AI will make a difference is in health care of the elderly. Personal assistants are already capable of many important tasks to help make sure older adults stay in their home. But adding to that emotion detection, more in- depth health monitoring and AI-based diagnostics will surely enhance the power of these tools.”\\n\\nDenis Parra, assistant professor of computer science in the school of engineering at the Pontifical Catholic University of Chile Chile, commented, “I live in a developing country. Whilst there are potential negative aspects of AI (loss of jobs), for people with disabilities AI technology could improve their lives. I imagine people entering a government office or health facility where people with eye- or ear-related disabilities could effortlessly interact to state their necessities and resolve their information needs.”\\n\\nTimothy Leffel, research scientist, National Opinion Research Center (NORC) at the University of Chicago, said, “Formulaic transactions and interactions are particularly ripe for automation. This can be good in cases where human error can cause problems, e.g., for well-understood diagnostic medical testing.”\\n\\nJean-Daniel Fekete, researcher in human-computer interaction at INRIA in France, said, “Humans and machines will integrate more, improving health through monitoring and easing via machine control. Personal data will then become even more revealing and intrusive and should be kept under personal control.”\\n\\nJoe Whittaker, a former professor of sciences and associate director of the NASA GESTAR program, now associate provost at Jackson State University, responded, “My hope is that AI/human-machine interface will become commonplace especially in the academic research and health care arena. I envision significant advances in brain-machine interface to facilitate mitigation of physical and mental challenges. Similar uses in robotics should also be used to assist the elderly.”\\n\\nJames Gannon, global head of eCompliance for emerging technology, cloud and cybersecurity at Novartis, responded, “AI will increase the speed and availability to develop drugs and therapies for orphan indications. AI will assist in general lifestyle and health care management for the average person.”\\n\\nJay Sanders, president and CEO of the Global Telemedicine Group, responded, “AI will bring collective expertise to the decision point, and in health care, bringing collective expertise to the bedside will save many lives now lost by individual medical errors.”\\n\\nwww.pewresearch.org\\n\\n104\\n\\nPEW RESEARCH CENTER\\n\\nGeoff Arnold, CTO for the Verizon Smart Communities organization, said, “One of the most important trends over the next 12 years is the aging population and the high costs of providing them with care and mobility. AI will provide better data-driven diagnoses of medical and cognitive issues and it will facilitate affordable AV-based paratransit for the less mobile. It will support, not replace, human care-givers.”\\n\\nJohn Lazzaro, retired professor of electrical engineering and computer science, University of California, Berkeley, commented, “When I visit my primary care physician today, she spends a fair amount time typing into an EMS application as she’s talking to me. In this sense, the computer has already arrived in the clinic. An AI system that frees her from this clerical task – that can listen and watch and distill the doctor-patient interaction into actionable data – would be an improvement. A more-advanced AI system would be able to form a ‘second opinion’ based on this data as the appointment unfolds, discreetly advising the doctor via a wearable. The end goal is a reduction in the number of ‘false starts’ in-patient diagnosis. If you've read Lisa Sander’s columns in the New York Times, where she traces the arc of difficult diagnoses, you understand the real clinical problem that this system addresses.”\\n\\nSteve Farnsworth, chief marketing officer at Demand Marketing, commented, “Machine learning and AI offer tools to turn that into actionable data. One project using machine learning and big data already was able to predict SIDS correctly 94% of the time. Imagine AI looking at diagnostics, tests and successful treatments of millions of medical cases. We would instantly have a deluge of new cures and know the most effective treatment options using only the data, medicines and therapies we have now. The jump in quality health care alone for humans is staggering. This is only one application for AI.”\\n\\nDaniel Siewiorek, a professor with the Human-Computer Interaction Institute at Carnegie Mellon University, predicted, “AI will enable systems to perform labor-intensive activities where there are labor shortages. For example, consider recovery from an injury. There is a shortage of physical therapists to monitor and correct exercises. AI would enable a virtual coach to monitor, correct and encourage a patient. Virtual coaches could take on the persona of a human companion or a pet, allowing the aging population to live independently.”\\n\\nJoly MacFie, president of the Internet Society, New York chapter, commented, “AI will have many benefits for people with disabilities and health issues. Much of the aging baby boomer generation will be in this category.”\\n\\nwww.pewresearch.org\\n\\n105\\n\\nPEW RESEARCH CENTER\\n\\nThe overall hopes for the future of health care are tempered by concerns that there will continue to be inequities in access to the best care and worries that private health data may be used to limit people’s options.\\n\\nCraig Burdett, a respondent who provided no identifying details, wrote, “While most AI will probably be a positive benefit, the possible darker side of AI could lead to a loss of agency for some. For example, in a health care setting an increasing use of AI could allow wealthier patients access to significantly-more-advanced diagnosis agents. When coupled with a supportive care team, these patients could receive better treatment and a greater range of treatment options. Conversely, less-affluent patients may be relegated to automated diagnoses and treatment plants with little opportunity for interaction to explore alternative treatments. AI could, effectively, manage long-term health care costs by offering lesser treatment (and sub-optimal recovery rates) to individuals perceived to have a lower status. Consider two patients with diabetes. One patient, upon diagnosis, modifies their eating and exercise patterns (borne out by embedded diagnostic tools) and would benefit from more advanced treatment. The second patient fails to modify their behaviour resulting in substantial ongoing treatment that could be avoided by simple lifestyle choices. An AI could subjectively evaluate that the patient has little interest in their own health and withhold more expensive treatment options leading to a shorter lifespan and an overall cost saving.”\\n\\nSumandra Majee, an architect at F5 Networks Inc., said, “AI, deep learning, etc., will become more a part of daily life in advanced countries. This will potentially widen the gap between technology-savvy people and economically well-to-do folks and the folks with limited access to technology. However, I am hopeful that in the field of healthcare, especially when it comes to diagnosis, AI will significantly augment the field, allowing doctors to do a far better job. Many of the routines aspects of checkups can be done via technology. There is no reason an expert human has to be involved in basic A/B testing to reach a conclusion. Machines can be implemented for those tasks and human doctors should only do the critical parts. I do see AI playing a negative role in education, where students may not often actually do the hard work of learning through experience. It might actually make the overall population dumber.”\\n\\nTimothy Graham, a postdoctoral research fellow in sociology and computer science at Australian National University, commented, “In health care, we see current systems already under heavy criticism (e.g., the My Health Record system in Australia, or the NHS Digital program), because they are nudging citizens into using the system through an ‘opt-out’ mechanism and there are concerns that those who do not opt out may be profiled, targeted and/or denied access to services based on their own data.”\\n\\nwww.pewresearch.org\\n\\n106\\n\\nPEW RESEARCH CENTER\\n\\nValarie Bell, a computational social scientist at the University of North Texas, commented, “Let’s say medical diagnosis is taken over by machines, computers and robotics – how will stressful prognoses be communicated? Will a hologram or a computer deliver ‘the bad news’ instead of a physician? Given the health care industry’s inherent profit motives it would be easy for them to justify how much cheaper it would be to simply have devices diagnose, prescribe treatment and do patient care, without concern for the importance of human touch and interactions. Thus, we may devolve into a health care system where the rich actually get a human doctor while everyone else, or at least the poor and uninsured, get the robot.”\\n\\nThe following one-liners from anonymous respondents also tie into the future of health care:\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n§\\n\\n“People could use a virtual doctor for information and first-level response; so much time could be saved!” “The merging of data science and AI could benefit strategic planning of the future research and development efforts that should be undertaken by humanity.” “I see economic efficiencies and advances in preventive medicine and treatment of disease, however, I do think there will be plenty of adverse consequences.” “Data can reduce errors – for instance, in clearly taking into account the side effects of a medicine or use of multiple medications.” “Human-machine/AI collaboration will reduce barriers to proper medical treatment through better recordkeeping and preventative measures.” “AI can take over many of the administrative tasks current doctors must do, allowing them more time with patients.”\\n\\nThe future of education: High hopes for advances in adaptive and individualized learning, but some doubt that there will be any significant progress and worry over digital divide\\n\\nOver the past few decades, experts and amateurs alike have predicted the internet would have large-scale impacts on education. Many of these hopes have not lived up to the hype. Some respondents to this canvassing said the advent of AI could foster those changes. They expect to see more options for affordable adaptive and individualized learning solutions, including digital agents or “AI assistants” that work to enhance student-teacher interactions and effectiveness.\\n\\nBarry Chudakov, founder and principal of Sertain Research and author of “Metalifestream,” commented, “In the learning environment, AI has the potential to finally demolish the retain-to- know learning (and regurgitate) model. Knowing is no longer retaining – machine intelligence\\n\\nwww.pewresearch.org\\n\\n107\\n\\nPEW RESEARCH CENTER\\n\\ndoes that; it is making significant connections. Connect and assimilate becomes the new learning model.”\\n\\nLou Gross, professor of mathematical ecology and expert in grid computing, spatial optimization and modeling of ecological systems at the University of Tennessee, Knoxville, said, “I see AI as assisting in individualized instruction and training in ways that are currently unavailable or too expensive. There are hosts of school systems around the world that have some technology but are using it in very constrained ways. AI use will provide better adaptive learning and help achieve a teacher’s goal of personalizing education based on each student’s progress.”\\n\\nGuy Levi, chief innovation officer for the Center for Educational Technology, based in Israel, wrote, “In the field of education AI will promote personalization, which almost by definition promotes motivation. The ability to move learning forward all the time by a personal AI assistant, which opens the learning to new paths, is a game changer. The AI assistants will also communicate with one another and will orchestrate teamwork and collaboration. The AI assistants will also be able to manage diverse methods of learning, such as productive failure, teach-back and other innovating pedagogies.”\\n\\nMicah Altman, a senior fellow at the Brookings Institution and head scientist in the program on information science at MIT Libraries, wrote, “These technologies will help to adapt learning (and other environments) to the needs of each individual by translating language, aiding memory and providing us feedback on our own emotional and cognitive state and on the environment. We all need adaptation; each of us, practically every day, is at times tired, distracted, fuzzy-headed or nervous, which limits how we learn, how we understand and how we interact with others. AI has the potential to assist us to engage with the world better – even when conditions are not ideal – and to better understand ourselves.”\\n\\nShigeki Goto, Asia-Pacific internet pioneer, Internet Hall of Fame member and a professor of computer science at Waseda University, commented, “AI is already applied to personalized medicine for an individual patient. Similarly, it will be applied to learning or education to realize ‘personalized learning’ or tailored education. We need to collect data which covers both of successful learning and failure experiences, because machine learning requires positive and negative data.”\\n\\nAndreas Kirsch, fellow at Newspeak House, formerly with Google and DeepMind in Zurich and London, wrote, “Higher education outside of normal academia will benefit further from AI progress and empower more people with access to knowledge and information. For example, question-and-answer systems will improve. Tech similar to Google Translate and WaveNet will\\n\\nwww.pewresearch.org\\n\\n108\\n\\nPEW RESEARCH CENTER\\n\\nlower the barrier of knowledge acquisition for non-English speakers. At the same time, child labor will be reduced because robots will be able to perform the tasks far cheaper and faster, forcing governments in Asia to find real solutions.”\\n\\nKristin Jenkins, executive director of BioQUEST Curriculum Consortium, said, “One of the benefits of this technology is the potential to have really effective, responsive education resources. We know that students benefit from immediate feedback and the opportunity to practice applying new information repeatedly to enhance mastery. AI systems are perfect for analyzing students’ progress, providing more practice where needed and moving on to new material when students are ready. This allows time with instructors to focus on more-complex learning, including 21st-century skills.”\\n\\nMike Meyer, chief information officer at Honolulu Community College, commented, “Adult education availability and relevance will undergo a major transformation. Community colleges will become more directly community centers for both occupational training and greatly expanded optional liberal arts, art, crafts and hobbies. Classes will, by 2030, be predominantly augmented- reality-based, with a full mix of physical and virtual students in classes presented in virtual classrooms by national and international universities and organizations. The driving need will be expansion of knowledge for personal interest and enjoyment as universal basic income or equity will replace the automated tasks that had provided subsistence jobs in the old system.”\\n\\nJennifer Groff, co-founder of the Center for Curriculum Redesign, an international non- governmental organization dedicated to redesigning education for the 21st century, wrote, “The impact on learning and learning environments has the potential to be one of the most positive future outcomes. Learning is largely intangible and invisible, making it a ‘black box’ – and our tools to capture and support learning to this point have been archaic. Think large-scale assessment. Learners need tools that help them understand where they are in a learning pathway, how they learn best, what they need next and so on. We're only just beginning to use technology to better answer these questions. AI has the potential to help us better understand learning, gain insights into learners at scale and, ultimately, build better learning tools and systems for them. But as a large social system, it is also prey to the complications of poor public policy that ultimately warps and diminishes AI’s potential positive impact.”\\n\\nNorton Gusky, an education-technology consultant, wrote, “By 2030 most learners will have personal profiles that will tap into AI/machine learning. Learning will happen everywhere and at any time. There will be appropriate filters that will limit the influence of AI, but ethical considerations will also be an issue.”\\n\\nwww.pewresearch.org\\n\\n109\\n\\nPEW RESEARCH CENTER\\n\\nCliff Zukin, professor of public policy and political science at Rutgers University’s School of Planning and Public Policy and the Eagleton Institute of Politics, said, “It takes ‘information’ out of the category of a commodity, and more information makes for better decisions and is democratizing. Education, to me, has always been the status leveler, correcting, to some extent, for birth luck and social mobility. This will be like Asimov’s ‘Foundation,’ where everyone is plugged into the data-sphere. There is a dark side (later) but overall a positive.”\\n\\nHowever, some expect that there will be a continuing digital divide in education, with the privileged having more access to advanced tools and more capacity for using them well, while the less-privileged lag behind.\\n\\nHenning Schulzrinne, co-chair of the Internet Technical Committee of the IEEE Communications Society, professor at Columbia University and Internet Hall of Fame member, said, “Human-mediated education will become a luxury good. Some high school- and college-level teaching will be conducted partially by video and AI-graded assignments, using similar platforms to the MOOC [massive open online courses] models today, with no human involvement, to deal with increasing costs for education (‘robo-TA’).”\\n\\nJoe Whittaker, a former professor of sciences and associate director of the NASA GESTAR program, now associate provost at Jackson State University, responded, “Huge segments of society will be left behind or excluded completely from the benefits of digital advances – many persons in underserved communities as well as others who are socio-economically challenged. This is due to the fact that these persons will be under-prepared generally, with little or no digital training or knowledge base. They rarely have access to the relatively ubiquitous internet, except when at school or in the workplace. Clearly, the children of these persons will be greatly disadvantaged.”\\n\\nSome witnesses of technology’s evolution over the past few decades feel that its most-positive potential has been disappointingly delayed. After witnessing the slower-than-expected progress of tech’s impact on public education since the 1990s, they are less hopeful than others.\\n\\nEd Lyell, longtime educational technologies expert and professor at Adams State University, said education has been held back to this point by the tyranny of the status quo. He wrote, “By 2030, lifelong learning will become more widespread for all ages. The tools already exist, including Khan Academy and YouTube. We don’t have to know as much, just how to find information when we want it. We will have on-demand, 24/7 ‘schooling.’ This will make going to sit-down classroom schools more and more a hindrance to our learning. The biggest negative will be from those protecting current, status-quo education including teachers/faculty, school boards and college administrators. They are protecting their paycheck- or ego-based role. They will need training,\\n\\nwww.pewresearch.org\\n\\n110\\n\\nPEW RESEARCH CENTER\\n\\ncounseling and help to embrace the existing and forthcoming change as good for all learners. Part of the problem now is that they do not want to acknowledge the reality of how current schools are today. Some do a good job, yet these are mostly serving already smarter, higher-income communities. Parents fight to have their children have a school like they experienced, forgetting how inefficient and often useless it was. AI can help customize curricula to each learner and guide/monitor their journey through multiple learning activities, including some existing schools, on-the-job learning, competency-based learning, internships and such. You can already learn much more, and more efficiently, using online resources than almost all of the classes I took in my public schooling and college, all the way through getting a Ph.D.”\\n\\nA consultant and analyst also said that advances in education have been held back by entrenched interests in legacy education systems, writing, “The use of technology in education is minimal today due to the existence and persistence of the classroom-in-a-school model. As we have seen over the last 30 years, the application of artificial intelligence in the field of man/machine interface has grown in many unexpected directions. Who would have thought back in the late 1970s that the breadth of today’s online (i.e., internet) capabilities could emerged? I believe we are just seeing the beginning of the benefits of the man/machine interface for mankind. The institutionalized education model must be eliminated to allow education of each and every individual to grow. The human brain can be ‘educated’ 24 hours a day by intelligent ‘educators’ who may not even be human in the future. Access to information is no longer a barrier as it was 50 years ago. The next step now is to remove the barrier of structured human delivery of learning in the classroom.”\\n\\nBrock Hinzmann, a partner in the Business Futures Network who worked for 40 years as a futures researcher at SRI International, was hopeful in his comments but also issued a serious warning. He wrote: “Most of the improvements in the technologies we call AI will involve machine learning from big data to improve the efficiency of systems, which will improve the economy and wealth. It will improve emotion and intention recognition, augment human senses and improve overall satisfaction in human-computer interfaces. There will also be abuses in monitoring personal data and emotions and in controlling human behavior, which we need to recognize early and thwart. Intelligent machines will recognize patterns that lead to equipment failures or flaws in final products and be able to correct a condition or shut down and pinpoint the problem. Autonomous vehicles will be able to analyze data from other vehicles and sensors in the roads or on the people nearby to recognize changing conditions and avoid accidents. In education and training, AI learning systems will recognize learning preferences, styles and progress of individuals and help direct them toward a personally satisfying outcome.\\n\\nwww.pewresearch.org\\n\\n111\\n\\nPEW RESEARCH CENTER\\n\\n“However, governments or religious organizations may monitor emotions and activities using AI to direct them to ‘feel’ a certain way, to monitor them and to punish them if their emotional responses at work, in education or in public do not conform to some norm. Education could become indoctrination; democracy could become autocracy or theocracy.”\\n\\nwww.pewresearch.org\\n\\n112\\n\\nPEW RESEARCH CENTER\\n\\nAbout this canvassing of experts\\n\\nThe expert predictions reported here about the impact of the internet between 2018 and 2030 came in response to questions asked by Pew Research Center and Elon University’s Imagining the Internet Center in an online canvassing conducted between July 4, 2018, and Aug. 6, 2018. This is the 10th Future of the Internet study the two organizations have conducted together. For this project, we invited more than 10,000 experts and members of the interested public to share their opinions on the likely future of the internet, and 985 responded to at least one of the questions we asked. This report covers only the answers to our questions about AI and the future of humans. We also asked respondents to answer a series of questions tied to the 50th anniversary of the ARPANET/internet; additional reports tied to those responses will be released in 2019, the anniversary year.\\n\\nSpecifically related to artificial intelligence, the participants in the nonscientific canvassing were asked:\\n\\n“Please think forward to the year 2030. Analysts expect that people will become even more dependent on networked artificial intelligence (AI) in complex digital systems. Some say we will continue on the historic arc of augmenting our lives with mostly positive results as we widely implement these networked tools. Some say our increasing dependence on these AI and related systems is likely to lead to widespread difficulties.\\n\\nOur question: By 2030, do you think it is most likely that advancing AI and related technology systems will enhance human capacities and empower them? That is, most of the time, will most people be better off than they are today? Or is it most likely that advancing AI and related technology systems will lessen human autonomy and agency to such an extent that most people will not be better off than the way things are today?”\\n\\nThe answers of the 979 respondents include:\\n\\n63% who said most people will be better off • 37% who said most people will not be better off • 25 respondents who chose not to select either option\\n\\nwww.pewresearch.org\\n\\n113\\n\\nPEW RESEARCH CENTER\\n\\nAdditionally, they were also asked:\\n\\n“Please explain why you chose the answer you did and sketch out a vision of how the human-machine/AI collaboration will function in 2030. Please consider giving an example of how a typical human-machine interaction will look and feel in a specific area, for instance, in the workplace, in family life, in a health care setting or in a learning environment. Why? What is your hope or fear? What actions might be taken to assure the best future?”\\n\\nThe web-based instrument was first sent directly to a list of targeted experts identified and accumulated by Pew Research Center and Elon University during previous “Future of the Internet” studies, as well as those identified in an earlier study of people who made predictions about the likely future of the internet between 1990 to 1995. Additional experts with proven interest in this particular research topic were also added to the list. Among those invited were artificial intelligence researchers, developers and business leaders from leading global organizations, including, to name a few, Oxford, Cambridge, MIT, Stanford and Carnegie Mellon universities, Google, Microsoft, Facebook, Amazon, Kernel, Kyndi, BT and Cloudflare; leaders active in global internet governance and internet research activities, such as the Internet Engineering Task Force (IETF), Internet Corporation for Assigned Names and Numbers (ICANN), Internet Society (ISOC), International Telecommunications Union (ITU), Association of Internet Researchers (AoIR), and the Organization for Economic Cooperation and Development (OECD). We also invited a large number of professionals and policy people working in government, including the National Science Foundation, Federal Communications Commission, U.S. military and European Union; think tanks and interest networks (for instance, those that include professionals and academics in anthropology, sociology, psychology, law, political science and communications); engineering/computer science and business/entrepreneurship faculty, graduate students and postgraduate researchers who have published work tied to these topics; plus many who are active in civil society organizations such as Association for Progressive Communications (APC), Electronic Privacy Information Center (EPIC), Electronic Frontier Foundation (EFF) and Access Now; and those affiliated with newly emerging nonprofits and other research units. Invitees were encouraged to share the survey link with others they believed would have an interest in participating, thus there was a small “snowball” effect as a small percentage of these invitees invited others to weigh in.\\n\\nSince the data are based on a nonrandom sample, the results are not projectable to any population other than the individuals expressing their points of view in this sample.\\n\\nwww.pewresearch.org\\n\\n114\\n\\nPEW RESEARCH CENTER\\n\\nThe respondents’ remarks reflect their personal positions and are not the positions of their employers; the descriptions of their leadership roles help identify their background and the locus of their expertise.\\n\\nAbout half of the expert respondents elected to remain anonymous. Because people’s level of expertise is an important element of their participation in the conversation, anonymous respondents were given the opportunity to share a description of their internet expertise or background and this was noted where relevant in this report.\\n\\nSome 519 respondents answered the demographic questions on the canvassing. About 70% identified themselves as being based in North America, while 30% hail from other corners of the world. When asked about their “primary area of internet interest,” 33% identified themselves as professor/teacher; 17% as research scientists; 13% as futurists or consultants; 8% as technology developers or administrators; 5% as entrepreneurs or business leaders; 5% as advocates or activist users; 4% as pioneers or originators; 1% as legislators, politicians or lawyers; and an additional 13% specified their primary area of interest as “other.”\\n\\nFollowing is a list of some of the key respondents in this canvassing:\\n\\nWalid Al-Saqaf, senior lecturer at Sodertorn University, Sweden, and member of the board of trustees of the Internet Society (ISOC); Aneesh Aneesh, author of “Global Labor: Algocratic Modes of Organization”; Kostas Alexandridis, author of “Exploring Complex Dynamics in Multi-agent-based Intelligent Systems”; Micah Altman, director of research and head scientist for the program on information science at MIT; Geoff Arnold, CTO for the Verizon Smart Communities organization; Rob Atkinson, president of the Information Technology and Innovation Foundation; Collin Baker, senior AI researcher at the International Computer Science Institute at the University of California, Berkeley; Brian Behlendorf, executive director of the Hyperledger project at The Linux Foundation; Nathaniel Borenstein, chief scientist at Mimecast; danah boyd, founder and president of the Data & Society Research Institute, and principal researcher at Microsoft; Stowe Boyd, founder and managing director at Work Futures; Henry E. Brady, dean, Goldman School of Public Policy, University of California, Berkeley; Erik Brynjolfsson, director of the MIT Initiative on the Digital Economy and author of “Machine, Platform, Crowd: Harnessing Our Digital Future”; Jamais Cascio, distinguished fellow at the Institute for the Future; Vint Cerf, Internet Hall of Fame member and vice president and chief internet evangelist at Google; Barry Chudakov, founder and principal at Sertain Research and StreamFuzion Corp.; Joël Colloc, professor at Université du Havre Normandy University and author of “Ethics of Autonomous Information Systems”; Steve Crocker, CEO and co-founder of Shinkuro Inc. and Internet Hall of Fame member; Kenneth Cukier, author and senior editor at\\n\\nwww.pewresearch.org\\n\\n115\\n\\nPEW RESEARCH CENTER\\n\\nThe Economist; Wout de Natris, internet cybercrime and security consultant; Eileen Donahoe, executive director of the Global Digital Policy Incubator at Stanford University, Judith Donath, Harvard University’s Berkman Klein Center for Internet & Society; William Dutton, Oxford Martin Fellow at the Global Cyber Security Capacity Centre; Robert Epstein, a senior research psychologist and founding director of the Loebner Prize Competition in Artificial Intelligence, Susan Etlinger, an industry analyst for Altimeter Group; Jean-Daniel Fekete, researcher in information visualization, visual analytics and human-computer interaction at INRIA, France; Seth Finkelstein, consulting programmer and EFF Pioneer Award winner; Charlie Firestone, executive director of the Aspen Institute’s communications and society program; Bob Frankston, internet pioneer and software innovator; Divina Frau-Meigs, UNESCO chair for sustainable digital development; Richard Forno, of the Center for Cybersecurity at the University of Maryland-Baltimore County; Oscar Gandy, professor emeritus of communication at the University of Pennsylvania; Charles Geiger, head of the executive secretariat for the UN's World Summit on the Information Society; Ashok Goel, director of the Human-Centered Computing Ph.D. Program at Georgia Tech; Ken Goldberg, distinguished chair in engineering, and founding member, Berkeley AI Research Lab; Marina Gorbis, executive director of the Institute for the Future; Shigeki Goto, Asia-Pacific internet pioneer and Internet Hall of Fame member; Theodore Gordon, futurist and co-founder of the Millennium Project; Kenneth Grady, futurist and founding author of The Algorithmic Society blog; Sam Gregory, director of WITNESS and digital human rights activist; Wendy Hall, executive director of the Web Science Institute; John C. Havens, executive director of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems and the Council on Extended Intelligence; Marek Havrda, director at NEOPAS and strategic adviser for the GoodAI project; Jim Hendler, director of the Rensselaer Polytechnic Institute for Data Exploration and Application; Perry Hewitt, a marketing, content and technology executive; Brock Hinzmann, a partner in the Business Futures Network who worked for 40 years as a futures researcher at SRI International; Bernie Hogan, senior research fellow, Oxford Internet Institute; Barry Hughes, senior scientist at the Center for International Futures, University of Denver; Jeff Jarvis, director of the Tow-Knight Center at City University of New York’s Craig Newmark School of Journalism; Bryan Johnson, founder and CEO of Kernel (developer of advanced neural interfaces) and OS Fund; Anthony Judge, editor of tbe Encyclopedia of World Problems and Human Potential; James Kadtke, expert on converging technologies at the Institute for National Strategic Studies, U.S. National Defense University; Sonia Katyal, co-director of the Berkeley Center for Law and Technology and a member of the inaugural U.S. Commerce Department Digital Economy Board of Advisors; Frank Kaufmann, founder and director of the Values in Knowledge Foundation; Fiona Kerr, professor of neural systems and complexity at the University of Adelaide; Annalie Killian, futurist and vice president at Sparks & Honey; Andreas Kirsch, fellow at Newspeak House, formerly with Google and DeepMind in Zurich and London; Michael Kleeman, a senior\\n\\nwww.pewresearch.org\\n\\n116\\n\\nPEW RESEARCH CENTER\\n\\nfellow at the University of California, San Diego and board member at the Institute for the Future; Leonard Kleinrock, Internet Hall of Fame member and professor of computer science at the University of California, Los Angeles; Bart Knijnenburg, researcher on decision-making and recommender systems at Clemson University; Gary L. Kreps, distinguished professor and director of the Center for Health and Risk Communication at George Mason University; Larry Lannom, internet pioneer and vice president at the Corporation for National Research Initiatives (CNRI); Peter Levine, professor and associate dean for research at Tufts University’s Tisch College of Civic Life; John Markoff, fellow at the Center for Advanced Study in the Behavioral Sciences at Stanford University; Matt Mason, roboticist and former director of the Robotics Institute at Carnegie Mellon University; Craig J. Mathias, principal for the Farpoint Group; Giacomo Mazzone, head of institutional relations at the European Broadcasting Union; Andrew McLaughlin, executive director of the Center for Innovative Thinking at Yale, previously deputy CTO of the U.S. and global public policy lead for Google; Panagiotis T. Metaxas, author of “Technology, Propaganda and the Limits of Human Intellect” and professor of computer science, Wellesley College; Robert Metcalfe, co-inventor of Ethernet, founder of 3Com and Internet Hall of Fame member; Jerry Michalski, founder of the Relationship Economy eXpedition (REX); Steven Miller, vice provost and professor of information systems at Singapore Management University; Mario Morino, chair of the Morino Institute and co-founder of Venture Philanthropy Partners; Monica Murero, director of the E-Life International Institute, Italy; Grace Mutung’u, co-leader of the Kenya ICT Action Network; Martijn van Otterlo, author of “Gatekeeping Algorithms with Human Ethical Bias,” Tilburg University, Netherlands; Ian Peter, internet pioneer and advocate and co-founder of the Association for Progressive Communications (APC); Justin Reich, executive director of the MIT Teaching Systems Lab; Peter Reiner, professor and co-founder of the National Core for Neuroethics at the University of British Columbia; Lawrence Roberts, designer and manager of ARPANET (the precursor to the global internet) and Internet Hall of Fame member; Michael Roberts, Internet Hall of Fame member and first president and CEO of ICANN; Marc Rotenberg, executive director of EPIC; Douglas Rushkoff, writer, documentarian, and lecturer who focuses on human autonomy in a digital age; David Sarokin, author of “Missed Information: Better Information for Building a Wealthier, More Sustainable Future”; Thomas Schneider, vice-director at the Federal Office of Communications (OFCOM) in Switzerland; L. Schomaker, professor at the University of Groningen and scientific director of the Artificial Intelligence and Cognitive Engineering (ALICE) research institute; Ben Shneiderman, distinguished professor and founder of the Human Computer Interaction Lab at the University of Maryland; Dan Schultz, senior creative technologist at Internet Archive; Henning Schulzrinne, Internet Hall of Fame member and professor at Columbia University; Evan Selinger, professor of philosophy at Rochester Institute of Technology; Wendy Seltzer, strategy lead and counsel at the World Wide Web Consortium; Greg Shannon, chief scientist for the CERT Division at Carnegie Mellon University’s Software\\n\\nwww.pewresearch.org\\n\\n117\\n\\nPEW RESEARCH CENTER\\n\\nEngineering Institute; Daniel Siewiorek, professor with the Human-Computer Interaction Institute at Carnegie Mellon University; Mark Surman, executive director of the Mozilla Foundation; Brad Templeton, chair emeritus for the Electronic Frontier Foundation; Baratunde Thurston, futurist and former director of digital at The Onion; Sherry Turkle, MIT professor and author of “Alone Together”; Joseph Turow, professor of communication at the University of Pennsylvania; Stuart A. Umpleby, professor emeritus at George Washington University; Karl M. van Meter, author of “Computational Social Science in the Era of Big Data”; Michael Veale, co-author of “Fairness and Accountability Designs Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making”; Amy Webb, futurist, professor and founder of the Future Today Institute; David Wells, chief financial officer at Netflix; David Weinberger, senior researcher at Harvard University’s Berkman Klein Center for Internet & Society; Paul Werbos, former program director at the U.S. National Science Foundation; Betsy Williams, Center for Digital Society and Data Studies at the University of Arizona; John Willinsky, professor and director of the Public Knowledge Project at Stanford Graduate School of Education; Yvette Wohn, director of the Social Interaction Lab at the New Jersey Institute of Technology and expert on human-computer interaction; Andrew Wycoff, the director of OECD’s directorate for science, technology and innovation; Cliff Zukin, professor of public policy and political science at the School for Planning and Public Policy and the Eagleton Institute of Politics, Rutgers University.\\n\\nA selection of institutions at which some of the respondents work or have affiliations:\\n\\nAbt Associates; Access Now; Aeon; Allen Institute for Artificial Intelligence; Alpine Technology Group; Altimeter Group; American Institute for Behavioral Research and Technology; American Library Association; Antelope Consulting; Anticipatory Futures Group; Arizona State University; Artificial Intelligence Research Institute, Universitat Autònoma de Barcelona; Aspen Institute; AT&T; Australian National University; Bad Idea Factory; Bar-Ilan University, Israel; Bloomberg Businessweek; Bogazici University, Turkey; Brookings Institution; BT Group; Business Futures Network; California Institute of Technology; Carnegie Mellon University; Center for Advanced Study in the Behavioral Sciences, Stanford University; Centre for Policy Modelling, Manchester Metropolitan University; Centre National de la Recherche Scientifique, France; Cisco Systems; Clemson University; Cloudflare; Columbia University; Comcast; Constellation Research; Cornell University; Corporation for National Research Initiatives; Council of Europe; Agency for Electronic Government and Information Society in Uruguay; Electronic Frontiers Australia; Electronic Frontier Foundation; Emergent Research; ENIAC Programmers Project; Eurac Research, Italy; FSA Technologies; Farpoint Group; Foresight Alliance; Future of Privacy Forum; Future Today Institute; Futurism.com; Gartner; General Electric; Georgia Tech; Ginkgo Bioworks; Global Forum for Media Development; Google; Harvard University; Hokkaido University, Japan;\\n\\nwww.pewresearch.org\\n\\n118\\n\\nPEW RESEARCH CENTER\\n\\nIBM; Internet Corporation for Assigned Names and Numbers (ICANN); Ignite Social Media; Information Technology and Innovation Foundation; Institute for Defense Analyses; Institute for the Future; Instituto Superior Técnico, Portugal; Institute for Ethics and Emerging Technologies; Internet Engineering Task Force (IETF); International Academy for Systems and Cybernetic Sciences; Internet Society; Institute for Communication & Leadership, Lucerne, Switzerland; Jet Propulsion Lab; Johns Hopkins University; Kansai University, Japan; Institute for Systems and Robotics, University of Lisbon; Institute of Electrical and Electronics Engineers (IEEE); Keio University, Japan; Kernel; Kyndi; Knowledge and Digital Culture Foundation, Mexico; KPMG; Leading Futurists; LeTourneau University; The Linux Foundation; Los Alamos National Laboratory; Machine Intelligence Research Institute; Massachusetts Institute of Technology; Maverick Technologies; McKinsey & Company; Media Psychology Research Center; Microsoft; Millennium Project; Monster Worldwide; Mozilla; Nanyang Technological University, Singapore; National Chengchi University, Taiwan; National Institute of Mental Health; NetLab; The New School; New York University; Netflix; NLnet Foundation; NORC at the University of Chicago; Novartis, Switzerland; Organization for Economic Cooperation and Development (OECD); Ontario College of Art and Design Strategic Foresight and Innovation; Open the Future; Open University of Israel; Oracle; O’Reilly Media; Global Cyber Security Capacity Center, Oxford University; Oxford Internet Institute; Packet Clearing House; People-Centered Internet; Perimeter Institute for Theoretical Physics; Politecnico di Milano; Princeton University; Privacy International; Purdue University; Queen Mary University of London; Quinnovation; RAND; Research ICT Africa; Rochester Institute of Technology; Rose-Hulman Institute of Technology; Russell Sage Foundation; Salesforce; SRI International; Sciteb, London; Shinkuro; Significance Systems; Singapore Management University; Sir Syed University of Engineering and Technology, Pakistan; SLAC National Accelerator Laboratory; Södertörn University, Sweden; Social Science Research Council; University of Paris III: Sorbonne Nouvelle; South China University of Technology; Stanford University; Straits Knowledge; Team Human; The Logic; Technische Universität Kaiserslautern, Germany; Tecnológico de Monterrey, Mexico; The Crucible; United Nations; University of California, Berkeley; University of California, Los Angeles; University of California, San Diego; University College London; University of Denver Pardee Center for International Futures; Universitat Oberta de Catalunya; Universidade NOVA de Lisboa, Portugal; the Universities of Alabama, Arizona, Delaware, Florida, Maryland, Michigan, Minnesota, Pennsylvania, Southern California, Utah and Vermont; the Universities of Calcutta, Cambridge, Cologne, Cyprus, Edinburgh, Granada, Groningen, Liverpool, Otago, Pavia, Salford and Waterloo; UNESCO; USENIX Association; U.S. Department of Energy; U.S. Naval Postgraduate School; U.S. Special Operations Command SOFWERX; Telecommunications and Radiocommunications Regulator of Vanuatu; Virginia Tech; Vision & Logic; Vizalytics; World Wide Web Foundation; Wellville; Wikimedia; Witness; Yale Law School Information Society Project.\\n\\nwww.pewresearch.org\\n\\n119\\n\\nPEW RESEARCH CENTER\\n\\nComplete sets of credited and anonymous responses can be found here:\\n\\nhttp://www.elon.edu/e-web/imagining/surveys/2018_survey/AI_and_the_Future_of_Humans_credit.xhtml\\n\\nhttp://www.elon.edu/e-web/imagining/surveys/2018_survey/AI_and_the_Future_of_Humans_anon.xhtml\\n\\nwww.pewresearch.org\\n\\n120\\n\\nPEW RESEARCH CENTER\\n\\nAcknowledgments\\n\\nThis report is a collaborative effort based on the input and analysis of the following individuals.\\n\\nWe are extremely thankful for the contributions of the 979 people who participated in this canvassing.\\n\\nPrimary researchers\\n\\nJanna Anderson, Director, Elon University’s Imagining the Internet Center Lee Rainie, Director, Internet and Technology Research Alex Luchsinger, Asst. Professor, Elon University\\n\\nResearch team\\n\\nClaudia Deane, Vice President, Research\\n\\nEditorial and graphic design\\n\\nMargaret Porteus, Information Graphics Designer Shannon Greenwood, Copy Editor\\n\\nCommunications and web publishing\\n\\nSara Atske, Assistant Digital Producer Shawnee Cohn, Communications Manager\\n\\nwww.pewresearch.org\")],\n",
              " [Document(metadata={'source': '/content/preparing for future of ai.pdf'}, page_content='PREPARING FOR THE FUTURE PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE OF ARTIFICIAL INTELLIGENCE\\n\\nExecutive Office of the President National Science and Technology Council National Science and Technology Council Committee on Technology\\n\\nOctober 2016\\n\\nAbout the National Science and Technology Council\\n\\nThe National Science and Technology Council (NSTC) is the principal means by which the Executive Branch coordinates science and technology policy across the diverse entities that make up the Federal research and development (R&D) enterprise. One of the NSTC’s primary objectives is establishing clear national goals for Federal science and technology investments. The NSTC prepares R&D packages aimed at accomplishing multiple national goals. The NSTC’s work is organized under five committees: Environment, Natural Resources, and Sustainability; Homeland and National Security; Science, Technology, Engineering, and Mathematics (STEM) Education; Science; and Technology. Each of these committees oversees subcommittees and working groups that are focused on different aspects of science and technology. More information is available at www.whitehouse.gov/ostp/nstc.\\n\\nAbout the Office of Science and Technology Policy\\n\\nThe Office of Science and Technology Policy (OSTP) was established by the National Science and Technology Policy, Organization, and Priorities Act of 1976. OSTP’s responsibilities include advising the President in policy formulation and budget development on questions in which science and technology are important elements; articulating the President’s science and technology policy and programs; and fostering strong partnerships among Federal, state, and local governments, and the scientific communities in industry and academia. The Director of OSTP also serves as Assistant to the President for Science and Technology and manages the NSTC. More information is available at www.whitehouse.gov/ostp.\\n\\nAcknowledgments\\n\\nThis document was developed through the contributions of staff from OSTP, other components of the Executive Office of the President, and other departments and agencies. A special thanks and appreciation to everyone who contributed.\\n\\nCopyright Information\\n\\nThis is a work of the U.S. Government and is in the public domain. It may be freely distributed, copied, and translated; acknowledgment of publication by the Office of Science and Technology Policy is appreciated. Any translation should include a disclaimer that the accuracy of the translation is the responsibility of the translator and not OSTP. It is requested that a copy of any translation be sent to OSTP. This work is available for worldwide use and reuse and under the Creative Commons CC0 1.0 Universal license.\\n\\nEXECUTIVE OFFICE OF THE PRESIDENT NATIONAL SCIENCE AND TECHNOLOGY COUNCIL WASHINGTON, D.C. 20502\\n\\nOctober 12, 2016\\n\\nDear colleagues:\\n\\nAdvances in Artificial Intelligence (AI) technology have opened up new markets and new opportunities for progress in critical areas such as health, education, energy, and the environment. In recent years, machines have surpassed humans in the performance of certain specific tasks, such as some aspects of image recognition. Experts forecast that rapid progress in the field of specialized artificial intelligence will continue. Although it is very unlikely that machines will exhibit broadly-applicable intelligence comparable to or exceeding that of humans in the next 20 years, it is to be expected that machines will reach and exceed human performance on more and more tasks.\\n\\nAs a contribution toward preparing the United States for a future in which AI plays a growing role, this report surveys the current state of AI, its existing and potential applications, and the questions that are raised for society and public policy by progress in AI. The report also makes recommendations for specific further actions by Federal agencies and other actors. A companion document lays out a strategic plan for Federally-funded research and development in AI. Additionally, in the coming months, the Administration will release a follow-on report exploring in greater depth the effect of AI-driven automation on jobs and the economy.\\n\\nThe report was developed by the NSTC’s Subcommittee on Machine Learning and Artificial Intelligence, which was chartered in May 2016 to foster interagency coordination, to provide technical and policy advice on topics related to AI, and to monitor the development of AI technologies across industry, the research community, and the Federal Government. The report was reviewed by the NSTC Committee on Technology, which concurred with its contents. The report follows a series of public-outreach activities spearheaded by the White House Office of Science and Technology Policy (OSTP) in 2016, which included five public workshops co-hosted with universities and other associations that are referenced in this report.\\n\\nOSTP also published a Request for Information (RFI) in June 2016, which received 161 responses. The submitted comments were published by OSTP on September 6, 2016. Consistent with the role of Big Data as an enabler of AI, this report builds on three previous Administration reports on Big Data referenced in this report.\\n\\nIn the coming years, AI will continue to contribute to economic growth and will be a valuable tool for improving the world, as long as industry, civil society, and government work together to develop the positive aspects of the technology, manage its risks and challenges, and ensure that everyone has the opportunity to help in building an AI-enhanced society and to participate in its benefits.\\n\\nSincerely,\\n\\nJohn P. Holdren\\n\\nMegan Smith\\n\\nAssistant to the President for Science and Technology U.S. Chief Technology Officer Director, Office of Science and Technology Policy\\n\\nNational Science and Technology Council\\n\\nChair John P. Holdren Assistant to the President for Science and Technology and Director, Office of Science and Technology Policy\\n\\nStaff Afua Bruce Executive Director Office of Science and Technology Policy\\n\\nSubcommittee on Machine Learning and Artificial Intelligence\\n\\nCo-Chair Ed Felten Deputy U.S. Chief Technology Officer Office of Science and Technology Policy\\n\\nExecutive Secretary Terah Lyons Policy Advisor to the U.S. Chief Technology Officer Office of Science and Technology Policy\\n\\nCo-Chair Michael Garris Senior Scientist National Institute of Standards and Technology U.S. Department of Commerce\\n\\nThe following Federal departments and agencies are represented on the Subcommittee on Machine Learning and Artificial Intelligence and through it, work together to monitor the state of the art in machine learning (ML) and AI (within the Federal Government, in the private sector, and internationally), to watch for the arrival of important technology milestones in the development of AI, to coordinate the use of and foster the sharing of knowledge and best practices about ML and AI by the Federal Government, and to consult in the development of Federal research and development priorities in AI:\\n\\nDepartment of Commerce (Co-Chair) Department of Defense Department of Education Department of Energy Department of Health and Human Services Department of Homeland Security Department of Justice Department of Labor Department of State Department of Transportation Department of Treasury\\n\\nDepartment of Veterans Affairs United States Agency for International Development Central Intelligence Agency General Services Administration National Science Foundation National Security Agency National Aeronautics and Space Administration Office of the Director of National Intelligence Social Security Administration\\n\\nThe following offices of the Executive Office of the President are also represented on the Subcommittee:\\n\\nCouncil of Economic Advisers Domestic Policy Council Office of Management and Budget Office of Science and Technology Policy (Co- Chair)\\n\\nOffice of the Vice President National Economic Council National Security Council\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nContents Executive Summary ..................................................................................................................................................... 1\\n\\nIntroduction ................................................................................................................................................................. 5\\n\\nA Brief History of AI ........................................................................................................................................... 5\\n\\nWhat is Artificial Intelligence? ............................................................................................................................ 6\\n\\nThe Current State of AI ........................................................................................................................................ 7\\n\\nPublic Outreach and Development of this Report .................................................................................................. 12\\n\\nApplications of AI for Public Good .......................................................................................................................... 13\\n\\nAI in the Federal Government .................................................................................................................................. 15\\n\\nAI and Regulation ...................................................................................................................................................... 17\\n\\nCase Study: Autonomous Vehicles and Aircraft ................................................................................................ 18\\n\\nResearch and Workforce .......................................................................................................................................... 23\\n\\nMonitoring Progress in AI.................................................................................................................................. 23\\n\\nFederal Support for AI Research ........................................................................................................................ 25\\n\\nWorkforce Development and Diversity ............................................................................................................. 26\\n\\nAI, Automation, and the Economy ....................................................................................................................... 29\\n\\nFairness, Safety, and Governance ............................................................................................................................ 30\\n\\nJustice, Fairness, and Accountability ................................................................................................................. 30\\n\\nSafety and Control.............................................................................................................................................. 32\\n\\nGlobal Considerations and Security ........................................................................................................................ 35\\n\\nInternational Cooperation ................................................................................................................................... 35\\n\\nAI and Cybersecurity ......................................................................................................................................... 36\\n\\nAI in Weapon Systems ....................................................................................................................................... 37\\n\\nConclusion .................................................................................................................................................................. 39\\n\\nRecommendations in this Report ............................................................................................................................. 40\\n\\nAcronyms.................................................................................................................................................................... 43\\n\\nReferences .................................................................................................................................................................. 45\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nExecutive Summary\\n\\nAs a contribution toward preparing the United States for a future in which Artificial Intelligence (AI) plays a growing role, we survey the current state of AI, its existing and potential applications, and the questions that are raised for society and public policy by progress in AI. We also make recommendations for specific further actions by Federal agencies and other actors. A companion document called the National Artificial Intelligence Research and Development Strategic Plan lays out a strategic plan for Federally-funded research and development in AI.\\n\\nApplications of AI for Public Good\\n\\nOne area of great optimism about AI and machine learning is their potential to improve people’s lives by helping to solve some of the world’s greatest challenges and inefficiencies. Many have compared the promise of AI to the transformative impacts of advancements in mobile computing. Public- and private- sector investments in basic and applied R&D on AI have already begun reaping major benefits to the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion. The effectiveness of government itself is being increased as agencies build their capacity to use AI to carry out their missions more quickly, responsively, and efficiently.\\n\\nAI and Regulation\\n\\nAI has applications in many products, such as cars and aircraft, which are subject to regulation designed to protect the public from harm and ensure fairness in economic competition. How will the incorporation of AI into these products affect the relevant regulatory approaches? In general, the approach to regulation of AI-enabled products to protect public safety should be informed by assessment of the aspects of risk that the addition of AI may reduce alongside the aspects of risk that it may increase. If a risk falls within the bounds of an existing regulatory regime, moreover, the policy discussion should start by considering whether the existing regulations already adequately address the risk, or whether they need to be adapted to the addition of AI. Also, where regulatory responses to the addition of AI threaten to increase the cost of compliance, or slow the development or adoption of beneficial innovations, policymakers should consider how those responses could be adjusted to lower costs and barriers to innovation without adversely impacting safety or market fairness.\\n\\nCurrently relevant examples of the regulatory challenges that AI-enabled products present are found in the cases of automated vehicles (AVs, such as self-driving cars) and AI-equipped unmanned aircraft systems (UAS, or “drones”). In the long run, AVs will likely save many lives by reducing driver error and increasing personal mobility, and UAS will offer many economic benefits. Yet public safety must be protected as these technologies are tested and begin to mature. The Department of Transportation (DOT) is using an approach to evolving the relevant regulations that is based on building expertise in the Department, creating safe spaces and test beds for experimentation, and working with industry and civil society to evolve performance-based regulations that will enable more uses as evidence of safe operation accumulates.\\n\\nResearch and Workforce\\n\\nGovernment also has an important role to play in the advancement of AI through research and development and the growth of a skilled, diverse workforce. A separate strategic plan for Federally- funded AI research and development is being released in conjunction with this report. The plan discusses the role of Federal R&D, identifies areas of opportunity, and recommends ways to coordinate R&D to maximize benefit and build a highly-trained workforce.\\n\\n1\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nGiven the strategic importance of AI, moreover, it is appropriate for the Federal Government to monitor developments in the field worldwide in order to get early warning of important changes arising elsewhere in case these require changes in U.S. policy.\\n\\nThe rapid growth of AI has dramatically increased the need for people with relevant skills to support and advance the field. An AI-enabled world demands a data-literate citizenry that is able to read, use, interpret, and communicate about data, and participate in policy debates about matters affected by AI. AI knowledge and education are increasingly emphasized in Federal Science, Technology, Engineering, and Mathematics (STEM) education programs. AI education is also a component of Computer Science for All, the President’s initiative to empower all American students from kindergarten through high school to learn computer science and be equipped with the computational thinking skills they need in a technology- driven world.\\n\\nEconomic Impacts of AI\\n\\nAI’s central economic effect in the short term will be the automation of tasks that could not be automated before. This will likely increase productivity and create wealth, but it may also affect particular types of jobs in different ways, reducing demand for certain skills that can be automated while increasing demand for other skills that are complementary to AI. Analysis by the White House Council of Economic Advisors (CEA) suggests that the negative effect of automation will be greatest on lower-wage jobs, and that there is a risk that AI-driven automation will increase the wage gap between less-educated and more- educated workers, potentially increasing economic inequality. Public policy can address these risks, ensuring that workers are retrained and able to succeed in occupations that are complementary to, rather than competing with, automation. Public policy can also ensure that the economic benefits created by AI are shared broadly, and assure that AI responsibly ushers in a new age in the global economy.\\n\\nFairness, Safety, and Governance\\n\\nAs AI technologies move toward broader deployment, technical experts, policy analysts, and ethicists have raised concerns about unintended consequences of widespread adoption. Use of AI to make consequential decisions about people, often replacing decisions made by human-driven bureaucratic processes, leads to concerns about how to ensure justice, fairness, and accountability—the same concerns voiced previously in the Administration’s Big Data: Seizing Opportunities, Preserving Values report of 2014,1 as well as the Report to the President on Big Data and Privacy: A Technological Perspective published by the President’s Council of Advisors on Science and Technology in 2014.2 Transparency concerns focus not only on the data and algorithms involved, but also on the potential to have some form of explanation for any AI-based determination. Yet AI experts have cautioned that there are inherent challenges in trying to understand and predict the behavior of advanced AI systems.\\n\\nUse of AI to control physical-world equipment leads to concerns about safety, especially as systems are exposed to the full complexity of the human environment. A major challenge in AI safety is building systems that can safely transition from the “closed world” of the laboratory into the outside “open world” where unpredictable things can happen. Adapting gracefully to unforeseen situations is difficult yet necessary for safe operation. Experience in building other types of safety-critical systems and infrastructure, such as aircraft, power plants, bridges, and vehicles, has much to teach AI practitioners\\n\\n1 “Big Data: Seizing Opportunities, Preserving Values,” Executive Office of the President, May 2014, https://www.whitehouse.gov/sites/default/files/docs/big_data_privacy_report_may_1_2014.pdf.\\n\\n2 The President’s Council of Advisors on Science and Technology, “Report to the President: Big Data and Privacy: A Technological Perspective,” Executive Office of the President, May 2014, https://www.whitehouse.gov/sites/default/files/microsites/ostp/PCAST/pcast_big_data_and_privacy_- _may_2014.pdf.\\n\\n2\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nabout verification and validation, how to build a safety case for a technology, how to manage risk, and how to communicate with stakeholders about risk.\\n\\nAt a technical level, the challenges of fairness and safety are related. In both cases, practitioners strive to avoid unintended behavior, and to generate the evidence needed to give stakeholders justified confidence that unintended failures are unlikely.\\n\\nEthical training for AI practitioners and students is a necessary part of the solution. Ideally, every student learning AI, computer science, or data science would be exposed to curriculum and discussion on related ethics and security topics. However, ethics alone is not sufficient. Ethics can help practitioners understand their responsibilities to all stakeholders, but ethical training should be augmented with technical tools and methods for putting good intentions into practice by doing the technical work needed to prevent unacceptable outcomes.\\n\\nGlobal Considerations and Security\\n\\nAI poses policy questions across a range of areas in international relations and security. AI has been a topic of interest in recent international discussions as countries, multilateral institutions, and other stakeholders have begun to access the benefits and challenges of AI. Dialogue and cooperation between these entities could help advance AI R&D and harness AI for good, while also addressing shared challenges.\\n\\nToday’s AI has important applications in cybersecurity, and is expected to play an increasing role for both defensive and offensive cyber measures. Currently, designing and operating secure systems requires significant time and attention from experts. Automating this expert work partially or entirely may increase security across a much broader range of systems and applications at dramatically lower cost, and could increase the agility of the Nation’s cyber-defenses. Using AI may help maintain the rapid response required to detect and react to the landscape of evolving threats.\\n\\nChallenging issues are raised by the potential use of AI in weapon systems. The United States has incorporated autonomy in certain weapon systems for decades, allowing for greater precision in the use of weapons and safer, more humane military operations. Nonetheless, moving away from direct human control of weapon systems involves some risks and can raise legal and ethical questions.\\n\\nThe key to incorporating autonomous and semi-autonomous weapon systems into American defense planning is to ensure that U.S. Government entities are always acting in accordance with international humanitarian law, taking appropriate steps to control proliferation, and working with partners and Allies to develop standards related to the development and use of such weapon systems. The United States has actively participated in ongoing international discussion on Lethal Autonomous Weapon Systems, and anticipates continued robust international discussion of these potential weapon systems. Agencies across the U.S. Government are working to develop a single, government-wide policy, consistent with international humanitarian law, on autonomous and semi-autonomous weapons.\\n\\nPreparing for the Future\\n\\nAI holds the potential to be a major driver of economic growth and social progress, if industry, civil society, government, and the public work together to support development of the technology with thoughtful attention to its potential and to managing its risks.\\n\\nThe U.S. Government has several roles to play. It can convene conversations about important issues and help to set the agenda for public debate. It can monitor the safety and fairness of applications as they develop, and adapt regulatory frameworks to encourage innovation while protecting the public. It can provide public policy tools to ensure that disruption in the means and methods of work enabled by AI increases productivity while avoiding negative economic consequences for certain sectors of the workforce. It can support basic research and the application of AI to public good. It can support development of a skilled, diverse workforce. And government can use AI itself to serve the public faster, 3\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nmore effectively, and at lower cost. Many areas of public policy, from education and the economic safety net, to defense, environmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. The U.S. Government must continue to build its capacity to understand and adapt to these changes.\\n\\nAs the technology of AI continues to develop, practitioners must ensure that AI-enabled systems are governable; that they are open, transparent, and understandable; that they can work effectively with people; and that their operation will remain consistent with human values and aspirations. Researchers and practitioners have increased their attention to these challenges, and should continue to focus on them.\\n\\nDeveloping and studying machine intelligence can help us better understand and appreciate our human intelligence. Used thoughtfully, AI can augment our intelligence, helping us chart a better and wiser path forward.\\n\\nA full list of the recommendations made in this report is on page 40.\\n\\n4\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nIntroduction\\n\\nArtificial Intelligence (AI) has the potential to help address some of the biggest challenges that society faces. Smart vehicles may save hundreds of thousands of lives every year worldwide, and increase mobility for the elderly and those with disabilities. Smart buildings may save energy and reduce carbon emissions. Precision medicine may extend life and increase quality of life. Smarter government may serve citizens more quickly and precisely, better protect those at risk, and save money. AI-enhanced education may help teachers give every child an education that opens doors to a secure and fulfilling life. These are just a few of the potential benefits if the technology is developed with an eye to its benefits and with careful consideration of its risks and challenges.\\n\\nThe United States has been at the forefront of foundational research in AI, primarily supported for most of the field’s history by Federal research funding and work at government laboratories. The Federal Government’s support for unclassified AI R&D is managed through the Networking and Information Technology Research and Development (NITRD) program, and supported primarily by the Defense Advanced Research Projects Agency (DARPA), the National Science Foundation (NSF), the National Institutes of Health (NIH), the Office of Naval Research (ONR), and the Intelligence Advanced Research Projects Activity (IARPA). Major national research efforts such as the National Strategic Computing Initiative, the Big Data Initiative, and the Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative also contribute indirectly to the progress of AI research. The current and projected benefits of AI technology are large, adding to the Nation’s economic vitality and to the productivity and well-being of its people. A companion document lays out a strategic plan for Federally-funded research and development in AI.\\n\\nAs a contribution toward preparing the United States for a future in which AI plays a growing role, we survey the current state of AI, its existing and potential applications, and the questions that progress in AI raise for society and public policy. We also make recommendations for specific further actions by Federal agencies and other actors.\\n\\nA Brief History of AI\\n\\nEndowing computers with human-like intelligence has been a dream of computer experts since the dawn of electronic computing. Although the term “Artificial Intelligence” was not coined until 1956, the roots of the field go back to at least the 1940s,3 and the idea of AI was crystalized in Alan Turing’s famous 1950 paper, “Computing Machinery and Intelligence.” Turing’s paper posed the question: “Can machines think?” It also proposed a test for answering that question,4 and raised the possibility that a machine might be programmed to learn from experience much as a young child does.\\n\\nIn the ensuing decades, the field of AI went through ups and downs as some AI research problems proved more difficult than anticipated and others proved insurmountable with the technologies of the time. It wasn’t until the late 1990s that research progress in AI began to accelerate, as researchers focused more on sub-problems of AI and the application of AI to real-world problems such as image recognition and medical diagnosis. An early milestone was the 1997 victory of IBM’s chess-playing computer Deep Blue\\n\\n3 See, e.g., Warren S. McCulloch and Walter H. Pitts, “A Logical Calculus of the Ideas Immanent in Nervous Activity,” Bulletin of Mathematical Biophysics, 5:115-133, 1943.\\n\\n4 Restated in modern terms, the “Turing Test” puts a human judge in a text-based chat room with either another person or a computer. The human judge can interrogate the other party and carry on a conversation, and then the judge is asked to guess whether the other party is a person or a computer. If a computer can consistently fool human judges in this game, then the computer is deemed to be exhibiting intelligence.\\n\\n5\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nover world champion Garry Kasparov. Other significant breakthroughs included DARPA’s Cognitive Agent that Learns and Organizes (CALO), which led to Apple Inc.’s Siri; IBM’s question-answering computer Watson’s victory in the TV game show “Jeopardy!”; and the surprising success of self-driving cars in the DARPA Grand Challenge competitions in the 2000s.\\n\\nThe current wave of progress and enthusiasm for AI began around 2010, driven by three factors that built upon each other: the availability of big data from sources including e-commerce, businesses, social media, science, and government; which provided raw material for dramatically improved machine learning approaches and algorithms; which in turn relied on the capabilities of more powerful computers.5 During this period, the pace of improvement surprised AI experts. For example, on a popular image recognition challenge6 that has a 5 percent human error rate according to one error measure, the best AI result improved from a 26 percent error rate in 2011 to 3.5 percent in 2015.\\n\\nSimultaneously, industry has been increasing its investment in AI. In 2016, Google Chief Executive Officer (CEO) Sundar Pichai said, “Machine learning [a subfield of AI] is a core, transformative way by which we’re rethinking how we’re doing everything. We are thoughtfully applying it across all our products, be it search, ads, YouTube, or Play. And we’re in early days, but you will see us\\u200a—\\u200ain a systematic way\\u200a—\\u200aapply machine learning in all these areas.”7 This view of AI broadly impacting how software is created and delivered was widely shared by CEOs in the technology industry, including Ginni Rometty of IBM, who has said that her organization is betting the company on AI.8\\n\\nWhat is Artificial Intelligence?\\n\\nThere is no single definition of AI that is universally accepted by practitioners. Some define AI loosely as a computerized system that exhibits behavior that is commonly thought of as requiring intelligence. Others define AI as a system capable of rationally solving complex problems or taking appropriate actions to achieve its goals in whatever real world circumstances it encounters.\\n\\nExperts offer differing taxonomies of AI problems and solutions. A popular AI textbook9 used the following taxonomy: (1) systems that think like humans (e.g., cognitive architectures and neural networks); (2) systems that act like humans (e.g., pass the Turing test via natural language processing; knowledge representation, automated reasoning, and learning), (3) systems that think rationally (e.g.,\\n\\n5 A more detailed history of AI is available in the Appendix of the AI 100 Report. Peter Stone, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, Kevin Leyton-Brown, David Parkes, William Press, AnnaLee Saxenian, Julie Shah, Milind Tambe, and Astro Teller, \"Artificial Intelligence and Life in 2030,\" One Hundred Year Study on Artificial Intelligence: Report of the 2015-2016 Study Panel, Stanford University, Stanford, CA, September 2016, http://ai100.stanford.edu/2016- report.\\n\\n6 The ImageNet Large Scale Visual Recognition Challenge provides a set of photographic images and asks for an accurate description of what is depicted in each image. Statistics in the text refer to the “classification error” metric in the “classification+localization with provided training data” task. See http://image-net.org/challenges/LSVRC/.\\n\\n7 Steven Levy, “How Google is Remaking Itself as a Machine Learning First Company,” Backchannel, June 22, 2016, https://backchannel.com/how-google-is-remaking-itself-as-a-machine-learning-first-company-ada63defcb70.\\n\\n8 See, e.g., Andrew Nusca, “IBM’s CEO Thinks Every Digital Business Will Become a Cognitive Computing Business,” Fortune, June 1 2016. (“[IBM] CEO Ginni Rometty is optimistic that the company’s wager on ‘cognitive computing,’ the term it uses for applied artificial intelligence and machine learning technologies, is the biggest bet the company will make in its 105-year history.”)\\n\\n9 Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach (3rd Edition) (Essex, England: Pearson, 2009).\\n\\n6\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nlogic solvers, inference, and optimization); and (4) systems that act rationally (e.g., intelligent software agents and embodied robots that achieve goals via perception, planning, reasoning, learning, communicating, decision-making, and acting). Separately, venture capitalist Frank Chen broke down the problem space of AI into five general categories: logical reasoning, knowledge representation, planning and navigation, natural language processing, and perception.10 And AI researcher Pedro Domingos ascribed AI researchers to five “tribes” based on the methods they use: “symbolists” use logical reasoning based on abstract symbols, “connectionists” build structures inspired by the human brain; “evolutionaries” use methods inspired by Darwinian evolution; “Bayesians” use probabilistic inference; and “analogizers” extrapolate from similar cases seen previously.11\\n\\nThis diversity of AI problems and solutions, and the foundation of AI in human evaluation of the performance and accuracy of algorithms, makes it difficult to clearly define a bright-line distinction between what constitutes AI and what does not. For example, many techniques used to analyze large volumes of data were developed by AI researchers and are now identified as “Big Data” algorithms and systems. In some cases, opinion may shift, meaning that a problem is considered as requiring AI before it has been solved, but once a solution is well known it is considered routine data processing. Although the boundaries of AI can be uncertain and have tended to shift over time, what is important is that a core objective of AI research and applications over the years has been to automate or replicate intelligent behavior.\\n\\nThe Current State of AI\\n\\nRemarkable progress has been made on what is known as Narrow AI, which addresses specific application areas such as playing strategic games, language translation, self-driving vehicles, and image recognition.12 Narrow AI underpins many commercial services such as trip planning, shopper recommendation systems, and ad targeting, and is finding important applications in medical diagnosis, education, and scientific research. These have all had significant societal benefits and have contributed to the economic vitality of the Nation.13\\n\\nGeneral AI (sometimes called Artificial General Intelligence, or AGI) refers to a notional future AI system that exhibits apparently intelligent behavior at least as advanced as a person across the full range of cognitive tasks. A broad chasm seems to separate today’s Narrow AI from the much more difficult challenge of General AI. Attempts to reach General AI by expanding Narrow AI solutions have made little headway over many decades of research. The current consensus of the private-sector expert community, with which the NSTC Committee on Technology concurs, is that General AI will not be achieved for at least decades.14\\n\\n10 Frank Chen, “AI, Deep Learning, and Machine Learning: A Primer,” Andreessen Horowitz, June 10, 2016, http://a16z.com/2016/06/10/ai-deep-learning-machines.\\n\\n11 Pedro Domingos, The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World (New York, New York: Basic Books, 2015).\\n\\n12 Narrow AI is not a single technical approach, but rather a set of discrete problems whose solutions rely on a toolkit of AI methods along with some problem-specific algorithms. The diversity of Narrow AI problems and solutions, and the apparent need to develop specific methods for each Narrow AI application, has made it infeasible to “generalize” a single Narrow AI solution to produce intelligent behavior of general applicability.\\n\\n13 Mike Purdy and Paul Daugherty, “Why Artificial Intelligence is the Future of Growth,” Accenture, 2016, https://www.accenture.com/us-en/_acnmedia/PDF-33/Accenture-Why-AI-is-the-Future-of-Growth.pdf.\\n\\n14 Expert opinion on the expected arrival date of AGI ranges from 2030 to centuries from now. There is a long history of excessive optimism about AI. For example, AI pioneer Herb Simon predicted in 1957 that computers\\n\\n7\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nPeople have long speculated on the implications of computers becoming more intelligent than humans. Some predict that a sufficiently intelligent AI could be tasked with developing even better, more intelligent systems, and that these in turn could be used to create systems with yet greater intelligence, and so on, leading in principle to an “intelligence explosion” or “singularity” in which machines quickly race far ahead of humans in intelligence.15\\n\\nIn a dystopian vision of this process, these super-intelligent machines would exceed the ability of humanity to understand or control. If computers could exert control over many critical systems, the result could be havoc, with humans no longer in control of their destiny at best and extinct at worst. This scenario has long been the subject of science fiction stories, and recent pronouncements from some influential industry leaders have highlighted these fears.\\n\\nA more positive view of the future held by many researchers sees instead the development of intelligent systems that work well as helpers, assistants, trainers, and teammates of humans, and are designed to operate safely and ethically.\\n\\nThe NSTC Committee on Technology’s assessment is that long-term concerns about super-intelligent General AI should have little impact on current policy. The policies the Federal Government should adopt in the near-to-medium term if these fears are justified are almost exactly the same policies the Federal Government should adopt if they are not justified. The best way to build capacity for addressing the longer-term speculative risks is to attack the less extreme risks already seen today, such as current security, privacy, and safety risks, while investing in research on longer-term capabilities and how their challenges might be managed. Additionally, as research and applications in the field continue to mature, practitioners of AI in government and business should approach advances with appropriate consideration of the long-term societal and ethical questions – in additional to just the technical questions – that such advances portend. Although prudence dictates some attention to the possibility that harmful super- intelligence might someday become possible, these concerns should not be the main driver of public policy for AI.\\n\\nMachine Learning\\n\\nMachine learning is one of the most important technical approaches to AI and the basis of many recent advances and commercial applications of AI. Modern machine learning is a statistical process that starts with a body of data and tries to derive a rule or procedure that explains the data or can predict future data. This approach—learning from data—contrasts with the older “expert system” approach to AI, in which programmers sit down with human domain experts to learn the rules and criteria used to make decisions, and translate those rules into software code. An expert system aims to emulate the principles used by human experts, whereas machine learning relies on statistical methods to find a decision procedure that works well in practice.\\n\\nAn advantage of machine learning is that it can be used even in cases where it is infeasible or difficult to write down explicit rules to solve a problem. For example, a company that runs an online service might use machine learning to detect user log-in attempts that are fraudulent. The company might start with a large data set of past login attempts, with each attempt labeled as fraudulent or not using the benefit of\\n\\nwould outplay humans at chess within a decade, an outcome that required 40 years to occur. Early predictions about automated language translation also proved wildly optimistic, with the technology only becoming usable (and by no means fully fluent) in the last several years. It is tempting but incorrect to extrapolate from the ability to solve one particular task to imagine machines with a much broader and deeper range of capabilities and to overlook the huge gap between narrow task-oriented performance and the type of general intelligence that people exhibit.\\n\\n15 It is far from certain that this sort of explosive growth in intelligence is likely, or even possible. Another plausible extrapolation from current knowledge is that machine intelligence will continue to increase gradually even after surpassing human intelligence.\\n\\n8\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nhindsight. Based on this data set, the company could use machine learning to derive a rule to apply to future login attempts that predicts which attempts are more likely to be fraudulent and should be subjected to extra security measures. In a sense, machine learning is not an algorithm for solving a specific problem, but rather a more general approach to finding solutions for many different problems, given data about them.\\n\\nTo apply machine learning, a practitioner starts with a historical data set, which the practitioner divides into a training set and a test set. The practitioner chooses a model, or mathematical structure that characterizes a range of possible decision-making rules with adjustable parameters. A common analogy is that the model is a “box” that applies a rule, and the parameters are adjustable knobs on the front of the box that control how the box operates. In practice, a model might have many millions of parameters.\\n\\nThe practitioner also defines an objective function used to evaluate the desirability of the outcome that results from a particular choice of parameters. The objective function will typically contain parts that reward the model for closely matching the training set, as well as parts that reward the use of simpler rules.\\n\\nTraining the model is the process of adjusting the parameters to maximize the objective function. Training is the difficult technical step in machine learning. A model with millions of parameters will have astronomically more possible outcomes than any algorithm could ever hope to try, so successful training algorithms have to be clever in how they explore the space of parameter settings so as to find very good settings with a feasible level of computational effort.\\n\\nOnce a model has been trained, the practitioner can use the test set to evaluate the accuracy and effectiveness of the model. The goal of machine learning is to create a trained model that will generalize—it will be accurate not only on examples in the training set, but also on future cases that it has never seen before. While many of these models can achieve better-than-human performance on narrow tasks such as image labeling, even the best models can fail in unpredictable ways. For example, for many image labeling models it is possible to create images that clearly appear to be random noise to a human but will be falsely labeled as a specific object with high confidence by a trained model.16\\n\\nAnother challenge in using machine learning is that it is typically not possible to extract or generate a straightforward explanation for why a particular trained model is effective. Because trained models have a very large number of adjustable parameters—often hundreds of millions or more—training may yield a model that \"works,\" in the sense of matching the data, but is not necessarily the simplest model that works. In human decision-making, any opacity in the process is typically due to not having enough information about why a decision was reached, because the decider may be unable to articulate why the decision “felt right.” With machine learning, everything about the decision procedure is known with mathematical precision, but there may be simply too much information to interpret clearly.\\n\\nDeep Learning\\n\\nIn recent years, some of the most impressive advancements in machine learning have been in the subfield of deep learning, also known as deep network learning. Deep learning uses structures loosely inspired by the human brain, consisting of a set of units (or “neurons”). Each unit combines a set of input values to produce an output value, which in turn is passed on to other neurons downstream. For example, in an image recognition application, a first layer of units might combine the raw data of the image to recognize simple patterns in the image; a second layer of units might combine the results of the first layer to recognize patterns-of-patterns; a third layer might combine the results of the second layer; and so on.\\n\\n16 See, e.g., Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy, “Explaining and Harnessing Adversarial Examples,” http://arxiv.org/pdf/1412.6572.pdf.\\n\\n9\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nDeep learning networks typically use many layers—sometimes more than 100— and often use a large number of units at each layer, to enable the recognition of extremely complex, precise patterns in data.\\n\\nIn recent years, new theories of how to construct and train deep networks have emerged, as have larger, faster computer systems, enabling the use of much larger deep learning networks. The dramatic success of these very large networks at many machine learning tasks has come as a surprise to some experts, and is the main cause of the current wave of enthusiasm for machine learning among AI researchers and practitioners.\\n\\nAutonomy and Automation\\n\\nAI is often applied to systems that can control physical actuators or trigger online actions. When AI comes into contact with the everyday world, issues of autonomy, automation, and human-machine teaming arise.\\n\\nAutonomy refers to the ability of a system to operate and adapt to changing circumstances with reduced or without human control. For example, an autonomous car could drive itself to its destination. Despite the focus in much of the literature on cars and aircraft, autonomy is a much broader concept that includes scenarios such as automated financial trading and automated content curation systems. Autonomy also includes systems that can diagnose and repair faults in their own operation, such as identifying and fixing security vulnerabilities.\\n\\nAutomation occurs when a machine does work that might previously have been done by a person.17 The term relates to both physical work and mental or cognitive work that might be replaced by AI. Automation, and its impact on employment, have been significant social and economic phenomena since at least the Industrial Revolution. It is widely accepted that AI will automate some jobs, but there is more debate about whether this is just the next chapter in the history of automation or whether AI will affect the economy differently than past waves of automation have previously.\\n\\nHuman-Machine Teaming\\n\\nIn contrast to automation, where a machine substitutes for human work, in some cases a machine will complement human work. This may happen as a side-effect of AI development, or a system might be developed specifically with the goal of creating a human-machine team. Systems that aim to complement human cognitive capabilities are sometimes referred to as intelligence augmentation.\\n\\nIn many applications, a human-machine team can be more effective than either one alone, using the strengths of one to compensate for the weaknesses of the other. One example is in chess playing, where a weaker computer can often beat a stronger computer player, if the weaker computer is given a human teammate—this is true even though top computers are much stronger players than any human.18 Another example is in radiology. In one recent study, given images of lymph node cells, and asked to determine\\n\\n17 Different definitions of “automation” are used in different settings. The definition used in the main text, involving the substitution of machine labor for human labor, is commonly used in economics. Another definition is used in the systems analysis setting in the Department of Defense (DoD): Automation means that the system functions with little or no human operator involvement. However the system performance is limited to the specific pre-programmed actions it has been designed to execute. Once the system is initiated by a human operator, it executes its task according to those instructions and subroutines, which have been tested and validated. Typically these are well- defined tasks that have predetermined responses, i.e., rule-based responses in reasonably well-known and structured environments. 18 Garry Kasparov, “The Chess Master and the Computer,” New York Review of Books, February 11, 2010. http://www.nybooks.com/articles/2010/02/11/the-chess-master-and-the-computer.\\n\\n10\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nwhether or not the cells contained cancer, an AI-based approach had a 7.5 percent error rate, where a human pathologist had a 3.5 percent error rate; a combined approach, using both AI and human input, lowered the error rate to 0.5 percent, representing an 85 percent reduction in error.19\\n\\n19 Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew H. Beck, “Deep Learning for Identifying Metastatic Breast Cancer,” June 18, 2016, https://arxiv.org/pdf/1606.05718v1.pdf.\\n\\n11\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nPublic Outreach and Development of this Report\\n\\nThis report was developed by the NSTC’s Subcommittee on Machine Learning and Artificial Intelligence, which was chartered in May 2016 to foster interagency coordination and provide technical and policy advice on topics related to AI, and to monitor the development of AI technologies across industry, the research community, and the Federal Government. The report follows a series of public outreach activities led by OSTP, designed to allow government officials thinking about these topics to learn from experts and from the public. This public outreach on AI included five co-hosted public workshops, and a public Request for Information (RFI). The public workshops were:\\n\\nAI, Law, and Governance (May 24, in Seattle, co-hosted by OSTP, the National Economic Council (NEC), and the University of Washington);\\n\\nAI for Social Good (June 7, in Washington DC, co-hosted by OSTP, the Association for the Advancement of AI (AAAI) and the Computing Community Consortium (CCC));\\n\\nFuture of AI: Emerging Topics and Societal Benefit at the Global Entrepreneurship Summit (June 23, in Palo Alto, co-hosted by OSTP and Stanford University);\\n\\nAI Technology, Safety, and Control (June 28, in Pittsburgh, co-hosted by OSTP and Carnegie Mellon University); and\\n\\nSocial and Economic Impacts of AI (July 7, in New York, co-hosted by OSTP, NEC, and New York University).\\n\\nIn conjunction with each of the five workshops, the private-sector co-hosts organized separate meetings or conference sessions which government staff attended. Total in-person attendance at the public events was more than 2,000 people, in addition to international online streaming audiences, which included more than 3,500 people for the Washington, DC workshop livestream alone.\\n\\nOSTP also published a Request for Information (RFI) seeking public comment on the topics of the workshops. The RFI closed on July 22, 2016 and received 161 responses. Comments submitted in response to the public RFI were published by OSTP on September 6, 2016.20\\n\\n20 Ed Felten and Terah Lyons, “Public Input and Next Steps on the Future of Artificial Intelligence,” Medium, September 6 2016, https://medium.com/@USCTO/public-input-and-next-steps-on-the-future-of-artificial- intelligence-458b82059fc3.\\n\\n12\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nApplications of AI for Public Good\\n\\nOne area of great optimism about AI and machine learning is their potential to improve people’s lives by helping to solve some of the world’s greatest challenges and inefficiencies. The promise of AI has been compared to the transformative impacts of advances in mobile computing.21 Public- and private-sector investments in basic and applied R&D on AI have already begun reaping major benefits for the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion.22\\n\\nAt Walter Reed Medical Center, the Department of Veteran Affairs is using AI to better predict medical complications and improve treatment of severe combat wounds, leading to better patient outcomes, faster healing, and lower costs.23 The same general approach—predicting complications to enable preventive treatment—has also reduced hospital-acquired infections at Johns Hopkins University.24 Given the current transition to electronic health records, predictive analysis of health data may play a key role across many health domains like precision medicine and cancer research.\\n\\nIn transportation, AI-enabled smarter traffic management applications are reducing wait times, energy use, and emissions by as much as 25 percent in some places.25 Cities are now beginning to leverage the type of responsive dispatching and routing used by ride-hailing services, and linking it with scheduling and tracking software for public transportation to provide just-in-time access to public transportation that can often be faster, cheaper and, in many cases, more accessible to the public.\\n\\nSome researchers are leveraging AI to improve animal migration tracking by using AI image classification software to analyze tourist photos from public social media sites. The software can identify individual animals in the photos and build a database of their migration using the data and location stamps on the photos. At OSTP’s AI for Social Good workshop, researchers talked about building some of the largest available datasets to-date on the populations and migrations of whales and large African animals, and about launching a project to track “The Internet of Turtles” to gain new insights about sea life.26 Other speakers described uses of AI to optimize the patrol strategy of anti-poaching agents, and to design habitat preservation strategies to maximize the genetic diversity of endangered populations.\\n\\n21 Frank Chen, “AI, Deep Learning, and Machine Learning: A Primer,” Andreessen Horowitz, June 10, 2016, http://a16z.com/2016/06/10/ai-deep-learning-machines.\\n\\n22 The potential benefits of increasing access to digital technologies are detailed further in the World Bank Group’s Digital Dividends report. (“World Development Report 2016: Digital Dividends,” The World Bank Group, 2016, http://documents.worldbank.org/curated/en/896971468194972881/pdf/102725-PUB-Replacement-PUBLIC.pdf.)\\n\\n23 Eric Elster, “Surgical Critical Care Initiative: Bringing Precision Medicine to the Critically Ill,” presentation at AI for Social Good workshop, Washington, DC, June 7, 2016, http://cra.org/ccc/wp- content/uploads/sites/2/2016/06/Eric-Elster-AI-slides-min.pdf.\\n\\n24 Katharine E. Henry, David N. Hager, Peter J. Pronovost, and Suchi Saria, \"A targeted real-time early warning score (TREWScore) for septic shock,\" Science Translational Medicine 7, no. 299 (2015): 299ra122-299ra122.\\n\\n25 Stephen F. Smith, “Smart Infrastructure for Urban Mobility,” presentation at AI for Social Good workshop, Washington, DC, June 7, 2016, http://cra.org/ccc/wp-content/uploads/sites/2/2016/06/Stephen-Smith-AI-slides.pdf.\\n\\n26 Aimee Leslie, Christine Hof, Diego Amorocho, Tanya Berger-Wolf, Jason Holmberg, Chuck Stewart, Stephen G. Dunbar, and Claire Jea,, “The Internet of Turtles,” April 12, 2016, https://www.researchgate.net/publication/301202821_The_Internet_of_Turtles.\\n\\n13\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nAutonomous sailboats and watercraft are already patrolling the oceans carrying sophisticated sensor instruments, collecting data on changes in Arctic ice and sensitive ocean ecosystems in operations that would be too expensive or dangerous for crewed vessels. Autonomous watercraft may be much cheaper to operate than manned ships, and may someday be used for enhanced weather prediction, climate monitoring, or policing illegal fishing.27\\n\\nAI also has the potential to improve aspects of the criminal justice system, including crime reporting, policing, bail, sentencing, and parole decisions. The Administration is exploring how AI can responsibly benefit current initiatives such as Data Driven Justice and the Police Data Initiative that seek to provide law enforcement and the public with data that can better inform decision-making in the criminal justice system, while also taking care to minimize the possibility that AI might introduce bias or inaccuracies due to deficiencies in the available data.\\n\\nSeveral U.S. academic institutions have launched initiatives to use AI to tackle economic and social challenges. For example, the University of Chicago created an academic program that uses data science and AI to address public challenges such as unemployment and school dropouts.28 The University of Southern California launched the Center for Artificial Intelligence in Society, an institute dedicated to studying how computational game theory, machine learning, automated planning and multi-agent reasoning techniques can help to solve socially relevant problems like homelessness. Meanwhile, researchers at Stanford University are using machine learning in efforts to address global poverty by using AI to analyze satellite images of likely poverty zones to identify where help is needed most.29\\n\\nMany uses of AI for public good rely on the availability of data that can be used to train machine learning models and test the performance of AI systems. Agencies and organizations with data that can be released without implicating personal privacy or trade secrets can help to enable the development of AI by making those data available to researchers. Standardizing data schemas and formats can reduce the cost and difficulty of making new data sets useful.\\n\\nRecommendation 1: Private and public institutions are encouraged to examine whether and how they can responsibly leverage AI and machine learning in ways that will benefit society. Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways.\\n\\nRecommendation 2: Federal agencies should prioritize open training data and open data standards in AI. The government should emphasize the release of datasets that enable the use of AI to address social challenges. Potential steps may include developing an “Open Data for AI” initiative with the objective of releasing a significant number of government data sets to accelerate AI research and galvanize the use of open data standards and best practices across government, academia, and the private sector.\\n\\n27 John Markoff, “No Sailors Needed: Robot Sailboats Scout the Oceans for Data,” The New York Times, September 4, 2016.\\n\\n28 “Data Science for Social Good,” University of Chicago, https://dssg.uchicago.edu/.\\n\\n29 Neal Jean, Marshall Burke, Michael Xie, W. Matthew Davis, David B. Lobell, and Stefano Ermon. \"Combining satellite imagery and machine learning to predict poverty.\" Science 353, no. 6301 (2016): 790-794.\\n\\n14\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nAI in the Federal Government\\n\\nThe Administration is working to develop policies and internal practices that will maximize the economic and societal benefits of AI and promote innovation. These policies and practices may include:\\n\\n\\uf0b7 \\uf0b7 \\uf0b7 making data sets available to the public; \\uf0b7 \\uf0b7 \\uf0b7\\n\\ninvesting in basic and applied research and development (R&D); serving as an early customer for AI technologies and their applications; supporting pilot projects and creating testbeds in real-world settings;\\n\\nsponsoring incentive prizes; identifying and pursuing Grand Challenges to set ambitious but achievable goals for AI; funding rigorous evaluations of AI applications to measure their impact and cost-effectiveness; and creating a policy, legal, and regulatory environment that allows innovation to flourish while protecting the public from harm.\\n\\n\\n\\nUsing AI in Government to Improve Services and Benefit the American People\\n\\nOne challenge in using AI to improve services is that the Federal Government’s capacity to foster and harness innovation in order to better serve the country varies widely across agencies. Some agencies are more focused on innovation, particularly those agencies with large R&D budgets, a workforce that includes many scientists and engineers, a culture of innovation and experimentation, and strong ongoing collaborations with private-sector innovators. Many also have organizations that are specifically tasked with supporting high-risk, high-return research (e.g., the advanced research projects agencies in the Departments of Defense and Energy, as well as the Intelligence Community), and fund R&D across the full range from basic research to advanced development. Other agencies like the NSF have research and development as their primary mission.\\n\\nBut some agencies, particularly those charged with reducing poverty and increasing economic and social mobility, have more modest levels of relevant capabilities, resources, and expertise.30 For example, while the National Institutes of Health (NIH) has an R&D budget of more than $30 billion, the Department of Labor’s R&D budget is only $14 million. This limits the Department of Labor’s capacity to explore applications of AI, such as applying AI-based “digital tutor” technology to increase the skills and incomes of non-college educated workers.\\n\\nDARPA’s “Education Dominance” program serves as an example of AI’s potential to fulfill and accelerate agency priorities. DARPA, intending to reduce from years to months the time required for new Navy recruits to become experts in technical skills, now sponsors the development of a digital tutor that uses AI to model the interaction between an expert and a novice. An evaluation of the digital tutor program concluded that Navy recruits using the digital tutor to become IT systems administrators frequently outperform Navy experts with 7-10 years of experience in both written tests of knowledge and real-world problem solving.31\\n\\nPreliminary evidence based on digital tutor pilot projects also suggests that workers who have completed a training program that uses the digital tutor are more likely to get a high-tech job that dramatically\\n\\n30 Thomas Kalil, “A Broader Vision for Government Research,” Issues in Science and Technology, 2003.\\n\\n31 “Winning the Education Future: The Role of ARPA-ED,” The U.S. Department of Education, March 8 2011, https://www.whitehouse.gov/sites/default/files/microsites/ostp/arpa-ed-factsheet.pdf.\\n\\n15\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nincreases their incomes.32 These wage increases appear to be much larger than the impacts of current workforce development programs.33 Ideally, these results would be confirmed with independently conducted, randomized, controlled trials. Currently, the cost of developing digital tutors is high, and there is no repeatable methodology for developing effective digital tutors. Research that enables the emergence of an industry that uses AI approaches such as digital tutors could potentially help workers acquire in- demand skills.\\n\\nRecommendation 3: The Federal Government should explore ways to improve the capacity of key agencies to apply AI to their missions. For example, Federal agencies should explore the potential to create DARPA-like organizations to support high-risk, high-reward AI research and its application, much as the Department of Education has done through its proposal to create an “ARPA-ED,” to support R&D to determine whether AI and other technologies could significantly improve student learning outcomes.\\n\\nRecommendation 4: The NSTC MLAI subcommittee should develop a community of practice for AI practitioners across government. Agencies should work together to develop and share standards and best practices around the use of AI in government operations. Agencies should ensure that Federal employee training programs include relevant AI opportunities.\\n\\n32 The President’s Council of Advisors on Science and Technology, letter to the President, September 2014, https://www.whitehouse.gov/sites/default/files/microsites/ostp/PCAST/pcast_workforce_edit_report_sept_2014.pdf.\\n\\n33 J.D. Fletcher, “Digital Tutoring in Information Systems Technology for Veterans: Data Report,” The Institute for Defense Analysis, September 2014.\\n\\n16\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nAI and Regulation\\n\\nAI has applications in many products, such as cars and aircraft, which are subject to regulation designed to protect the public from harm and ensure fairness in economic competition. How will the incorporation of AI into these products affect the relevant regulatory approaches? In general, the approach to regulation of AI-enabled products to protect public safety should be informed by assessment of the aspects of risk that the addition of AI may reduce, alongside the aspects of risk that it may increase. If a risk falls within the bounds of an existing regulatory regime, moreover, the policy discussion should start by considering whether the existing regulations already adequately address the risk, or whether they need to be adapted to the addition of AI. Also, where regulatory responses to the addition of AI threaten to increase the cost of compliance or slow the development or adoption of beneficial innovations, policymakers should consider how those responses could be adjusted to lower costs and barriers to innovation without adversely impacting safety or market fairness.\\n\\nThe general consensus of the RFI commenters was that broad regulation of AI research or practice would be inadvisable at this time.34 Instead, commenters said that the goals and structure of existing regulations were sufficient, and commenters called for existing regulation to be adapted as necessary to account for the effects of AI. For example, commenters suggested that motor vehicle regulation should evolve to account for the anticipated arrival of autonomous vehicles, and that the necessary evolution could be carried out within the current structure of vehicle safety regulation. In doing so, agencies must remain mindful of the fundamental purposes and goals of regulation to safeguard the public good, while creating space for innovation and growth in AI.\\n\\nEffective regulation of technologies such as AI requires agencies to have in-house technical expertise to help guide regulatory decision-making. The need for senior-level expert participation exists at regulating departments and agencies, and at all stages of the regulatory process. A range of personnel assignment and exchange models (e.g. hiring authorities) can be used to develop a Federal workforce with more diverse perspectives on the current state of technological development. One example of such an authority is the Intergovernmental Personnel Act (IPA) Mobility Program, which provides for the temporary assignment of personnel between the Federal Government and state and local governments, colleges and universities, Indian tribal governments, federally funded research and development centers, and other eligible organizations. If used strategically, the IPA program can help agencies meet their needs for hard- to-fill positions and increase their ability to hire candidates from diverse technical backgrounds. Federal employees serving in IPA assignments can serve as both recruiters and ambassadors for the Federal workforce. For example, agency staff sent to colleges and universities as instructors can inspire students to consider Federal employment. Likewise, programs that rotate employees through different jobs and sectors can help government employees gain knowledge and experience to inform regulation and policy, especially as it relates to emergent technologies like AI.\\n\\n34 Ed Felten and Terah Lyons, “Public Input and Next Steps on the Future of Artificial Intelligence.”\\n\\n17\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nRecommendation 5: Agencies should draw on appropriate technical expertise at the senior level when setting regulatory policy for AI-enabled products. Effective regulation of AI-enabled products requires collaboration between agency leadership, staff knowledgeable about the existing regulatory framework and regulatory practices generally, and technical experts with knowledge of AI. Agency leadership should take steps to recruit the necessary technical talent, or identify it in existing agency staff, and should ensure that there are sufficient technical “seats at the table” in regulatory policy discussions.\\n\\nRecommendation 6: Agencies should use the full range of personnel assignment and exchange models (e.g. hiring authorities) to foster a Federal workforce with more diverse perspectives on the current state of technology.\\n\\nCase Study: Autonomous Vehicles and Aircraft\\n\\nA relevant example of the regulatory challenges associated with an agency updating legacy regulations to account for new AI-based products is the work of the Department of Transportation (DOT) on automated vehicles and unmanned aircraft systems (UAS, or “drones”). Within DOT, automated cars are regulated by the National Highway Traffic Safety Administration (NHTSA) and aircraft are regulated by the Federal Aviation Administration (FAA).\\n\\nThe Promise of Autonomy\\n\\nThe application of AI to vehicles and aircraft has captured the public imagination. Today’s new cars have AI-based driver assist features like self-parking and advanced cruise controls that keep a car in its lane and adjust speed based on surrounding vehicles. Experimental fully automated cars monitored by humans can already be seen driving on the roads. The consensus of experts is that automated surface vehicle technology will eventually be safer than human drivers and may someday prevent most of the tens of thousands of fatalities that occur annually on the Nation’s roads and highways.\\n\\nAutomated vehicles also offer the possibility of greater mobility for the elderly and Americans with disabilities who may not be able to drive. First- and last-mile access to transit and other novel transportation approaches may provide communities isolated from essential services such as jobs, health care, and groceries unprecedented access to opportunity. A well-designed system of automated vehicles able to predict and avoid collisions may also significantly reduce transportation-related emissions and energy consumption. The Administration is taking steps to make this vision a reality, including the proposed $3.9 billion investment in the President’s Fiscal Year (FY) 2017 Budget by the Department of Transportation in automated and connected vehicle research, development, and deployment efforts, to ensure that the United States maintains its lead in automated vehicle technologies.35\\n\\n35 “Secretary Foxx Unveils President Obama’s FY17 Budget Proposal of Nearly $4 Billion for Automated Vehicles and Announces DOT Initiatives to Accelerate Vehicle Safety Innovations,” U.S. Department of Transportation, January 14 2016, https://www.transportation.gov/briefing-room/secretary-foxx-unveils-president- obama%E2%80%99s-fy17-budget-proposal-nearly-4-billion.\\n\\n18\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nMoving to the air, since the early 1990s, commercial UAS have operated on a limited basis in the National Airspace System (NAS).36 Until recently, UAS mainly supported government operations, such as military and border security operations. But in recent years, potential applications have rapidly expanded to include aerial photography, surveying land and crops, monitoring forest fires, responding to disasters, and inspecting critical infrastructure. Several government agencies are already operating UAS to advance their missions, and thousands of Americans have obtained the necessary authority from the Federal Aviation Administration (FAA) for commercial UAS operations, a process accelerated under the FAA’s “Small UAS Rule” that took effect in August 2016 and the FAA’s Small UAS Aircraft Registration Service that launched in December 2015. The FAA estimates that the number of UAS registered for commercial use will exceed 600,000 by August 2017.37\\n\\nOne estimate of the economic impact of integrating of UAS into the airspace predicted more than $13.6 billion of economic value created by UAS in the first three years of integration, with sustainable growth predicted to follow.38 A 2013 study from the Association for Unmanned Vehicle Systems International predicted that the commercial drone industry could generate more than $82 billion for the U.S. economy and create more than 100,000 new jobs over the next 10 years. Tax revenue to the states was predicted to increase by more than $482 million in the first decade after integration.39\\n\\nEnsuring Safety\\n\\nRealizing the potential benefits of these promising technologies requires that government take steps to ensure the safety of the airspace and roads, while continuing to foster a culture of innovation and growth. The United States has the safest and most complex aviation system in the world, and the public relies on FAA oversight to establish safety standards. Federal Motor Vehicle Safety Standards (FMVSS) place requirements on manufacturers to develop safe surface vehicles, and NHTSA has the authority to recall vehicles in the event of an unreasonable risk to safety. While there is considerable opportunity to reduce the fatalities on roads and highways, current practices result in approximately one fatality for every 100 million vehicle miles traveled. Equaling or exceeding such performance in automated vehicles is a formidable challenge.\\n\\nApplying techniques of AI in such safety-critical environments raises several challenges. First among these is the need to translate human responsibilities while driving or flying into software. Unlike in some other successful applications of Narrow AI, there are no concise descriptions for the task of operating ground or air vehicles. Each of these operations is multifaceted, with responsibilities including guiding the vehicle, detecting and avoiding obstacles, and handing mechanical failures such as flat tires. While subtasks such as navigation or certain types of perception may align with certain existing Narrow AI solutions, the integration and prioritization of these tasks may not. It may seem straightforward to simply obey all traffic laws, but a skilled human driver may cross a double-yellow road boundary to avoid an accident or move past a double-parked vehicle. Though such situations may be rare, they cannot be\\n\\n36 The National Airspace System is the network of air navigation facilities, air traffic control facilities, airports, technology, and rules and regulations that are needed to protect persons and property on the ground, and to establish a safe and efficient airspace environment for civil, commercial, and military aviation.\\n\\n37 “Aerospace Forecast Report Fiscal Years 2016 to 2036,” The Federal Aviation Administration, March 24 016, https://www.faa.gov/data_research/aviation/aerospace_forecasts/media/Unmanned_Aircraft_Systems.pdf.\\n\\n38 Derryl Jenkins and Bijan Vasigh, “The Economic Impact of Unmanned Aircraft Systems Integration in the United States,” The Association for Unmanned Vehicle Systems International, 2013, https://higherlogicdownload.s3.amazonaws.com/AUVSI/958c920a-7f9b-4ad2-9807- f9a4e95d1ef1/UploadedImages/New_Economic%20Report%202013%20Full.pdf.\\n\\n39 Ibid.\\n\\n19\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nignored—simple arithmetic dictates that in order for failures to occur at least as infrequently as they do with human drivers, a system must handle many such rare cases without failure.\\n\\nFor systems that rely on machine learning, the need to get rare cases right has implications for system design and testing. Machine learning approaches can be more confident that a case will be handled correctly if a similar case is in the training set. The challenge is how to develop a data set that includes enough of the rare cases that contribute to the risk of an accident. Commercial aviation has mechanisms for sharing incident and safety data across the industry, but reporting may not be second nature to recently credentialed UAS operators who are new to the safety and accountability culture of the traditional aviation industry. No comparable system currently exists for the automotive industry—only fatal accidents are reported, and the collection and reporting of other traffic safety information is done, if at all, in a disparate manner at the state or local level. The lack of consistently reported incident or near-miss data increases the number of miles or hours of operation necessary to establish system safety, presenting an obstacle to certain AI approaches that require extensive testing for validation.\\n\\nTo facilitate safe testing, the FAA has designated six UAS Test Sites across the country and provided blanket authorization for UAS operations within these sites. Activities at the sites include a project to extend NASA’s multi-year research on UAS traffic management (UTM) to identify operational requirements for large-scale beyond visual line-of-sight UAS operations in low-altitude airspace. Similarly, ground vehicle testbeds such as the Connected Vehicle Pilots and the deployment of automated vehicles in Columbus, Ohio, winner of the Department of Transportation’s $40 million Smart City Challenge in 2016, will provide rich baseline and interaction data for AI researchers.\\n\\nRecommendation 7: The Department of Transportation should work with industry and researchers on ways to increase sharing of data for safety, research, and other purposes. In light of the future importance of AI in surface and air, Federal actors should focus in the near-term on developing increasingly rich sets of data, consistent with consumer privacy, that can better inform policy-making as these technologies mature.\\n\\nAdapting Current Regulations\\n\\nWhile the regulatory approaches for the Nation’s airspace and highways differ, the approaches to integrating autonomous vehicles and aircraft share a common goal: both the FAA and NHTSA are working to establish nimble and flexible frameworks that ensure safety while encouraging innovation.\\n\\nWith respect to airspace regulation, a significant step toward enabling the safe integration of UAS into the airspace was the FAA’s promulgation of the Part 107, or “Small UAS,” final rule, which took effect on August 29, 2016. For the first time, the rule authorizes widespread non-recreational flights of UAS under 55 pounds. The rule limits flights to daytime, at an altitude of 400 feet or less, with the vehicle controlled by a licensed operator and within the operator’s direct line of sight. Flights over people are not allowed. Subsequent rules are planned, to relax these restrictions as experience and data show how to do so safely. In particular, DOT is currently developing a Notice of Proposed Rulemaking proposing a regime for certain types of “micro UAS” to conduct operations over people, with a rule on expanded operations expected to follow.\\n\\nThe FAA has not yet publicly announced a clear path to a regulation allowing fully autonomous40 flight. Though safe integration of autonomous aircraft into the airspace will be a complex process, the FAA is\\n\\n40 This report uses the term “autonomous” for an aircraft that is controlled by a machine rather than a human. “Piloted” refers to an aircraft that has a human onboard who is controlling the aircraft. “Remotely-piloted” refers to\\n\\n20\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\npreparing for a not-so-distant technological future in which autonomous and piloted aircraft fly together in a seamlessly integrated airspace system.\\n\\nNew approaches to airspace management may also include AI-based enhancement of the air traffic control system. Projected future air traffic densities and diversity of operations are unlikely to be feasible within the current airspace management architecture, due to current limits on air/ground integration, and reliance on human-to-human communication in air and ground practices.41 The cost of U.S. air transportation delays in 2007, the latest year for which there is reliable public data, was estimated to be $31.2 billion—a number that has presumably grown as user volume has increased since that year.42 Though some flight delays are unavoidable due to weather and other constraints, adopting new aviation technologies, enabling policies, and infrastructure upgrades could significantly increase efficiency of operation in the U.S. airspace. Such solutions include AI and machine learning-based architectures that have the potential to better accommodate a wider range of airspace users, including piloted and unpiloted aircraft, and to use airspace more efficiently without undermining safety. Development and deployment of such technologies would help ensure global competitiveness for airspace users and service providers, while increasing safety and reducing cost.43\\n\\nWith respect to surface transportation, the most significant step currently underway to establish a common framework is the Federal Automated Vehicles Policy that the Administration released on September 20, 2016.44 The policy had several parts:\\n\\nguidance for manufacturers, developers, and other organizations outlining a 15 point “Safety Assessment” for the safe design, development, testing, and deployment of highly automated vehicles; a model state policy, which clearly distinguishes Federal and State responsibilities and recommends policy areas for states to consider, with a goal of generating a consistent national framework for the testing and operation of automated vehicles, while leaving room for experimentation by states; an analysis of current regulatory tools that NHTSA can use to aid the safe development of automated vehicles, such as interpreting current rules to allow for appropriate flexibility in design, providing limited exemptions to allow for testing of nontraditional vehicle designs, and ensuring that unsafe automated vehicles are removed from the road; and\\n\\n\\n\\n\\n\\nan aircraft that is controlled by a human who is not on board. “Manned” means there is a human onboard who may or may not be in control.\\n\\n41 Heinz Erzberger, “The Automated Airspace Concept,” prepared for the 4th USA/Europe Air Traffic Management R&D Seminar Dec. 3-7, 2001, Santa Fe, New Mexico, USA, http://www.aviationsystemsdivision.arc.nasa.gov/publications/tactical/erzberger_12_01.pdf.\\n\\n42 Michael Ball, Cynthia Barnhart, Martin Dresner, Mark Hansen, Kevin Neels, Amedeo Odoni, Everett Peterson, Lance Sherry, Antonio Trani, Bo Zou, “Total Delay Impact Study: A Comprehensive Assessment of the Costs and Impacts of Flight Delay in the United States,” The National Center of Excellence for Aviation Operations Research, November 2010, http://www.nextor.org/pubs/TDI_Report_Final_11_03_10.pdf.\\n\\n43 Robert W. Poole, Jr., “The Urgent Need to Reform the FAA’s Air Traffic Control System,” The Heritage Foundation, 2007, http://www.heritage.org/research/reports/2007/02/the-urgent-need-to-reform-the-faas-air-traffic- control-system.\\n\\n44 “Federal Automated Vehicles Policy,” The U.S. Department of Transportation, September 21 2016, https://www.transportation.gov/AV.\\n\\n21\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\n\\n\\na discussion of new tools and authorities that the agency could consider seeking in the future to aid the safe and efficient deployment of new lifesaving technologies and ensure that technologies deployed on the road are safe.\\n\\nDOT intends for the guidance and the model state policy to be routinely updated as new data are learned and research completed.\\n\\nRecommendation 8: The U.S. Government should invest in developing and implementing an advanced and automated air traffic management system that is highly scalable, and can fully accommodate autonomous and piloted aircraft alike.\\n\\nRecommendation 9: The Department of Transportation should continue to develop an evolving framework for regulation to enable the safe integration of fully automated vehicles and UAS, including novel vehicle designs, into the transportation system.\\n\\n22\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nResearch and Workforce\\n\\nGovernment also has an important role to play in advancing the AI field by investing in research and development, developing a workforce that is skilled and diverse, and managing the economic impacts of these technologies as they develop. A separate National Artificial Intelligence Research and Development Strategic Plan is being published in conjunction with this report. This section discusses additional policy issues related to research and workforce development.\\n\\nMonitoring Progress in AI\\n\\nGiven the potential impacts of AI, society would benefit from accurate and timely methods for monitoring and forecasting AI developments.45 Several projects have attempted to forecast AI futures. The 2009 AAAI Presidential Panel on Long-Term AI Futures46 and the 2015 Future of AI Conference47 brought together AI experts to predict the future of their field. In addition, Stanford’s One-Hundred Year Study on Artificial Intelligence48 plans to conduct \"a series of periodic studies on how AI will affect automation, national security, psychology, ethics, law, privacy, democracy, and other issues.\" The first of these studies was published in September 2016.49\\n\\nOne potentially useful line of research is to survey expert judgments over time. As one example, a survey of AI researchers found that 80 percent of respondents believed that human-level General AI will eventually be achieved, and half believed it is at least 50 percent likely to be achieved by the year 2040. Most respondents also believed that General AI will eventually surpass humans in general intelligence.50 While these particular predictions are highly uncertain, as discussed above, such surveys of expert judgment are useful, especially when they are repeated frequently enough to measure changes in judgment over time. One way to elicit frequent judgments is to run \"forecasting tournaments\" such as prediction markets, in which participants have financial incentives to make accurate predictions.51 Other\\n\\n45 The track record of technology forecasts, in general, and AI forecasts, in particular, suggests this may be difficult. One of the largest retrospective reviews of technology forecasts over the last 50 years found that forecasts with time horizons beyond 10 years were rarely better than coin-flips45. (Carrie Mullins, “Retrospective Analysis of Technology Forecasting,” The Tauri Group, August 13, 2012.) One review of 95 timeline predictions for AI from 1950 to 2012 found that most forecasts predicted General AI would be achieved \"in the next 20 years.\" (Stuart Armstrong, Kaj Sotala, Seán S. ÓhÉigeartaigh, “The errors, insights and lessons of famous AI predictions – and what they mean for the future,” Journal of Experimental & Theoretical Artificial Intelligence, May 20, 2014.)\\n\\n46 “AAAI Presidential Panel on Long-Term AI Futures: 2008-2009 Study,” The Association for the Advancement of Artificial Intelligence, http://www.aaai.org/Organization/presidential-panel.php.\\n\\n47 “AI Safety Conference in Puerto Rico,” The Future of Life Institute, October 12, 2015, http://futureoflife.org/2015/10/12/ai-safety-conference-in-puerto-rico.\\n\\n48 Peter Stone, et al., “Artificial Intelligence and Life in 2030,” http://ai100.stanford.edu/2016-report.\\n\\n49 Ibid.\\n\\n50 Vincent Müller and Nick Bostrom, “Future progress in artificial intelligence: A Survey of Expert Opinion,” Fundamental Issues of Artificial Intelligence, 2014.\\n\\n51 Charles Twardy, Robin Hanson, Kathryn Laskey, Tod S. Levitt, Brandon Goldfedder, Adam Siegel, Bruce D’Ambrosio, and Daniel Maxwell, “SciCast: Collective Forecasting of Innovation,” Collective Intelligence, 2014.\\n\\n23\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nresearch has found that technology developments can often be accurately predicted by analyzing trends in publication and patent data52.\\n\\nAt present, the majority of basic research in AI is conducted by academics and by commercial labs that regularly announce their findings and publish them in the research literature. If competition drives commercial labs towards increased secrecy, monitoring of progress may become more difficult, and public concern may increase.\\n\\nOne particularly valuable line of research is to identify milestones that could represent or foreshadow significant leaps in AI capabilities. When asked during the outreach workshops and meetings how government could recognize milestones of progress in the field, especially those that indicate the arrival of General AI may be approaching, researchers tended to give three distinct but related types of answers:\\n\\n1. Success at broader, less structured tasks: In this view, the transition from present Narrow AI to an eventual General AI will occur by gradually broadening the capabilities of Narrow AI systems so that a single system can cover a wider range of less structured tasks. An example milestone in this area would be a housecleaning robot that is as capable as a person at the full range of routine housecleaning tasks.\\n\\n2. Unification of different “styles” of AI methods: In this view, AI currently relies on a set of separate methods or approaches, each useful for different types of applications. The path to General AI would involve a progressive unification of these methods. A milestone would involve finding a single method that is able to address a larger domain of applications that previously required multiple methods.\\n\\n3. Solving specific technical challenges, such as transfer learning: In this view, the path to General AI does not lie in progressive broadening of scope, nor in unification of existing methods, but in progress on specific technical grand challenges, opening up new ways forward. The most commonly cited challenge is transfer learning, which has the goal of creating a machine learning algorithm whose result can be broadly applied (or transferred) to a range of new applications. For example, transfer learning might allow a model to be trained to translate English to Spanish, in such a way that the resulting model could “transfer” its knowledge to similar tasks such as Chinese to French translation, or writing poetry in Russian, enabling these new tasks to be learned much more quickly.\\n\\nRecommendation 10: The NSTC Subcommittee on Machine Learning and Artificial Intelligence should monitor developments in AI, and report regularly to senior Administration leadership about the status of AI, especially with regard to milestones. The Subcommittee should update the list of milestones as knowledge advances and the consensus of experts changes over time. The Subcommittee should consider reporting to the public on AI developments, when appropriate.\\n\\nRecommendation 11: The Government should monitor the state of AI in other countries, especially with respect to milestones.\\n\\nRecommendation 12: Industry should work with government to keep government updated on the general progress of AI in industry, including the likelihood of milestones being reached soon.\\n\\n52 Sara Reardon, “Text-mining offers clues to success: US intelligence programme analyses language in patents and papers to identify next big technologies,” Nature no. 509, 410 (May 22 2014).\\n\\n24\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nFederal Support for AI Research\\n\\nIn 2015, the U.S. Government’s investment in unclassified R&D in AI-related technologies was approximately $1.1 billion, with preliminary estimates showing growth to $1.2 billion in 2016. Throughout the workshops and public outreach on AI conducted by OSTP, government officials heard calls for greater government investment in AI research and development, from business leaders, technologists, and economists.\\n\\nLeading researchers in AI were optimistic about sustaining the recent rapid progress in AI and its application to an ever wider range of applications. At the same time they emphasized that there are many deep unanswered questions, and no clear path toward General AI.\\n\\nResearchers reported that enthusiasm for and investment in AI research has fluctuated over recent decades—one low period was known as the “AI winter”—and they emphasized the importance of sustained investment given the history of major computer science advances taking 15 years or more to transition from conception in the lab to industrial maturity.\\n\\nA strong case can be made in favor of increased Federal funding for research in AI. Analysis by the Council of Economic Advisers (CEA) indicates that beyond AI, across all research areas, doubling or tripling research investment would be a net positive for the Nation due to the resulting increase in economic growth.53 Although it may not be feasible fiscally to increase funding for all research by that amount, a targeted increase in areas of high economic and strategic value may offer many benefits with much smaller budgetary impact than an across-the-board increase. AI qualifies as a high-leverage area, and research agencies report that the AI research community can absorb a significant funding increase productively, leading to faster progress on AI and a larger cadre of trained AI practitioners. In a speech delivered at an AI workshop in New York City in July 2016, CEA Chairman Jason Furman said, “We have had substantial innovation in robotics, AI, and other areas in the last decade. But we will need a much faster pace of innovation in these areas to really move the dial on productivity growth going forward,” noting that the biggest worry that he had about AI is “that we do not have enough of [it].”54\\n\\nTo be sure, the private sector will be the main engine of progress on AI. But as it stands, there is an underinvestment in basic research—research with long time horizons conducted for the sole purpose of furthering the scientific knowledge base—in part because it is difficult for a private firm to get a return from its investment in such research in a reasonable time frame. Basic research benefits everyone, but only the firm doing the research pays the costs. The literature suggests that, as a result, current levels of R&D spending are half to one-quarter of the level of R&D investment that would produce the optimal level of economic growth.55\\n\\n53 Jason Furman, “Is This Time Different? The Opportunities and Challenges of Artificial Intelligence,” (presentation, AI Now: The Social and Economic Implications of Artificial Intelligence Technologies in the Near Term, New York, NY, July 7, 2016), Available at https://www.whitehouse.gov/sites/default/files/page/files/20160707_cea_ai_furman.pdf.\\n\\n54 Jason Furman, “Is This Time Different? The Opportunities and Challenges of Artificial Intelligence.”\\n\\n55 Nicholas Bloom, Mark Schankerman, John Van Reene, “Identifying Technology Spillovers and Product Market Rivalry,” Econometrica, 81: 1347–1393. doi:10.3982/ECTA9466.\\n\\n25\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nRecommendation 13: The Federal government should prioritize basic and long-term AI research. The Nation as a whole would benefit from a steady increase in Federal and private-sector AI R&D, with a particular emphasis on basic research and long-term, high-risk research initiatives. Because basic and long-term research especially are areas where the private sector is not likely to invest, Federal investments will be important for R&D in these areas.\\n\\nWorkforce Development and Diversity\\n\\nThe rapid growth of AI has dramatically increased the need for people with relevant skills to support and advance the field. The AI workforce includes AI researchers who drive fundamental advances in AI, a larger number of specialists who refine AI methods for specific applications, and a much larger number of users who operate those applications in specific settings. For researchers, AI training is inherently interdisciplinary, often requiring a strong background in computer science, statistics, mathematical logic, and information theory.56 For specialists, training typically requires a background in software engineering and in the application area. For users, familiarity with AI technologies is needed to apply AI technologies reliably.\\n\\nThe Role of Government\\n\\nThe AI workforce challenge is in part a science, technology, engineering, and mathematics (STEM) education challenge that remains a priority focus of the NSTC, OSTP, and other agencies. NSF and the Department of Education are working with the private sector and across government to advance education quality, flexibility, and domain impact, to address goals such as sustained economic development, increased inclusion and diversity, and improved outcome measures. The NSTC Committee on Science, Technology and Mathematics Education (CoSTEM) brings together Federal agencies supporting STEM education programs to coordinate efforts on multiple topics, including AI education.\\n\\nAI knowledge and education are increasingly emphasized in Federal STEM education programs. There are several key roles for the Federal government in AI workforce development, including supporting graduate students, funding research on AI curriculum design and impact, and accrediting AI education programs.\\n\\nThe Role of Schools and Universities\\n\\nIntegrating AI, data science, and related fields throughout the Nation’s education system is essential to developing a workforce that can address national priorities. Educational institutions are establishing and growing AI programs at all levels. Universities, colleges, and even secondary schools are expanding AI and data science curricula, but more programs and teachers are needed.\\n\\nThere are several key roles for academic institutions:\\n\\nBronwyn H. Hall, Jacques Mairesse, and Pierre Mohnen, “Measuring the Returns to R&D,” Chapter prepared for the Handbook of the Economics of Innovation, B. H. Hall and N. Rosenberg (editors), December 10, 2009, https://eml.berkeley.edu/~bhhall/papers/HallMairesseMohnen09_rndsurvey_HEI.pdf.\\n\\nCharles I. Jones and John C. Williams, “Measuring the Social Returns to R&D,” The Quarterly Journal of Economics (1998) 113 (4): 1119-1135, doi: 10.1162/003355398555856.\\n\\n56 There is also a need for the development of a strong research community in fields outside of technical disciplines related to AI, to examine the impacts and implications of AI on economics, social science, health, and other areas of research.\\n\\n26\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nbuilding and sustaining the researcher workforce, including computer scientists, statisticians, database and software programmers, curators, librarians, and archivists with specialization in data science; training the specialist workforce, by emphasizing AI methods within software development courses, offering applied AI courses that demonstrate the applications of AI to other domains, and incorporating AI and data science challenges posed by industry, civil society, and government into active case studies; ensuring that the user workforce has the necessary familiarity with AI systems to meet the needs of users and of institutions across industry, government, and academia; supporting training through seed grants, professional development stipends, internships, fellowships, and summer research experiences; and recruiting and retaining faculty, as industrial salaries grow faster than academic salaries for skilled researchers.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCommunity colleges, two-year colleges, and certificate programs play an important role in providing opportunities for students and professionals to acquire necessary skills for a modest investment of their time and money. These opportunities may be especially relevant to workers expanding their skills, veterans returning to the workforce, and unemployed people seeking a way to reenter the workforce.\\n\\nAn AI-enabled world demands a data-literate citizenry that is able to read, use, interpret, and communicate about data, and participate in policy debates about matters affected by AI. Data science education as early as primary or secondary school can help to improve nationwide data literacy, while also preparing students for more advanced data science concepts and coursework after high school.\\n\\nAI education is also a component of Computer Science for All, the President’s initiative to empower all American students from kindergarten through high school to learn computer science and be equipped with the computational thinking skills they need to be creators, not just consumers, in the digital economy, and to be active citizens in a technology-driven world. The American economy is rapidly shifting, and both educators and business leaders are increasingly recognizing that computer science (CS) is a “new basic” skill necessary for economic opportunity and social mobility. CS for All builds on efforts already being led by parents, teachers, school districts, states, and private sector leaders from across the country and is one way to meet the challenge of preparing a future workforce for the needs of an AI-driven economy.\\n\\nThe Diversity Challenge\\n\\nAll sectors face the challenge of how to diversify the AI workforce. The lack of gender and racial diversity in the AI workforce mirrors the lack of diversity in the technology industry and the field of computer science generally. Unlocking the full potential of the American people, especially in STEM fields, in entrepreneurship, and in the technology industry is a priority of this Administration. The importance of including individuals from diverse backgrounds, experiences, and identities, especially women and members of racial and ethnic groups traditionally underrepresented in STEM, is one of the most critical and high-priority challenges for computer science and AI.\\n\\nJust 18 percent of computer science graduates today are women, down from a peak of 37 percent in 1984.57 Though there is a lack of consistently-reported demographic data on the AI workforce, some statistics are available. At the Neural Information Processing Systems (NIPS) Conference in 2015—one of the year’s largest conferences on AI research—just 13.7 percent of conference participants were\\n\\n57 Christianne Corbett and Catherine Hill, “Solving the Equation: The Variables for Women’s Success in Engineering and Computing,” The American Association of University Women, March 2015, http://www.aauw.org/files/2015/03/Solving-the-Equation-report-nsa.pdf.\\n\\n27\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nfemale.58 After seeing similarly low representation at a machine intelligence conference, at which she was the only female speaker from industry, the CEO and Co-Founder of Textio, a startup that applies AI to the text of job postings and recruiting emails, decided to further investigate recruitment language in the industry. When the company analyzed 78,768 engineering job listings, they found that job postings for software engineers in the machine intelligence sector had a gender-bias score in favor of men more than twice as high as any other sector.59\\n\\nThe diversity challenge is not limited to gender. African Americans, Hispanics, and members of other racial and ethnic minority groups are severely underrepresented, compared to their shares of the U.S. population, in the STEM workforce, in computer science, and in the technology industry workforce, including in the field of AI.\\n\\nMany of the comments submitted to the OSTP RFI discussed the diversity challenge. Commenters focused on the importance of AI being produced by and for diverse populations. Doing so helps to avoid the negative consequences of narrowly focused AI development, including the risk of biases in developing algorithms, by taking advantage of a broader spectrum of experience, backgrounds, and opinions. These topics were also covered extensively during the public workshops. There is some research on the effects of a lack of diversity in the AI workforce on AI technology design and on the societal impacts of AI. This rich body of research is growing but still lagging behind the literature on broader AI workforce development needs. More research would be beneficial.\\n\\nRecommendation 14: The NSTC Subcommittees on MLAI and NITRD, in conjunction with the NSTC Committee on Science, Technology, Engineering, and Education (CoSTEM), should initiate a study on the AI workforce pipeline in order to develop actions that ensure an appropriate increase in the size, quality, and diversity of the workforce, including AI researchers, specialists, and users.\\n\\n58 Jack Clark, “Artificial Intelligence Has a ‘Sea of Dudes’ Problem,” Bloomberg, June 21, 2016, https://www.bloomberg.com/news/articles/2016-06-23/artificial-intelligence-has-a-sea-of-dudes-problem.\\n\\n59 The next three high-scoring sectors were back-end engineering, full-stack engineering, and general software engineering. See more: https://textio.ai/gendered-language-in-your-job-post-predicts-the-gender-of-the-person- youll-hire-cd150452407d#.rht0s16ov.\\n\\n28\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nAI, Automation, and the Economy\\n\\nAI’s central economic effect in the short term will be the automation of tasks that could not be automated before. There is some historical precedent for waves of new automation from which we can learn, and some ways in which AI will be different. Government must understand the potential impacts so it can put in place policies and institutions that will support the benefits of AI, while mitigating the costs.60\\n\\nLike past waves of innovation, AI will create both benefits and costs. The primary benefit of previous waves of automation has been productivity growth; today’s wave of automation is no different. For example, a 2015 study of robots in 17 countries found that they added an estimated 0.4 percentage point on average to those countries’ annual GDP growth between 1993 and 2007, accounting for just over one- tenth of those countries’ overall GDP growth during that time.61\\n\\nOne important concern arising from prior waves of automation, however, is the potential impact on certain types of jobs and sectors, and the resulting impacts on income inequality. Because AI has the potential to eliminate or drive down wages of some jobs, especially low- and medium-skill jobs, policy interventions will likely be needed to ensure that AI’s economic benefits are broadly shared and that inequality is diminished and not worsened as a consequence.\\n\\nThe economic policy questions raised by AI-driven automation are important but they are best addressed by a separate White House working group. The White House will conduct an additional interagency study on the economic impact of automation on the economy and recommended policy responses, to be published in the coming months.\\n\\nRecommendation 15: The Executive Office of the President should publish a follow-on report by the end of this year, to further investigate the effects of AI and automation on the U.S. job market, and outline recommended policy responses.\\n\\n60 Jason Furman, “Is This Time Different? The Opportunities and Challenges of Artificial Intelligence.”\\n\\n61 Georg Graetz and Guy Michaels, “Robots at Work,” CEPR Discussion Paper No. DP10477, March 2015, http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2575781.\\n\\n29\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nFairness, Safety, and Governance\\n\\nAs AI technologies gain broader deployment, technical experts and policy analysts have raised concerns about unintended consequences. The use of AI to make consequential decisions about people, often replacing decisions made by human actors and institutions, leads to concerns about how to ensure justice, fairness, and accountability—the same concerns voiced previously in the “Big Data” context.62 The use of AI to control physical-world equipment leads to concerns about safety, especially as systems are exposed to the full complexity of the human environment.\\n\\nAt a technical level, the challenges of fairness and safety are related. In both cases, practitioners strive to prevent intentional discrimination or failure, to avoid unintended consequences, and to generate the evidence needed to give stakeholders justified confidence that unintended failures are unlikely.\\n\\nJustice, Fairness, and Accountability\\n\\nA common theme in the Law and Governance, AI for Social Good, and Social and Economic Impacts workshops was the need to ensure that AI promotes justice and fairness, and that AI-based processes are accountable to stakeholders. This issue was highlighted previously in the Administration’s first Big Data report63 published in May 2014, and the follow-up report on Big Data, Algorithmic Systems, Opportunity, and Civil Rights,64 published in May 2016.\\n\\nIn the criminal justice system, some of the biggest concerns with Big Data are the lack of data and the lack of quality data.65 AI needs good data. If the data is incomplete or biased, AI can exacerbate problems of bias. It is important that anyone using AI in the criminal justice context is aware of the limitations of current data.\\n\\nA commonly cited example at the workshops is the use of apparently biased “risk prediction” tools by some judges in criminal sentencing and bail hearings as well as by some prison officials in assignment and parole decisions, as detailed in an extensively researched ProPublica article.66 The article presented evidence suggesting that a commercial risk scoring tool used by some judges generates racially biased risk scores. A separate report from Upturn questioned the fairness and efficacy of some predictive policing tools.67\\n\\n62 The White House, “Big Data: Seizing Opportunities, Preserving Values,” May 2014, https://www.whitehouse.gov/sites/default/files/docs/big_data_privacy_report_may_1_2014.pdf; and The White House, “Big Data: A Report on Algorithmic Systems, Opportunity, and Civil Rights,” May 2016, https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf.\\n\\n63 The White House, “Big Data: Seizing Opportunities, Preserving Values,” Executive Office of the President, May 2014.\\n\\n64 The White House, “Big Data: A Report on Algorithmic Systems, Opportunity, and Civil Rights,” Executive Office of the President, May 2016.\\n\\n65 Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what-we-dont-know-about-mass-incarceration/394520/\\n\\n66 Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\\n\\n67 David Robinson and Logan Koepke, “Stuck in a Pattern: Early evidence on ‘predictive policing’ and civil rights,” Upturn, August 2016, http://www.stuckinapattern.org.\\n\\n30\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nSimilar issues could impact hiring practices. If a machine learning model is used to screen job applicants, and if the data used to train the model reflects past decisions that are biased, the result could be to perpetuate past bias. For example, looking for candidates who resemble past hires may bias a system toward hiring more people like those already on a team, rather than considering the best candidates across the full diversity of potential applicants.\\n\\nIn response to these concerns, several workshop speakers argued for greater transparency when AI tools are used for public purposes. One speaker compared the role of AI to the role of administrative agencies in public decision-making. Authority is delegated to an agency due to the agency’s subject-matter expertise, but the delegation is constrained by due process protections, measures promoting transparency and oversight, and limits on the scope of the delegated authority. Some speakers called for the development of an analogous theory of how to maintain accountability when delegating decision-making power to machines. Transparency concerns focused not only on the data and algorithms used, but also on the potential to have some form of explanation for any AI-based determination.\\n\\nAt the same workshops, AI experts cautioned that there are inherent challenges in trying to understand, predict, and explain the behavior of advanced AI systems, due to the complexity of the systems and the large volume of data they use.\\n\\nThe difficulty of understanding machine learning results is at odds with the common misconception that complex algorithms always do what their designers choose to have them do, and therefore that bias will creep into an algorithm if and only if its developers themselves suffer from conscious or unconscious bias. It is certainly true that a technology developer who wants to produce a biased algorithm can do so, and that unconscious bias may cause practitioners to apply insufficient effort to preventing bias. In practice, however, unbiased developers with the best intentions can inadvertently produce systems with biased results, because even the developers of an AI system may not understand it well enough to prevent unintended outcomes.\\n\\nMoritz Hardt suggested an illustrative example of how bias might emerge unintentionally from the machine learning process.68 He postulated a machine learning model trained to distinguish people’s real names from false names.69 The model might determine that a name is more likely to be false if the first- name part of it is unique in the data set. This rule might have predictive power across the whole population, because false names are more likely to be fanciful and therefore unique. However, if there is an ethnic group that is a small minority of the population and tends to use a different set of first names than the majority population, these distinctive names are more likely to be unique in the sample, and therefore more likely to be incorrectly classified as false names. This effect would arise not because of any special treatment of the minority group’s names, and not because the input data is unrepresentative of the overall population, but simply because the minority group is less numerous.70\\n\\nAndrew Moore, the Dean of Computer Science at Carnegie Mellon University, offered a perspective on the challenge of AI and unforeseen consequences at the workshop on AI Technology, Safety, and Control.\\n\\n68 Moritz Hardt, “How big data is unfair,” Medium, September 26 2014, https://medium.com/@mrtz/how-big-data- is-unfair-9aa544d739de.\\n\\n69 Some online services require that users sign up for accounts using their real names. Some such services use AI models to detect names suspected of being false, in order to cancel the associated accounts. In such a system, a user whose name is incorrectly classified as false may be unable to sign up for an account, or may have their account canceled unexpectedly.\\n\\n70 Hardt points to another way that disparate impact may occur. ML models typically become more accurate as the number of examples in the training set increases. In some circumstances, this may cause prediction to be more accurate for a majority group than for a minority. Again, this disparity arises simply because the majority group is more numerous, even if the dataset is representative of the population.\\n\\n31\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nHe argued that today, because of the opacity of AI algorithms, the most effective way to minimize the risk of unintended outcomes is through extensive testing—essentially to make a long list of the types of bad outcomes that could occur, and to rule out these outcomes by creating many specialized tests to look for them.\\n\\nAn example of what can go wrong in the absence of extensive testing comes from a trained model for automatically captioning photos, which infamously put the caption “gorilla” on some close-up photos of dark-skinned human faces. This was antithetical to the developers’ values, and it occurred despite testing that showed the model produced accurate results on a high percentage of all photos. These particular errors, although rare, had negative consequences that were beyond the understanding of the model, which had no built-in concept of race, nor any understanding of the relevant historical context. One way to prevent this type of error would have involved extensive testing of the algorithm to scrutinize how human faces, in particular, are labeled, including examination of some results by people who could recognize unacceptable outcomes that the model wouldn’t catch.\\n\\nEthical training for AI practitioners and students is a necessary part of the solution. Ideally, every student learning AI, computer science, or data science would be exposed to curriculum and discussion on related ethics and security topics.71 However, ethics alone is not sufficient. Ethics can help practitioners understand their responsibilities to all stakeholders, but ethical training needs to be augmented with the technical capability to put good intentions into practice by taking technical precautions as a system is built and tested.\\n\\nAs practitioners strive to make AI systems more just, fair and accountable, there are often opportunities to make technology an aid to accountability rather than a barrier to it. Research to improve the interpretability of machine learning results is one example. Having an interpretable model that helps people understand a decision empowers them to interrogate the assumptions and processes behind it.\\n\\nThere are several technical approaches to enhancing the accountability and robustness of complex algorithmic decisions. A system can be tested “in the wild” by presenting it with situations and observing its behavior. A system can be subjected to black-box testing, in which it is presented with synthetic inputs and its behavior is observed, enabling behavior to be tested in scenarios that might not occur naturally.72 Some or all of the technical details of a system’s design can be published, enabling analysts to replicate it and analyze aspects of its internal behavior that might be difficult to characterize with testing alone. In some cases it is possible to publish information that helps the public evaluate a system’s risk of bias, while withholding other information about the system as proprietary or private.\\n\\nSafety and Control\\n\\nAt the workshops, AI experts said that one of the main factors limiting the deployment of AI in the real world is concern about safety and control. If practitioners cannot achieve justified confidence that a system is safe and controllable, so that deploying the system does not create an unacceptable risk of serious negative consequences, then the system cannot and should not be deployed.\\n\\n71 Some institutions may choose to incorporate ethics into existing courses. Others may choose to introduce separate courses on ethics.\\n\\n72 Black-box testing allows a system to be presented with fictionalized data, which enables comprehensive experiments that vary individual attributes of an individual as well as larger numbers of experiments than might be possible for in-the-wild testing. See, e.g., Anupam Datta, Shayak Sen, and Yair Zick, “Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems,” Proceedings of 37th IEEE Symposium on Security and Privacy, 2016.\\n\\n32\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nA major challenge in safety and control is building systems that can safely transition from the “closed world” of the laboratory into the outside “open world” where unpredictable things can happen. In the open world, a system is likely to encounter objects and situations that were not anticipated when it was designed and built. Adapting gracefully to unforeseen situations is difficult yet necessary for safe operation.\\n\\nOn the topic of safety and predictability in AI, several speakers referenced a recent paper entitled “Concrete Problems in AI Safety,”73 and the first author of the paper spoke at the workshop on Technology, Safety, and Control. The paper uses a running example of an autonomous robot that does housecleaning. The paper’s overview section gives an extended list of the sorts of practical problems that arise in making such a robot effective and safe, which is quoted here:\\n\\nAvoiding Negative Side Effects: How can we ensure that our cleaning robot will not disturb the environment in negative ways while pursuing its goals, e.g., by knocking over a vase because it can clean faster by doing so? Can we do this without manually specifying everything the robot should not disturb?\\n\\nAvoiding Reward Hacking: How can we ensure that the cleaning robot won’t game its reward function? For example, if we reward the robot for achieving an environment free of messes, it might disable its vision so that it won’t find any messes, or cover over messes with materials it can’t see through, or simply hide when humans are around so they can’t tell it about new types of messes.\\n\\nScalable Oversight: How can we efficiently ensure that the cleaning robot respects aspects of the objective that are too expensive to be frequently evaluated during training? For instance, it should throw out things that are unlikely to belong to anyone, but put aside things that might belong to someone (it should handle stray candy wrappers differently from stray cellphones). Asking the humans involved whether they lost anything can serve as a check on this, but this check might have to be relatively infrequent—can the robot find a way to do the right thing despite limited information?\\n\\nSafe Exploration: How do we ensure that the cleaning robot doesn’t make exploratory moves with very bad repercussions? For example, the robot should experiment with mopping strategies, but putting a wet mop in an electrical outlet is a very bad idea.\\n\\nRobustness to Distributional Shift: How do we ensure that the cleaning robot recognizes, and behaves robustly, when in an environment different from its training environment? For example, heuristics it learned for cleaning factory work floors may be outright dangerous in an office.\\n\\nThese examples illustrate how the “intelligence” of an AI system can be deep but narrow: the system might have a superhuman ability to detect dirt and optimize its mopping strategy, yet not know to avoid swiping a wet mop over an electrical outlet. One way to describe this overall problem is: how can we give intelligent machines common sense? Researchers are making slow progress on these sorts of problems.\\n\\nAI Safety Engineering\\n\\nA common theme at the Technology, Safety, and Control workshop was the need to connect open-world AI methods with the broader field of safety engineering. Experience in building other types of safety- critical systems, such as aircraft, power plants, bridges, and vehicles, has much to teach AI practitioners about verification and validation, how to build a safety case for a technology, how to manage risk, and how to communicate with stakeholders about risk.\\n\\n73 Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané, “Concrete Problems in AI Safety,” https://arxiv.org/abs/1606.06565.\\n\\n33\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nAt present, the practice of AI, especially in fast-moving areas of machine learning, can be as much art as science. Certain aspects of practice are not backed by a well-developed theory but instead rely on intuitive judgment and experimentation by practitioners. This is not unusual in newly emerging areas of technology, but it does limit the application of the technology in practice. Some stakeholders have suggested a need to grow AI into a more mature engineering field.\\n\\nAs engineering fields mature, they typically move from an initial “craft” stage characterized by intuition- driven creation by talented amateurs and a do-it-yourself spirit; to a second commercial stage involving skilled practitioners, pragmatic improvement, widely accepted rules-of-thumb, and organized manufacture for sale; to a mature stage that integrates more rigorous methods, educated professionals, well-established theory, and greater specialization of products.74 Most engineering fields, having a much longer history than modern AI, have reached a mature stage.\\n\\nIn general, mature engineering fields have greater success in creating systems that are predictable, reliable, robust, safe, and secure. Continuing the progress toward AI becoming a mature engineering field will be one of the key enablers of safety and controllability as more complex systems are built.\\n\\nRecommendation 16: Federal agencies that use AI-based systems to make or provide decision support for consequential decisions about individuals should take extra care to ensure the efficacy and fairness of those systems, based on evidence-based verification and validation.\\n\\nRecommendation 17: Federal agencies that make grants to state and local governments in support of the use of AI-based systems to make consequential decisions about individuals should review the terms of grants to ensure that AI-based products or services purchased with Federal grant funds produce results in a sufficiently transparent fashion and are supported by evidence of efficacy and fairness.\\n\\nRecommendation 18: Schools and universities should include ethics, and related topics in security, privacy, and safety, as an integral part of curricula on AI, machine learning, computer science, and data science.\\n\\nRecommendation 19: AI professionals, safety professionals, and their professional societies should work together to continue progress toward a mature field of AI safety engineering.\\n\\n74 See, e.g., Mary Shaw, Prospects for an Engineering Discipline of Software, IEEE Software 7(6), November 1990.\\n\\n34\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nGlobal Considerations and Security\\n\\nIn addition to the long-term challenges of AI and the specific issues relating to fairness and safety, AI poses consequential policy questions in international relations, cybersecurity, and defense.\\n\\nInternational Cooperation\\n\\nAI has been a topic of interest in recent international discussions as countries, multilateral institutions, and other stakeholders have begun to assess the benefits and challenges of AI. Dialogue and cooperation between these entities could help advance AI R&D and harness AI for good, while also addressing pertinent challenges. In particular, several breakthroughs in AI are the direct or indirect result of collaborative research involving people, resources, and institutions in multiple countries. As with other digital policies, countries will need to work together to identify opportunities for cooperation and develop international frameworks that will help promote AI R&D and address any challenges. The United States, a leader in AI R&D, can continue to play a key role in global research coordination through government- to-government dialogues and partnerships.\\n\\nInternational engagement is necessary to fully explore the applications of AI in health care, automation in manufacturing, and information and communication technologies (ICTs). AI applications also have the potential to address global issues such as disaster preparedness and response, climate change, wildlife trafficking, the digital divide, jobs, and smart cities. The State Department foresees privacy concerns, safety of autonomous vehicles, and AI’s impact on long-term employment trends as AI-related policy areas to watch in the international context.\\n\\nIn support of U.S. foreign policy priorities in this space—including ensuring U.S. international leadership and economic competitiveness—the U.S. Government has engaged on AI R&D and policy issues in bilateral discussions with other countries, including Japan, the Republic of Korea, Germany, Poland, the United Kingdom, and Italy, as well as in multilateral fora. International AI policy issues and the economic impacts of AI have also been raised in the UN, the G-7, the Organization for Economic Cooperation and Development (OECD), and the Asia-Pacific Economic Cooperation (APEC). The U.S. Government expects AI to be a topic of increasing interest in international engagements.\\n\\nThe United States has been committed to working with industry and relevant standards organizations, in order to facilitate the development of international standards in a manner that is industry-led; voluntary; consensus-driven; and based on principles of transparency, openness, and market needs. The U.S. approach is formalized in law (NTTAA, PL 104-113) and policy (OMB Circular A-119) and reiterated in the United States Standards Strategy.75\\n\\nRecommendation 20: The U.S. Government should develop a government-wide strategy on international engagement related to AI, and develop a list of AI topical areas that need international engagement and monitoring.\\n\\nRecommendation 21: The U.S. Government should deepen its engagement with key international stakeholders, including foreign governments, international organizations, industry, academia, and others, to exchange information and facilitate collaboration on AI R&D.\\n\\n75 United States Standards Strategy Committee, \"United States standards strategy,\" New York: American National Standards Institute (2015), https://share.ansi.org/shared%20documents/Standards%20Activities/NSSC/USSS_Third_edition/ANSI_USSS_2015 .pdf.\\n\\n35\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nAI and Cybersecurity\\n\\nToday’s Narrow AI has important applications in cybersecurity, and is expected to play an increasing role for both defensive (reactive) measures and offensive (proactive) measures.\\n\\nCurrently, designing and operating secure systems requires a large investment of time and attention from experts. Automating this expert work, partially or entirely, may enable strong security across a much broader range of systems and applications at dramatically lower cost, and may increase the agility of cyber defenses. Using AI may help maintain the rapid response required to detect and react to the landscape of ever evolving cyber threats. There are many opportunities for AI and specifically machine learning systems to help cope with the sheer complexity of cyberspace and support effective human decision making in response to cyberattacks.\\n\\nFuture AI systems could perform predictive analytics to anticipate cyberattacks by generating dynamic threat models from available data sources that are voluminous, ever-changing, and often incomplete. These data include the topology and state of network nodes, links, equipment, architecture, protocols, and networks. AI may be the most effective approach to interpreting these data, proactively identifying vulnerabilities, and taking action to prevent or mitigate future attacks.\\n\\nResults to-date in DARPA’s Cyber Grand Challenge (CGC) competition demonstrate the potential of this approach.76 The CGC was designed to accelerate the development of advanced, autonomous systems that can detect, evaluate, and patch software vulnerabilities before adversaries have a chance to exploit them. The CGC Final Event was held on August 4, 2016. To fuel follow-on research and parallel competition, all of the code produced by the automated systems during the CGC Final Event has been released as open source to allow others to reverse engineer it and learn from it.\\n\\nAI systems also have their own cybersecurity needs. AI-driven applications should implement sound cybersecurity controls to ensure integrity of data and functionality, protect privacy and confidentiality, and maintain availability. The recent Federal Cybersecurity R&D Strategic Plan77 highlighted the need for “sustainably secure systems development and operation.” Advances in cybersecurity will be critical in making AI solutions secure and resilient against malicious cyber activities, particularly as the volume and type of tasks conducted by governments and private sector businesses using Narrow AI increases.\\n\\nFinally, AI could support planning, coordinating, integrating, synchronizing, and directing activities to operate and defend U.S. government networks and systems effectively, provide assistance in support of secure operation of private-sector networks and systems, and enable action in accordance with all applicable laws, regulations and treaties.\\n\\n76 https://www.cybergrandchallenge.com\\n\\n77 “Federal Cybersecurity Research and Development Strategic Plan,” Executive Office of the President, February 2016, https://www.whitehouse.gov/sites/whitehouse.gov/files/documents/2016_Federal_Cybersecurity_Research_and_De velopment_Stratgeic_Plan.pdf.\\n\\n36\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nRecommendation 22: Agencies’ plans and strategies should account for the influence of AI on cybersecurity, and of cybersecurity on AI. Agencies involved in AI issues should engage their U.S. Government and private-sector cybersecurity colleagues for input on how to ensure that AI systems and ecosystems are secure and resilient to intelligent adversaries. Agencies involved in cybersecurity issues should engage their U.S. Government and private sector AI colleagues for innovative ways to apply AI for effective and efficient cybersecurity.\\n\\nAI in Weapon Systems The United States has incorporated autonomy into certain weapon systems for decades.78 These technological improvements may allow for greater precision in the use of these weapon systems and safer, more humane military operations. Precision-guided munitions allow an operation to be completed with fewer weapons expended and with less collateral damage, and remotely-piloted vehicles can lessen the risk to military personnel by placing greater distance between them and danger. Nonetheless, moving away from direct human control of weapon systems involves some risks and can raise legal and ethical questions. The key to further incorporating autonomous and semi-autonomous weapon systems into U.S. defense planning and force structure is to continue ensuring that all our weapon systems, including autonomous weapon systems, are being used in a manner consistent with international humanitarian law. In addition, the U.S. Government should continue taking appropriate steps to control proliferation, and working with partners and Allies to develop standards related to the development and use of such weapon systems.\\n\\nOver the past several years, in particular, issues concerning the development of so-called “Lethal Autonomous Weapon Systems” (LAWS) have been raised by technical experts, ethicists, and others in the international community.79 The United States has actively participated in the ongoing international discussion on LAWS in the context of the Convention on Certain Conventional Weapons (CCW),80 and anticipates continued robust international discussion of these potential weapon systems going forward.\\n\\nState Parties to the CCW are discussing technical, legal, military, ethical, and other issues involved with emerging technologies, although it is clear that there is no common understanding of LAWS. Some States have conflated LAWS with remotely piloted aircraft (military “drones”), a position which the United States opposes, as remotely-piloted craft are, by definition, directly controlled by humans just as manned aircraft are. Other States have focused on artificial intelligence, robot armies, or whether “meaningful human control” – an undefined term – is exercised over life-and-death decisions. The U.S. priority has been to reiterate that all weapon systems, autonomous or otherwise, must adhere to international\\n\\n78 See, e.g., Jeffrey L. Caton, “Autonomous Weapons Systems: A Brief Survey of Developmental, Operational, Legal, and Ethical Issues,” Strategic Studies Institute, U.S. Army War College, December 2015, http://www.strategicstudiesinstitute.army.mil/pdffiles/PUB1309.pdf.\\n\\n79 See, e.g., DeepMind comment, Human Rights Watch comment to the OSTP Request for Information on Artificial Intelligence.\\n\\n80 The Convention on Prohibitions on the Use of Certain Conventional Weapons Which May Be Deemed to Be Excessively Injurious or to Have Indiscriminate Effects (CCW) regulates certain weapons under its five Protocols. The States Parties to the CCW began informal discussions related to LAWS in 2014.\\n\\n37\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nhumanitarian law, including the principles of distinction81 and proportionality.82 For this reason, the United States has consistently noted the importance of the weapons review process in the development and adoption of new weapon systems. The CCW will decide on whether and how to conduct future meetings on LAWS and associated issues during its Review Conference in December 2016.\\n\\nThe U.S. government is also conducting a comprehensive review of the implications of autonomy in defense systems. In November 2012, the Department of Defense (DoD) issued DoD Directive 3000.09, “Autonomy in Weapon Systems,” which outlines requirements for the development and fielding of autonomous and semi-autonomous weapons. Weapon systems capable of autonomously selecting and engaging targets with lethal force require senior-level DoD reviews and approval before those weapon systems enter formal development and again before fielding. The DoD Directive neither prohibits nor encourages such development, but requires it to proceed carefully and only after review and approval by senior defense officials. Among other things, the DoD Directive requires that autonomous and semi- autonomous weapon systems are rigorously tested and that personnel are trained appropriately in their use to advance international norms pertaining to armed conflict.\\n\\nAI has the potential to provide significant benefits across a range of defense-related activities. Non-lethal activities such as logistics, maintenance, base operations, veterans’ healthcare, lifesaving battlefield medical assistance and casualty evacuation, personnel management, navigation, communication, cyber- defense, and intelligence analysis can benefit from AI, making American forces safer and more effective. AI may also play an important role in new systems for protecting people and high-value fixed assets and deterring attacks through non-lethal means. Ultimately, these applications may turn out to be the most important for DoD.\\n\\nGiven advances in military technology and artificial intelligence more broadly, scientists, strategists, and military experts all agree that the future of LAWS is difficult to predict and the pace of change is rapid. Many new capabilities may soon be possible, and quickly able to be developed and operationalized. The Administration is engaged in active, ongoing interagency discussions to work toward a government-wide policy on autonomous weapons consistent with shared human values, national security interests, and international and domestic obligations.\\n\\nRecommendation 23: The U.S. Government should complete the development of a single, government-wide policy, consistent with international humanitarian law, on autonomous and semi-autonomous weapons.\\n\\n81 The principle of distinction requires parties to a conflict to distinguish between the civilian population and combatants and between civilian objects and military objectives, and to direct their operations only against military objectives.\\n\\n82 The principle of proportionality prohibits attacks that may be expected to cause incidental loss of civilian life, injury to civilians, damage to civilian objects, or a combination thereof, which would be excessive in relation to the concrete and direct military advantage anticipated.\\n\\n38\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nConclusion\\n\\nAI can be a major driver of economic growth and social progress, if industry, civil society, government, and the public work together to support development of the technology, with thoughtful attention to its potential and to managing its risks.\\n\\nGovernment has several roles to play. It should convene conversations about important issues and help to set the agenda for public debate. It should monitor the safety and fairness of applications as they develop, and adapt regulatory frameworks to encourage innovation while protecting the public. It should support basic research and the application of AI to public goods, as well as the development of a skilled, diverse workforce. And government should use AI itself, to serve the public faster, more effectively, and at lower cost.\\n\\nMany areas of public policy, from education and the economic safety net, to defense, environmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. Government must continue to build its capacity to understand and adapt to these changes.\\n\\nAs the technology of AI continues to develop, practitioners must ensure that AI-enabled systems are governable; that they are open, transparent, and understandable; that they can work effectively with people; and that their operation will remain consistent with human values and aspirations. Researchers and practitioners have increased their attention to these challenges, and should continue to focus on them.\\n\\nDeveloping and studying machine intelligence can help us better understand and appreciate our human intelligence. Used thoughtfully, AI can augment our intelligence, helping us chart a better and wiser path forward.\\n\\n39\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nRecommendations in this Report\\n\\nThis section collects all of the recommendations in this report, for ease of reference.\\n\\nRecommendation 1: Private and public institutions are encouraged to examine whether and how they can responsibly leverage AI and machine learning in ways that will benefit society. Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways.\\n\\nRecommendation 2: Federal agencies should prioritize open training data and open data standards in AI. The government should emphasize the release of datasets that enable the use of AI to address social challenges. Potential steps may include developing an “Open Data for AI” initiative with the objective of releasing a significant number of government data sets to accelerate AI research and galvanize the use of open data standards and best practices across government, academia, and the private sector.\\n\\nRecommendation 3: The Federal Government should explore ways to improve the capacity of key agencies to apply AI to their missions. For example, Federal agencies should explore the potential to create DARPA-like organizations to support high-risk, high-reward AI research and its application, much as the Department of Education has done through its proposal to create an “ARPA-ED,” to support R&D to determine whether AI and other technologies could significantly improve student learning outcomes.\\n\\nRecommendation 4: The NSTC MLAI subcommittee should develop a community of practice for AI practitioners across government. Agencies should work together to develop and share standards and best practices around the use of AI in government operations. Agencies should ensure that Federal employee training programs include relevant AI opportunities.\\n\\nRecommendation 5: Agencies should draw on appropriate technical expertise at the senior level when setting regulatory policy for AI-enabled products. Effective regulation of AI-enabled products requires collaboration between agency leadership, staff knowledgeable about the existing regulatory framework and regulatory practices generally, and technical experts with knowledge of AI. Agency leadership should take steps to recruit the necessary technical talent, or identify it in existing agency staff, and should ensure that there are sufficient technical “seats at the table” in regulatory policy discussions.\\n\\nRecommendation 6: Agencies should use the full range of personnel assignment and exchange models (e.g. hiring authorities) to foster a Federal workforce with more diverse perspectives on the current state of technology.\\n\\nRecommendation 7: The Department of Transportation should work with industry and researchers on ways to increase sharing of data for safety, research, and other purposes. The future roles of AI in surface and air transportation are undeniable. Accordingly, Federal actors should focus in the near-term on developing increasingly rich sets of data, consistent with consumer privacy, that can better inform policy-making as these technologies mature.\\n\\n40\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nRecommendation 8: The U.S. Government should invest in developing and implementing an advanced and automated air traffic management system that is highly scalable, and can fully accommodate autonomous and piloted aircraft alike.\\n\\nRecommendation 9: The Department of Transportation should continue to develop an evolving framework for regulation to enable the safe integration of fully automated vehicles and UAS, including novel vehicle designs, into the transportation system.\\n\\nRecommendation 10: The NSTC Subcommittee on Machine Learning and Artificial Intelligence should monitor developments in AI, and report regularly to senior Administration leadership about the status of AI, especially with regard to milestones. The Subcommittee should update the list of milestones as knowledge advances and the consensus of experts changes over time. The Subcommittee should consider reporting to the public on AI developments, when appropriate.\\n\\nRecommendation 11: The Government should monitor the state of AI in other countries, especially with respect to milestones.\\n\\nRecommendation 12: Industry should work with government to keep government updated on the general progress of AI in industry, including the likelihood of milestones being reached soon.\\n\\nRecommendation 13: The Federal government should prioritize basic and long-term AI research. The Nation as a whole would benefit from a steady increase in Federal and private-sector AI R&D, with a particular emphasis on basic research and long-term, high-risk research initiatives. Because basic and long-term research especially are areas where the private sector is not likely to invest, Federal investments will be important for R&D in these areas.\\n\\nRecommendation 14: The NSTC Subcommittees on MLAI and NITRD, in conjunction with the NSTC Committee on Science, Technology, Engineering, and Education (CoSTEM),, should initiate a study on the AI workforce pipeline in order to develop actions that ensure an appropriate increase in the size, quality, and diversity of the workforce, including AI researchers, specialists, and users.\\n\\nRecommendation 15: The Executive Office of the President should publish a follow-on report by the end of this year, to further investigate the effects of AI and automation on the U.S. job market, and outline recommended policy responses.\\n\\nRecommendation 16: Federal agencies that use AI-based systems to make or provide decision support for consequential decisions about individuals should take extra care to ensure the efficacy and fairness of those systems, based on evidence-based verification and validation.\\n\\nRecommendation 17: Federal agencies that make grants to state and local governments in support of the use of AI-based systems to make consequential decisions about individuals should review the terms of grants to ensure that AI-based products or services purchased with Federal grant funds produce results in a sufficiently transparent fashion and are supported by evidence of efficacy and fairness.\\n\\n41\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nRecommendation 18: Schools and universities should include ethics, and related topics in security, privacy, and safety, as an integral part of curricula on AI, machine learning, computer science, and data science.\\n\\nRecommendation 19: AI professionals, safety professionals, and their professional societies should work together to continue progress toward a mature field of AI safety engineering.\\n\\nRecommendation 20: The U.S. Government should develop a government-wide strategy on international engagement related to AI, and develop a list of AI topical areas that need international engagement and monitoring.\\n\\nRecommendation 21: The U.S. Government should deepen its engagement with key international stakeholders, including foreign governments, international organizations, industry, academia, and others, to exchange information and facilitate collaboration on AI R&D.\\n\\nRecommendation 22: Agencies’ plans and strategies should account for the influence of AI on cybersecurity, and of cybersecurity on AI. Agencies involved in AI issues should engage their U.S. Government and private-sector cybersecurity colleagues for input on how to ensure that AI systems and ecosystems are secure and resilient to intelligent adversaries. Agencies involved in cybersecurity issues should engage their U.S. Government and private sector AI colleagues for innovative ways to apply AI for effective and efficient cybersecurity.\\n\\nRecommendation 23: The U.S. Government should complete the development of a single, government- wide policy, consistent with international humanitarian law, on autonomous and semi-autonomous weapons.\\n\\n42\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nAcronyms\\n\\nAAAI\\n\\nAssociation for the Advancement of Artificial Intelligence\\n\\nAGI\\n\\nArtificial General Intelligence\\n\\nAI\\n\\nArtificial Intelligence\\n\\nAPEC\\n\\nAsia-Pacific Economic Cooperation\\n\\nBRAIN\\n\\nBrain Research through Advancing Innovative Neurotechnologies\\n\\nCALO\\n\\nCognitive Agent that Learns and Organizes\\n\\nCCC\\n\\nComputing Community Consortium\\n\\nCCW\\n\\nConvention on Certain Conventional Weapons\\n\\nCEA\\n\\nCouncil of Economic Advisers\\n\\nCEO\\n\\nChief Executive Officer\\n\\nCGC\\n\\nCyber Grand Challenge (run by DARPA)\\n\\nCoSTEM\\n\\nCommittee on Science Technology, Engineering, and Education (component of NSTC)\\n\\nCS\\n\\nComputer Science\\n\\nDARPA\\n\\nDefense Advanced Research Projects Agency\\n\\nDoD\\n\\nDepartment of Defense\\n\\nDOT\\n\\nDepartment of Transportation\\n\\nFAA\\n\\nFederal Aviation Administration\\n\\nFMVSS\\n\\nFederal Motor Vehicle Safety Standards\\n\\nIARPA\\n\\nIntelligence Advanced Research Projects Activity\\n\\nICTs\\n\\nInformation and Communication Technologies\\n\\nIPA\\n\\nIntergovernmental Personnel Act\\n\\nLAWS\\n\\nLethal Autonomous Weapon Systems\\n\\nMLAI\\n\\nMachine Learning and Artificial Intelligence (subcommittee of NSTC)\\n\\nNAS\\n\\nNational Airspace System\\n\\nNEC\\n\\nNational Economic Council\\n\\nNHTSA\\n\\nNational Highway Traffic Safety Administration\\n\\nNIH\\n\\nNational Institutes of Health\\n\\nNIPS\\n\\nNeural Information Processing Systems conference\\n\\nNITRD\\n\\nNetworking and Information Technology Research and Development (subcommittee of NSTC)\\n\\nNSF\\n\\nNational Science Foundation\\n\\nNSTC\\n\\nNational Science and Technology Council\\n\\n43\\n\\nOECD\\n\\nOMB\\n\\nONR\\n\\nOSTP\\n\\nR&D\\n\\nRFI\\n\\nSTEM\\n\\nUAS\\n\\nUTM\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nOrganization for Economic Cooperation and Development\\n\\nOffice of Management and Budget\\n\\nOffice of Naval Research\\n\\nOffice of Science and Technology Policy\\n\\nResearch and Development\\n\\nRequest For Information\\n\\nScience, Technology, Engineering, and Mathematics\\n\\nUnmanned Aerial System\\n\\nUAS Traffic Management\\n\\n44\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nReferences\\n\\n“AAAI Presidential Panel on Long-Term AI Futures: 2008-2009 Study,” The Association for the Advancement of Artificial Intelligence, http://www.aaai.org/Organization/presidential-panel.php.\\n\\n“Aerospace Forecast Report Fiscal Years 2016 to 2036,” The Federal Aviation Administration, March 24 016, https://www.faa.gov/data_research/aviation/aerospace_forecasts/media/Unmanned_Aircraft_Systems.pdf.\\n\\n“AI Safety Conference in Puerto Rico,” The Future of Life Institute, October 12, 2015, http://futureoflife.org/2015/10/12/ai-safety-conference-in-puerto-rico.\\n\\n“Big Data: Seizing Opportunities, Preserving Values,” Executive Office of the President, May 2014, https://www.whitehouse.gov/sites/default/files/docs/big_data_privacy_report_may_1_2014.pdf.\\n\\n“Data Science for Social Good,” University of Chicago, https://dssg.uchicago.edu/.\\n\\n“Federal Automated Vehicles Policy,” The U.S. Department of Transportation, September 21 2016, https://www.transportation.gov/AV.\\n\\n“Federal Cybersecurity Research and Development Strategic Plan,” Executive Office of the President, February 2016, https://www.whitehouse.gov/sites/whitehouse.gov/files/documents/2016_Federal_Cybersecurity_Research_and_De velopment_Stratgeic_Plan.pdf.\\n\\n“Secretary Foxx Unveils President Obama’s FY17 Budget Proposal of Nearly $4 Billion for Automated Vehicles and Announces DOT Initiatives to Accelerate Vehicle Safety Innovations,” U.S. Department of Transportation, January 14 2016, https://www.transportation.gov/briefing-room/secretary-foxx-unveils-president- obama%E2%80%99s-fy17-budget-proposal-nearly-4-billion.\\n\\n“Winning the Education Future: The Role of ARPA-ED,” The U.S. Department of Education, March 8 2011, https://www.whitehouse.gov/sites/default/files/microsites/ostp/arpa-ed-factsheet.pdf.\\n\\n“World Development Report 2016: Digital Dividends,” The World Bank Group, 2016, http://documents.worldbank.org/curated/en/896971468194972881/pdf/102725-PUB-Replacement-PUBLIC.pdf.\\n\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané, “Concrete Problems in AI Safety,” https://arxiv.org/abs/1606.06565.\\n\\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\\n\\nStuart Armstrong, Kaj Sotala, Seán S. ÓhÉigeartaigh, “The errors, insights and lessons of famous AI predictions – and what they mean for the future,” Journal of Experimental & Theoretical Artificial Intelligence, May 20, 2014.\\n\\nMichael Ball, Cynthia Barnhart, Martin Dresner, Mark Hansen, Kevin Neels, Amedeo Odoni, Everett Peterson, Lance Sherry, Antonio Trani, Bo Zou, “Total Delay Impact Study: A Comprehensive Assessment of the Costs and Impacts of Flight Delay in the United States,” The National Center of Excellence for Aviation Operations Research, November 2010, http://www.nextor.org/pubs/TDI_Report_Final_11_03_10.pdf.\\n\\nNicholas Bloom, Mark Schankerman, John Van Reene, “Identifying Technology Spillovers and Product Market Rivalry,” Econometrica, 81: 1347–1393. doi:10.3982/ECTA9466. Frank Chen, “AI, Deep Learning, and Machine Learning: A Primer,” Andreessen Horowitz, June 10, 2016, http://a16z.com/2016/06/10/ai-deep-learning-machines.\\n\\nJeffrey L. Caton, “Autonomous Weapons Systems: A Brief Survey of Developmental, Operational, Legal, and Ethical Issues,” Strategic Studies Institute, U.S. Army War College, December 2015, http://www.strategicstudiesinstitute.army.mil/pdffiles/PUB1309.pdf.\\n\\n45\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nJack Clark, “Artificial Intelligence Has a ‘Sea of Dudes’ Problem,” Bloomberg, June 21, 2016, https://www.bloomberg.com/news/articles/2016-06-23/artificial-intelligence-has-a-sea-of-dudes-problem.\\n\\nChristianne Corbett and Catherine Hill, “Solving the Equation: The Variables for Women’s Success in Engineering and Computing,” The American Association of University Women, March 2015, http://www.aauw.org/files/2015/03/Solving-the-Equation-report-nsa.pdf.\\n\\nAnupam Datta, Shayak Sen, and Yair Zick, “Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems,” Proceedings of 37th IEEE Symposium on Security and Privacy, 2016.\\n\\nPedro Domingos, The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World (New York, New York: Basic Books, 2015).\\n\\nEric Elster, “Surgical Critical Care Initiative: Bringing Precision Medicine to the Critically Ill,” presentation at AI for Social Good workshop, Washington, DC, June 7, 2016, http://cra.org/ccc/wp- content/uploads/sites/2/2016/06/Eric-Elster-AI-slides-min.pdf.\\n\\nHeinz Erzberger, “The Automated Airspace Concept,” prepared for the 4th USA/Europe Air Traffic Management R&D Seminar Dec. 3-7, 2001, Santa Fe, New Mexico, USA, http://www.aviationsystemsdivision.arc.nasa.gov/publications/tactical/erzberger_12_01.pdf.\\n\\nEd Felten and Terah Lyons, “Public Input and Next Steps on the Future of Artificial Intelligence,” Medium, September 6 2016, https://medium.com/@USCTO/public-input-and-next-steps-on-the-future-of-artificial- intelligence-458b82059fc3.\\n\\nJ.D. Fletcher, “Digital Tutoring in Information Systems Technology for Veterans: Data Report,” The Institute for Defense Analysis, September 2014.\\n\\nMatt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what-we-dont-know-about-mass-incarceration/394520/\\n\\nJason Furman, “Is This Time Different? The Opportunities and Challenges of Artificial Intelligence,” (presentation, AI Now: The Social and Economic Implications of Artificial Intelligence Technologies in the Near Term, New York, NY, July 7, 2016), Available at https://www.whitehouse.gov/sites/default/files/page/files/20160707_cea_ai_furman.pdf.\\n\\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy, “Explaining and Harnessing Adversarial Examples,” http://arxiv.org/pdf/1412.6572.pdf.\\n\\nGeorg Graetz and Guy Michaels, “Robots at Work,” CEPR Discussion Paper No. DP10477, March 2015, http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2575781.\\n\\nBronwyn H. Hall, Jacques Mairesse, and Pierre Mohnen, “Measuring the Returns to R&D,” Chapter prepared for the Handbook of the Economics of Innovation, B. H. Hall and N. Rosenberg (editors), December 10, 2009, https://eml.berkeley.edu/~bhhall/papers/HallMairesseMohnen09_rndsurvey_HEI.pdf.\\n\\nMoritz Hardt, “How big data is unfair,” Medium, September 26 2014, https://medium.com/@mrtz/how-big-data-is- unfair-9aa544d739de.\\n\\nNeal Jean, Marshall Burke, Michael Xie, W. Matthew Davis, David B. Lobell, and Stefano Ermon. \"Combining satellite imagery and machine learning to predict poverty.\" Science 353, no. 6301 (2016): 790-794.\\n\\nDerryl Jenkins and Bijan Vasigh, “The Economic Impact of Unmanned Aircraft Systems Integration in the United States,” The Association for Unmanned Vehicle Systems International, 2013, https://higherlogicdownload.s3.amazonaws.com/AUVSI/958c920a-7f9b-4ad2-9807- f9a4e95d1ef1/UploadedImages/New_Economic%20Report%202013%20Full.pdf.\\n\\nCharles I. Jones and John C. Williams, “Measuring the Social Returns to R&D,” The Quarterly Journal of Economics (1998) 113 (4): 1119-1135, doi: 10.1162/003355398555856.\\n\\n46\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nThomas Kalil, “A Broader Vision for Government Research,” Issues in Science and Technology, 2003.\\n\\nGarry Kasparov, “The Chess Master and the Computer,” New York Review of Books, February 11, 2010. http://www.nybooks.com/articles/2010/02/11/the-chess-master-and-the-computer.\\n\\nKatharine E. Henry, David N. Hager, Peter J. Pronovost, and Suchi Saria, \"A targeted real-time early warning score (TREWScore) for septic shock,\" Science Translational Medicine 7, no. 299 (2015): 299ra122-299ra122.\\n\\nAimee Leslie, Christine Hof, Diego Amorocho, Tanya Berger-Wolf, Jason Holmberg, Chuck Stewart, Stephen G. Dunbar, and Claire Jea,, “The Internet of Turtles,” April 12, 2016, https://www.researchgate.net/publication/301202821_The_Internet_of_Turtles.\\n\\nSteven Levy, “How Google is Remaking Itself as a Machine Learning First Company,” Backchannel, June 22, 2016, https://backchannel.com/how-google-is-remaking-itself-as-a-machine-learning-first-company-ada63defcb70.\\n\\nJohn Markoff, “No Sailors Needed: Robot Sailboats Scout the Oceans for Data,” The New York Times, September 4, 2016.\\n\\nWarren S. McCulloch and Walter H. Pitts, “A Logical Calculus of the Ideas Immanent in Nervous Activity,” Bulletin of Mathematical Biophysics, 5:115-133, 1943.\\n\\nVincent Müller and Nick Bostrom, “Future progress in artificial intelligence: A Survey of Expert Opinion,” Fundamental Issues of Artificial Intelligence, 2014.\\n\\nCarrie Mullins, “Retrospective Analysis of Technology Forecasting,” The Tauri Group, August 13, 2012.Andrew Nusca, “IBM’s CEO Thinks Every Digital Business Will Become a Cognitive Computing Business,” Fortune, June 1 2016.\\n\\nRobert W. Poole, Jr., “The Urgent Need to Reform the FAA’s Air Traffic Control System,” The Heritage Foundation, 2007, http://www.heritage.org/research/reports/2007/02/the-urgent-need-to-reform-the-faas-air-traffic- control-system.\\n\\nThe President’s Council of Advisors on Science and Technology, letter to the President, September 2014, https://www.whitehouse.gov/sites/default/files/microsites/ostp/PCAST/pcast_workforce_edit_report_sept_2014.pdf.\\n\\nThe President’s Council of Advisors on Science and Technology, “Report to the President: Big Data and Privacy: A Technological Perspective,” Executive Office of the President, May 2014, https://www.whitehouse.gov/sites/default/files/microsites/ostp/PCAST/pcast_big_data_and_privacy_- _may_2014.pdf.\\n\\nMike Purdy and Paul Daugherty, “Why Artificial Intelligence is the Future of Growth,” Accenture, 2016, https://www.accenture.com/us-en/_acnmedia/PDF-33/Accenture-Why-AI-is-the-Future-of-Growth.pdf.\\n\\nSara Reardon, “Text-mining offers clues to success: US intelligence programme analyses language in patents and papers to identify next big technologies,” Nature no. 509, 410 (May 22 2014).\\n\\nDavid Robinson and Logan Koepke, “Stuck in a Pattern: Early evidence on ‘predictive policing’ and civil rights,” Upturn, August 2016, http://www.stuckinapattern.org.\\n\\nStuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach (3rd Edition) (Essex, England: Pearson, 2009).\\n\\nMary Shaw, Prospects for an Engineering Discipline of Software, IEEE Software 7(6), November 1990.\\n\\nStephen F. Smith, “Smart Infrastructure for Urban Mobility,” presentation at AI for Social Good workshop, Washington, DC, June 7, 2016, http://cra.org/ccc/wp-content/uploads/sites/2/2016/06/Stephen-Smith-AI-slides.pdf.\\n\\nPeter Stone, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, Kevin Leyton-Brown, David Parkes, William Press, AnnaLee Saxenian,\\n\\n47\\n\\nPREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nJulie Shah, Milind Tambe, and Astro Teller, \"Artificial Intelligence and Life in 2030,\" One Hundred Year Study on Artificial Intelligence: Report of the 2015-2016 Study Panel, Stanford University, Stanford, CA, September 2016, http://ai100.stanford.edu/2016-report.\\n\\nCharles Twardy, Robin Hanson, Kathryn Laskey, Tod S. Levitt, Brandon Goldfedder, Adam Siegel, Bruce D’Ambrosio, and Daniel Maxwell, “SciCast: Collective Forecasting of Innovation,” Collective Intelligence, 2014.\\n\\nUnited States Standards Strategy Committee, \"United States standards strategy,\" New York: American National Standards Institute (2015), https://share.ansi.org/shared%20documents/Standards%20Activities/NSSC/USSS_Third_edition/ANSI_USSS_2015 .pdf.\\n\\nDayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew H. Beck, “Deep Learning for Identifying Metastatic Breast Cancer,” June 18, 2016, https://arxiv.org/pdf/1606.05718v1.pdf.\\n\\n48')],\n",
              " [Document(metadata={'source': '/content/artificial intelligence.pdf'}, page_content='Artificial Intelligence\\n\\nAbout the Tutorial\\n\\nThis tutorial provides introductory knowledge on Artificial Intelligence. It would come to a great help if you are about to select Artificial Intelligence as a course subject. You can briefly know about the areas of AI in which research is prospering.\\n\\nAudience\\n\\nThis tutorial is prepared for the students at beginner level who aspire to learn Artificial Intelligence.\\n\\nPrerequisites\\n\\nThe basic knowledge of Computer Science is mandatory. The knowledge of Mathematics, Languages, Science, Mechanical or Electrical engineering is a plus.\\n\\nDisclaimer & Copyright\\n\\n\\uf0e3 Copyright 2015 by Tutorials Point (I) Pvt. Ltd.\\n\\nAll the content and graphics published in this e-book are the property of Tutorials Point (I) Pvt. Ltd. The user of this e-book is prohibited to reuse, retain, copy, distribute or republish any contents or a part of contents of this e-book in any manner without written consent of the publisher.\\n\\nWe strive to update the contents of our website and tutorials as timely and as precisely as possible, however, the contents may contain inaccuracies or errors. Tutorials Point (I) Pvt. Ltd. provides no guarantee regarding the accuracy, timeliness or completeness of our website or its contents including this tutorial. If you discover any errors on our website or in this tutorial, please notify us at contact@tutorialspoint.com.\\n\\ni\\n\\nArtificial Intelligence\\n\\nTable of Contents\\n\\nAbout the Tutorial ......................................................................................................................................... i\\n\\nAudience ....................................................................................................................................................... i\\n\\nPrerequisites ................................................................................................................................................. i\\n\\nDisclaimer & Copyright .................................................................................................................................. i\\n\\nTable of Contents ......................................................................................................................................... ii\\n\\n1. OVERVIEW OF AI ...................................................................................................................... 1\\n\\nWhat is Artificial Intelligence? ...................................................................................................................... 1\\n\\nPhilosophy of AI ........................................................................................................................................... 1\\n\\nGoals of AI .................................................................................................................................................... 1\\n\\nWhat Contributes to AI? ............................................................................................................................... 2\\n\\nProgramming Without and With AI .............................................................................................................. 2\\n\\nWhat is AI Technique? .................................................................................................................................. 3\\n\\nApplications of AI ......................................................................................................................................... 3\\n\\nHistory of AI ................................................................................................................................................. 4\\n\\n2.\\n\\nINTELLIGENT SYSTEMS ............................................................................................................. 6\\n\\nWhat is Intelligence? .................................................................................................................................... 6\\n\\nTypes of Intelligence..................................................................................................................................... 6\\n\\nWhat is Intelligence Composed of? .............................................................................................................. 7\\n\\nDifference between Human and Machine Intelligence ................................................................................. 9\\n\\n3.\\n\\nRESEARCH AREAS OF AI .......................................................................................................... 10\\n\\nReal Life Applications of Research Areas .................................................................................................... 11\\n\\nTask Classification of AI .............................................................................................................................. 12\\n\\nii\\n\\n4.\\n\\n5.\\n\\n6.\\n\\nArtificial Intelligence\\n\\nAGENTS AND ENVIRONMENTS ............................................................................................... 14\\n\\nWhat are Agent and Environment? ............................................................................................................ 14\\n\\nAgents Terminology ................................................................................................................................... 14\\n\\nRationality .................................................................................................................................................. 15\\n\\nWhat is Ideal Rational Agent? .................................................................................................................... 15\\n\\nThe Structure of Intelligent Agents ............................................................................................................. 15\\n\\nThe Nature of Environments ...................................................................................................................... 18\\n\\nProperties of Environment ......................................................................................................................... 19\\n\\nPOPULAR SEARCH ALGORITHMS ............................................................................................ 20\\n\\nSingle Agent Pathfinding Problems............................................................................................................. 20\\n\\nSearch Terminology .................................................................................................................................... 20\\n\\nBrute-Force Search Strategies .................................................................................................................... 20\\n\\nInformed (Heuristic) Search Strategies ....................................................................................................... 23\\n\\nLocal Search Algorithms ............................................................................................................................. 24\\n\\nFUZZY LOGIC SYSTEMS ........................................................................................................... 27\\n\\nWhat is Fuzzy Logic? ................................................................................................................................... 27\\n\\nWhy Fuzzy Logic? ........................................................................................................................................ 27\\n\\nFuzzy Logic Systems Architecture ............................................................................................................... 28\\n\\nExample of a Fuzzy Logic System ................................................................................................................ 29\\n\\nApplication Areas of Fuzzy Logic ................................................................................................................. 32\\n\\nAdvantages of FLSs ..................................................................................................................................... 33\\n\\nDisadvantages of FLSs ................................................................................................................................ 33\\n\\niii\\n\\n7.\\n\\n8.\\n\\n9.\\n\\nArtificial Intelligence\\n\\nNATURAL LANGUAGE PROCESSING ........................................................................................ 34\\n\\nComponents of NLP .................................................................................................................................... 34\\n\\nDifficulties in NLU ....................................................................................................................................... 34\\n\\nNLP Terminology ........................................................................................................................................ 35\\n\\nSteps in NLP................................................................................................................................................ 35\\n\\nImplementation Aspects of Syntactic Analysis............................................................................................ 36\\n\\nEXPERT SYSTEMS.................................................................................................................... 40\\n\\nWhat are Expert Systems? .......................................................................................................................... 40\\n\\nCapabilities of Expert Systems .................................................................................................................... 40\\n\\nComponents of Expert Systems .................................................................................................................. 41\\n\\nKnowledge Base ......................................................................................................................................... 41\\n\\nInference Engine ......................................................................................................................................... 42\\n\\nUser Interface ............................................................................................................................................. 43\\n\\nExpert Systems Limitations......................................................................................................................... 44\\n\\nApplications of Expert System .................................................................................................................... 44\\n\\nExpert System Technology .......................................................................................................................... 45\\n\\nDevelopment of Expert Systems: General Steps ......................................................................................... 45\\n\\nBenefits of Expert Systems ......................................................................................................................... 46\\n\\nROBOTICS .............................................................................................................................. 47\\n\\nWhat are Robots? ....................................................................................................................................... 47\\n\\nWhat is Robotics? ....................................................................................................................................... 47\\n\\nDifference in Robot System and Other AI Program ..................................................................................... 47\\n\\nRobot Locomotion ...................................................................................................................................... 48\\n\\nComponents of a Robot .............................................................................................................................. 50\\n\\niv\\n\\nArtificial Intelligence\\n\\nComputer Vision ......................................................................................................................................... 50\\n\\nTasks of Computer Vision ........................................................................................................................... 50\\n\\nApplication Domains of Computer Vision ................................................................................................... 51\\n\\nApplications of Robotics ............................................................................................................................. 51\\n\\n10. NEURAL NETWORKS ............................................................................................................... 53\\n\\nWhat are Artificial Neural Networks (ANNs)? ............................................................................................. 53\\n\\nBasic Structure of ANNs .............................................................................................................................. 53\\n\\nTypes of Artificial Neural Networks ............................................................................................................ 54\\n\\nWorking of ANNs ........................................................................................................................................ 55\\n\\nMachine Learning in ANNs ......................................................................................................................... 55\\n\\nBayesian Networks (BN) ............................................................................................................................. 56\\n\\nApplications of Neural Networks ................................................................................................................ 59\\n\\n11. AI ISSUES ................................................................................................................................ 61\\n\\n12. AI TERMINOLOGY ................................................................................................................... 62\\n\\nv\\n\\n1. Overview of AI\\n\\nArtificial Intelligence\\n\\nSince the invention of computers or machines, their capability to perform various tasks went on growing exponentially. Humans have developed the power of computer systems in terms of their diverse working domains, their increasing speed, and reducing size with respect to time.\\n\\nA branch of Computer Science named Artificial Intelligence pursues creating the computers or machines as intelligent as human beings.\\n\\nWhat is Artificial Intelligence?\\n\\nAccording to the father of Artificial Intelligence John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.\\n\\nArtificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.\\n\\nAI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.\\n\\nPhilosophy of AI\\n\\nWhile exploiting the power of the computer systems, the curiosity of human, lead him to wonder, “Can a machine think and behave like humans do?”\\n\\nThus, the development of AI started with the intention of creating similar intelligence in machines that we find and regard high in humans.\\n\\nGoals of AI\\n\\nTo Create Expert Systems: The systems which exhibit intelligent behavior, learn,\\n\\ndemonstrate, explain, and advice its users.\\n\\nTo Implement Human Intelligence in Machines: Creating systems that\\n\\nunderstand, think, learn, and behave like humans.\\n\\n1\\n\\nArtificial Intelligence\\n\\nWhat Contributes to AI?\\n\\nArtificial intelligence is a science and technology based on disciplines such as Computer Science, Biology, Psychology, Linguistics, Mathematics, and Engineering. A major thrust of AI is in the development of computer functions associated with human intelligence, such as reasoning, learning, and problem solving.\\n\\nOut of the following areas, one or multiple areas can contribute to build an intelligent system.\\n\\nProgramming Without and With AI\\n\\nThe programming without and with AI is different in following ways:\\n\\nProgramming Without AI\\n\\nProgramming With AI\\n\\nA computer program without AI can answer the specific questions it is meant to solve.\\n\\nA computer program with AI can answer the generic questions it is meant to solve.\\n\\nModification in the program leads to change in its structure.\\n\\nAI programs can absorb new modifications by putting highly of information together. Hence you can modify even a minute piece of information of program without affecting its structure.\\n\\nindependent\\n\\npieces\\n\\nModification is not quick and easy. It may lead to affecting the program adversely.\\n\\nQuick and Easy program modification.\\n\\n2\\n\\nArtificial Intelligence\\n\\nWhat is AI Technique?\\n\\nIn the real world, the knowledge has some unwelcomed properties:\\n\\n\\n\\nIts volume is huge, next to unimaginable.\\n\\n\\n\\nIt is not well-organized or well-formatted.\\n\\n\\n\\nIt keeps changing constantly.\\n\\nAI Technique is a manner to organize and use the knowledge efficiently in such a way that:\\n\\n\\n\\nIt should be perceivable by the people who provide it.\\n\\n\\n\\nIt should be easily modifiable to correct errors.\\n\\n\\n\\nIt should be useful in many situations though it is incomplete or inaccurate.\\n\\nAI techniques elevate the speed of execution of the complex program it is equipped with.\\n\\nApplications of AI\\n\\nAI has been dominant in various fields such as:\\n\\nGaming\\n\\nAI plays crucial role in strategic games such as chess, poker, tic-tac-toe, etc., where\\n\\nmachine can think of large number of possible positions based on heuristic knowledge.\\n\\nNatural Language Processing\\n\\nIt is possible to interact with the computer that understands natural language spoken\\n\\nby humans.\\n\\nExpert Systems\\n\\nThere are some applications which integrate machine, software, and special\\n\\ninformation to impart reasoning and advising. They provide explanation and advice to\\n\\nthe users.\\n\\nVision Systems\\n\\nThese systems understand, interpret, and comprehend visual input on the computer.\\n\\nFor example,\\n\\no A spying aeroplane takes photographs which are used to figure out spatial\\n\\ninformation or map of the areas.\\n\\no Doctors use clinical expert system to diagnose the patient.\\n\\no Police use computer software that can recognize the face of criminal with the\\n\\nstored portrait made by forensic artist.\\n\\n3\\n\\nArtificial Intelligence\\n\\nSpeech Recognition\\n\\nSome intelligent systems are capable of hearing and comprehending the language in terms of sentences and their meanings while a human talks to it. It can handle different accents, slang words, noise in the background, change in human’s noise due to cold, etc.\\n\\nHandwriting Recognition\\n\\nThe handwriting recognition software reads the text written on paper by a pen or on screen by a stylus. It can recognize the shapes of the letters and convert it into editable text.\\n\\nIntelligent Robots\\n\\nRobots are able to perform the tasks given by a human. They have sensors to detect physical data from the real world such as light, heat, temperature, movement, sound, bump, and pressure. They have efficient processors, multiple sensors and huge memory, to exhibit intelligence. In addition, they are capable of learning from their mistakes and they can adapt to the new environment.\\n\\nHistory of AI\\n\\nHere is the history of AI during 20th century:\\n\\nYear\\n\\nMilestone / Innovation\\n\\n1923\\n\\nKarel Čapek’s play named “Rossum\\'s Universal Robots” (RUR) opens in London, first use of the word \"robot\" in English.\\n\\n1943\\n\\nFoundations for neural networks laid.\\n\\n1945\\n\\nIsaac Asimov, a Columbia University alumni, coined the term Robotics.\\n\\n1950\\n\\nAlan Turing introduced Turing Test for evaluation of intelligence and published Computing Machinery and Intelligence. Claude Shannon published Detailed Analysis of Chess Playing as a search.\\n\\n1956\\n\\nJohn McCarthy coined the term Artificial Intelligence. Demonstration of the first running AI program at Carnegie Mellon University.\\n\\n1958\\n\\nJohn McCarthy invents LISP programming language for AI.\\n\\n1964\\n\\nDanny Bobrow\\'s dissertation at MIT showed that computers can understand natural language well enough to solve algebra word problems correctly.\\n\\n1965\\n\\nJoseph Weizenbaum at MIT built ELIZA, an interactive problem that carries on a dialogue in English.\\n\\n1969\\n\\nScientists at Stanford Research Institute Developed Shakey, a robot, equipped with locomotion, perception, and problem solving.\\n\\n4\\n\\n1973\\n\\n1979\\n\\n1985\\n\\n1990\\n\\n1997\\n\\n2000\\n\\nArtificial Intelligence\\n\\nThe Assembly Robotics group at Edinburgh University built Freddy, the Famous Scottish Robot, capable of using vision to locate and assemble models.\\n\\nThe first computer-controlled autonomous vehicle, Stanford Cart, was built.\\n\\nHarold Cohen created and demonstrated the drawing program, Aaron.\\n\\nMajor advances in all areas of AI:\\n\\nSignificant demonstrations in machine learning\\n\\nCase-based reasoning\\n\\nMulti-agent planning\\n\\nScheduling\\n\\nData mining, Web Crawler\\n\\nnatural language understanding and translation\\n\\nVision, Virtual Reality\\n\\nGames\\n\\nThe Deep Blue Chess Program beats the then world chess champion, Garry Kasparov.\\n\\nInteractive robot pets become commercially available. MIT displays Kismet, a robot with a face that expresses emotions. The robot Nomad explores remote regions of Antarctica and locates meteorites.\\n\\n5\\n\\n2. IntelligenT Systems\\n\\nArtificial Intelligence\\n\\nWhile studying artificially intelligence, you need to know what intelligence is. This chapter covers Idea of intelligence, types, and components of intelligence.\\n\\nWhat is Intelligence?\\n\\nThe ability of a system to calculate, reason, perceive relationships and analogies, learn from experience, store and retrieve information from memory, solve problems, comprehend complex ideas, use natural language fluently, classify, generalize, and adapt new situations.\\n\\nTypes of Intelligence\\n\\nAs described by Howard Gardner, an American developmental psychologist, the Intelligence comes in multifold:\\n\\nIntelligence\\n\\nDescription\\n\\nExample\\n\\nLinguistic intelligence\\n\\nThe ability to speak, recognize, and use mechanisms of phonology (speech sounds), syntax (grammar), and semantics (meaning).\\n\\nNarrators, Orators\\n\\nMusical intelligence\\n\\nThe ability to create, communicate with, and understand meanings made of sound, understanding of pitch, rhythm.\\n\\nMusicians, Singers, Composers\\n\\nLogical- mathematical intelligence\\n\\nThe ability of use and understand relationships in the absence of action or objects. Understanding complex and abstract ideas.\\n\\nMathematicians, Scientists\\n\\nSpatial intelligence\\n\\nThe ability to perceive visual or spatial information, change it, and re-create visual images without reference to the objects, construct 3D images, and to move and rotate them.\\n\\nMap Astronauts, Physicists\\n\\nreaders,\\n\\nBodily-Kinesthetic intelligence\\n\\nThe ability to use complete or part of the body to solve problems or fashion products, control over fine and coarse motor skills, and manipulate the objects.\\n\\nPlayers, Dancers\\n\\nIntra-personal intelligence\\n\\nThe ability to distinguish among one’s own feelings, intentions, and motivations.\\n\\nGautam Buddha\\n\\n6\\n\\nArtificial Intelligence\\n\\nInterpersonal intelligence\\n\\nThe ability to recognize and make distinctions among other people’s feelings, beliefs, and intentions.\\n\\nMass Communicators, Interviewers\\n\\nYou can say a machine or a system is artificially intelligent when it is equipped with at least one and at most all intelligences in it.\\n\\nWhat is Intelligence Composed of?\\n\\nThe intelligence is intangible. It is composed of:\\n\\n1. Reasoning 2. Learning 3. Problem Solving 4. Perception 5. Linguistic Intelligence\\n\\nLet us go through all the components briefly:\\n\\n1. Reasoning: It is the set of processes that enables us to provide basis for judgement,\\n\\nmaking decisions, and prediction. There are broadly two types:\\n\\nInductive Reasoning\\n\\nDeductive Reasoning\\n\\nIt conducts specific observations to makes broad general statements.\\n\\nIt starts with a general statement and examines the possibilities to reach a specific, logical conclusion.\\n\\nEven if all of the premises are true in a statement, inductive reasoning allows for the conclusion to be false.\\n\\nIf something is true of a class of things in general, it is also true for all members of that class.\\n\\n7\\n\\nArtificial Intelligence\\n\\nExample: “Nita is a teacher. All teachers are studious. Therefore, Nita is studious.”\\n\\nExample: \"All women of age above 60 years are grandmothers. Shalini is 65 years. Therefore, Shalini is a grandmother.\"\\n\\n2. Learning: It is the activity of gaining knowledge or skill by studying, practising, being taught, or experiencing something. Learning enhances the awareness of the subjects of the study.\\n\\nThe ability of learning is possessed by humans, some animals, and AI-enabled\\n\\nsystems. Learning is categorized as:\\n\\no Auditory Learning: It is learning by listening and hearing. For example, students\\n\\nlistening to recorded audio lectures.\\n\\no Episodic Learning: To learn by remembering sequences of events that one has\\n\\nwitnessed or experienced. This is linear and orderly.\\n\\no Motor Learning: It is learning by precise movement of muscles. For example,\\n\\npicking objects, Writing, etc.\\n\\no Observational Learning: To learn by watching and imitating others. For example,\\n\\nchild tries to learn by mimicking her parent.\\n\\no Perceptual Learning: It is learning to recognize stimuli that one has seen before.\\n\\nFor example, identifying and classifying objects and situations.\\n\\no Relational Learning: It involves learning to differentiate among various stimuli on the basis of relational properties, rather than absolute properties. For Example, Adding ‘little less’ salt at the time of cooking potatoes that came up salty last time, when cooked with adding say a tablespoon of salt.\\n\\no Spatial learning: It is learning through visual stimuli such as images, colors, maps, etc. For Example, A person can create roadmap in mind before actually following the road.\\n\\no Stimulus-Response Learning: It is learning to perform a particular behavior when a certain stimulus is present. For example, a dog raises its ear on hearing doorbell.\\n\\n3. Problem solving: It is the process in which one perceives and tries to arrive at a desired solution from a present situation by taking some path, which is blocked by known or unknown hurdles.\\n\\nProblem solving also includes decision making, which is the process of selecting the best suitable alternative out of multiple alternatives to reach the desired goal are available.\\n\\n4. Perception: It is the process of acquiring, interpreting, selecting, and organizing sensory information.\\n\\n8\\n\\nArtificial Intelligence\\n\\nPerception presumes sensing. In humans, perception is aided by sensory organs. In the domain of AI, perception mechanism puts the data acquired by the sensors together in a meaningful manner.\\n\\n5. Linguistic Intelligence: It is one’s ability to use, comprehend, speak, and write the\\n\\nverbal and written language. It is important in interpersonal communication.\\n\\nDifference between Human and Machine Intelligence\\n\\nHumans perceive by patterns whereas the machines perceive by set of rules and data.\\n\\nHumans store and recall information by patterns, machines do it by searching algorithms. For example, the number 40404040 is easy to remember, store and recall as its pattern is simple.\\n\\nHumans can figure out the complete object even if some part of it is missing or distorted; whereas the machines cannot correctly.\\n\\n9\\n\\n3. Research Areas of AI\\n\\nArtificial Intelligence\\n\\nThe domain of artificial intelligence is huge in breadth and width. While proceeding, we consider the broadly common and prospering research areas in the domain of AI:\\n\\nSpeech and Voice Recognition These both terms are common in robotics, expert systems and natural language processing. Though these terms are used interchangeably, their objectives are different.\\n\\nSpeech Recognition\\n\\nVoice Recognition\\n\\nThe at understanding and comprehending WHAT was spoken.\\n\\nspeech\\n\\nrecognition\\n\\naims\\n\\nThe objective of voice recognition is to recognize WHO is speaking.\\n\\nIt is used in hand-free computing, map or menu navigation\\n\\nIt analyzes person’s tone, voice pitch, and accent, etc., to identify a person.\\n\\nMachine does not need training as it is not speaker dependent.\\n\\nThe recognition system needs training as it is person-oriented.\\n\\n10\\n\\nArtificial Intelligence\\n\\nSpeaker independent Speech Recognition systems are difficult to develop.\\n\\nSpeaker-dependent Speech Recognition systems are comparatively easy to develop.\\n\\nWorking of Speech and Voice Recognition Systems The user input spoken at a microphone goes to sound card of the system. The converter turns the analog signal into equivalent digital signal for the speech processing. The database is used to compare the patterns to recognize the words. Finally, a reverse feedback is given to the database.\\n\\nThis source-language text becomes input to the Translation Engine, which converts it to the target language text. They are supported with interactive GUI, large database of vocabulary etc.\\n\\nReal Life Applications of Research Areas\\n\\nThere is a large array of applications where AI is serving common people in their day-to-day lives:\\n\\nSr. No.\\n\\nResearch Area\\n\\nReal Life Application\\n\\nExpert Systems\\n\\n1\\n\\nExamples: systems\\n\\nFlight-tracking\\n\\nsystems, Clinical\\n\\nNatural Language Processing\\n\\nExamples: Google Now recognition, Automatic voice output\\n\\nfeature,\\n\\nspeech\\n\\n2\\n\\nNeural Networks\\n\\n3\\n\\nExamples: Pattern recognition systems such as face recognition, character recognition, handwriting recognition.\\n\\n11\\n\\nArtificial Intelligence\\n\\nRobotics\\n\\n4\\n\\nExamples: Industrial robots for moving, spraying, painting, precision checking, drilling, cleaning, coating, carving etc.\\n\\nFuzzy Logic\\n\\n5\\n\\nExamples: Consumer electronics, automobiles, etc.\\n\\nTask Classification of AI\\n\\nThe domain of AI is classified into Formal tasks, Mundane tasks, and Expert tasks.\\n\\n12\\n\\nArtificial Intelligence\\n\\nTask Domains of Artificial Intelligence\\n\\nMundane (Ordinary) Tasks\\n\\nFormal Tasks\\n\\nExpert Tasks\\n\\nPerception\\n\\nMathematics\\n\\nEngineering\\n\\nComputer Vision\\n\\nGeometry\\n\\nFault finding\\n\\nSpeech, Voice\\n\\nLogic\\n\\nManufacturing\\n\\n\\n\\nIntegration\\n\\nand\\n\\nMonitoring\\n\\nDifferentiation\\n\\nNatural Language Processing\\n\\nGames\\n\\nScientific Analysis\\n\\nUnderstanding\\n\\nGo\\n\\nLanguage Generation\\n\\nChess (Deep Blue)\\n\\nLanguage Translation\\n\\nCheckers\\n\\nCommon Sense\\n\\nVerification\\n\\nFinancial Analysis\\n\\nReasoning\\n\\nTheorem Proving\\n\\nMedical Diagnosis\\n\\nPlanning\\n\\nCreativity\\n\\nRobotics\\n\\nLocomotive\\n\\nHumans learn mundane (ordinary) tasks since their birth. They learn by perception, speaking, using language, and locomotives. They learn Formal Tasks and Expert Tasks later, in that order.\\n\\nFor humans, the mundane tasks are easiest to learn. The same was considered true before trying to implement mundane tasks in machines. Earlier, all work of AI was concentrated in the mundane task domain.\\n\\nLater, it turned out that the machine requires more knowledge, complex knowledge representation, and complicated algorithms for handling mundane tasks. This is the reason why AI work is more prospering in the Expert Task domain now, as the expert task domain needs expert knowledge without common sense, which can be easier to represent and handle.\\n\\n13\\n\\n4. Agents and Environments\\n\\nArtificial Intelligence\\n\\nAn AI system is composed of an agent and its environment. The agents act in their environment. The environment may contain other agents.\\n\\nWhat are Agent and Environment?\\n\\nAn agent is anything that can perceive its environment through sensors and acts upon that environment through effectors.\\n\\nA human agent has sensory organs such as eyes, ears, nose, tongue and skin parallel\\n\\nto the sensors, and other organs such as hands, legs, mouth, for effectors.\\n\\nA robotic agent replaces cameras and infrared range finders for the sensors, and\\n\\nvarious motors and actuators for effectors.\\n\\nA software agent has encoded bit strings as its programs and actions.\\n\\nAgents Terminology\\n\\nPerformance Measure of Agent: It is the criteria, which determines how successful\\n\\nan agent is.\\n\\nBehavior of Agent: It is the action that agent performs after any given sequence of\\n\\npercepts.\\n\\nPercept: It is agent’s perceptual inputs at a given instance.\\n\\nPercept Sequence: It is the history of all that an agent has perceived till date.\\n\\n14\\n\\nArtificial Intelligence\\n\\nAgent Function: It is a map from the precept sequence to an action.\\n\\nRationality\\n\\nRationality is nothing but status of being reasonable, sensible, and having good sense of judgment.\\n\\nRationality is concerned with expected actions and results depending upon what the agent has perceived. Performing actions with the aim of obtaining useful information is an important part of rationality.\\n\\nWhat is Ideal Rational Agent?\\n\\nAn ideal rational agent is the one, which is capable of doing expected actions to maximize its performance measure, on the basis of:\\n\\n\\n\\nIts percept sequence\\n\\n\\n\\nIts built-in knowledge base\\n\\nRationality of an agent depends on the following:\\n\\n1. The performance measures, which determine the degree of success. 2. Agent’s Percept Sequence till now. 3. The agent’s prior knowledge about the environment. 4. The actions that the agent can carry out.\\n\\nA rational agent always performs right action, where the right action means the action that causes the agent to be most successful in the given percept sequence. The problem the agent solves is characterized by Performance Measure, Environment, Actuators, and Sensors (PEAS).\\n\\nThe Structure of Intelligent Agents\\n\\nAgent’s structure can be viewed as:\\n\\nAgent = Architecture + Agent Program\\n\\nArchitecture = the machinery that an agent executes on.\\n\\nAgent Program = an implementation of an agent function.\\n\\nSimple Reflex Agents\\n\\nThey choose actions only based on the current percept.\\n\\nThey are rational only if a correct decision is made only on the basis of current precept.\\n\\nTheir environment is completely observable.\\n\\nCondition-Action Rule – It is a rule that maps a state (condition) to an action.\\n\\n15\\n\\nArtificial Intelligence\\n\\nModel-Based Reflex Agents They use a model of the world to choose their actions. They maintain an internal state.\\n\\nModel: knowledge about “how the things happen in the world”.\\n\\nInternal State: It is a representation of unobserved aspects of current state depending on percept history.\\n\\nUpdating state requires the information about\\n\\nHow the world evolves.\\n\\nHow the agent’s actions affect the world.\\n\\n16\\n\\nArtificial Intelligence\\n\\nGoal-Based Agents\\n\\nThey choose their actions in order to achieve goals. Goal-based approach is more flexible than reflex agent since the knowledge supporting a decision is explicitly modeled, thereby allowing for modifications.\\n\\nGoal: It is the description of desirable situations.\\n\\nUtility-Based Agents\\n\\nThey choose actions based on a preference (utility) for each state.\\n\\n17\\n\\nArtificial Intelligence\\n\\nGoals are inadequate when:\\n\\nThere are conflicting goals only some of which can be achieved.\\n\\nGoals have some uncertainty of being achieved and one needs to weigh likelihood of success against the importance of a goal.\\n\\nThe Nature of Environments\\n\\nSome programs operate in the entirely artificial environment confined to keyboard input, database, computer file systems and character output on a screen.\\n\\nIn contrast, some software agents (software robots or softbots) exist in rich, unlimited softbots domains. The simulator has a very detailed, complex environment. The software agent needs to choose from a long array of actions in real time. A softbot designed to scan the online preferences of the customer and show interesting items to the customer works in the real as well as an artificial environment.\\n\\nThe most famous artificial environment is the Turing Test environment, in which one real and other artificial agents are tested on equal ground. This is a very challenging environment as it is highly difficult for a software agent to perform as well as a human.\\n\\nTuring Test The success of an intelligent behavior of a system can be measured with Turing Test.\\n\\nTwo persons and a machine to be evaluated participate in the test. Out of the two persons, one plays the role of the tester. Each of them sits in different rooms. The tester is unaware of who is machine and who is a human. He interrogates the questions by typing and sending them to both intelligences, to which he receives typed responses.\\n\\n18\\n\\nArtificial Intelligence\\n\\nThis test aims at fooling the tester. If the tester fails to determine machine’s response from the human response, then the machine is said to be intelligent.\\n\\nProperties of Environment\\n\\nThe environment has multifold properties:\\n\\nDiscrete / Continuous: If there are a limited number of distinct, clearly defined, states of the environment, the environment is discrete (For example, chess); otherwise it is continuous (For example, driving).\\n\\nObservable / Partially Observable: If it is possible to determine the complete state of the environment at each time point from the percepts it is observable; otherwise it is only partially observable.\\n\\nStatic / Dynamic: If the environment does not change while an agent is acting, then it is static; otherwise it is dynamic.\\n\\nSingle agent / Multiple agents: The environment may contain other agents which may be of the same or different kind as that of the agent.\\n\\nAccessible vs. inaccessible: If the agent’s sensory apparatus can have access to the complete state of the environment, then the environment is accessible to that agent.\\n\\nDeterministic vs. Non-deterministic: If the next state of the environment is completely determined by the current state and the actions of the agent, then the environment is deterministic; otherwise it is non-deterministic.\\n\\nEpisodic vs. Non-episodic: In an episodic environment, each episode consists of the agent perceiving and then acting. The quality of its action depends just on the episode itself. Subsequent episodes do not depend on the actions in the previous episodes. Episodic environments are much simpler because the agent does not need to think ahead.\\n\\n19\\n\\n5. Popular Search Algorithms\\n\\nArtificial Intelligence\\n\\nSearching is the universal technique of problem solving in AI. There are some single-player games such as tile games, Sudoku, crossword, etc. The search algorithms help you to search for a particular position in such games.\\n\\nSingle Agent Pathfinding Problems\\n\\nThe games such as 3X3 eight-tile, 4X4 fifteen-tile, and 5X5 twenty four tile puzzles are single- agent-path-finding challenges. They consist of a matrix of tiles with a blank tile. The player is required to arrange the tiles by sliding a tile either vertically or horizontally into a blank space with the aim of accomplishing some objective.\\n\\nThe other examples of single agent pathfinding problems are Travelling Salesman Problem, Rubik’s Cube, and Theorem Proving.\\n\\nSearch Terminology\\n\\nProblem Space: It is the environment in which the search takes place. (A set of states and set of operators to change those states)\\n\\nProblem Instance: It is Initial state + Goal state\\n\\nProblem Space Graph: It represents problem state. States are shown by nodes and operators are shown by edges.\\n\\nDepth of a problem: Length of a shortest path or shortest sequence of operators from Initial State to goal state.\\n\\nSpace Complexity: The maximum number of nodes that are stored in memory.\\n\\nTime Complexity: The maximum number of nodes that are created.\\n\\nAdmissibility: A property of an algorithm to always find an optimal solution.\\n\\nBranching Factor: The average number of child nodes in the problem space graph.\\n\\nDepth: Length of the shortest path from initial state to goal state.\\n\\nBrute-Force Search Strategies\\n\\nThey are most simple, as they do not need any domain-specific knowledge. They work fine with small number of possible states.\\n\\nRequirements –\\n\\nState description\\n\\n20\\n\\nArtificial Intelligence\\n\\nA set of valid operators\\n\\n\\n\\nInitial state\\n\\nGoal state description\\n\\nBreadth-First Search It starts from the root node, explores the neighboring nodes first and moves towards the next level neighbors. It generates one tree at a time until the solution is found. It can be implemented using FIFO queue data structure. This method provides shortest path to the solution.\\n\\nIf branching factor (average number of child nodes for a given node) = b and depth = d, then number of nodes at level d = bd.\\n\\nThe total no of nodes created in worst case is b + b2 + b3 + … + bd.\\n\\nDisadvantage: Since each level of nodes is saved for creating next one, it consumes a lot of memory space. Space requirement to store nodes is exponential.\\n\\nIts complexity depends on the number of nodes. It can check duplicate nodes.\\n\\nDepth-First Search It is implemented in recursion with LIFO stack data structure. It creates the same set of nodes as Breadth-First method, only in the different order.\\n\\nAs the nodes on the single path are stored in each iteration from root to leaf node, the space requirement to store nodes is linear. With branching factor b and depth as m, the storage space is bm.\\n\\nDisadvantage: This algorithm may not terminate and go on infinitely on one path. The solution to this issue is to choose a cut-off depth. If the ideal cut-off is d, and if chosen cut- off is lesser than d, then this algorithm may fail. If chosen cut-off is more than d, then execution time increases.\\n\\nIts complexity depends on the number of paths. It cannot check duplicate nodes.\\n\\n21\\n\\nArtificial Intelligence\\n\\nBidirectional Search It searches forward from initial state and backward from goal state till both meet to identify a common state.\\n\\nThe path from initial state is concatenated with the inverse path from the goal state. Each search is done only up to half of the total path.\\n\\nUniform Cost Search Sorting is done in increasing cost of the path to a node. It always expands the least cost node. It is identical to Breadth First search if each transition has the same cost.\\n\\nIt explores paths in the increasing order of cost.\\n\\nDisadvantage: There can be multiple long paths with the cost ≤ C*. Uniform Cost search must explore them all.\\n\\nIterative Deepening Depth-First Search It performs depth-first search to level 1, starts over, executes a complete depth-first search to level 2, and continues in such way till the solution is found.\\n\\nIt never creates a node until all lower nodes are generated. It only saves a stack of nodes. The algorithm ends when it finds a solution at depth d. The number of nodes created at depth d is bd and at depth d-1 is bd-1.\\n\\n22\\n\\nArtificial Intelligence\\n\\nComparison of Various Algorithms Complexities Let us see the performance of algorithms based on various criteria:\\n\\nCriterion\\n\\nBreadth First\\n\\nDepth First\\n\\nBidirectional\\n\\nUniform Cost\\n\\nIterative Deepening\\n\\nTime\\n\\nbd\\n\\nbm\\n\\nb d/2\\n\\nbd\\n\\nbd\\n\\nSpace\\n\\nbd\\n\\nbm\\n\\nb d/2\\n\\nbd\\n\\nbd\\n\\nOptimality\\n\\nY\\n\\nN\\n\\nY\\n\\nY\\n\\nY\\n\\nCompleteness\\n\\nY\\n\\nN\\n\\nY\\n\\nY\\n\\nY\\n\\nInformed (Heuristic) Search Strategies\\n\\nTo solve large problems with large number of possible states, problem-specific knowledge needs to be added to increase the efficiency of search algorithms.\\n\\nHeuristic Evaluation Functions They calculate the cost of optimal path between two states. A heuristic function for sliding- tiles games is computed by counting number of moves that each tile makes from its goal state and adding these number of moves for all tiles.\\n\\nPure Heuristic Search It expands nodes in the order of their heuristic values. It creates two lists, a closed list for the already expanded nodes and an open list for the created but unexpanded nodes.\\n\\nIn each iteration, a node with a minimum heuristic value is expanded, all its child nodes are created and placed in the closed list. Then, the heuristic function is applied to the child nodes and they are placed in the open list according to their heuristic value. The shorter paths are saved and the longer ones are disposed.\\n\\n23\\n\\nArtificial Intelligence\\n\\nA* Search It is best-known form of Best First search. It avoids expanding paths that are already expensive, but expands most promising paths first.\\n\\nf(n) = g(n) + h(n), where\\n\\ng(n) the cost (so far) to reach the node\\n\\nh(n) estimated cost to get from the node to the goal\\n\\nf(n) estimated total cost of path through n to goal. It is implemented using priority queue by increasing f(n).\\n\\nGreedy Best First Search It expands the node that is estimated to be closest to goal. It expands nodes based on f(n) = h(n). It is implemented using priority queue.\\n\\nDisadvantage: It can get stuck in loops. It is not optimal.\\n\\nLocal Search Algorithms\\n\\nThey start from a prospective solution and then move to a neighboring solution. They can return a valid solution even if it is interrupted at any time before they end.\\n\\nHill-Climbing Search It is an iterative algorithm that starts with an arbitrary solution to a problem and attempts to find a better solution by changing a single element of the solution incrementally. If the change produces a better solution, an incremental change is taken as a new solution. This process is repeated until there are no further improvements.\\n\\nfunction Hill-Climbing (problem), returns a state that is a local maximum.\\n\\ninputs: problem, a problem\\n\\nlocal variables: current, a node\\n\\nneighbor, a node\\n\\ncurrent ←Make_Node(Initial-State[problem])\\n\\nloop\\n\\ndo neighbor ← a highest_valued successor of current\\n\\nif Value[neighbor] ≤ Value[current] then\\n\\nreturn State[current]\\n\\ncurrent ← neighbor\\n\\nend\\n\\nDisadvantage: This algorithm is neither complete, nor optimal.\\n\\n24\\n\\nArtificial Intelligence\\n\\nLocal Beam Search In this algorithm, it holds k number of states at any given time. At the start, these states are generated randomly. The successors of these k states are computed with the help of objective function. If any of these successors is the maximum value of the objective function, then the algorithm stops.\\n\\nOtherwise the (initial k states and k number of successors of the states = 2k) states are placed in a pool. The pool is then sorted numerically. The highest k states are selected as new initial states. This process continues until a maximum value is reached.\\n\\nfunction BeamSearch( problem, k), returns a solution state.\\n\\nstart with k randomly generated states\\n\\nloop\\n\\ngenerate all successors of all k states\\n\\nif any of the states = solution, then return the state\\n\\nelse select the k best successors\\n\\nend\\n\\nSimulated Annealing Annealing is the process of heating and cooling a metal to change its internal structure for modifying its physical properties. When the metal cools, its new structure is seized, and the metal retains its newly obtained properties. In simulated annealing process, the temperature is kept variable.\\n\\nWe initially set the temperature high and then allow it to ‘cool\\' slowly as the algorithm proceeds. When the temperature is high, the algorithm is allowed to accept worse solutions with high frequency.\\n\\nStart\\n\\n5. Initialize k = 0; L = integer number of variables; 6. From i -> j, search the performance difference ∆. 7. If ∆ <= 0 then accept else if exp(-\\uf044/T(k)) > random(0,1) then accept; 8. Repeat steps 1 and 2 for L(k) steps. 9. k = k + 1; Repeat steps 1 through 4 till the criteria is met.\\n\\nEnd\\n\\nTravelling Salesman Problem In this algorithm, the objective is to find a low-cost tour that starts from a city, visits all cities en-route exactly once and ends at the same starting city.\\n\\nStart\\n\\nFind out all (n -1)! Possible solutions, where n is the total number of cities.\\n\\n25\\n\\nArtificial Intelligence\\n\\nDetermine the minimum cost by finding out the cost of each of these (n -1)!\\n\\nsolutions.\\n\\nFinally, keep the one with the minimum cost.\\n\\nend\\n\\n26\\n\\n6. Fuzzy Logic Systems\\n\\nArtificial Intelligence\\n\\nFuzzy Logic Systems (FLS) produce acceptable but definite output in response to incomplete, ambiguous, distorted, or inaccurate (fuzzy) input.\\n\\nWhat is Fuzzy Logic?\\n\\nFuzzy Logic (FL) is a method of reasoning that resembles human reasoning. The approach of FL imitates the way of decision making in humans that involves all intermediate possibilities between digital values YES and NO.\\n\\nThe conventional logic block that a computer can understand takes precise input and produces a definite output as TRUE or FALSE, which is equivalent to human’s YES or NO.\\n\\nThe inventor of fuzzy logic, Lotfi Zadeh, observed that unlike computers, the human decision making includes a range of possibilities between YES and NO, such as:\\n\\nCERTAINLY YES\\n\\nPOSSIBLY YES\\n\\nCANNOT SAY\\n\\nPOSSIBLY NO\\n\\nCERTAINLY NO\\n\\nThe fuzzy logic works on the levels of possibilities of input to achieve the definite output.\\n\\nImplementation\\n\\n\\n\\nIt can be implemented in systems with various sizes and capabilities ranging from small\\n\\nmicro-controllers to large, networked, workstation-based control systems.\\n\\n\\n\\nIt can be implemented in hardware, software, or a combination of both.\\n\\nWhy Fuzzy Logic?\\n\\nFuzzy logic is useful for commercial and practical purposes.\\n\\n\\n\\nIt can control machines and consumer products.\\n\\n\\n\\nIt may not give accurate reasoning, but acceptable reasoning.\\n\\nFuzzy logic helps to deal with the uncertainty in engineering.\\n\\n27\\n\\nArtificial Intelligence\\n\\nFuzzy Logic Systems Architecture\\n\\nIt has four main parts as shown:\\n\\n1. Fuzzification Module: transforms the system inputs, which are crisp numbers, into\\n\\nfuzzy sets.\\n\\nIt splits the input signal into five steps such as:\\n\\nLP\\n\\nx is Large Positive\\n\\nMP\\n\\nx is Medium Positive\\n\\nS\\n\\nx is Small\\n\\nMN\\n\\nx is Medium Negative\\n\\nLN\\n\\nx is Large Negative\\n\\n2. Knowledge Base: It stores IF-THEN rules provided by experts. 3. Inference Engine: It simulates the human reasoning process by making fuzzy inference on the inputs and IF-THEN rules.\\n\\n4. Defuzzification Module: It transforms the fuzzy set obtained by the inference engine\\n\\ninto a crisp value.\\n\\nThese membership functions work on fuzzy sets of variables.\\n\\nMembership Functions Membership functions allow you to quantify linguistic term and represent a fuzzy set graphically. A membership function for a fuzzy set A on the universe of discourse X is defined as µA:X → [0,1].\\n\\n28\\n\\nArtificial Intelligence\\n\\nHere, each element of X is mapped to a value between 0 and 1. It is called membership value or degree of membership. It quantifies the degree of membership of the element in X to the fuzzy set A.\\n\\nx axis represents the universe of discourse.\\n\\ny axis represents the degrees of membership in the [0, 1] interval.\\n\\nThere can be multiple membership functions applicable to fuzzify a numerical value. Simple membership functions are used as use of complex functions does not add more precision in the output.\\n\\nAll membership functions for LP, MP, S, MN, and LN are shown as below:\\n\\nThe triangular membership function shapes are most common among various other membership function shapes such as trapezoidal, singleton, and Gaussian.\\n\\nHere, the input to 5-level fuzzifier varies from -10 volts to +10 volts. Hence the corresponding output also changes.\\n\\nExample of a Fuzzy Logic System\\n\\nLet us consider an air conditioning system with 5-lvel fuzzy logic system. This system adjusts the temperature of air conditioner by comparing the room temperature and the target temperature value.\\n\\n29\\n\\nArtificial Intelligence\\n\\nAlgorithm\\n\\n1. Define linguistic Variables and terms (start) 2. Construct membership functions for them. (start) 3. Construct knowledge base of rules (start) 4. Convert crisp data into fuzzy data sets using membership functions (fuzzification) 5. Evaluate rules in the rule base (inference engine) 6. Combine results from each rule (inference engine) 7. Convert output data into non-fuzzy values. (defuzzification)\\n\\nDevelopment Step 1: Define linguistic variables and terms\\n\\nLinguistic variables are input and output variables in the form of simple words or sentences. For room temperature, cold, warm, hot, etc., are linguistic terms.\\n\\nTemperature (t) = {very-cold, cold, warm, very-warm, hot}\\n\\nEvery member of this set is a linguistic term and it can cover some portion of overall temperature values.\\n\\nStep 2: Construct membership functions for them\\n\\nThe membership functions of temperature variable are as shown:\\n\\n30\\n\\nArtificial Intelligence\\n\\nStep3: Construct knowledge base rules\\n\\nCreate a matrix of room temperature values versus target temperature values that an air conditioning system is expected to provide.\\n\\nRoomTemp/Target Very_Cold\\n\\nCold\\n\\nWarm\\n\\nHot\\n\\nVery_Hot\\n\\nVery_Cold\\n\\nNo_Change Heat\\n\\nHeat\\n\\nHeat\\n\\nHeat\\n\\nCold\\n\\nCool\\n\\nNo_Change Heat\\n\\nHeat\\n\\nHeat\\n\\nWarm\\n\\nCool\\n\\nCool\\n\\nNo_Change Heat\\n\\nHeat\\n\\nHot\\n\\nCool\\n\\nCool\\n\\nCool\\n\\nNo_Change Heat\\n\\nVery_Hot\\n\\nCool\\n\\nCool\\n\\nCool\\n\\nCool\\n\\nNo_Change\\n\\nBuild a set of rules into the knowledge base in the form of IF-THEN-ELSE structures.\\n\\nSr. No.\\n\\nCondition\\n\\nAction\\n\\n1\\n\\nIF temperature=(Cold OR Very_Cold) AND target=Warm THEN HEAT\\n\\n2\\n\\nIF temperature=(Hot OR Very_Hot) AND target=Warm THEN\\n\\nCOOL\\n\\n3\\n\\nIF (temperature=Warm) AND (target=Warm) THEN\\n\\nNOCHANGE\\n\\nStep5\\n\\n31\\n\\nArtificial Intelligence\\n\\nFuzzy set operations perform evaluation of rules. The operations used for OR and AND are Max and Min respectively. All results of evaluation are combined to form a final result. This result is a fuzzy value.\\n\\nStep 6\\n\\nDefuzzification is then performed according to membership function for output variable.\\n\\nApplication Areas of Fuzzy Logic\\n\\nThe key application areas of fuzzy logic are as given:\\n\\nAutomotive Systems\\n\\nAutomatic Gearboxes\\n\\nFour-Wheel Steering\\n\\nVehicle environment control\\n\\nConsumer Electronics\\n\\nHi-Fi Systems\\n\\nPhotocopiers\\n\\nStill and Video Cameras\\n\\nTelevision\\n\\nDomestic Goods\\n\\nMicrowave Ovens\\n\\nRefrigerators\\n\\nToasters\\n\\nVacuum Cleaners\\n\\n32\\n\\nArtificial Intelligence\\n\\nWashing Machines\\n\\nEnvironment Control\\n\\nAir Conditioners/Dryers/Heaters\\n\\nHumidifiers\\n\\nAdvantages of FLSs\\n\\nMathematical concepts within fuzzy reasoning are very simple.\\n\\nYou can modify a FIS by just adding or deleting rules due to flexibility of fuzzy logic.\\n\\nFuzzy logic Systems can take imprecise, distorted, noisy input information.\\n\\nFLSs are easy to construct and understand.\\n\\nFuzzy logic is a solution to complex problems in all fields of life, including medicine, as\\n\\nit resembles human reasoning and decision making.\\n\\nDisadvantages of FLSs\\n\\nThere is no systematic approach to fuzzy system designing.\\n\\nThey are understandable only when simple.\\n\\nThey are suitable for the problems which do not need high accuracy.\\n\\n33\\n\\n7. Natural Language Processing\\n\\nArtificial Intelligence\\n\\nNatural Language Processing (NLP) refers to AI method of communicating with an intelligent systems using a natural language such as English.\\n\\nProcessing of Natural Language is required when you want an intelligent system like robot to perform as per your instructions, when you want to hear decision from a dialogue based clinical expert system, etc.\\n\\nThe field of NLP involves making computers to perform useful tasks with the natural languages humans use. The input and output of an NLP system can be:\\n\\nSpeech \\uf0b7 Written Text\\n\\nComponents of NLP\\n\\nThere are two components of NLP as given:\\n\\nNatural Language Understanding (NLU) Understanding involves the following tasks:\\n\\nMapping the given input in natural language into useful representations. \\uf0b7 Analyzing different aspects of the language.\\n\\nNatural Language Generation (NLG) It is the process of producing meaningful phrases and sentences in the form of natural language from some internal representation.\\n\\nIt involves:\\n\\nText planning: It includes retrieving the relevant content from knowledge base.\\n\\nSentence planning: It includes choosing required words, forming meaningful\\n\\nphrases, setting tone of the sentence.\\n\\nText Realization: It is mapping sentence plan into sentence structure.\\n\\nThe NLU is harder than NLG.\\n\\nDifficulties in NLU\\n\\nNL has an extremely rich form and structure.\\n\\n\\n\\nIt is very ambiguous. There can be different levels of ambiguity:\\n\\no Lexical ambiguity: It is at very primitive level such as word-level.\\n\\n34\\n\\nArtificial Intelligence\\n\\no For example, treating the word “board” as noun or verb?\\n\\no Syntax Level ambiguity: A sentence can be parsed in different ways.\\n\\no For example, “He lifted the beetle with red cap.” – Did he use cap to lift the\\n\\nbeetle or he lifted a beetle that had red cap?\\n\\no Referential ambiguity: Referring to something using pronouns. For example,\\n\\nRima went to Gauri. She said, “I am tired.” - Exactly who is tired?\\n\\no One input can mean different meanings.\\n\\no Many inputs can mean the same thing.\\n\\nNLP Terminology\\n\\nPhonology: It is study of organizing sound systematically.\\n\\nMorphology: It is a study of construction of words from primitive meaningful units.\\n\\nMorpheme: It is primitive unit of meaning in a language.\\n\\nSyntax: It refers to arranging words to make a sentence. It also involves determining\\n\\nthe structural role of words in the sentence and in phrases.\\n\\nSemantics: It is concerned with the meaning of words and how to combine words into\\n\\nmeaningful phrases and sentences.\\n\\nPragmatics: It deals with using and understanding sentences in different situations\\n\\nand how the interpretation of the sentence is affected.\\n\\nDiscourse: It deals with how the immediately preceding sentence can affect the\\n\\ninterpretation of the next sentence.\\n\\nWorld Knowledge: It includes the general knowledge about the world.\\n\\nSteps in NLP\\n\\nThere are general five steps:\\n\\n1. Lexical Analysis\\n\\nIt involves identifying and analyzing the structure of words. Lexicon of a language means the collection of words and phrases in a language. Lexical analysis is dividing the whole chunk of txt into paragraphs, sentences, and words.\\n\\n2. Syntactic Analysis (Parsing)\\n\\nIt involves analysis of words in the sentence for grammar and arranging words in a manner that shows the relationship among the words. The sentence such as “The school goes to boy” is rejected by English syntactic analyzer.\\n\\n35\\n\\nArtificial Intelligence\\n\\n3. Semantic Analysis\\n\\nIt draws the exact meaning or the dictionary meaning from the text. The text is checked for meaningfulness. It is done by mapping syntactic structures and objects in the task domain. The semantic analyzer disregards sentence such as “hot ice-cream”.\\n\\n4. Discourse Integration\\n\\nThe meaning of any sentence depends upon the meaning of the sentence just before it. In addition, it also brings about the meaning of immediately succeeding sentence.\\n\\n5. Pragmatic Analysis\\n\\nDuring this, what was said is re-interpreted on what it actually meant. It involves deriving those aspects of language which require real world knowledge.\\n\\nImplementation Aspects of Syntactic Analysis\\n\\nThere are a number of algorithms researchers have developed for syntactic analysis, but we consider only the following simple methods:\\n\\nContext-Free Grammar\\n\\nTop-Down Parser\\n\\nLet us see them in detail:\\n\\nContext-Free Grammar It is the grammar that consists rules with a single symbol on the left-hand side of the rewrite rules. Let us create grammar to parse a sentence –\\n\\n“The bird pecks the grains”\\n\\nArticles (DET): a | an | the.\\n\\n36\\n\\nArtificial Intelligence\\n\\nNouns: bird | birds | grain | grains\\n\\nNoun Phrase (NP): Article + Noun | Article + Adjective + Noun\\n\\n= DET N | DET ADJ N\\n\\nVerbs: pecks | pecking | pecked\\n\\nVerb Phrase (VP): NP V | V NP\\n\\nAdjectives (ADJ): beautiful | small | chirping\\n\\nThe parse tree breaks down the sentence into structured parts so that the computer can easily understand and process it. In order for the parsing algorithm to construct this parse tree, a set of rewrite rules, which describe what tree structures are legal, need to be constructed.\\n\\nThese rules say that a certain symbol may be expanded in the tree by a sequence of other symbols. According to first order logic rule, ff there are two strings Noun Phrase (NP) and Verb Phrase (VP), then the string combined by NP followed by VP is a sentence. The rewrite rules for the sentence are as follows:\\n\\nS -> NP VP NP -> DET N | DET ADJ N VP -> V NP\\n\\nLexocon: DET -> a | the ADJ -> beautiful | perching N -> bird | birds | grain | grains\\n\\nV -> peck | pecks | pecking\\n\\nThe parse tree can be created as shown:\\n\\n37\\n\\nArtificial Intelligence\\n\\nNow consider the above rewrite rules. Since V can be replaced by both, \"peck\" or \"pecks\", sentences such as \"The bird peck the grains\" can be wrongly permitted. i. e. the subject-verb agreement error is approved as correct.\\n\\nMerit: The simplest style of grammar, therefore widely used one.\\n\\nDemerits:\\n\\nThey are not highly precise. For example, “The grains peck the bird”, is a syntactically\\n\\ncorrect according to parser, but even if it makes no sense, parser takes it as a correct\\n\\nsentence.\\n\\nTo bring out high precision, multiple sets of grammar need to be prepared. It may\\n\\nrequire a completely different sets of rules for parsing singular and plural variations,\\n\\npassive sentences, etc., which can lead to creation of huge set of rules that are\\n\\nunmanageable.\\n\\nTop-Down Parser Here, the parser starts with the S symbol and attempts to rewrite it into a sequence of terminal symbols that matches the classes of the words in the input sentence until it consists entirely of terminal symbols.\\n\\nThese are then checked with the input sentence to see if it matched. If not, the process is started over again with a different set of rules. This is repeated until a specific rule is found which describes the structure of the sentence.\\n\\n38\\n\\nArtificial Intelligence\\n\\nMerit: It is simple to implement.\\n\\nDemerits:\\n\\n\\uf0b7 Slow speed of working.\\n\\nIt is inefficient, as the search process has to be repeated if an error occurs.\\n\\n39\\n\\n8. Expert Systems\\n\\nArtificial Intelligence\\n\\nExpert systems (ES) are one of the prominent research domains of AI. It is introduced by the researchers at Stanford University, Computer Science Department.\\n\\nWhat are Expert Systems?\\n\\nThe expert systems are the computer applications developed to solve complex problems in a particular domain, at the level of extra-ordinary human intelligence and expertise.\\n\\nCharacteristics of Expert Systems\\n\\nHigh performance\\n\\nUnderstandable\\n\\nReliable\\n\\nHighly responsive\\n\\nCapabilities of Expert Systems\\n\\nThe expert systems are capable of:\\n\\nAdvising\\n\\n\\n\\nInstructing and assisting human in decision making\\n\\nDemonstrating\\n\\nDeriving a solution\\n\\nDiagnosing\\n\\nExplaining\\n\\n\\n\\nInterpreting input\\n\\nPredicting results\\n\\n\\n\\nJustifying the conclusion\\n\\nSuggesting alternative options to a problem\\n\\nThey are incapable of:\\n\\nSubstituting human decision makers\\n\\nPossessing human capabilities\\n\\nProducing accurate output for inadequate knowledge base\\n\\nRefining their own knowledge\\n\\n40\\n\\nArtificial Intelligence\\n\\nComponents of Expert Systems\\n\\nThe components of ES include:\\n\\nKnowledge Base\\n\\n\\n\\nInference Engine\\n\\nUser Interface\\n\\nLet us see them one by one briefly:\\n\\nKnowledge Base\\n\\nIt contains domain-specific and high-quality knowledge.\\n\\nKnowledge is required to exhibit intelligence. The success of any ES majorly depends upon the collection of highly accurate and precise knowledge.\\n\\nWhat is Knowledge? The data is collection of facts. The information is organized as data and facts about the task domain. Data, information, and past experience combined together are termed as knowledge.\\n\\n41\\n\\nArtificial Intelligence\\n\\nComponents of Knowledge Base The knowledge base of an ES is a store of both, factual and heuristic knowledge.\\n\\nFactual Knowledge – It is the information widely accepted by the Knowledge\\n\\nEngineers and scholars in the task domain.\\n\\nHeuristic Knowledge – It is about practice, accurate judgment, one’s ability of\\n\\nevaluation, and guessing.\\n\\nKnowledge representation\\n\\nIt is the method used to organize and formalize the knowledge in the knowledge base. It is in the form of IF-THEN-ELSE rules.\\n\\nKnowledge Acquisition The success of any expert system majorly depends on the quality, completeness, and accuracy of the information stored in the knowledge base.\\n\\nThe knowledge base is formed by readings from various experts, scholars, and the Knowledge Engineers. The knowledge engineer is a person with the qualities of empathy, quick learning, and case analyzing skills.\\n\\nHe acquires information from subject expert by recording, interviewing, and observing him at work, etc. He then categorizes and organizes the information in a meaningful way, in the form of IF-THEN-ELSE rules, to be used by interference machine. The knowledge engineer also monitors the development of the ES.\\n\\nInference Engine\\n\\nUse of efficient procedures and rules by the Inference Engine is essential in deducting a correct, flawless solution.\\n\\nIn case of knowledge-based ES, the Inference Engine acquires and manipulates the knowledge from the knowledge base to arrive at a particular solution.\\n\\nIn case of rule based ES, it:\\n\\nApplies rules repeatedly to the facts, which are obtained from earlier rule application.\\n\\nAdds new knowledge into the knowledge base if required.\\n\\nResolves rules conflict when multiple rules are applicable to a particular case\\n\\nTo recommend a solution, the inference engine uses the following strategies:\\n\\nForward Chaining\\n\\nBackward Chaining\\n\\nForward Chaining It is a strategy of an expert system to answer the question, “What can happen next?”\\n\\n42\\n\\nArtificial Intelligence\\n\\nHere, the inferance engine follows the chain of conditions and derivations and finally deduces the outcome. It considers all the facts and rules, and sorts them before concluding to a solution.\\n\\nThis strategy is followed for working on conclusion, result, or effect. For example, prediction of share market status as an effect of changes in interest rates.\\n\\nBackward Chaining With this strategy, an expert system finds out the answer to the question, “Why this happened?”\\n\\nOn the basis of what has already happened, the inference engine tries to find out which conditions could have happened in the past for this result. This strategy is followed for finding out cause or reason. For example, diagnosis of blood cancer in humans.\\n\\nUser Interface\\n\\nUser interface provides interaction between user of the ES and the ES itself. It is generally Natural Language Processing so as to be used by the user who is well-versed in the task domain. The user of the ES need not be necessarily an expert in Artificial Intelligence.\\n\\nIt explains how the ES has arrived at a particular recommendation. The explanation may in the following forms:\\n\\nNatural language displayed on screen\\n\\nVerbal narrations in natural language\\n\\n43\\n\\nArtificial Intelligence\\n\\nListing of rule numbers displayed on the screen.\\n\\nThe user interface makes it easy to trace the credibility of the deductions.\\n\\nRequirements of Efficient ES User Interface\\n\\n\\n\\nIt should help users to accomplish their goals in shortest possible ay.\\n\\n\\n\\nIt should be designed to work for user’s existing or desired work practices.\\n\\n\\n\\nIts technology should be adaptable to user’s requirements; not the other way round.\\n\\n\\n\\nIt should make efficient use of user input.\\n\\nExpert Systems Limitations\\n\\nNo technology can offer easy and complete solution. Large systems are costly, require significant development time, and computer resources. ESs have their limitations which include:\\n\\nLimitations of the technology\\n\\nDifficult knowledge acquisition\\n\\nES are Difficult to maintain\\n\\nHigh Development costs\\n\\nApplications of Expert System\\n\\nThe following table shows where ES can be applied.\\n\\nApplication\\n\\nDescription\\n\\nDesign Domain\\n\\nCamera lens design, automobile design.\\n\\nMedical Domain\\n\\nDiagnosis Systems to deduce cause of disease from observed data, conduction medical operations on humans.\\n\\nMonitoring Systems\\n\\nComparing data continuously with observed system or with prescribed behavior such as leakage monitoring in long petroleum pipeline.\\n\\nProcess Control Systems\\n\\nControlling a physical process based on monitoring.\\n\\nKnowledge Domain\\n\\nFinding out faults in vehicles, computers.\\n\\nFinance/Commerce\\n\\nDetection of possible fraud, suspicious transactions, stock market trading, Airline scheduling, cargo scheduling.\\n\\n44\\n\\nArtificial Intelligence\\n\\nExpert System Technology\\n\\nThere are several levels of ES technologies available. Expert systems technologies include:\\n\\n1. Expert System Development Environment: The ES development environment\\n\\nincludes hardware and tools. They are:\\n\\no Workstations, minicomputers, mainframes\\n\\no High level Symbolic Programming Languages such as LISt Programming (LISP)\\n\\nand PROgrammation en LOGique (PROLOG).\\n\\no Large databases\\n\\n2. Tools: They reduce the effort and cost involved in developing an expert system to\\n\\nlarge extent.\\n\\no Powerful editors and debugging tools with multi-windows.\\n\\no They provide rapid prototyping\\n\\no Have Inbuilt definitions of model, knowledge representation, and inference\\n\\ndesign.\\n\\n1. Shells: A shell is nothing but an expert system without knowledge base. A shell provides the developers with knowledge acquisition, inference engine, user interface, and explanation facility. For example, few shells are given below:\\n\\no\\n\\nJava Expert System Shell (JESS) that provides fully developed Java API for\\n\\ncreating an expert system.\\n\\no Vidwan, a shell developed at the National Centre for Software Technology,\\n\\nMumbai in 1993. It enables knowledge encoding in the form of IF-THEN rules.\\n\\nDevelopment of Expert Systems: General Steps\\n\\nThe process of ES development is iterative. Steps in developing the ES include:\\n\\n1. Identify Problem Domain\\n\\nThe problem must be suitable for an expert system to solve it.\\n\\nFind the experts in task domain for the ES project.\\n\\nEstablish cost-effectiveness of the system.\\n\\n2. Design the System\\n\\n\\n\\nIdentify the ES Technology.\\n\\nKnow and establish the degree of integration with the other systems and databases.\\n\\nRealize how the concepts can represent the domain knowledge best.\\n\\n45\\n\\nArtificial Intelligence\\n\\n3. Develop the Prototype\\n\\nForm Knowledge Base: The knowledge engineer works to:\\n\\n\\n\\nAcquire domain knowledge from the expert.\\n\\n\\n\\nRepresent it in the form of If-THEN-ELSE rules.\\n\\n4. Test and Refine the Prototype\\n\\nThe knowledge engineer uses sample cases to test the prototype for any deficiencies\\n\\nin performance.\\n\\nEnd users test the prototypes of the ES.\\n\\n5. Develop and Complete the ES\\n\\nTest and ensure the interaction of the ES with all elements of its environment, including end users, databases, and other information systems.\\n\\nDocument the ES project well.\\n\\nTrain the user to use ES.\\n\\n6. Maintain the System\\n\\nKeep the knowledge base up-to-date by regular review and update. \\uf0b7 Cater for new interfaces with other information systems, as those systems evolve.\\n\\nBenefits of Expert Systems\\n\\nAvailability: They are easily available due to mass production of software.\\n\\nLess Production Cost: Production cost is reasonable. This makes them affordable.\\n\\nSpeed: They offer great speed. They reduce the amount of work an individual puts in.\\n\\nLess Error Rate: Error rate is low as compared to human errors.\\n\\nReducing Risk: They can work in the environment dangerous to humans.\\n\\nSteady response: They work steadily without getting motional, tensed or fatigued.\\n\\n46\\n\\n9. Robotics\\n\\nArtificial Intelligence\\n\\nRobotics is a domain in artificial intelligence that deals with the study of creating intelligent and efficient robots.\\n\\nWhat are Robots?\\n\\nRobots are the artificial agents acting in real world environment.\\n\\nObjective Robots are aimed at manipulating the objects by perceiving, picking, moving, modifying the physical properties of object, destroying it, or to have an effect thereby freeing manpower from doing repetitive functions without getting bored, distracted, or exhausted.\\n\\nWhat is Robotics?\\n\\nRobotics is a branch of AI, which is composed of Electrical Engineering, Mechanical Engineering, and Computer Science for designing, construction, and application of robots.\\n\\nAspects of Robotics\\n\\nThe robots have mechanical construction, form, or shape designed to accomplish a\\n\\nparticular task.\\n\\nThey have electrical components which power and control the machinery.\\n\\nThey contain some level of computer program that determines what, when and how\\n\\na robot does something.\\n\\nDifference in Robot System and Other AI Program\\n\\nHere is the difference between the two:\\n\\nAI Programs\\n\\nRobots\\n\\nThey usually operate in computer-stimulated worlds.\\n\\nThey operate in real physical world\\n\\nThe input to an AI program is in symbols and rules.\\n\\nInputs to robots is analog signal in the form of speech waveform or images\\n\\nThey need general purpose computers to operate on.\\n\\nThey need special hardware with sensors and effectors.\\n\\n47\\n\\nArtificial Intelligence\\n\\nRobot Locomotion\\n\\nLocomotion is the mechanism that makes a robot capable of moving in its environment. There are various types of locomotions:\\n\\nLegged\\n\\nWheeled\\n\\nCombination of Legged and Wheeled Locomotion\\n\\nTracked slip/skid\\n\\nLegged Locomotion\\n\\nThis type of locomotion consumes more power while demonstrating walk, jump, trot,\\n\\nhop, climb up or down, etc.\\n\\n\\n\\nIt requires more number of motors to accomplish a movement. It is suited for rough\\n\\nas well as smooth terrain where irregular or too smooth surface makes it consume\\n\\nmore power for a wheeled locomotion. It is little difficult to implement because of\\n\\nstability issues.\\n\\n\\n\\nIt comes with the variety of one, two, four, and six legs. If a robot has multiple legs\\n\\nthen leg coordination is necessary for locomotion.\\n\\nThe total number of possible gaits (a periodic sequence of lift and release events for each of the total legs) a robot can travel depends upon the number of its legs.\\n\\nIf a robot has k legs, then the number of possible events N = (2k-1)!.\\n\\nIn case of a two-legged robot (k=2), the number of possible events is N = (2k-1)!\\n\\n= (2*2-1)! = 3! = 6.\\n\\nHence there are six possible different events:\\n\\n1. Lifting the Left leg\\n\\n2. Releasing the Left leg\\n\\n3. Lifting the Right leg\\n\\n4. Releasing the Right leg\\n\\n5. Lifting both the legs together\\n\\n6. Releasing both the legs together.\\n\\nIn case of k=6 legs, there are 39916800 possible events. Hence the complexity of robots is directly proportional to the number of legs.\\n\\n48\\n\\nArtificial Intelligence\\n\\nWheeled Locomotion It requires fewer number of motors to accomplish a movement. It is little easy to implement as there are less stability issues in case of more number of wheels. It is power efficient as compared to legged locomotion.\\n\\nStandard wheel: Rotates around the wheel axle and around the contact\\n\\nCastor wheel: Rotates around the wheel axle and the offset steering joint\\n\\nSwedish 45° and Swedish 90° wheels: Omni-wheel, rotates around the contact\\n\\npoint, around the wheel axle, and around the rollers.\\n\\nBall or spherical wheel: Omnidirectional wheel, technically difficult to implement.\\n\\nSlip/Skid Locomotion In this type, the vehicles use tracks as in a tank. The robot is steered by moving the tracks with different speeds in the same or opposite direction. It offers stability because of large contact area of track and ground.\\n\\n49\\n\\nArtificial Intelligence\\n\\nComponents of a Robot\\n\\nRobots are constructed with the following:\\n\\nPower Supply: The robots are powered by batteries, solar power, hydraulic, or\\n\\npneumatic power sources.\\n\\nActuators: They convert energy into movement.\\n\\nElectric motors (AC/DC): They are required for rotational movement.\\n\\nPneumatic Air Muscles: They contract almost 40% when air is sucked in them.\\n\\nMuscle Wires: They contract by 5% when electric current is passed through them.\\n\\nPiezo Motors and Ultrasonic Motors: Best for industrial robots.\\n\\nSensors: They provide knowledge of real time information on the task environment.\\n\\nRobots are equipped with vision sensors to be to compute the depth in the\\n\\nenvironment. A tactile sensor imitates the mechanical properties of touch receptors of\\n\\nhuman fingertips.\\n\\nComputer Vision\\n\\nThis is a technology of AI with which the robots can see. The computer vision plays vital role in the domains of safety, security, health, access, and entertainment.\\n\\nComputer vision automatically extracts, analyzes, and comprehends useful information from a single image or an array of images. This process involves development of algorithms to accomplish automatic visual comprehension.\\n\\nHardware of Computer Vision System This involves:\\n\\nPower supply\\n\\n\\n\\nImage acquisition device such as camera\\n\\na processor\\n\\na software\\n\\nA display device for monitoring the system\\n\\nAccessories such as camera stands, cables, and connectors\\n\\nTasks of Computer Vision\\n\\nOCR: In the domain of computers, Optical Character Reader, a software to convert scanned documents into editable text, which accompanies a scanner.\\n\\n50\\n\\nArtificial Intelligence\\n\\nFace Detection: Many state-of-the-art cameras come with this feature, which enables to read the face and take the picture of that perfect expression. It is used to let a user access the software on correct match.\\n\\nObject Recognition: They are installed in supermarkets, cameras, high-end cars such as BMW, GM, and Volvo.\\n\\nEstimating Position: It is estimating position of an object with respect to camera as in position of tumor in human’s body.\\n\\nApplication Domains of Computer Vision\\n\\nagriculture\\n\\nautonomous vehicles\\n\\nbiometrics\\n\\n\\n\\ncharacter recognition\\n\\n\\n\\nforensics, security, and surveillance\\n\\n\\n\\nindustrial quality inspection\\n\\n\\n\\nface recognition\\n\\ngesture analysis\\n\\ngeoscience\\n\\nmedical imagery\\n\\npollution monitoring\\n\\nprocess control\\n\\n\\n\\nremote sensing\\n\\n\\n\\nrobotics\\n\\n\\n\\ntransport\\n\\nApplications of Robotics\\n\\nThe robotics has been instrumental in the various domains such as:\\n\\nIndustries: Robots are used for handling material, cutting, welding, color coating,\\n\\ndrilling, polishing, etc.\\n\\nMilitary: Autonomous robots can reach inaccessible and hazardous zones during war.\\n\\nA robot named Daksh, developed by Defense Research and Development Organization\\n\\n(DRDO), is in function to destroy life-threatening objects safely.\\n\\nMedicine: The robots are capable of carrying out hundreds of clinical tests\\n\\nsimultaneously, rehabilitating permanently disabled people, and performing complex\\n\\nsurgeries such as brain tumors.\\n\\n51\\n\\nArtificial Intelligence\\n\\nExploration: The robot rock climbers used for space exploration, underwater drones\\n\\nused for ocean exploration are to name a few.\\n\\nEntertainment: Disney’s engineers have created hundreds of robots for movie\\n\\nmaking.\\n\\n52\\n\\n10. Neural Networks\\n\\nArtificial Intelligence\\n\\nYet another research area in AI, neural networks, is inspired from the natural neural network of human nervous system.\\n\\nWhat are Artificial Neural Networks (ANNs)?\\n\\nThe inventor of the first neurocomputer, Dr. Robert Hecht-Nielsen, defines a neural network as:\\n\\n\"...a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.”\\n\\nBasic Structure of ANNs\\n\\nThe idea of ANNs is based on the belief that working of human brain by making the right connections, can be imitated using silicon and wires as living neurons and dendrites.\\n\\nThe human brain is composed of 100 billion nerve cells called neurons. They are connected to other thousand cells by Axons. Stimuli from external environment or inputs from sensory organs are accepted by dendrites. These inputs create electric impulses, which quickly travel through the neural network. A neuron can then send the message to other neuron to handle the issue or does not send it forward.\\n\\nANNs are composed of multiple nodes, which imitate biological neurons of human brain. The neurons are connected by links and they interact with each other. The nodes can take input\\n\\n53\\n\\nArtificial Intelligence\\n\\ndata and perform simple operations on the data. The result of these operations is passed to other neurons. The output at each node is called its activation or node value.\\n\\nEach link is associated with weight. ANNs are capable of learning, which takes place by altering weight values. The following illustration shows a simple ANN:\\n\\nTypes of Artificial Neural Networks\\n\\nThere are two Artificial Neural Network topologies: FeedForward and Feedback.\\n\\nFeedForward ANN In this ANN, the information flow is unidirectional. A unit sends information to other unit from which it does not receive any information. There are no feedback loops. They are used in pattern generation/recognition/classification. They have fixed inputs and outputs.\\n\\nFeedback ANN Here, feedback loops are allowed. They are used in content addressable memories.\\n\\n54\\n\\nArtificial Intelligence\\n\\nWorking of ANNs\\n\\nIn the topology diagrams shown, each arrow represents a connection between two neurons and indicates the pathway for the flow of information. Each connection has a weight, an integer number that controls the signal between the two neurons.\\n\\nIf the network generates a “good or desired” output, there is no need to adjust the weights. However, if the network generates a “poor or undesired” output or an error, then the system alters the weights in order to improve subsequent results.\\n\\nMachine Learning in ANNs\\n\\nANNs are capable of learning and they need to be trained. There are several learning strategies:\\n\\nSupervised Learning: It involves a teacher that is scholar than the ANN itself. For\\n\\nexample, the teacher feeds some example data about which the teacher already knows\\n\\nthe answers.\\n\\nFor example, pattern recognizing. The ANN comes up with guesses while recognizing.\\n\\nThen the teacher provides the ANN with the answers. The network then compares it\\n\\nguesses with the teacher’s “correct” answers and makes adjustments according to\\n\\nerrors.\\n\\nUnsupervised Learning: It is required when there is no example data set with known\\n\\nanswers. For example, searching for a hidden pattern. In this case, clustering i.e.\\n\\ndividing a set of elements into groups according to some unknown pattern is carried\\n\\nout based on the existing data sets present.\\n\\n55\\n\\nArtificial Intelligence\\n\\nReinforcement Learning: This strategy built on observation. The ANN makes a\\n\\ndecision by observing its environment. If the observation is negative, the network\\n\\nadjusts its weights to be able to make a different required decision the next time.\\n\\nBack Propagation Algorithm It is the training or learning algorithm. It learns by example. If you submit to the algorithm the example of what you want the network to do, it changes the network’s weights so that it can produce desired output for a particular input on finishing the training.\\n\\nBack Propagation networks are ideal for simple Pattern Recognition and Mapping Tasks.\\n\\nBayesian Networks (BN)\\n\\nThese are the graphical structures used to represent the probabilistic relationship among a set of random variables. Bayesian networks are also called Belief Networks or Bayes Nets. BNs reason about uncertain domain.\\n\\nIn these networks, each node represents a random variable with specific propositions. For example, in a medical diagnosis domain, the node Cancer represents the proposition that a patient has cancer.\\n\\nThe edges connecting the nodes represent probabilistic dependencies among those random variables. If out of two nodes, one is affecting the other then they must be directly connected in the directions of the effect. The strength of the relationship between variables is quantified by the probability associated with each node.\\n\\nThere is an only constraint on the arcs in a BN that you cannot return to a node simply by following directed arcs. Hence the BNs are called Directed Acyclic Graphs (DAGs).\\n\\nBNs are capable of handling multivalued variables simultaneously. The BN variables are composed of two dimensions:\\n\\n1. Range of prepositions 2. Probability assigned to each of the prepositions.\\n\\nConsider a finite set X = {X1, X2, …,Xn} of discrete random variables, where each variable Xi may take values from a finite set, denoted by Val(Xi). If there is a directed link from variable Xi to variable, Xj, then variable Xi will be a parent of variable Xj showing direct dependencies between the variables.\\n\\nThe structure of BN is ideal for combining prior knowledge and observed data. BN can be used to learn the causal relationships and understand various problem domains and to predict future events, even in case of missing data.\\n\\nBuilding a Bayesian Network A knowledge engineer can build a Bayesian network. There are a number of steps the knowledge engineer needs to take while building it.\\n\\n56\\n\\nArtificial Intelligence\\n\\nExample problem: Lung cancer. A patient has been suffering from breathlessness. He visits the doctor, suspecting he has lung cancer. The doctor knows that barring lung cancer, there are various other possible diseases the patient might have such as tuberculosis and bronchitis.\\n\\nGather Relevant Information of Problem\\n\\n\\n\\nIs the patient a smoker? If yes, then high chances of cancer and bronchitis.\\n\\n\\n\\nIs the patient exposed to air pollution? If yes, what sort of air pollution?\\n\\nTake an X-Ray positive X-ray would indicate either TB or lung cancer.\\n\\nIdentify Interesting Variables\\n\\nThe knowledge engineer tries to answer the questions:\\n\\nWhich nodes to represent?\\n\\nWhat values can they take? In which state can they be?\\n\\nFor now let us consider nodes, with only discrete values. The variable must take on exactly one of these values at a time.\\n\\nCommon types of discrete nodes are:\\n\\nBoolean nodes: They represent propositions, taking binary values TRUE (T) and FALSE (F).\\n\\nOrdered values: A node Pollution might represent and take values from {low, medium, high} describing degree of a patient’s exposure to pollution.\\n\\nIntegral values: A node called Age might represent patient’s age with possible values from 1 to 120. Even at this early stage, modeling choices are being made.\\n\\nPossible nodes and values for the lung cancer example:\\n\\nNode Name\\n\\nType\\n\\nValue\\n\\nPollution\\n\\nBinary\\n\\n{LOW, HIGH, MEDIUM}\\n\\nSmoker\\n\\nBoolean\\n\\n{TRUE, FASLE}\\n\\nLung-Cancer\\n\\nBoolean\\n\\n{TRUE, FASLE}\\n\\nX-Ray\\n\\nBinary\\n\\n{Positive, Negative}\\n\\nCreate Arcs between Nodes\\n\\nTopology of the network should capture qualitative relationships between variables.\\n\\nFor example, what causes a patient to have lung cancer? - Pollution and smoking. Then add arcs from node Pollution and node Smoker to node Lung-Cancer.\\n\\nSimilarly if patient has lung cancer, then X-ray result will be positive. Then add arcs from Lung-Cancer to X-Ray.\\n\\n57\\n\\nArtificial Intelligence\\n\\nSpecify Topology\\n\\nConventionally, BNs are laid out so that the arcs point from top to bottom. The set of parent nodes of a node X is given by Parents(X).\\n\\nThe Lung-Cancer node has two parents (reasons or causes): Pollution and Smoker, while node Smoker is an ancestor of node X-Ray. Similarly, X-Ray is a child (consequence or effects) of node Lung-Cancer and successor of nodes Smoker and Pollution.\\n\\nConditional Probabilities\\n\\nNow quantify the relationships between connected nodes: this is done by specifying a conditional probability distribution for each node. As only discrete variables are considered here, this takes the form of a Conditional Probability Table (CPT).\\n\\nFirst, for each node we need to look at all the possible combinations of values of those parent nodes. Each such combination is called an instantiation of the parent set. For each distinct instantiation of parent node values, we need to specify the probability that the child will take.\\n\\nFor example, the Lung-Cancer node’s parents are Pollution and Smoking. They take the possible values = { (H,T), ( H,F), (L,T), (L,F)}. The CPT specifies the probability of cancer for each of these cases as <0.05, 0.02, 0.03, 0.001> respectively.\\n\\nEach node will have conditional probability associated as follows:\\n\\n58\\n\\nArtificial Intelligence\\n\\nApplications of Neural Networks\\n\\nThey can perform tasks that are easy for a human but difficult for a machine:\\n\\nAerospace: Autopilot aircrafts, aircraft fault detection.\\n\\nAutomotive: Automobile guidance systems.\\n\\nMilitary: Weapon steering, target tracking, object discrimination, facial recognition,\\n\\nsignal/image identification.\\n\\nElectronics: Code sequence prediction, IC chip layout, chip failure analysis, machine\\n\\nvision, voice synthesis.\\n\\nFinancial: Real estate appraisal, loan advisor, mortgage screening, corporate bond\\n\\nrating, portfolio trading program, corporate financial analysis, currency value\\n\\nprediction, document readers, credit application evaluators.\\n\\nIndustrial: Manufacturing process control, product design and analysis, quality\\n\\ninspection systems, welding quality analysis, paper quality prediction, chemical\\n\\nproduct design analysis, dynamic modeling of chemical process systems, machine\\n\\nmaintenance analysis, project bidding, planning, and management.\\n\\n59\\n\\nArtificial Intelligence\\n\\nMedical: Cancer cell analysis, EEG and ECG analysis, prosthetic design, transplant\\n\\ntime optimizer.\\n\\nSpeech: Speech recognition, speech classification, text to speech conversion.\\n\\nTelecommunications: Image and data compression, automated information\\n\\nservices, real-time spoken language translation.\\n\\nTransportation: Truck brake diagnosis, vehicle scheduling, routing systems.\\n\\nSoftware: Pattern Recognition in facial recognition, optical character recognition, etc.\\n\\nTime Series Prediction: ANNs are used to make predictions on stocks and natural\\n\\ncalamities.\\n\\nSignal Processing: Neural networks can be trained to process an audio signal and\\n\\nfilter it appropriately in the hearing aids.\\n\\nControl: ANNs are often used to make steering decisions of physical vehicles.\\n\\nAnomaly Detection: As ANNs are expert at recognizing patterns, they can also be\\n\\ntrained to generate an output when something unusual occurs that misfits the pattern.\\n\\n60\\n\\n11. AI Issues\\n\\nArtificial Intelligence\\n\\nAI is developing with such an incredible speed, sometimes it seems magical. There is an opinion among researchers and developers that AI could grow so immensely strong that it would be difficult for humans to control.\\n\\nHumans developed AI systems by introducing into them every possible intelligence they could, for which the humans themselves now seem threatened.\\n\\nThreat to Privacy An AI program that recognizes speech and understands natural language is theoretically capable of understanding each conversation on e-mails and telephones.\\n\\nThreat to Human Dignity AI systems have already started replacing the human beings in few industries. It should not replace people in the sectors where they are holding dignified positions which are pertaining to ethics such as nursing, surgeon, judge, police officer, etc.\\n\\nThreat to Safety The self-improving AI systems can become so mighty than humans that could be very difficult to stop from achieving their goals, which may lead to unintended consequences.\\n\\n61\\n\\n12. AI Terminology\\n\\nArtificial Intelligence\\n\\nHere is the list of frequently used terms in the domain of AI:\\n\\nTerm\\n\\nMeaning\\n\\nAgent\\n\\nAgents are systems or software programs capable of autonomous, purposeful and reasoning directed towards one or more goals. They are also called assistants, brokers, bots, droids, intelligent agents, and software agents.\\n\\nAutonomous Robot\\n\\nRobot free from external control or influence and able to control itself independently.\\n\\nBackward Chaining\\n\\nStrategy of working backward for Reason/Cause of a problem.\\n\\nBlackboard\\n\\nIt is the memory inside computer, which is used for communication between the cooperating expert systems.\\n\\nEnvironment\\n\\nIt is the part of real or computational world inhabited by the agent.\\n\\nForward Chaining\\n\\nHeuristics\\n\\nStrategy of working forward for conclusion/solution of a problem. It is the knowledge based on Trial-and-error, evaluations, and experimentation.\\n\\nKnowledge Engineering Percepts\\n\\nPruning\\n\\nAcquiring knowledge from human experts and other resources.\\n\\nIt is the format in which the agent obtains information about the environment. Overriding unnecessary and irrelevant considerations in AI systems.\\n\\nRule\\n\\nIt is a format of representing knowledge base in Expert System. It is in the form of IF-THEN-ELSE.\\n\\nShell\\n\\nA shell is a software that helps in designing inference engine, knowledge base, and user interface of an expert system.\\n\\nTask\\n\\nTuring Test\\n\\nIt is the goal the agent is tries to accomplish. A test developed by Allan Turing to test the intelligence of a machine as compared to human intelligence.\\n\\n62')],\n",
              " [Document(metadata={'source': '/content/future of ai.pdf'}, page_content=\"The Future of AI or AI for the Future Eunika Mercier-Laurent\\n\\nTo cite this version:\\n\\nEunika Mercier-Laurent. The Future of AI or AI for the Future. Leon Strous; Roger Johnson; David Alan Grier; Doron Swade. Unimagined Futures – ICT Opportunities and Challenges :, AICT-555, Springer International Publishing, pp.20-37, 2020, IFIP Advances in Information and Communication Technology, 978-3-030-64245-7. \\uffff10.1007/978-3-030-64246-4_3\\uffff. \\uffffhal-03194237\\uffff\\n\\nHAL Id: hal-03194237\\n\\nhttps://inria.hal.science/hal-03194237\\n\\nSubmitted on 9 Apr 2021\\n\\nHAL is a multi-disciplinary open access archive for the deposit and dissemination of sci- entific research documents, whether they are pub- lished or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers.\\n\\nL’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.\\n\\nDistributed under a Creative Commons Attribution 4.0 International License\\n\\nThe Future of AI or AI for the Future\\n\\nEunika Mercier-Laurent [0000-0003-2303-7263]\\n\\nUniversity of Reims Champagne Ardenne, Reims, France Chair IFIP TC12 eunika.mercier-laurent@univ-reims.fr\\n\\nAbstract. The third hype of AI and enthusiasm for applying last techniques in all fields raise great interest and some important questions on the future directions in AI research and applications. Guiding by the principle of combing the best from human and computers capacities this chapter lists some important chal- lenges to face and related directions in AI research. Multiple interrelated crises such as natural disasters, pandemics and other generated by humans require new approaches, combining existing techniques and set new directions for research. This chapter presents briefly the Artificial General Intelligence (AGI) concept and the challenges to face in sustainability, smart resources management, future connectivity, industry, agriculture, health, economy and education. The presented vision for the future of AI includes both researchers’ dreams and emergencies.\\n\\nKeywords: Artificial Intelligence, Future, Planet protection.\\n\\n1\\n\\nArtificial General Intelligence or/and AI for Human Purpose?\\n\\nThe third hype of AI triggered some trends, but above all, new definitions such as nar- row AI and large AI, weak and strong AI, unconscious and conscious AI [1,2]. All these new definitions are efforts to split AI into communities, while in fact intelligence is a whole system.\\n\\nLike the first generation of AI founders, some researchers still work on trying to build a machine more intelligent than humans are. They claim to be able to build Arti- ficial General Intelligence [3]. In his provocative video entitled: Machines playing God, Max Tegmark [4] reduces AI to deep learning. However, what he mentions as future work has been already developed since 1970s by researchers in Machine Learning such as R.S Michalski, R. Quinlan (EBG, generalization from examples) and some others [6].\\n\\nSome build humanoid robots, but is it a priority in the world today while we have increasing number of unemployed and homeless? Or for super-intelligent killer-robots and drones [7]?\\n\\nDo we need super-intelligence? What will be the place of humans in the artificial\\n\\nsupra-intelligent society [4]?\\n\\nCan such AI help facing today’s complex challenges, mostly generated by human\\n\\nactivities or influence people to be respectful?\\n\\n2\\n\\nFighting Covid 19 pandemic, managing the economic crisis generated by confine- ment, understanding the new virus and elaborating vaccine are priorities. How can AI assist us in managing the Planet and biosphere protection?\\n\\nAccording to experts, we entered to Anthropocene epoch [8] and it is urgent to mul- tiply the efforts to protect our planet Earth. It requires minimizing our footprint by min- imizing the use of energy [9] and water and minimizing all kind of pollution in order to preserve the air we breathe, water we drink and quality of food. All fields of human activity are involved: growing cities, transportation, technology with race for perfor- mance, agriculture, health, industry, etc. The current situation requires evaluation and monitoring of the impact of human activities.\\n\\nThis chapter discusses two possible futures – AGI and AI for human and planet pur-\\n\\npose.\\n\\n1.1\\n\\nFour generations of AI\\n\\nWhile Aristotle, Archimedes, Descartes and Leibnitz have laid theoretical foundations for AI, Norbert Wiener [10], Warren McCulloch, Walter Pitts, Donald Hebb [11] and Ludwig von Bertalanffy [12] have introduced cybernetics, artificial neural networks and a base for evolutionary algorithms before the official birth of AI in 1956. Alan Turing proposed the famous test in 1950.\\n\\nFirst generation of AI is linked with the beginning of computers. Some call this period Early enthusiasm (1950-1970). First robot, called Perceptron, chess game, LISP, the first AI programming language designed by John McCarthy inspired the work on object programming and triggered the second generation of AI, those of Knowledge- based AI that has begun in the 1960s.\\n\\nObject programming languages, Natural Language Processing (Prolog), various knowledge representation models, Case-based reasoning, Knowledge discovery tech- niques, constraint programming were born between 1970s and 1990. Many successful applications (1980-1994) demonstrated the usefulness of AI that since has been embed- ded in many applications in all fields [13].\\n\\nIn the middle of the 1990s Internet became the star and AI was temporarily shelved.\\n\\nSome talk about AI Winter (1995-2012).\\n\\nThird generation of AI was born in 2012 from the necessity of exploring the expo- nentially growing amount of data generated by among others electronic commerce and social networks. This generation will not be possible without previous research and applications of known AI techniques such as Artificial Neural Networks enhanced with better computer performance, improvement of robots, humanoid robots, drones and In- ternet of Things (IoT).\\n\\nFourth generation is coming. It will combine deep learning exploration of unstruc- tured data and knowledge-based AI to obtain the robust AI systems able to provide decision support and AI Systems as a service [14].\\n\\nIn his video John Launchbury (DARPA) illustrates the differences between three generations of AI (he forgot the first) by the capacities of Perceiving, Learning, Ab- stracting and Reasoning [15]. However, he forgot to include symbolic machine learn- ing, initiated in the early 1970 by Ryszard S. Michalski, John R. Quinlan, Jaime G.\\n\\nCarbonell, Tom M. Mitchell and some others [6]. Fig. 1 presents his 3rd generation (in fact 4th) of AI that will balance all four components.\\n\\nFig. 1. DARPA Third wave of AI, source [15]\\n\\n2\\n\\nArtificial General Intelligence\\n\\nThe founders of the AI fields were largely concerned with the creation of hardware and software acting as human, even more intelligent. Mark Gubrud proposed the expression of Artificial General Intelligence in military context [16]. What is the progress from General Problem Solver (Newell et al, 1959)? Certainly, computers’ performance al- lows quicker processing but the available systems still lack intelligence. Forbes states “AI systems that can diagnose cancers with greater accuracy than human doctors, there are many other fields where specialized artificial intelligence is replicating human-like reasoning and cognition.” [17]. Did Forbes journalist never try googling image search? The quality of diagnosis however depends on the accuracy of data used for learning model and on the quality of learning algorithm. If the accuracy is similar to the results of available search engines, a diagnosis can be erroneous, while experienced oncologist still diagnoses better than those by the best algorithm. Deep learning combined with expert system may give better results. Some work on this kind of applications was done in the late 1980s in the Faculté de Médecine, Paris.\\n\\nAnother limitation of deep learning is the exploring of past data to predict the future.\\n\\nIt may work in linear world, but not in today’s dynamic one.\\n\\nOne of the main actors of AGI is Open AI [18]. They focus on the development of highly autonomous systems that outperform humans and will be beneficial for human- ity. They aim to build safe and beneficial AGI, but also consider their mission has been fulfilled if their work aids others to achieve this outcome. Among their projects, we find the following:\\n\\n3\\n\\n4\\n\\na neural network based generator of music; similar work has been done by Sony CSL [19]\\n\\nComponents of robots, robot hand able to solve Rubik’s cube, robots learning dex- terity\\n\\nexploring multi-agents learning capabilities • automated text generation • learning sentiments\\n\\nThey explore mainly various deep learning algorithms. This research lacks examples of overall projects combining all these elements in an artificial intelligent system. Ex- ploring multi-agent learning capability is a topic of research since the beginning of multi-agent systems in the late 1980s. Automated text generation has been studied and practiced since the invention of Prolog in 1970 by Alain Colmerauer team, Marseille. Military research is more advanced, but most of the projects are confidential. We can only guess what is inside of Kalashnikov robots, UAV and other military advanced equipment [20].\\n\\nBefore designing AGI systems, it is vital to understand what intelligence is and how\\n\\nsuch systems can effectively collaborate with humans.\\n\\nIn 2005 Kurtzweil states, “Singularity is Near”. Robots still have no intuition and\\n\\nare unable to hypnotize human, but is it necessary?\\n\\nSome researchers focus on the simulation of the brain. Is it possible to simulate\\n\\nsomething if we have only partial knowledge about it?\\n\\nNeuroimaging technology can deliver images. Experiments such as specific activity in MRI helps discovering some functionalities [21]. The brain is NOT just a supercom- puter, it is much more than that and works in interaction with the other organs. Accord- ing to multidisciplinary scientists [22], the brain is a system component of our body and interacts with the other organs. Some talk about connection between three “brains” equipped with neurons: brain, heart and gut or stomach, each plays a role in decision making.\\n\\nArtificial neural network is simple implementation of biological neurons, which are\\n\\nmuch more complex.\\n\\nThe Human Brain Project [23] has begun in 2013. Sponsored by European Union it connects scientific and industrial researchers to advance our knowledge in the fields of neuroscience, computing, and brain-related medicine. It is composed of following plat- forms:\\n\\nNeuroinformatics (access to shared brain data) • Brain Simulation (replication of brain architecture and activity on computers) • High Performance Analytics and Computing (providing the required computing and analytics capabilities)\\n\\nMedical Informatics (access to patient data, identification of disease signatures) • Neuromorphic Computing (development of brain-inspired computing) • Neurorobotics (use of robots to test brain simulations)\\n\\nThe same year, 2013, White House announced the Brain Initiative. It is supported by several federal agencies as well as dozens of technology firms, academic institutions, scientists and other key contributors to the field of neuroscience [24].\\n\\nThe impact of such research may be beneficial in medicine, to cure serious diseases\\n\\nsuch as Alzheimer, other brain defects and in psychiatry.\\n\\nIn science fiction, AGI is associated with consciousness, sentience, sapiens and self- awareness. The current research is in its infancy for such capacities. Marketing is much more interested in “sentiment” analysis and eye tracking than in providing us with prod- ucts that we really need. They never ask if someone did not find what he/she looked for. Is it so difficult to do or are they not interested?\\n\\nConsidering that it is more important to empower humanity by combining the best\\n\\nof human and best of computer we focus on AI for human and planet purpose.\\n\\n3\\n\\nAI for Human and Planet Purpose – what we expect from future AI?\\n\\nAnother trend in AI research and applications is collaborative intelligence human-ma- chine. “Computers are incredibly fast, accurate and stupid. Human beings are incred- ibly slow, inaccurate and brilliant. Together they are powerful beyond imagination”. This citation attributed to Einstein proposes certainly better future than those transform- ing human into slave of “intelligent” systems or into “shopping machine”.\\n\\nYears ago AI researchers and practitioners invented and have since experimented with various AI techniques such as natural language programming, expert systems, case based reasoning, constraint programming and multi-agent systems.\\n\\nFig. 2. Available AI techniques, source [9]\\n\\n5\\n\\n6\\n\\nThese techniques combined into hybrid systems and applied knowingly and wisely, allow solving the majority of complex problems generated by present challenges. How- ever, it requires prior deep problem understanding and experience in applying the ade- quate AI techniques.\\n\\nToday AI is “inside” numerous applications in all fields. The future of AI has to\\n\\nconsider all these experiences to progress in right direction.\\n\\nBetween 1995 and today the AI research was awoken to business goals – sell more and quicker. This engine motivated development and improvement of various deep learning algorithms [25]. Most believe that it is possible to solve all kind of problems using deep learning; it has become “general problem solver” of the moment. Applied to navigation data to deduce client experience, for face recognition, eye tracking, chat bots, automated translation these algorithms give satisfactory results if the training set is correctly elaborated and if the algorithm is able to improve itself. For example, DeepL translator can learn from users who can correct the provided result if they know well a given language.\\n\\nNevertheless, sometimes the challenge is to find what is not in data. Life and intel-\\n\\nligence is not about data.\\n\\nIn parallel, research in robotics progressed thanks to the innovation in electronics and miniaturization. The disasters such as Tchernobyl and Fukushima demonstrated the need for small flying robots able to evaluate the damage and act in the places that human cannot access. Similarly, for other risk management such as earthquakes and typhoons, where drones and flying robots provide great help to the human.\\n\\nSurgical robots equipped with vision systems are also of significant help, especially in the situation when high precision is required. Disinfecting robots and vehicles demonstrated their usefulness during Covid 19 pandemics.\\n\\nIndustry 4.0 implemented the principle of collaboration human-machine in co-bots [26] and some factories of the future, such as those of Schneider Electric in Vaudreuil. AI powers the cyber physical systems and digital twins [27].\\n\\nFig. 3. Schneider Factory of the Future, Vaudreuil, source Schneider Electric [28]\\n\\nIntuition and imagination combined with quick access to world base of problem solving give certainly better results than asking Sophia robot. We will still need truly intelligent personal assistants able to learn with the user, not necessary Alexa, Siri or more sophisticated robot.\\n\\nIn the nearest future, the AI approach to problem solving and combining deep learn- ing with knowledge-based AI may bring significant help in many fields. It is very im- portant and much more useful and ethical to build systems combining the best of human and of computer capacities instead of trying to reproduce human intelligence.\\n\\nDeep understanding of inter-influences of human activity on environment may help finding acceptable co-designed solutions for preserving our biosphere and decelerating the Planet decline. (AI for sustainability)\\n\\nCompetition promoted by various ranking systems and research limited to a given field only are the barriers to progress by more collaborative and multidisciplinary re- search.\\n\\nTechnological innovation makes our lives easier. However, the progress without considering the impacts of human activities led to degradation of our living conditions. Many factors affect the sustainability. One of them is quick technological progress, considered as powerful engine of economy. It brings many benefits for humanity, but contributes also to Planet Crisis. Computers, smartphones, IoT and other devices are quickly outdated. The combination of various communicating software requires “up- to-date” hardware to run correctly. Most of hardware are not eco-designed and need raw material that has become scarce. Despite the large introduction of Corporate Social Responsibility, some companies still practice planned obsolescence to generate more revenues.\\n\\nSocial networks and various applications generate an exponential amount of data stored in data centers that need cooling. Fortunately, some apply circular energy to re- duce impact on environment, but still those in Scandinavian countries clearly contribute to the rise of temperature and melting of ice.\\n\\nWhat do we have that can be reused/improved and what needs to be invented?\\n\\n3.1 Challenges to face\\n\\nAmong the challenges for humans and for planet: fix existing disaster and make IT smarter and greener.\\n\\nAfter health, time is probably one of the most important assets. Traditional IT can be smarter with AI inside. In her invited talk to 6th AI4KM, Helena Lindskog pointed out the importance of being “time rich” today and set up the challenge for AI is to help us have more time for innovation, for family, to discover and enjoy nature and other activities [29]. Make the IT friendly and intuitive, greening the software, smartening data centers are among the wishes. Computer learning with its user, his/her interests is the opposite of what we have today - pushing all kind of advertisement. AI should be able to understand the content of my emails, clean my email box and answer easy ones. I dream about intelligent assistant helping me find the file talking about a given topic or drawing/image to illustrate what I am writing.\\n\\n7\\n\\n8\\n\\nIntelligent electronic commerce with immediate association of offer and demand, without categories; client describes his/her wishes or present a picture to “say” what I want.\\n\\nAssociation of offer and demand such as job searching for me in my place, like res- taurant on google map, service provider, spare pieces, repair café, 3D printer close to my place.\\n\\nImagine I switch on my computer or other device and immediately I get the results\\n\\nof relevant search proposed by embedded intelligence, knowing my profile.\\n\\nIntelligent translator exists already, eg https://www.deepl.com/translator. The pro- nunciation of the translated text will be beneficial. When traveling, the real-time accu- rate conversation translator is of considerable help. Can such translators in the long term prevent us from learning languages (and using our brain)? Will our brain become lazy?\\n\\nToday we have to face important challenges, such as those of UN for sustainability. AI can directly improve goal 6, 7, 9, 11 and next from Fig. 4. It may also influence improvement of others.\\n\\nFig. 4. Sustainable Development Goals, source un.org\\n\\nAccording to Anthony Wang [30] it is over 70 various principles for AI ethics. One of the most repeated is that AI has to be beneficial for humanity. I add “and the planet”.\\n\\n3.2\\n\\nFrom “intelligent” assistants to helping the user\\n\\nIntelligent assistants can take several forms:\\n\\npersonal, working for one user • for children protection • linked to a company website or a platform answering the clients questions • “street” assistant helping visitors/tourists\\n\\nin shop assistant, especially in big one guiding clients to the products they need • inside of museum, expositions • for people with disabilities All have access to available resources and are designed to provide the user with imme- diate and relevant information, help, advice or a solution to a given problem. They have to “know” what the users do not know.\\n\\nMy dream is to have personal assistant able to learn my interests, scan all available and fake checked resources and provide me with the timely relevant information and this way participate in opportunity “hunting”. It has to be capable of immediate finding a document in my computer related to the topic I ask, picture or drawing. Producer of Korean TV series “My holo love” imagined such an assistant, able to learn from user and improve its “knowledge”. It is only visible and available for its user wearing special glasses. The only problem to solve is to preserve the user’s intimacy when he/she is talking to invisible assistant [31].\\n\\nThe relevance of answers from automated assistant depends today on the quality of data and of learning algorithm. Both chat bots and connected assistants are involved. However, assistants available today need the capacity of “understanding” the question and provide relevant and verified answer. Equipped with multimodal interface it should follow me in my travels and “talk” languages that I do not talk, help finding the right word or expression in languages I talk, provide help for writing in other than the lan- guages I master. Many comparator applications are available such as for tourism or insurance, but no one provides the optimal offer for the end user, because they work for their clients.\\n\\nHaving the access to medical cases, an intelligent assistant can provide the basic help\\n\\nbefore going to the doctor. It can also teach the user the preventive actions.\\n\\n3.3 AI in school\\n\\nDeep learning facilitated the development of intelligent assistants. For the most, they are helpful; however, they do not encourage people to think. Thinking and ability to use alternative cognitive approaches makes the difference between someone who just follow the school program and those able to solve problems using knowledge and lim- ited resources. Thinking “without borders between fields” and ability to find alterna- tive, greener solutions. Many professions change with technology and school has to prepare students for being flexible.\\n\\nE-learning was introduced in schools at the beginning of 21st century. Many courses (MOOC) are now available online opening access to large knowledge at all levels and providing education to the rural zones without schools and to developing countries.\\n\\nDigitalization of educational activities introduced tablets and now robots in schools. Educational games make learning more fun and attractive. What is missing is a sort of “super professor” able to evaluate a level of the student and to propose the best suited material from the web matching to the student profile and request. Still intelligent as- sistants can provide help explaining topic or exercise.\\n\\n9\\n\\n10\\n\\nDuring Covid19 lockdown children had to learn at home. An i-teacher detecting dif- ficulties of each, explaining and challenging is certainly helpful. It requires AI not only tracking activity but able to “understand” what child is doing and when he/she needs help. Challenging assistant – not switching off the brain, but stimulating thinking and suggesting various approaches.\\n\\nMost of schools follow the teaching program, but few take care of the specific chil- dren talents with the aim of helping them to choose their professional future. We can imagine an AI –based Future Advice office combining the adequate AI techniques to evaluate talents, propose a game to test some professions with projection how it will evolve during the next ten or more years. Several years ago, the entertainment park Kidzania [32] was open for children to make them try various professions. The IT and AI-related professions have to be added to the spectrum of those proposed.\\n\\n3.4 AI & food\\n\\nIn the age of processed food, the related industry aims to produce more food for less price regardless of nutritional quality. Pesticides and artificial fertilizers are massively used on the pretext of a duty to nourish the planet. Doing this they destroy and impov- erish the soil, pollute water and pests become more resistant. Globalization allowed pests to travel longdistances hidden in goods; some are very harmful for local environ- ment and difficult to fight with, because they are often unknown, as COVID is. Ac- cording to Intel and some others, the world will need to produce 50 percent more food by 2050. Nevertheless pushed by food business, the food waste today is evaluated around 50% only in the United States [33]. Food is lost or wasted for a variety of rea- sons: bad weather, processing problems, overproduction and unstable markets cause food loss long before it arrives in a grocery store, while overbuying, poor planning and confusion over labels and safety contribute to food waste at stores and in homes. Un- eaten food also puts unneeded strain on the environment by wasting valuable resources like water and farmland. Reducing food waste by just 15 percent could provide enough sustenance to feed more than 25 million people, annually. Food loss occurs on farms for a variety of reasons. To hedge against pests and weather, farmers often plant more than consumers demand. Food may not be harvested because of damage by weather, pests and disease. Market conditions off the farm can lead farmers to throw out edible food because the shape is out of the norms. If the price of produce on the market is lower than the cost of transportation and labor, sometimes farmers will leave their crops un-harvested. This practice, called dumping, happens when farmers are producing more of a product that people are willing to buy, or when demand for a product falls unex- pectedly.\\n\\nDuring the COVID-19 pandemic, for example, farmers lost a major portion of their business due to restaurant and school lunchroom closures as well as a lack of workforce for gathering.\\n\\nIn the recent video entitles Sustainability European Space Agency (ESA) shows-up the satellite images demonstrating how far COVID affects the food production. They deplore the impact of border closures on the harvesting of fruits and vegetables [34]. Yet, because of the economic crisis, many people are partially unemployed and this\\n\\nworkforce is not correctly managed. AI can help manage all kind of resource, but this potential is underused today.\\n\\nAI system is able to detect from satellite images the zones to harvest or infected ones. Thinking – They have images of underexplored zones that can be cultivated to produce food locally. Some use harvesting robots; however, it should be cost effective.\\n\\nVarious AI techniques are already used to help farmers. Drones “decide” when cof- fee and oranges are ripe enough for picking (Hawaï, Bresil). They can also detect pests’ invasion. Hopefully in the near future, we will have devices not only detecting, but selectively (pest recognition) absorbing them instead of using pesticides. Logistics planned with constraints programming to optimize time and trucks should be more widely used. Sensors connected to automated watering help optimizing the use of wa- ter.\\n\\nGreenhouse tomatoes grow in a bed of pulped coconut husks, a nutrient-free envi- ronment that allows the growers controlling what goes into the plant. Sensors monitor the fruit’s progress toward perfect ripeness, adjusting light to accelerate or slow the pace of maturation. However, this kind of farming requires considerable processing power.\\n\\nWe can imagine another approach combining the knowledge about soil and environ- ment with knowledge about crop rotation, association of vegetable/fruits to avoid pests attack, chose the right period for sow and plant in function of available seeds. Such farmer advisor programmed applying green software principle can be powered with solar energy.\\n\\nFig. 5. Connected Farmer, source [35]\\n\\n11\\n\\n12\\n\\nWhile the harvesting machines have been used for years, they are now replaced by robots. Robotic harvesting equipment, partially in response to labor gaps that have left farmers scrambling to harvest crops like fruits and berries. Harvest Croo berry picker operate on the basis of machine vision and sensor fusion to “see” where harvest fruits and berries are. They use sophisticated directed movements to pick precisely [36].\\n\\nUnmanned aerial vehicles or drones being outfitted with precision sensors, in order to run the fields and get the data that’s needed. These airborne surveillance devices can look for stunted crops, signs of pest or weed damage, dryness and many other variables that are part of the difficulty of farming in general. With all of this data in hand, farmers can enhance their production models and their strategies across the lay of the land to decrease risk, waste and liability [37].\\n\\nMain challenge is still protecting plants against weeds and various kinds of pests outdoors. Another alternative is to grow in greenhouses, which is being done as well, but some of the most amazing farming technology is being deployed outside. The “See & Spray” machines are an excellent example of combining artificial intelligence and computer vision [38]. Deploying mobile technologies with AI and computer vision built in, farmers can find weeds and eradicate them, instead of blanket spraying an entire crop. That makes the food cleaner, and it saves enormous amounts of money. It’s just another example of real new technologies that are having a dramatic impact on yields and everything else.\\n\\nThe transition from conventional agriculture practices into a sustainable mode of\\n\\ngrowing food can lead to social and economic equity and a healthy environment. In the nearest future we expect connecting the modern, AI powered systems with an- cestral knowledge about how to cultivate without pesticides and with natural fertilizers. Maybe the future agriculture will be not about large farms but smart ones. Autonomous houses and farms can be monitored by AI ensuring optimized use of locally available resources and minimized impact on the planet.\\n\\n3.5 AI for risks/crisis management\\n\\nIrresponsible human activities have led to Planet disaster. The increased frequency and magnitude of natural risks and those caused by humans requires new, quicker and more effective ways of managing them. Frequent fires mostly of criminal origin destroy forest, our lungs and its ecosystem. In many cases, AI demonstrated its potential to help managing these disasters [39, 40]. Earth observation systems can be applied for various risk management, whether caused by humans or natural risks. Fukushima, tsunami, earthquake, flooding require quick organizing of emergency actions involving people, hospitals, vehicles and other resources aligned to given disaster.\\n\\nFacing Covid19 pandemic has multiple effects on health, on jobs, on economy, ed- ucation, agriculture and environment. The researchers and medical staff are learning from examples and experiments. Handling such crises require related knowledge and smart managing of existing resources, real-time planning of hospital staff, equipment and quick finding of vaccine. Existing AI techniques can help in first stage diagnosis online to refer the patient to the right doctor. AI can optimize the allocation of beds,\\n\\nequipment, staff in function of their competencies and specialties. AI support explora- tion of clinical trials and accelerate finding of vaccine. Evaluation of efficiency of health policy [41] can be used as model for pandemic management and evaluation of induced risks.\\n\\nIn his TED Talk from 2015, Bill Gates states that pandemics are the greatest threat we will have to face in the future [42]. He said we need global alert system, technology, expertise, collaboration medical-military, simulation and diagnostic. Alternative is the understanding of real causes of pandemics and other serious risks in aim of avoiding and preventing them. Investments are necessary to fix the problem once it happens, but informed prevention avoids human, economic and environmental losses.\\n\\n3.6 AI for sustainability\\n\\nParadoxically AI needs devices, mostly designed with “planned obsolescence” princi- ple to preserve continuous business. While “green software” has been slowly intro- duced, few really apply these principles. All fields are concerned and evaluation of impacts before doing should become mandatory. AI may play a greater role in simula- tion before doing, choosing the right raw material and design easy to update or to recy- cle [43]. IT and Information Systems should be eco-designed, which is not the case today. The management and storage of big data generated massively can be controlled by using conceptual models instead of storing all data. AI can effectively support opti- mization of hardware and software design. Neural computers are not new, but with the new hype researchers works again on this architecture. We do not know yet how far it can contribute to sustainability because nobody is in charge of this aspect. Similarly, designers of quantum computers focus on computation power and do not consider the eco-design.\\n\\nRace for performance and connectivity pushes designers to 5G, which is not neces- sary the best choice because of the impact on living. AI-based decision support systems connected with innovative design methods such as TRIZ [44] may help designers find- ing alternative solutions. The trendy design thinking, known longtime before as “inno- vation with clients” [45] or extreme programming [46], has to integrate these principles. Maybe researchers and designers by nature (biomimetics) take into consideration these aspects. Nevertheless, AI offers a spectrum of techniques helpful in optimization and verification of environmental and other constraints.\\n\\nThe case of Smart City, mainly based on technology offer a playground for water and energy (including renewable ones) optimization, smart eco-buildings, green and optimized transportation of people and goods, opportunity finding (job, service, train- ing).\\n\\nAll human activity that have affected our biosphere and planet is concerned and AI can help minimizing this impact and do things smarter [8]. Preserving the balance in our biosphere will be beneficial for all living today and tomorrow.\\n\\n13\\n\\n14\\n\\n3.7 AI and the Financial Service Industry\\n\\nWithout any doubt AI has a huge impact on the future of the financial services industry. This is widely acknowledged and described in many articles and reports [47, 48, 49, 50, 51, 52, 53], labelling it “revolutionizing / transforming / disrupting the industry”. In an already highly digitized sector, AI adds an extra dimension in a number of areas. It offers new opportunities, both for new players and for incumbent institutions. It also brings threats, in particular to the established parties who have a legacy in their infra- structure and services.\\n\\nOften mentioned areas where AI is transforming the industry are:\\n\\nFraud detection and risk management. AI is extremely helpful in detecting and iden- tifying fraudulent transactions, learning from past spending behaviors.\\n\\nRegulatory compliance. The financial services industry is heavily regulated. AI can help an institution to keep up to date with changing regulations and to be compliant with rules like Know Your Customer (KYC), anti-money laundering regulations, etc.\\n\\nCustomer experience. Customers nowadays expect a more personalized offer of ser- vices. Developments like chatbots that, with the help of AI, can identify the individ- ual customer, “understand” and interpret his or her emotions via voice and/or facial recognition and subsequently can offer “tailormade” advice found in existing data- base.\\n\\nManaging personal finances. With the move to mobile banking and the use of wal- lets, AI offers opportunities to help customers make smart decisions on spending, saving and investing money. This can help improve the “financial health” of many people. Managing finance, AI simply accumulates all the data from your web and other footprints and creates your spending graph.\\n\\nInvestment advisory. Here predictive analytics and recommendation engines turn into digital advisors that can even fully automate purchase and management of in- vestments. The result is no longer a need for financial advisors / relationship man- agers.\\n\\nPredict stock performance. Trading and investment depend on the ability to predict the future of (stock) markets accurately. Predicting this in a consistent way seems impossible for humans. Deep learning algorithms could perhaps achieve this as a result of using massive amounts of market data complemented with real-time eco- nomic and political data. An example of the trading issue is high frequency trading (HFT). While not new in itself, since a little over a decade the execution time of transactions has moved from seconds to milli- and microseconds. HFT is a form of algorithmic trading which can arguably be labelled as an AI implementation of trading. The profits of HFT however seem to have passed their peak performance. Besides limitations caused by the infra- structure (hardware, networks) also the limitations of the algorithms play a role. Per- haps deep learning could give HFT a new boost. In [54, 55], as in many other articles, benefits but also risks are listed. Algorithms are not infallible, understanding how the algorithms and neural networks predict specific outcomes is difficult if not impossible\\n\\n(black box), high investments needed which could result in only a few players surviving (compare the world of the big tech firms).\\n\\nAll these developments will have a major effect on the workforce, not only in the number of employees but also in the types of jobs and skills needed for them. Another major impact is a change in the players in the financial landscape. The incumbents (current banks, insurance companies, pension funds, investment companies, etc) al- ready face strong competition from the so-called fintechs in some areas. But also the bigtechs (Amazon, Google, Facebook, Alibaba, …) are entering the financial services industry. We seem to move towards a platform economy where the bigtechs have an advantage with their strong customer base to easily offer additional (financial) services to their customers, knowing what their customers want thanks to AI.\\n\\nBenefits of AI in the financial services industry are clear but so are the risks. Not only for a single financial institution but also for customers, for the trust in the financial system and for the financial stability in a country or even on a global scale. That is why in [48] it is stated that “Financial firms using artificial intelligence (AI) should adhere to principles of sound and controlled business operations. A responsible use of AI in financial services means that firms should pay due attention to the soundness, account- ability, fairness, ethics, skills and transparency aspects of the applications they de- velop.”\\n\\n4 What perspectives for research – what humanity expects\\n\\nfrom Future AI?\\n\\nThroughout this chapter we have already mentioned some needs, expectations and wishes for the future AI. In term of disruptive innovation, the current AI represents a little progress since its beginning. The AI techniques that we have today has been in- vented before and are recently extended to more powerful and to new applications. The wise combination of them in the hybrid systems allows facing today’s challenges. What AI we need today and tomorrow?\\n\\nMichael Zeldich (Artificial Labour Leasing) designs subjective robots. Such robots used for example for house cleaning learn with their users and are limited to perform what users ask for. He believes that our duty is to create an artificial society able to resist the planet’s destruction and preserve our civilization.\\n\\nBefore we invent something else or all die, there are two main options for AI: to continue to progress with AGI or work on the challenges we have to face and then derive new theories and invent new approaches and techniques. The first requires deep understanding of how the human body works, how the organs are connected, how we interact with our environment. What future humans can expect from the AGI world?\\n\\nAccording to WEF [53] “Nevertheless, it is evident that more research needs to be done in order to better understand the opportunities and challenges brought about by the eventual mass adoption of AI in Financial Services. For instance, how can finance firms open up the ‘black box’ of AI and facilitate more explainable and transparent applications? As AI is becoming increasingly autonomous, what will the roles of hu- mans be and how would an effective human-in-the-loop AI system manifest itself? What\\n\\n15\\n\\n16\\n\\nare some socioeconomic repercussions and ethical implications of AI-induced biases and risks? How can regulators and policymakers harness technology solutions to ef- fectively regulate and supervise AI in finance?”\\n\\nThese remarks about research trends, regulators and policymakers do not only apply to the financial industry but also to many others if not all sectors where AI is having or will have a major impact. So research is also needed to help regulators and policymak- ers, especially by providing them with simulators.\\n\\nThe issue of confidentiality of personal data is not easy to deal with. The same ap- plies to our navigation and tracking data. Many websites refuse of access to information if the user does not accept them. However, it is much more relevant to obtain the right information from the user by asking instead bombing him/her with cookies and all sort of add-ons. AI can do much more than just analyze the data. Nevertheless, it requires a different way of thinking.\\n\\nAI research should be multi- and interdisciplinary because intelligence is. We still\\n\\nneed to progress in comprehension of our brain/body capacities.\\n\\nFuture AI research should balance needs and ambitions. The Covid 19 crisis clearly demonstrated that collaboration may lead to better and quicker results. World experi- ence in AI applications for solving complex problems should be available allowing finding immediately the solution for the problem someone has to solve.\\n\\nInformation processing, still conceived using traditional methods such as categori- zation, “data thinking”, processes should evolve not by adding AI layer, but “AI think- ing”. We need systems able to adapt the configuration automatically working with the user, able to find immediately the file, image or video the user look for.\\n\\nHardware and software should be eco-designed and easy to recompose/recycle. AI based simulators help finding right components, minimize and “smartize” the software for the minimal energy use. MIT offers Climat Interactive simulator [56].\\n\\nWe need smart AI-powered search engines. For example, the EU database Cordis is a real “goldmine” containing information about the funded research projects and their results that may inspire researchers, industrial people and investors but it need to be equipped with smart, business free, search engine for immediate finding what visitor is looking for. It can be also useful for exploring the available results for quicker progress, for quick and relevant access to references in a given field.\\n\\nWe need decision support systems rather than fully automated decision making, that\\n\\nare not 100% reliable in case of missing data in critical situations.\\n\\nMany challenging research problems can be found in sectorial applications such as in agriculture, health, education, banking and others. In the nearest future we expect connecting the modern, AI powered systems with ancestral knowledge about smart farming.\\n\\nHealth offers a great opportunity for AI research, especially in understanding and\\n\\nprevention of new viruses and serious and other diseases.\\n\\nEducation is among the pillars of modern society. Existing MOOC, e-learning and other online teaching and training systems may be improved by interactive e-teacher, however we still need a talent detector able to encourage learners to think and direct them to the area they are gifted for.\\n\\nPlanet protection, smart sustainability, innovation management offer also interesting\\n\\nchallenges.\\n\\nDetecting and tracking cyber and other criminals, identity thieves or global security\\n\\nare just a few fields of exploration for AI.\\n\\nMultidisciplinary research without “borders” between fields and full exploring of all machine learning techniques, including symbolic, may help in elaborating of general solvers. Of course, this requires evolution of research evaluation criteria.\\n\\nIn this context, is it necessary to build a machine able of consciousness, sentience,\\n\\nsapiens and self-awareness?\\n\\nInstead of splitting AI lets connect the research fields for more spectacular results\\n\\nand for human purpose.\\n\\nWe expect a global smartening of AI and IT researchers.\\n\\nAcknowledgement\\n\\nI would like to thank Leon Strous for his contribution to paragraph 3.7 AI and the Fi- nancial Service Industry.\\n\\nReferences\\n\\n1. Jajal T. Distinguishing between Narrow AI, General AI and Super AI, May 2028, https://me- dium.com/@tjajal/distinguishing-between-narrow-ai-general-ai-and-super-ai- a4bc44172e22\\n\\n2. Bersini H.: The two A.I. – conscious and unconscious, Keynote at Data Driven Innovation, Rome, 2018\\n\\n3. Artificial General Intelligence 4. Tegmark M.: Life 3.0: Being Human at the Age of Artificial Intelligence (2017). 5. Tegmark M. Machines playing God: How AI will overcome humans, video, May 2018 https://www.youtube.com/watch?v=p9eLpRbRk4c\\n\\n6. Michalski R.S Machine Learning 7. Ten secret military\\n\\ntechnologies,\\n\\nvideo\\n\\nJune\\n\\nhttps://www.youtube.com/watch?v=LLYPDJTLr4I\\n\\n8. Mercier-Laurent E. The Innovation Biosphere – Planet and Brains in Digital Era, Wiley, 2015\\n\\n9. Kayakutlu G. and Mercier-Laurent E.: Intelligence in Energy, Elsevier, 2017 10. Wiener N.: The Human Use of Human Being, Cybernetics and Society, Houghton Mifflin Company, Boston, 1950\\n\\n11. Johnson R.C.: Cognizers: Neural Networks and Machines that think, Wiley, 1988 12. von Bertalanfy General System Theory, 1968 13. Rauch-Hindin W. B.: Artificial Intelligence in Business, Science and Industry, Prentice- Hall, 1986\\n\\n14. Mercier-Laurent E. Implementing Horizon Europe, 11th Innovation Summit, Feb 2020\\n\\n17\\n\\n2019,\\n\\n18\\n\\n15. Launchbury\\n\\nJ.: DARPA\\n\\nperspective\\n\\non Artificial\\n\\nIntelligence,\\n\\nhttps://www.youtube.com/watch?v=-O01G3tSYpU\\n\\n16. Gubrud M. 17. Naveen J.: How far we are from achieving General Artificial Intelligence?, Forbes, Jan 10, 2019.\\n\\n18. Open AI https://openai.com, last accessed 2020/06/07 19. Pachet F. research https://www.francoispachet.fr/ last accessed 2020/06/07 20. Military (and others) robots https://www.youtube.com/watch?v=OEIeS12TcWU, February 2018\\n\\n21. Le Bihan D.: Looking inside the brain: The power of neuroimaging. Princeton University Press, Princeton, 2014\\n\\n22. Rajvanshi A.K. The Three Minds of the Body – brain, heart and gut, Speaking Tree, India, May 2011\\n\\n23. https://www.humanbrainproject.eu/en/ last accessed 2020/06/07 24. https://www.darpa.mil/program/our-research/darpa-and-the-brain-initiative, last accessed 2020/06/07\\n\\n25. Deep learning algorithms https://www.predictiveanalyticstoday.com/deep-learning-soft- ware-libraries/, last accessed 2020/06/07\\n\\n26. Co-bots https://www.safran-group.com/media/20151218_cobots-collaborative-robots, last accessed 2020/06/07\\n\\n27. Monsone C., Mercier-Laurent E.: Ecosystem of Industry 4.0 – Combining Technology and Human Power, MEDES 2019\\n\\n28. Schneider Future Factory https://www.se.com/fr/fr/about-us/newsroom/actualites/le-site- du-vaudreuil-de-schneider-electric-labellise-vitrine-industrie-du-futur-c155-636ff.html 29. Lindskog H.: Globalization - Understanding the correlations between attitudes towards glob- alization, time, resources and Financial Resources, Invited talk 6th Artificial Intelligence for Knowledge Management on Ijcai 18, Stockholm, Sweden 2018. Extended version will be published in Springer AICT-571, 2020\\n\\n30. Wang A. Ethics and Regulation of Artificial Intelligence, Keynote to NITC 2019, Colombo, Sri Lanka, October 2019\\n\\n31. My holo love https://www2.dramacool.movie/my-holo-love-episode-1.html 32. Kidzania https://kidzania.com/en 33. Food waste 2020/05/16 accessed\\n\\n34. A\\n\\nsustainable\\n\\nFuture,\\n\\nEuropean\\n\\nSpace\\n\\nhttp://www.esa.int/ESA_Multimedia/Videos/2020/05/A_sustainable_future, May 2020 35. Precision Farming – sowing the seeds of new agricultural revolution, https://cordis.eu- ropa.eu/article/id/400295-precision-farming-sowing-the-seeds-of-a-new-agricultural-revo- lution\\n\\n36. Harvest croo https://harvestcroo.com/ last accessed 2020/05/29 37. Gonzalez-De-Santos P., Fernández R., Sepúlveda D., Navas E., Armada M.: Unmanned Ground Vehicles for Smart Farm, February 2020, DOI: 10.5772/intechopen.90683\\n\\n38. See & Spray machines http://smartmachines.bluerivertechnology.com/ last accessed 2020/05/29\\n\\n39. Mercier-Laurent E.: Preventing and facing new crisis and risks in complex environment, IJMDM, 2018, Vol 17, N°2\\n\\n2017\\n\\nAgency,\\n\\n40. L’Heritier, C., Imussaten A., Harispe S., Dusserre G., Roig B.: Identifying Criteria Most Influencing Strategy Performance: Application to Humanitarian Logistical Strategy Plan- ning, Chapter in Information Processing and Management of Uncertainty in Knowledge- Based Systems. Applications, DOI 10.1007/978-3-319-91479-4_10\\n\\n41. Evaluation of efficiency of health policy https://www.futura-sciences.com/sante/actual- ites/coronavirus-coronavirus-ia-evaluer-efficacite-politiques-sanitaires-80992/\\n\\n42. TED Talk Bill Gates, 2015 https://www.youtube.com/watch?v=QdSyKEBiOnE 43. Mercier-Laurent E.: AI for Innovation Ecosystems 44. TRIZ https://triz.org/triz 45. Amidon D. Innovating with clients, Chapter in The Innovation Strategy for The Knowledge Economy, Butterworth Heinemann, 1997\\n\\n46. Extreme programming http://www.extremeprogramming.org/ 47. BAFIN2018 – Big Data trifft auf künstliche Intelligenz, Herausforderungen und Implika- tionen für Aufsicht und Regulierung von Finanzdienstleistungen, Bundesanstalt für Fi- nanzdienstleistungsaufsicht (Bafin), 2018, https://www.bafin.de/SharedDocs/Down- loads/DE/dl_bdai_studie.pdf?__blob=publicationFile&v=3\\n\\n48. DNB2019 – General principles for the use of Artificial Intelligence in the financial sector, De Nederlandsche Bank (DNB), 25 July 2019, https://www.dnb.nl/en/binaries/Gen- eral%20principles%20for%20the%20use%20of%20Artificial%20Intelli- gence%20in%20the%20financial%20sector2_tcm47-385055.pdf\\n\\n49. FinTech2019 – How Data Analytics Backed By AI And ML Is Transforming The BFSI Sector, FinTech News, Neeraj Goyal, 23 August 2019, https://www.fintechnews.org/how- the-financial-industry-is-affected-by-ai-and-ml/\\n\\n50. Infosecurity2019 – How AI is Revolutionizing the Banking Sector, Infosecurity Group, Ol- iver Smith, 24 December 2019, https://www.infosecurity-magazine.com/opinions/ai-revo- lutionizing-banking/\\n\\n51. Maruti2020 – 5 Ways AI is Transforming the Finance Industry, Maruti Techlabs, 2020, https://marutitech.com/ways-ai-transforming-finance/\\n\\n52. WEF2020-1 – AI has started a financial revolution - here's how, World Economic Forum (WEF), 04 February 2020, https://www.weforum.org/agenda/2020/02/how-ai-is-shaping-fi- nancial-services/\\n\\n53. WEF2020-2 – Transforming Paradigms - A Global AI in Financial Services Survey, World / University of Cambridge, January 2020, http://www3.wefo- Economic Forum rum.org/docs/WEF_AI_in_Financial_Services_Survey.pdf\\n\\n54. Peter Akioyamen, Neural Networks & Deep Learning — The Revival of HFT? https://to- wardsdatascience.com/neural-networks-deep-learning-the-revival-of-hft-2bc2c271fba2 , last accessed 7 June 2020\\n\\n55. Shobhit Seth, The World of High-Frequency Algorithmic Trading, https://www.in- vestopedia.com/articles/investing/091615/world-high-frequency-algorithmic-trading.asp , last accessed 7 June 2020\\n\\n56. MIT Sloan simulator Climat Interactive https://vimeo.com/359091159\\n\\n19\")],\n",
              " [Document(metadata={'source': '/content/future of A.I intelligence.pdf'}, page_content='The Future of Artificial Intelligence\\n\\n“Every major player is working on this technology of artificial intelligence. As of now, it\\'s benign... but I would say that the day is not far off when artificial intelligence as applied to cyber warfare becomes a threat to everybody.”\\n\\nTed Bell, Bestselling novelist and Writer-in- Residence at Cambridge University\\n\\nIf you use technology today, a smartphone, a computer, or any connected electronic device, then you are bound to be using artificial intelligence as part of the embedded software of that technology. Here are just a few commonplace examples:\\n\\nGoogle search • • Netflix or Amazon predictive analytics as to purchases • Navigation apps like Waze (owned by Google), Google Maps, MapQuest (yes, remember this one?)\\n\\nVoice recognition (Siri, Alexa, Cortana, again Google)\\n\\nModern video games: Super Mario Bros., NBA 2K (my son\\'s favorite), Call of Duty, etc. Fraud detection Translation software\\n\\n•\\n\\nBehind all of this technology is a complex set of algorithms, central processing units (CPUs), and computer servers with increasing levels of sophistication that are designed to accelerate that input and output of information, increasing the quality, complexity, and volume of interactions with our devices, and enhance our quality of life.\\n\\nCurated by Andrew Boyarsky, MSM, PMP Clinical Associate Professor, and Academic Director of the MS in Enterprise Risk Management, Katz School of Graduate and Professional Studies, Yeshiva University\\n\\nWe are already living in the world of AI. Read the resources below in order to engage the past, present, and future of this world from a variety of perspectives.\\n\\nSo, what comes next? Where are we headed with AI, and what level of responsibility do the designers and providers have with managing AI technology? Will we control AI technology or will it control us? How do we handle the economic\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n1\\n\\nramifications that are likely to be added to the existing stresses in our local, national, and global communities?\\n\\nTo come back to the title of this edition of Nexus, can we make AI, robots, etc. into the useful tools we intend them to be, with mensch-like attributes; or will it be a Golom that will rage beyond our control? I very much look forward to our community discussion on this topic.\\n\\nAndrew Boyarski\\n\\nTable of Contents\\n\\nThe Basics: What is AI?................................................................................................................................................................................... 3\\n\\nWhat’s the Difference between Artificial Intelligence, Machine Learning, and Deep Learning? ............................................................... 3\\n\\nMichael Copeland, Nvidia Blog ................................................................................................................................................................... 3 From Bust to Boom ................................................................................................................................................................................ 3 Machine Learning—An Approach to Achieve Artificial Intelligence ...................................................................................................... 4 Deep Learning — A Technique for Implementing Machine Learning .................................................................................................... 4 Thanks to Deep Learning, AI Has a Bright Future................................................................................................................................... 5\\n\\nNext Steps: How is AI Impacting Society? ..................................................................................................................................................... 5\\n\\nRabbinic Artificial Intelligence? Babylonian Talmud; Sanhedrin 65b ......................................................................................................... 5\\n\\nHow Judaism Predicted the First Humanoid Robot, Mark Goldfeder, CNN ............................................................................................... 5\\n\\nThe Robot Revolution Will Be the Quietest One, Liu Cixin, The New York Times ...................................................................................... 7\\n\\nThis isn’t crying wolf: Machines will take white-collar jobs during the next administration, Martin Ford, Linkedin ................................. 8\\n\\nThe Hype—and Hope—of Artificial Intelligence, Om Malik, The New Yorker............................................................................................ 9\\n\\nDeep Dive: What are the Next Frontiers and Further Implications of AI? ................................................................................................. 11\\n\\nDeepMind and Blizzard Open StarCraft II as an AI Research Environment, Oriyal VInyals, Stephen Gaffney, Timo Ewalds; DeepMind 11\\n\\nNeuroscience-Inspired Artificial Intelligence, Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, Matthew Botvinick; Neuron ...................................................................................................................................................................................................... 12\\n\\nThe Past: ................................................................................................................................................................................................... 14 Deep Learning ...................................................................................................................................................................................... 14 Reinforcement Learning ....................................................................................................................................................................... 15\\n\\nThe Present: .............................................................................................................................................................................................. 15 Attention .............................................................................................................................................................................................. 15 Episodic Memory ................................................................................................................................................................................. 16 Working Memory ................................................................................................................................................................................. 17 Continual Learning ............................................................................................................................................................................... 18\\n\\nThe Future: ............................................................................................................................................................................................... 18 Intuitive Understanding of the Physical World .................................................................................................................................... 19 Efficient Learning ................................................................................................................................................................................. 19 Transfer Learning ................................................................................................................................................................................. 20 Imagination and Planning .................................................................................................................................................................... 20 Virtual Brain Analytics .......................................................................................................................................................................... 22 From AI to Neuroscience ...................................................................................................................................................................... 22 Conclusions .......................................................................................................................................................................................... 24\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n2\\n\\nCurated Sources\\n\\nThe Basics: What is AI?\\n\\nWhat’s the Difference between Artificial Intelligence, Machine Learning, and Deep Learning? Michael Copeland, Nvidia Blog\\n\\nArtificial intelligence is the future. Artificial intelligence is science fiction. Artificial intelligence is already part of our everyday lives. All those statements are true, it just depends on what flavor of AI you are referring to.\\n\\nFor example, when Google DeepMind’s AlphaGo program defeated South Korean Master Lee Se-dol in the board game Go earlier this year, the terms AI, machine learning, and were used in the media to describe how DeepMind won. And all three are part of the reason why AlphaGo trounced Lee Se-Dol. But they are not the same things.\\n\\nThe easiest way to think of their relationship is to visualize them as concentric circles with AI — the idea that came first — the largest, then machine learning — which blossomed later, and finally deep learning — which is driving today’s AI explosion — fitting inside both.\\n\\nFrom Bust to Boom\\n\\nAI has been part of our imaginations and simmering in research labs since a handful of computer scientists rallied around the term at the Dartmouth Conferences in 1956 and birthed the field of AI. In the decades since, AI has alternately been heralded as the key to our civilization’s brightest future, and tossed on technology’s trash heap as a harebrained notion of over-reaching propeller heads. Frankly, until 2012, it was a bit of both.\\n\\nOver the past few years AI has exploded, and especially since 2015. Much of that has to do with the wide availability of GPUs that make parallel processing ever faster, cheaper, and more powerful. It also has to do with the simultaneous one-two punch of practically infinite storage and a flood of data of every stripe (that whole Big Data movement) – images, text, transactions, mapping data, you name it.\\n\\nLet’s walk through how computer scientists have moved from something of a bust — until 2012 — to a boom that has unleashed applications used by hundreds of millions of people every day.\\n\\nArtificial Intelligence—Human Intelligence Exhibited by Machines\\n\\nKing me: computer programs that played checkers were among the earliest examples of artificial intelligence, stirring an early wave of excitement in the 1950s.\\n\\nBack in that summer of ’56 conference the dream of those AI pioneers was to construct complex machines — enabled by emerging computers — that possessed the same characteristics of human intelligence. This is the concept we think of as “General AI” — fabulous machines that have all our senses (maybe even more), all our reason, and think just like we do. You’ve seen these machines endlessly in movies as friend — C-3PO — and foe — The Terminator. General AI machines have remained in the movies and science fiction novels for good reason; we can’t pull it off, at least not yet. What we can do falls into the concept of “Narrow AI.” Technologies that are able to perform specific tasks as well as, or better than, we humans can. Examples of narrow AI are things such as image classification on a service like Pinterest and face recognition on Facebook.\\n\\nThose are examples of Narrow AI in practice. These technologies exhibit some facets of human intelligence. But how? Where does that intelligence come from? That get us to the next circle, Machine Learning.\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n3\\n\\nMachine Learning—An Approach to Achieve Artificial Intelligence\\n\\nSpam free diet: machine learning helps keep your inbox (relatively) free of spam.\\n\\nMachine learning at its most basic is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world. So rather than hand-coding software routines with a specific set of instructions to accomplish a particular task, the machine is “trained” using large amounts of data and algorithms that give it the ability to learn how to perform the task.\\n\\nMachine learning came directly from minds of the early AI crowd, and the algorithmic approaches over the years included decision tree learning, inductive logic programming, clustering, reinforcement learning, and Bayesian networks among others. As we know, none achieved the ultimate goal of General AI, and even Narrow AI was mostly out of reach with early machine learning approaches.\\n\\nAs it turned out, one of the very best application areas for machine learning for many years was computer vision, though it still required a great deal of hand-coding to get the job done. People would go in and write hand-coded classifiers like edge detection filters so the program could identify where an object started and stopped; shape detection to determine if it had eight sides; a classifier to recognize the letters “S-T-O-P.” From all those hand-coded classifiers they would develop algorithms to make sense of the image and “learn” to determine whether it was a stop sign.\\n\\nGood, but not mind-bendingly great. Especially on a foggy day when the sign isn’t perfectly visible, or a tree obscures part of it. There’s a reason computer vision and image detection didn’t come close to rivaling humans until very recently, it was too brittle and too prone to error. Time, and the right learning algorithms made all the difference.\\n\\nDeep Learning — A Technique for Implementing Machine Learning Herding cats: Picking images of cats out of YouTube videos was one of the first breakthrough demonstrations of deep learning.\\n\\nAnother algorithmic approach from the early machine-learning crowd, Artificial Neural Networks, came and mostly went over the decades. Neural Networks are inspired by our understanding of the biology of our brains – all those interconnections between the neurons. But, unlike a biological brain where any neuron can connect to any other neuron within a certain physical distance, these artificial neural networks have discrete layers, connections, and directions of data propagation.\\n\\nYou might, for example, take an image, chop it up into a bunch of tiles that are inputted into the first layer of the neural network. In the first layer individual neurons, then passes the data to a second layer. The second layer of neurons does its task, and so on, until the final layer and the final output is produced.\\n\\nEach neuron assigns a weighting to its input — how correct or incorrect it is relative to the task being performed. The final output is then determined by the total of those weightings. So think of our stop sign example. Attributes of a stop sign image are chopped up and “examined” by the neurons — its octagonal shape, its fire-engine red color, its distinctive letters, its traffic-sign size, and its motion or lack thereof. The neural network’s task is to conclude whether this is a stop sign or not. It comes up with a “probability vector,” really a highly educated guess, based on the weighting. In our example the system might be 86% confident the image is a stop sign, 7% confident it’s a speed limit sign, and 5% it’s a kite stuck in a tree, and so on — and the network architecture then tells the neural network whether it is right or not.\\n\\nEven this example is getting ahead of itself, because until recently neural networks were all but shunned by the AI research community. They had been around since the earliest days of AI, and had produced very little in the way of “intelligence.” The problem was even the most basic neural networks were very computationally intensive, it just wasn’t\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n4\\n\\na practical approach. Still, a small heretical research group led by Geoffrey Hinton at the University of Toronto kept at it, finally parallelizing the algorithms for supercomputers to run and proving the concept, but it wasn’t until GPUs were deployed in the effort that the promise was realized.\\n\\nIf we go back again to our stop sign example, chances are very good that as the network is getting tuned or “trained” it’s coming up with wrong answers — a lot. What it needs is training. It needs to see hundreds of thousands, even millions of images, until the weightings of the neuron inputs are tuned so precisely that it gets the answer right practically every time — fog or no fog, sun or rain. It’s at that point that the neural network has taught itself what a stop sign looks like; or your mother’s face in the case of Facebook; or a cat, which is what Andrew Ng did in 2012 at Google.\\n\\nNg’s breakthrough was to take these neural networks, and essentially make them huge, increase the layers and the neurons, and then run massive amounts of data through the system to train it. In Ng’s case it was images from 10 million YouTube videos. Ng put the “deep” in deep learning, which describes all the layers in these neural networks.\\n\\nToday, image recognition by machines trained via deep learning in some scenarios is better than humans, and that ranges from cats to identifying indicators for cancer in blood and tumors in MRI scans. Google’s AlphaGo learned the game, and trained for its Go match — it tuned its neural network — by playing against itself over and over and over.\\n\\nThanks to Deep Learning, AI Has a Bright Future\\n\\nDeep Learning has enabled many practical applications of Machine Learning and by extension the overall field of AI. Deep Learning breaks down tasks in ways that makes all kinds of machine assists seem possible, even likely. Driverless cars, better preventive healthcare, even better movie recommendations, are all here today or on the horizon. AI is the present and the future. With Deep Learning’s help, AI may even get to that science fiction state we’ve so long imagined. You have a C-3PO, I’ll take it. You can keep your Terminator.\\n\\nNext Steps: How is AI Impacting Society?\\n\\nRabbinic Artificial Intelligence? Babylonian Talmud; Sanhedrin 65b\\n\\nRava says: If the righteous wish to do so, they can create a world, as it is stated: “But your iniquities have separated between you and your God.” In other words, there is no distinction between God and a righteous person who has no sins, and just as God created the world, so can the righteous.\\n\\nIndeed, Rava created a man, a golem, using forces of sanctity. Rava sent his creation before Rabbi Zeira. Rabbi Zeira would speak to him but he would not reply. Rabbi Zeira said to him: You were created by one of the members of the group, one of the Sages. Return to your dust.\\n\\nHow Judaism Predicted the First Humanoid Robot, Mark Goldfeder, CNN\\n\\n(CNN) - To the team of researchers, Eugene Goostman seemed like a nice Jewish boy from Odessa, Ukraine.\\n\\nIn fact, he was a computer.\\n\\nIn convincing some of the researchers that Goostman was real, the computer program became the first to pass the Turing Test for artificial intelligence. The Turing Test, named for British mathematician Alan Turing, is often thought of as the benchmark test for true machine intelligence. Since 1950, thousands of scientific teams have tried to create something capable of passing, but none has succeeded. That is, until Saturday – and, appropriately for the Goostman advance, our brave new world can learn a bit from Jewish history.\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n5\\n\\nAs we start to think about whether to grant human-like beings special status, Judaism’s highly developed ethical sense, with its willing over-inclusiveness, is not a bad model to follow. What makes this so fascinating is that long ago Judaism came up with a test for humanity that was quite similar to the Turing Test. Jewish law ascribes to and develops several “descriptive” tests for humanity - for instance \"born of woman\" (that is, a biological test). But it also recognizes the limitations of letting a technicality be the only definition of moral personhood.\\n\\nIf there was a creature that looked human, and acted human, but was somehow not born of woman, Jewish law would not feel comfortable denying its basic human rights. And so the Jerusalem Talmud developed a secondary test for humanity, a contextual/functional test. In the fourth century collection of teachings, rabbis argue that if something looks human and acts human enough that when interacting with it we are not sure, the creature should be considered a person, at least for some things. Having human features is important under Jewish law because Judaism believes that man is created in the image of God.\\n\\nBut what exactly does it mean to act human?\\n\\nMany of the early biblical commentators say that what separates man from animals is the ability to speak - not only to communicate but also to express some level of moral intelligence. While the early rabbis obviously didn’t have bots or computer programs, they did deal with creatures that were human-ish, if not human.\\n\\nFamously, the rabbis give partial human status to something called a yadua. While the rabbinic descriptions are terse, the creature seems something like Bigfoot; a giant man-like animal usually spotted in the field. Maimonides, in describing these creatures, notes that their speech is similar to humans, but is unintelligible. The famous Jewish scholar refers to the creatures in his commentary as monkeys. But he doesn\\'t dispute the Talmudic teaching that in some cases yadua can be considered persons. After all, so the argument goes, the yadua looks (somewhat) like a human, and exhibits a level of intelligence that makes it seem, in some ways human. Therefore it deserves to be treated like a human for some things, even though it fails the biological test of being born of a woman.\\n\\nSimply put: The rule is that if something looks and acts human in a particular context, to the point that it seems like a person, do not start poking it to see if it bleeds. Just go ahead and treat it like a person.\\n\\nWhere then, does that leave computers, or more specifically, human-like robots?\\n\\nWhat if Eugene Goostman had been put into a life-like robotic body that had some human features?\\n\\nThe golem in Jewish lore is typically depicted as a man-shaped creature made of clay, imbued with a sense of life by means of a specific series of letters programmed into it by a specialist. It is quite similar, in fact, to the robot: a man- shaped creature made of metal, imbued with a sense of life by means of a very specific series of numbers programmed into it by a specialist. Interestingly, the term “robot” (from the Czech word “robota” meaning “drudgery” or “hard work”) was invented by the Czech novelist and playwright Karel Capek. Capek lived in Prague, and was well acquainted with the well-known legend of the Golem of Prague. Golems are usually associated with kabbalah (Jewish mysticism), but not always.\\n\\nLest you think that golems are not a good analogy for robots because of a special supernatural status, some influential Jewish scholars claim that the most famous golem was created by natural science and was not magic at all. The Talmud in Sanhedrin tells the story of how one rabbi created an artificial man and sent him to a colleague.\\n\\n“Rava created a man and sent him to Rabbi Zeira. The rabbi spoke to the man but he did not answer. Then he (Zeira) said: \"You are from my colleagues. Return to your dust.” Why was Zeira allowed to dismantle Rava\\'s golem, i.e. to return it to its dust? Why was this not considered murder? Because he talked to it, and it could not answer. That is, it could not pass for human. Which leaves open the possibility that another, better, golem, perhaps a 13-year-old boy from Odessa, given the proper outfit, might have fared better.\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n6\\n\\nThe Robot Revolution Will Be the Quietest One, Liu Cixin, The New York Times\\n\\nTurning Point: Though the first fatal crash involving an autonomous car took place in July 2016, self-driving vehicles have been adopted around the world.\\n\\nIn 2016, self-driving cars made inroads in several countries, many of which rewrote their laws to accommodate the new technology. As a science-fiction writer, it’s my duty to warn the human race that the robot revolution has begun — even if no one has noticed yet.\\n\\nWhen a few autonomous test cars appeared on the roads over the last few years, we didn’t think of them as robots because they didn’t have the humanoid shape that science-fiction movies taught us to expect. In 2016, they were adopted widely: as buses in the United Arab Emirates and the Netherlands, taxis in Singapore and private cars in the United States and China. There was a fatal accident in Florida involving an autonomous car, which caused some concerns, but this did not significantly affect our embrace of this technology.\\n\\nInstead of arming ourselves against this alien presence, as some of my fellow science-fiction writers have fearfully suggested, we gawked as the vehicles pulled up to the curb. The driverless vehicles, some of which had no steering wheels or gas pedals, merged into traffic and stopped at stop signs, smoothly taking us to our destinations. We lounged in comfort, occasionally taking selfies.\\n\\nMachine learning has been an important tool for autonomous car companies as they develop the systems that pilot their vehicles. Instead of rigidly following programming as an app on your phone does, an A.I. system can try to learn to do a task itself, using techniques borrowed from human learning, like pattern recognition and trial and error, and may use hardware modeled on the architecture of a human brain. Currently, the responsibilities of artificial intelligence are mostly limited to tasks like translating texts, helping with medical diagnoses and writing simple articles for media companies. But we can expect to see unimaginable progress in this field in future — and the widespread use of the autonomous car is going to accelerate that process as automobile and technology companies invest ever more resources in its development.\\n\\nLet’s try to envision that future. As during every other technological revolution, the robots will first transform our economy. People who drive for a living will lose their jobs — around 3 million in the United States alone. E-commerce may experience further booms because of automation, and car ownership is likely to become nearly obsolete as more targeted car sharing and public transportation systems are developed. Eventually, the robot cars could be integrated with other transportation systems. Say that you live in New York City and want to go to China’s Henan Province: You will enter the address into an app, a car will take you to your plane at the airport, and after you land, another will take you directly to your destination.\\n\\nRobots will begin to creep into other areas of our lives — serving as busboys or waiters, for example — as our investments in robotic transport improve their prowess in areas such as environmental detection and modeling, hyper- complex problem solving and fuzzy-logic applications. With every advance, the use of A.I.-powered robots will expand into other fields: health care, policing, national defense and education.\\n\\nThere will be scandals when things go wrong and backlash movements from the new Luddites. But I don’t think we’ll protest very much. The A.I. systems that drive our cars will teach us to trust machine intelligence over the human variety — car accidents will become very rare, for example — and when given an opportunity to delegate a job to a robot, we will placidly do so without giving it much thought.\\n\\nIn all previous technological revolutions, people who lost their jobs mostly moved to new ones, but that will be less likely when the robots take over. A.I. that can learn from experience will replace many accountants, lawyers, bankers, insurance adjusters, doctors, scientific researchers and some creative professionals. Intelligence and advanced training will no longer mean job stability.\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n7\\n\\nGradually the A.I. era will transform the essence of human culture. When we’re no longer more intelligent than our machines, when they can easily outthink and outperform us, making the sort of intuitive leaps in research and other areas that we currently associate with genius, a sort of learned helplessness is likely to set in for us, and the idea of work itself may cease to hold meaning.\\n\\nAs A.I. takes over, the remaining jobs may dwindle to a fraction of what they were, employing perhaps 10 percent or even less of the total population. These may be highly creative or complex jobs that robots can’t do, such as senior management, directing scientific research or nursing and child care.\\n\\nIn the dystopian scenario, as jobless numbers rise across the globe, our societies sink into prolonged turmoil. The world could be engulfed by endless conflicts between those who control the A.I. and the rest of us. The technocratic 10 percent could end up living in a gated community with armed robot guards.\\n\\nThere is a second, utopian scenario, where we’ve anticipated these changes and come up with solutions beforehand. Those in political power have planned a smoother, gentler transition, perhaps using A.I. to help them anticipate and modulate the strife. At the end of it, almost all of us live on social welfare\\n\\nHow we will spend our time is hard to predict. “He who does not work, neither shall he eat” has been the cornerstone of civilizations through the ages, but that will have vanished. History shows that those who haven’t had to work — aristocrats, say — have often spent their time entertaining and developing their artistic and sporting talents while scrupulously observing elaborate rituals of dress and manners.\\n\\nIn this future, creativity is highly valued. We sport ever more fantastic makeup, hairstyles and clothing. The labor of past ages seems barbaric.\\n\\nBut the aristocrats ruled nations; in the A.I. era, machines are doing all the thinking. Because, over the decades, we’ve gradually given up our autonomy, step by step, allowing ourselves to be transformed into A.I.’s docile, fabulously pampered pets. As A.I. whisks us from place to place — visits to family members, art galleries and musical events — we will look out the windows, as unaware of its plans for us as a poodle on its way to the groomer’s.\\n\\nThis isn’t crying wolf: Machines will take white-collar jobs during the next administration, Martin Ford, Linkedin\\n\\nIn this series, professionals provide advice for the next U.S. president. What do you want POTUS focused on?\\n\\nDear Madam / Mr. President:\\n\\nOver fifty years ago, in March 1964, a document known as the “Triple Revolution Report” landed on the desk of your predecessor, Lyndon Johnson. That report, written by a prominent group of intellectuals that included two Nobel laureates, argued that the United States was on the brink of dramatic social and economic disruption as rapidly advancing industrial automation technology was poised to throw millions out of work.\\n\\nNeedless to say, that dire prediction did not come to pass. However, there are good reasons to believe that technology has finally advanced to the point where such concerns need to be taken seriously. The fear that machines might displace workers and create unemployment has a long history, and because the alarm has been prematurely sounded so many times in the past, there is a real danger that a “little boy who cried wolf” effect will leave us complacent and unprepared if and when the disruption finally arrives.\\n\\nRecent advances in artificial intelligence and robotics suggest that it is entirely possible that a significant impact on the job market could begin to unfold during the course of your presidency. The most important thing to understand about all this progress is that computers no longer have to be programmed step-by-step. Machine learning—a technology that involves smart algorithms churning through vast amounts of data—in effect allows computers figure out for themselves how to perform tasks or reach specific goals.\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n8\\n\\nThe recent triumph of Google’s DeepMind technology at learning to play the ancient game of “Go” and then triumphing against one of the world’s best players was an especially vivid demonstration of the technology, but, in fact, machine learning is already in widespread use across both industries and occupations. Smart algorithms have already displaced lawyers and paralegals who once reviewed documents as part of the legal discovery process. An increasing number of news articles published my major U.S. media companies are being generated autonomously by systems that analyze data and create content that is often indistinguishable from a story written by a human journalist. Machine learning is also powering the latest generation of robots, and the machines are rapidly becoming more flexible and dexterous.\\n\\nAs technology continues to accelerate, the number and types of jobs that can be automated is certain to expand dramatically. It\\'s not just factory workers that can be replaced by robots and machines: Rapidly improving software automation and specialized artificial intelligence applications will make knowledge worker and professional occupations requiring college educations and advanced skills increasingly vulnerable. This demonstrated capability for information technology to climb the skills ladder and threaten the jobs taken by college graduates is a special cause for concern because it calls into question the only conventional solution we have to offer workers displaced by automation: ever more training and education.\\n\\nIf technology eventually results in wide-spread unemployment, or if it drives down wages for the majority of workers as jobs are deskilled and commoditized, then we could also run into a serious problem with consumer demand. Jobs are the primary mechanism that gets purchasing power into the hands of consumers so that they buy the products and services generated by the economy. If automation has a negative impact on consumer demand and confidence, then we run the risk of economic stagnation or even a downward, deflationary spiral.\\n\\nWhile these concerns may seem either far-fetched science fiction or a return to the Ludditism we’ve experienced in the past, many of us in the technology community believe the risk is real--and that it deserves serious consideration. At a time when our political system is intensely polarized and seems unable to respond to even the most mundane challenges, the prospect of a dramatic and unanticipated economic and social disruption is not sometime we can afford to take lightly.\\n\\nIf the automation of jobs proves to be a relentless trend, then there will eventually be no alternative but to consider unconventional solutions--perhaps including a guaranteed basic income for all Americans. Needless to say, the implementation of such policies would present a staggering political challenge. Given that there is no reliable way to predict when the disruption will occur, or how fast it will unfold, it is imperative that planning begin well in advance. A logical first step would be to initiate some experimental pilot programs designed to test various policy responses. The data generated by these programs would be invaluable in eventually crafting an effective national policy to adapt our economy and society to the implications of disruptive technology.\\n\\nI urge you to consider including among those who staff your new administration experts who are familiar with recent advances in artificial intelligence and robotics and with the potential economic and social impact of these technologies, and who are prepared to initiate the planning process.\\n\\nThe Hype—and Hope—of Artificial Intelligence, Om Malik, The New Yorker\\n\\nEarlier this month, on his HBO show “Last Week Tonight,” John Oliver skewered media companies’ desperate search for clicks. Like many of his bits, it became a viral phenomenon, clocking in at nearly six million views on YouTube. At around the ten-minute mark, Oliver took his verbal bat to the knees of Tronc, the new name for Tribune Publishing Company, and its parody-worthy promotional video, in which a robotic spokeswoman describes the journalistic benefits of artificial intelligence, as a string section swells underneath.\\n\\nTronc is not the only company to enthusiastically embrace the term “artificial intelligence.” A.I. is hot, and every company worth its stock price is talking about how this magical potion will change everything. Even Macy’s recently announced that it was testing an I.B.M. artificial-intelligence tool in ten of its department stores, in order to bring back 9\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\ncustomers who are abandoning traditional retail in favor of online shopping.\\n\\nMuch like “the cloud,” “big data,” and “machine learning” before it, the term “artificial intelligence” has been hijacked by marketers and advertising copywriters. A lot of what people are calling “artificial intelligence” is really data analytics—in other words, business as usual. If the hype leaves you asking “What is A.I., really?,” don’t worry, you’re not alone. I asked various experts to define the term and got different answers. The only thing they all seem to agree on is that artificial intelligence is a set of technologies that try to imitate or augment human intelligence. To me, the emphasis is on augmentation, in which intelligent software helps us interact and deal with the increasingly digital world we live in.\\n\\nThree decades ago, I read newspapers, wrote on an electric typewriter, and watched a handful of television channels. Today, I have streaming video from Netflix, Amazon, HBO, and other places, and I’m sometimes paralyzed by the choices. It is becoming harder for us to stay on top of the onslaught—e-mails, messages, appointments, alerts. Augmented intelligence offers the possibility of winnowing an increasing number of inputs and options in a way that humans can’t manage without a helping hand.\\n\\nComputers in general, and software in particular, are much more difficult than other kinds of technology for most people to grok, and they overwhelm us with a sense of mystery. There was a time when you would record a letter or a document on a dictaphone and someone would transcribe it for you. A human was making the voice-to-text conversion with the help of a machine. Today, you can speak into your iPhone and it will transcribe your messages itself. If people could have seen our current voice-to-text capabilities fifty years ago, it would have looked as if technology had become sentient. Now it’s just a routine way to augment how we interact with the world. Kevin Kelly, the writer and futurist, whose most recent book is “The Inevitable: Understanding the 12 Technological Forces That Will Shape Our Future,” said, “What we can do now would be A.I. fifty years ago. What we can do in fifty years will not be called A.I.”\\n\\nYou don’t have to look up from Facebook to get his point. Before we had the Internet, we would either call or write to our friends, one at a time, and keep up with their lives. It was a slow process, and took a lot of effort and time to learn about each other. As a result, we had fewer interactions—there was a cost attached to making long-distance phone calls and a time commitment attached to writing letters. With the advent of the Internet, e-mail emerged as a way to facilitate and speed up those interactions. Facebook did one better—it turned your address book into a hub, allowing you to simultaneously stay in touch with hundreds, even thousands, of friends. The algorithm allows us to maintain more relationships with much less effort at almost no cost.\\n\\nMichelle Zhou spent over a decade and a half at I.B.M. Research and I.B.M. Watson Group before leaving to become a co-founder of Juji, a sentiment-analysis startup. An expert in a field where artificial intelligence and human-computer interaction intersect, Zhou breaks down A.I. into three stages. The first is recognition intelligence, in which algorithms running on ever more powerful computers can recognize patterns and glean topics from blocks of text, or perhaps even derive the meaning of a whole document from a few sentences. The second stage is cognitive intelligence, in which machines can go beyond pattern recognition and start making inferences from data. The third stage will be reached only when we can create virtual human beings, who can think, act, and behave as humans do.\\n\\nWe are a long way from creating virtual human beings. Despite what you read in the media, no technology is perfect, and the most valuable function of A.I. lies in augmenting human intelligence. To even reach that point, we need to train computers to mimic humans. An April, 2016, story in Bloomberg Business provided a good example. It described how companies that provide automated A.I. personal assistants (of the sort that arrange schedules or help with online shopping) had hired human “trainers” to check and evaluate the A.I. assistants’ responses before they were sent out. “It’s ironic that we define artificial intelligence with respect to its ability to replicate human intelligence,” said Sean Gourley, the founder of Primer, a data-analytics company, and an expert on deriving intelligence from large data sets with the help of algorithms.\\n\\nWhether it is Spotify or Netflix or a new generation of A.I. chat bots, all of these tools rely on humans themselves to\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n10\\n\\nprovide the data. When we listen to songs, put them on playlists, and share them with others, we are sending vital signals to Spotify that train its algorithms not only to discover what we might like but also to predict hits.\\n\\nEven the much talked-about “computer vision” has become effective only because humans have uploaded billions of photos and tagged them with metadata to give those photos context. Increasingly powerful computers can scan through these photos and find patterns and meaning. Similarly, Google can use billions of voice samples it has collected over the years to build a smart system that understands accents and nuances, which make its voice-based search function possible.\\n\\nUsing Zhou’s three stages as a yardstick, we are only in the “recognition intelligence” phase—today’s computers use deep learning to discover patterns faster and better. It’s true, however, that some companies are working on technologies that can be used for inferring meanings, which would be the next step. “It does not matter whether we will end up at stage 3,” Zhou wrote to me in an e-mail. “I’m still a big fan of man-machine symbiosis, where computers do the best they can (that is being consistent, objective, precise), and humans do our best (creative, imprecise but adaptive).” For a few more decades, at least, humans will continue to train computers to mimic us. And, in the meantime, we’re going to have to deal with the hyperbole surrounding A.I.\\n\\nDeep Dive: What are the Next Frontiers and Further Implications of AI?\\n\\nDeepMind and Blizzard Open StarCraft II as an AI Research Environment, Oriyal VInyals, Stephen Gaffney, Timo Ewalds; DeepMind\\n\\nDeepMind\\'s scientific mission is to push the boundaries of AI by developing systems that can learn to solve complex problems. To do this, we design agents and test their ability in a wide range of environments from the purpose- built DeepMind Lab to established games, such as Atari and Go.\\n\\nTesting our agents in games that are not specifically designed for AI research, and where humans play well, is crucial to benchmark agent performance. That is why we, along with our partner Blizzard Entertainment, are excited to announce the release of SC2LE, a set of tools that we hope will accelerate AI research in the real-time strategy game StarCraft II. The SC2LE release includes:\\n\\nA Machine Learning API developed by Blizzard that gives researchers and developers hooks into the game. This includes the release of tools for Linux for the first time.\\n\\nA dataset of anonymised game replays, which will increase from 65k to more than half a million in the coming weeks.\\n\\nAn open source version of DeepMind’s toolset, PySC2, to allow researchers to easily use Blizzard’s feature-layer API with their agents.\\n\\nA series of simple RL mini-games to allow researchers to test the performance of agents on specific tasks.\\n\\nA joint paper that outlines the environment, and reports initial baseline results on the mini-games, supervised learning from replays, and the full 1v1 ladder game against the built-in AI.\\n\\nStarCraft and StarCraft II are among the biggest and most successful games of all time, with players competing in tournaments for more than 20 years. The original game is also already used by AI and ML researchers, who compete annually in the AIIDE bot competition. Part of StarCraft’s longevity is down to the rich, multi-layered gameplay, which also makes it an ideal environment for AI research.\\n\\nFor example, while the objective of the game is to beat the opponent, the player must also carry out and balance a number of sub-goals, such as gathering resources or building structures. In addition, a game can take from a few\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n11\\n\\nminutes to one hour to complete, meaning actions taken early in the game may not pay-off for a long time. Finally, the map is only partially observed, meaning agents must use a combination of memory and planning to succeed.\\n\\nThe game also has other qualities that appeal to researchers, such as the large pool of avid players that compete online every day. This ensures that there is a large quantity of replay data to learn from - as well as a large quantity of extremely talented opponents for AI agents.\\n\\nEven StarCraft’s action space presents a challenge with a choice of more than 300 basic actions that can be taken. Contrast this with Atari games, which only have about 10 (e.g. up, down, left, right etc). On top of this, actions in StarCraft are hierarchical, can be modified and augmented, with many of them requiring a point on the screen. Even assuming a small screen size of 84x84 there are roughly 100 million possible actions available.\\n\\nThis release means researchers can now tackle some of these challenges using Blizzard’s own tools to build their own tasks and models.\\n\\nOur PySC2 environment wrapper helps by offering a flexible and easy-to-use interface for RL agents to play the game. In this initial release, we break the game down into “feature layers”, where elements of the game such as unit type, health and map visibility are isolated from each other, whilst preserving the core visual and spatial elements of the game.\\n\\nThe release also contains a series of ‘mini-games’ - an established technique for breaking down the game into manageable chunks that can be used to test agents on specific tasks, such as moving the camera, collecting mineral shards or selecting units. We hope that researchers can test their techniques on these as well as propose new mini- games for other researchers to compete and evaluate on.\\n\\nOur initial investigations show that our agents perform well on these mini-games. But when it comes to the full game, even strong baseline agents, such as A3C, cannot win a single game against even the easiest built-in AI. For instance, the following video shows an early-stage training agent (left) which fails to keep its workers mining, a task that humans find trivial. After training (right), the agents perform more meaningful actions, but if they are to be competitive, we will need further breakthroughs in deep RL and related areas.\\n\\nOne technique that we know allows our agents to learn stronger policies is imitation learning. This kind of training will soon be far easier thanks to Blizzard, which has committed to ongoing releases of hundreds of thousands of anonymized replays gathered from the StarCraft II ladder. These will not only allow researchers to train supervised agents to play the game, but also opens up other interesting areas of research such as sequence prediction and long-term memory.\\n\\nOur hope is that the release of these new tools will build on the work that the AI community has already done in StarCraft, encouraging more DeepRL research and making it easier for researchers to focus on the frontiers of our field.\\n\\nWe look forward to seeing what the community discovers.\\n\\nNeuroscience-Inspired Artificial Intelligence, Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, Matthew Botvinick; Neuron\\n\\nThe fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields.\\n\\nIn recent years, rapid progress has been made in the related fields of neuroscience and artificial intelligence (AI). At the dawn of the computer age, work on AI was inextricably intertwined with neuroscience and psychology, and many of\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n12\\n\\nthe early pioneers straddled both fields, with collaborations between these disciplines proving highly productive. (Churchland and Sejnowski, 1988, Hebb, 1949, Hinton et al., 1986, Hopfield, 1982, McCulloch and Pitts, 1943, Turing, 1950). However, more recently, the interaction has become much less commonplace, as both subjects have grown enormously in complexity and disciplinary boundaries have solidified. In this review, we argue for the critical and ongoing importance of neuroscience in generating ideas that will accelerate and guide AI research (see Hassabis commentary in Brooks et al., 2012).\\n\\nWe begin with the premise that building human-level general AI (or “Turing-powerful” intelligent systems; Turing, 1936) is a daunting task, because the search space of possible solutions is vast and likely only very sparsely populated. We argue that this therefore underscores the utility of scrutinizing the inner workings of the human brain— the only existing proof that such an intelligence is even possible. Studying animal cognition and its neural implementation also has a vital role to play, as it can provide a window into various important aspects of higher-level general intelligence.\\n\\nThe benefits to developing AI of closely examining biological intelligence are two-fold. First, neuroscience provides a rich source of inspiration for new types of algorithms and architectures, independent of and complementary to the mathematical and logic-based methods and ideas that have largely dominated traditional approaches to AI. For example, were a new facet of biological computation found to be critical to supporting a cognitive function, then we would consider it an excellent candidate for incorporation into artificial systems. Second, neuroscience can provide validation of AI techniques that already exist. If a known algorithm is subsequently found to be implemented in the brain, then that is strong support for its plausibility as an integral component of an overall general intelligence system. Such clues can be critical to a long-term research program when determining where to allocate resources most productively. For example, if an algorithm is not quite attaining the level of performance required or expected, but we observe it is core to the functioning of the brain, then we can surmise that redoubled engineering efforts geared to making it work in artificial systems are likely to pay off.\\n\\nOf course from a practical standpoint of building an AI system, we need not slavishly enforce adherence to biological plausibility. From an engineering perspective, what works is ultimately all that matters. For our purposes then, biological plausibility is a guide, not a strict requirement. What we are interested in is a systems neuroscience-level understanding of the brain, namely the algorithms, architectures, functions, and representations it utilizes. This roughly corresponds to the top two levels of the three levels of analysis that Marr famously stated are required to understand any complex biological system (Marr and Poggio, 1976): the goals of the system (the computational level) and the process and computations that realize this goal (the algorithmic level). The precise mechanisms by which this is physically realized in a biological substrate are less relevant here (the implementation level). Note this is where our approach to neuroscience-inspired AI differs from other initiatives, such as the Blue Brain Project (Markram, 2006) or the field of neuromorphic computing systems (Esser et al., 2016), which attempt to closely mimic or directly reverse engineer the specifics of neural circuits (albeit with different goals in mind). By focusing on the computational and algorithmic levels, we gain transferrable insights into general mechanisms of brain function, while leaving room to accommodate the distinctive opportunities and challenges that arise when building intelligent machines in silico.\\n\\nThe following sections unpack these points by considering the past, present, and future of the AI-neuroscience interface. Before beginning, we offer a clarification. Throughout this article, we employ the terms “neuroscience” and “AI.” We use these terms in the widest possible sense. When we say neuroscience, we mean to include all fields that are involved with the study of the brain, the behaviors that it generates, and the mechanisms by which it does so, including cognitive neuroscience, systems neuroscience and psychology. When we say AI, we mean work in machine learning, statistics, and AI research that aims to build intelligent machines (Legg and Hutter, 2007).\\n\\nWe begin by considering the origins of two fields that are pivotal for current AI research, deep learning and reinforcement learning, both of which took root in ideas from neuroscience. We then turn to the current state of play in AI research, noting many cases where inspiration has been drawn (sometimes without explicit acknowledgment) from concepts and findings in neuroscience. In this section, we particularly emphasize instances where we have\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n13\\n\\ncombined deep learning with other approaches from across machine learning, such as reinforcement learning (Mnih et al., 2015), Monte Carlo tree search (Silver et al., 2016), or techniques involving an external content-addressable memory (Graves et al., 2016). Next, we consider the potential for neuroscience to support future AI research, looking at both the most likely research challenges and some emerging neuroscience-inspired AI techniques. While our main focus will be on the potential for neuroscience to benefit AI, our final section will briefly consider ways in which AI may be helpful to neuroscience and the broader potential for synergistic interactions between these two fields.\\n\\nThe Past:\\n\\nDeep Learning\\n\\nAs detailed in a number of recent reviews, AI has been revolutionized over the past few years by dramatic advances in neural network, or “deep learning,” methods (LeCun et al., 2015, Schmidhuber, 2014). As the moniker “neural network” might suggest, the origins of these AI methods lie directly in neuroscience. In the 1940s, investigations of neural computation began with the construction of artificial neural networks that could compute logical functions (McCulloch and Pitts, 1943). Not long after, others proposed mechanisms by which networks of neurons might learn incrementally via supervisory feedback (Rosenblatt, 1958) or efficiently encode environmental statistics in an unsupervised fashion (Hebb, 1949). These mechanisms opened up the field of artificial neural network research, and they continue to provide the foundation for contemporary research on deep learning (Schmidhuber, 2014).\\n\\nNot long after this pioneering work, the development of the backpropagation algorithm allowed learning to occur in networks composed of multiple layers (Rumelhart et al., 1985, Werbos, 1974). Notably, the implications of this method for understanding intelligence, including AI, were first appreciated by a group of neuroscientists and cognitive scientists, working under the banner of parallel distributed processing (PDP) (Rumelhart et al., 1986). At the time, most AI research was focused on building logical processing systems based on serial computation, an approach inspired in part by the notion that human intelligence involves manipulation of symbolic representations (Haugeland, 1985). However, there was a growing sense in some quarters that purely symbolic approaches might be too brittle and inflexible to solve complex real-world problems of the kind that humans routinely handle. Instead, a growing foundation of knowledge about the brain seemed to point in a very different direction, highlighting the role of stochastic and highly parallelized information processing. Building on this, the PDP movement proposed that human cognition and behavior emerge from dynamic, distributed interactions within networks of simple neuron-like processing units, interactions tuned by learning procedures that adjust system parameters in order to minimize error or maximize reward.\\n\\nAlthough the PDP approach was at first applied to relatively small-scale problems, it showed striking success in accounting for a wide range of human behaviors (Hinton et al., 1986). Along the way, PDP research introduced a diverse collection of ideas that have had a sustained influence on AI research. For example, current machine translation research exploits the notion that words and sentences can be represented in a distributed fashion (i.e., as vectors) (LeCun et al., 2015), a principle that was already ingrained in early PDP-inspired models of sentence processing (St. John and McClelland, 1990). Building on the PDP movement’s appeal to biological computation, current state-of-the- art convolutional neural networks (CNNs) incorporate several canonical hallmarks of neural computation, including nonlinear transduction, divisive normalization, and maximum-based pooling of inputs (Yamins and DiCarlo, 2016). These operations were directly inspired by single-cell recordings from the mammalian visual cortex that revealed how visual input is filtered and pooled in simple and complex cells in area V1 (Hubel and Wiesel, 1959). Moreover, current network architectures replicate the hierarchical organization of mammalian cortical systems, with both convergent and divergent information flow in successive, nested processing layers (Krizhevsky et al., 2012, LeCun et al., 1989, Riesenhuber and Poggio, 1999, Serre et al., 2007), following ideas first advanced in early neural network models of visual processing (Fukushima, 1980). In both biological and artificial systems, successive non-linear computations transform raw visual input into an increasingly complex set of features, permitting object recognition that is invariant to transformations of pose, illumination, or scale.\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n14\\n\\nAs the field of deep learning evolved out of PDP research into a core area within AI, it was bolstered by new ideas, such as the development of deep belief networks (Hinton et al., 2006) and the introduction of large datasets inspired by research on human language (Deng et al., 2009). During this period, it continued to draw key ideas from neuroscience. For example, biological considerations informed the development of successful regularization schemes that support generalization beyond training data. One such scheme, in which only a subset of units participate in the processing of a given training example (“dropout”), was motivated by the stochasticity that is inherent in biological systems populated by neurons that fire with Poisson-like statistics (Hinton et al., 2012). Here and elsewhere, neuroscience has provided initial guidance toward architectural and algorithmic constraints that lead to successful neural network applications for AI.\\n\\nReinforcement Learning\\n\\nAlongside its important role in the development of deep learning, neuroscience was also instrumental in erecting a second pillar of contemporary AI, stimulating the emergence of the field of reinforcement learning (RL). RL methods address the problem of how to maximize future reward by mapping states in the environment to actions and are among the most widely used tools in AI research (Sutton and Barto, 1998). Although it is not widely appreciated among AI researchers, RL methods were originally inspired by research into animal learning. In particular, the development of temporal-difference (TD) methods, a critical component of many RL models, was inextricably intertwined with research into animal behavior in conditioning experiments. TD methods are real-time models that learn from differences between temporally successive predictions, rather than having to wait until the actual reward is delivered. Of particular relevance was an effect called second-order conditioning, where affective significance is conferred on a conditioned stimulus (CS) through association with another CS rather than directly via association with the unconditioned stimulus (Sutton and Barto, 1981). TD learning provides a natural explanation for second-order conditioning and indeed has gone on to explain a much wider range of findings from neuroscience, as we discuss below.\\n\\nHere, as in the case of deep learning, investigations initially inspired by observations from neuroscience led to further developments that have strongly shaped the direction of AI research. From their neuroscience-informed origins, TD methods and related techniques have gone on to supply the core technology for recent advances in AI, ranging from robotic control (Hafner and Riedmiller, 2011) to expert play in backgammon (Tesauro, 1995) and Go (Silver et al., 2016).\\n\\nThe Present:\\n\\nReading the contemporary AI literature, one gains the impression that the earlier engagement with neuroscience has diminished. However, if one scratches the surface, one can uncover many cases in which recent developments have been inspired and guided by neuroscientific considerations. Here, we look at four specific examples.\\n\\nAttention\\n\\nThe brain does not learn by implementing a single, global optimization principle within a uniform and undifferentiated neural network (Marblestone et al., 2016). Rather, biological brains are modular, with distinct but interacting subsystems underpinning key functions such as memory, language, and cognitive control (Anderson et al., 2004, Shallice, 1988). This insight from neuroscience has been imported, often in an unspoken way, into many areas of current AI.\\n\\nOne illustrative example is recent AI work on attention. Up until quite lately, most CNN models worked directly on entire images or video frames, with equal priority given to all image pixels at the earliest stage of processing. The primate visual system works differently. Rather than processing all input in parallel, visual attention shifts strategically among locations and objects, centering processing resources and representational coordinates on a series of regions in turn (Koch and Ullman, 1985, Moore and Zirnsak, 2017, Posner and Petersen, 1990). Detailed neurocomputational models have shown how this piecemeal approach benefits behavior, by prioritizing and isolating the information that is relevant\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n15\\n\\nat any given moment (Olshausen et al., 1993, Salinas and Abbott, 1997). As such, attentional mechanisms have been a source of inspiration for AI architectures that take “glimpses” of the input image at each step, update internal state representations, and then select the next location to sample (Larochelle and Hinton, 2010, Mnih et al., 2014) (Figure 1A). One such network was able to use this selective attentional mechanism to ignore irrelevant objects in a scene, allowing it to perform well in challenging object classification tasks in the presence of clutter (Mnih et al., 2014). Further, the attentional mechanism allowed the computational cost (e.g., number of network parameters) to scale favorably with the size of the input image. Extensions of this approach were subsequently shown to produce impressive performance at difficult multi-object recognition tasks, outperforming conventional CNNs that process the entirety of the image, both in terms of accuracy and computational efficiency (Ba et al., 2015), as well as enhancing image-to- caption generation (Xu et al., 2015).\\n\\nWhile attention is typically thought of as an orienting mechanism for perception, its “spotlight” can also be focused internally, toward the contents of memory. This idea, a recent focus in neuroscience studies (Summerfield et al., 2006), has also inspired work in AI. In some architectures, attentional mechanisms have been used to select information to be read out from the internal memory of the network. This has helped provide recent successes in machine translation (Bahdanau et al., 2014) and led to important advances on memory and reasoning tasks (Graves et al., 2016). These architectures offer a novel implementation of content-addressable retrieval, which was itself a concept originally introduced to AI from neuroscience (Hopfield, 1982).\\n\\nOne further area of AI where attention mechanisms have recently proven useful focuses on generative models, systems that learn to synthesize or “imagine” images (or other kinds of data) that mimic the structure of examples presented during training. Deep generative models (i.e., generative models implemented as multi-layered neural networks) have recently shown striking successes in producing synthetic outputs that capture the form and structure of real visual scenes via the incorporation of attention-like mechanisms (Hong et al., 2015, Reed et al., 2016). For example, in one state-of-the-art generative model known as DRAW, attention allows the system to build up an image incrementally, attending to one portion of a “mental canvas” at a time (Gregor et al., 2015).\\n\\nEpisodic Memory\\n\\nA canonical theme in neuroscience is that that intelligent behavior relies on multiple memory systems (Tulving, 1985). These will include not only reinforcement-based mechanisms, which allow the value of stimuli and actions to be learned incrementally and through repeated experience, but also instance-based mechanisms, which allow experiences to be encoded rapidly (in “one shot”) in a content-addressable store (Gallistel and King, 2009). The latter form of memory, known as episodic memory (Tulving, 2002), is most often associated with circuits in the medial temporal lobe, prominently including the hippocampus (Squire et al., 2004).\\n\\nOne recent breakthrough in AI has been the successful integration of RL with deep learning (Mnih et al., 2015, Silver et al., 2016). For example, the deep Q-network (DQN) exhibits expert play on Atari 2600 video games by learning to transform a vector of image pixels into a policy for selecting actions (e.g., joystick movements). One key ingredient in DQN is “experience replay,” whereby the network stores a subset of the training data in an instance-based way, and then “replays” it offline, learning anew from successes or failures that occurred in the past. Experience replay is critical to maximizing data efficiency, avoids the destabilizing effects of learning from consecutive correlated experiences, and allows the network to learn a viable value function even in complex, highly structured sequential environments such as video games.\\n\\nCritically, experience replay was directly inspired by theories that seek to understand how the multiple memory systems in the mammalian brain might interact. According to a prominent view, animal learning is supported by parallel or “complementary” learning systems in the hippocampus and neocortex (Kumaran et al., 2016, McClelland et al., 1995). The hippocampus acts to encode novel information after a single exposure (one-shot learning), but this information is gradually consolidated to the neocortex in sleep or resting periods that are interleaved with periods of activity. This\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n16\\n\\nconsolidation is accompanied by replay in the hippocampus and neocortex, which is observed as a reinstatement of the structured patterns of neural activity that accompanied the learning event (O’Neill et al., 2010, Skaggs and McNaughton, 1996) (Figure 1B). This theory was originally proposed as a solution to the well-known problem that in conventional neural networks, correlated exposure to sequential task settings leads to mutual interference among policies, resulting in catastrophic forgetting of one task as a new one is learned. The replay buffer in DQN might thus be thought of as a very primitive hippocampus, permitting complementary learning in silico much as is proposed for biological brains. Later work showed that the benefits of experience replay in DQN are enhanced when replay of highly rewarding events is prioritized (Schaul et al., 2015), just as hippocampal replay seems to favor events that lead to high levels of reinforcement (Singer and Frank, 2009).\\n\\nExperiences stored in a memory buffer can not only be used to gradually adjust the parameters of a deep network toward an optimal policy, as in DQN, but can also support rapid behavioral change based on an individual experience. Indeed, theoretical neuroscience has argued for the potential benefits of episodic control, whereby rewarded action sequences can be internally re-enacted from a rapidly updateable memory store, implemented in the biological case in the hippocampus (Gershman and Daw, 2017). Further, normative accounts show that episodic control is particularly advantageous over other learning mechanisms when limited experience of the environment has been obtained (Lengyel and Dayan, 2007).\\n\\nRecent AI research has drawn on these ideas to overcome the slow learning characteristics of deep RL networks, developing architectures that implement episodic control (Blundell et al., 2016). These networks store specific experiences (e.g., actions and reward outcomes associated with particular Atari game screens) and select new actions based on the similarity between the current situation input and the previous events stored in memory, taking the reward associated with those previous events into account (Figure 1B). As predicted from the initial, neuroscience- based work (Lengyel and Dayan, 2007), artificial agents employing episodic control show striking gains in performance over deep RL networks, particularly early on during learning (Blundell et al., 2016). Further, they are able to achieve success on tasks that depend heavily on one-shot learning, where typical deep RL architectures fail. Moreover, episodic- like memory systems more generally have shown considerable promise in allowing new concepts to be learned rapidly based on only a few examples (Vinyals et al., 2016). In the future, it will be interesting to harness the benefits of rapid episodic-like memory and more traditional incremental learning in architectures that incorporate both of these components within an interacting framework that mirrors the complementary learning systems in mammalian brain. We discuss these future perspectives below in more detail later, in “Imagination and planning.”\\n\\nWorking Memory\\n\\nHuman intelligence is characterized by a remarkable ability to maintain and manipulate information within an active store, known as working memory, which is thought to be instantiated within the prefrontal cortex and interconnected areas (Goldman-Rakic, 1990). Classic cognitive theories suggest that this functionality depends on interactions between a central controller (“executive”) and separate, domain-specific memory buffers (e.g., visuo-spatial sketchpad) (Baddeley, 2012). AI research has drawn inspiration from these models, by building architectures that explicitly maintain information over time. Historically, such efforts began with the introduction of recurrent neural network architectures displaying attractor dynamics and rich sequential behavior, work directly inspired by neuroscience (Elman, 1990, Hopfield and Tank, 1986, Jordan, 1997). This work enabled later, more detailed modeling of human working memory (Botvinick and Plaut, 2006, Durstewitz et al., 2000), but it also laid the foundation for further technical innovations that have proved pivotal in recent AI research. In particular, one can see close parallels between the learning dynamics in these early, neuroscience-inspired networks and those in long-short-term memory (LSTM) networks, which subsequently achieved state of the art performance across a variety of domains. LTSMs allow information to be gated into a fixed activity state and maintained until an appropriate output is required (Hochreiter and Schmidhuber, 1997). Variants of this type of network have shown some striking behaviors in challenging domains, such as learning to respond to queries about the latent state of variables after training on computer code (Zaremba and 17\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\nSutskever, 2014).\\n\\nIn ordinary LSTM networks, the functions of sequence control and memory storage are closely intertwined. This contrasts with classic models of human working memory, which, as mentioned above, separate these two. This neuroscience-based schema has recently inspired more complex AI architectures where control and storage are supported by distinct modules (Graves et al., 2014, Graves et al., 2016, Weston et al., 2014). For example, the differential neural computer (DNC) involves a neural network controller that attends to and reads/writes from an external memory matrix (Graves et al., 2016). This externalization allows the network controller to learn from scratch (i.e., via end-to-end optimization) to perform a wide range of complex memory and reasoning tasks that currently elude LSTMs, such as finding the shortest path through a graph-like structure, such as a subway map, or manipulating blocks in a variant of the Tower of Hanoi task (Figure 1C). These types of problems were previously argued to depend exclusively on symbol processing and variable binding and therefore beyond the purview of neural networks (Fodor and Pylyshyn, 1988, Marcus, 1998). Of note, although both LSTMs and the DNC are described here in the context of working memory, they have the potential to maintain information over many thousands of training cycles and so may thus be suited to longer-term forms of memory, such as retaining and understanding the contents of a book.\\n\\nContinual Learning\\n\\nIntelligent agents must be able to learn and remember many different tasks that are encountered over multiple timescales. Both biological and artificial agents must thus have a capacity for continual learning, that is, an ability to master new tasks without forgetting how to perform prior tasks (Thrun and Mitchell, 1995). While animals appear relatively adept at continual learning, neural networks suffer from the problem of catastrophic forgetting (French, 1999, McClelland et al., 1995). This occurs as the network parameters shift toward the optimal state for performing the second of two successive tasks, overwriting the configuration that allowed them to perform the first. Given the importance of continual learning, this liability of neural networks remains a significant challenge for the development of AI.\\n\\nIn neuroscience, advanced neuroimaging techniques (e.g., two-photon imaging) now allow dynamic in vivo visualization of the structure and function of dendritic spines during learning, at the spatial scale of single synapses (Nishiyama and Yasuda, 2015). This approach can be used to study neocortical plasticity during continual learning (Cichon and Gan, 2015, Hayashi-Takagi et al., 2015, Yang et al., 2009). There is emerging evidence for specialized mechanisms that protect knowledge about previous tasks from interference during learning on a new task. These include decreased synaptic lability (i.e., lower rates of plasticity) in a proportion of strengthened synapses, mediated by enlargements to dendritic spines that persist despite learning of other tasks (Cichon and Gan, 2015, Yang et al., 2009) (Figure 1D). These changes are associated with retention of task performance over several months, and indeed, if they are “erased” with synaptic optogenetics, this leads to forgetting of the task (Hayashi-Takagi et al., 2015). These empirical insights are consistent with theoretical models that suggest that memories can be protected from interference through synapses that transition between a cascade of states with different levels of plasticity (Fusi et al., 2005) (Figure 1D).\\n\\nTogether, these findings from neuroscience have inspired the development of AI algorithms that address the challenge of continual learning in deep networks by implementing of a form of “elastic” weight consolidation (EWC) (Kirkpatrick et al., 2017), which acts by slowing down learning in a subset of network weights identified as important to previous tasks, thereby anchoring these parameters to previously found solutions (Figure 1D). This allows multiple tasks to be iearned without an increase in network capacity, with weights shared efficiently between tasks with related structure. In this way, the EWC algorithm allows deep RL networks to support continual learning at large scale.\\n\\nThe Future:\\n\\nIn AI, the pace of recent research has been remarkable. Artificial systems now match human performance in challenging\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n18\\n\\nobject recognition tasks (Krizhevsky et al., 2012) and outperform expert humans in dynamic, adversarial environments such as Atari video games (Mnih et al., 2015), the ancient board game of Go (Silver et al., 2016), and imperfect information games such as heads-up poker (Moravčík et al., 2017). Machines can autonomously generate synthetic natural images and simulations of human speech that are almost indistinguishable from their real-world counterparts (Lake et al., 2015, van den Oord et al., 2016), translate between multiple languages (Wu et al., 2016), and create “neural art” in the style of well-known painters (Gatys et al., 2015).\\n\\nHowever, much work is still needed to bridge the gap between machine and human-level intelligence. In working toward closing this gap, we believe ideas from neuroscience will become increasingly indispensable. In neuroscience, the advent of new tools for brain imaging and genetic bioengineering have begun to offer a detailed characterization of the computations occurring in neural circuits, promising a revolution in our understanding of mammalian brain function (Deisseroth and Schnitzer, 2013). The relevance of neuroscience, both as a roadmap for the AI research agenda and as a source of computational tools is particularly salient in the following key areas.\\n\\nIntuitive Understanding of the Physical World\\n\\nRecent perspectives emphasize key ingredients of human intelligence that are already well developed in human infants but lacking in most AI systems (Gilmore et al., 2007, Gopnik and Schulz, 2004, Lake et al., 2016). Among these capabilities are knowledge of core concepts relating to the physical world, such as space, number, and objectness, which allow people to construct compositional mental models that can guide inference and prediction (Battaglia et al., 2013, Spelke and Kinzler, 2007).\\n\\nAI research has begun to explore methods for addressing this challenge. For example, novel neural network architectures have been developed that interpret and reason about scenes in a humanlike way, by decomposing them into individual objects and their relations (Battaglia et al., 2016, Chang et al., 2016, Eslami et al., 2016) (Figures 2A and 2B ). In some cases, this has resulted in human-level performance on challenging reasoning tasks (Santoro et al., 2017). In other work, deep RL has been used to capture the processes by which children gain commonsense understanding of the world through interactive experiments (Denil et al., 2016). Relatedly, deep generative models have been developed that are able to construct rich object models from raw sensory inputs (Higgins et al., 2016). These leverage constraints first identified in neuroscience, such as redundancy reduction (Barlow, 1959), which encourage the emergence of disentangled representations of independent factors such as shape and position (Figure 2C). Importantly, the latent representations learned by such generative models exhibit compositional properties, supporting flexible transfer to novel tasks (Eslami et al., 2016, Higgins et al., 2016, Rezende et al., 2016a). In the caption associated with Figure 2, we provide more detailed information about these networks.\\n\\nEfficient Learning Human cognition is distinguished by its ability to rapidly learn about new concepts from only a handful of examples, leveraging prior knowledge to enable flexible inductive inferences. In order to highlight this human ability as a challenge for AI, Lake and colleagues recently posed a “characters challenge” (Lake et al., 2016). Here, an observer must distinguish novel instances of an unfamiliar handwritten character from other, similar items after viewing only a single exemplar. Humans can perform this task well, but it is difficult for classical AI systems.\\n\\nEncouragingly, recent AI algorithms have begun to make progress on tasks like the characters challenge, through both structured probabilistic models (Lake et al., 2015) and deep generative models based on the abovementioned DRAW model (Rezende et al., 2016b). Both classes of system can make inferences about a new concept despite a poverty of data and generate new samples from a single example concept (Figure 2D). Further, recent AI research has developed networks that “learn to learn,” acquiring knowledge on new tasks by leveraging prior experience with related problems, to support one-shot concept learning (Santoro et al., 2016, Vinyals et al., 2016) and accelerating learning in RL tasks (Wang et al., 2016). Once again, this builds on concepts from neuroscience: learning to learn was first explored in studies of animal learning (Harlow, 1949), and has subsequently been studied in developmental psychology (Adolph,\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n19\\n\\n2005, Kemp et al., 2010, Smith, 1995).\\n\\nTransfer Learning\\n\\nHumans also excel at generalizing or transferring generalized knowledge gained in one context to novel, previously unseen domains (Barnett and Ceci, 2002, Holyoak and Thagard, 1997). For example, a human who can drive a car, use a laptop computer, or chair a committee meeting is usually able act effectively when confronted with an unfamiliar vehicle, operating system, or social situation. Progress is being made in developing AI architectures capable of exhibiting strong generalization or transfer, for example by enabling zero-shot inferences about novel shapes outside the training distribution based on compositional representations (Higgins et al., 2016; Figure 2C). Others have shown that a new class of architecture, known as a progressive network, can leverage knowledge gained in one video game to learn rapidly in another, promising the sort of “far transfer” that is characteristic of human skill acquisition (Rusu et al., 2016a). Progressive networks have also been successfully employed to transfer knowledge for a simulated robotic environment to a real robot arm, massively reducing the training time required on the real world (Rusu et al., 2016b). Intriguingly, the proposed architecture bears some resemblance to a successful computational model of sequential task learning in humans (Collins and Koechlin, 2012, Donoso et al., 2014). In the neuroscience literature, one hallmark of transfer learning has been the ability to reason relationally, and AI researchers have also begun to make progress in building deep networks that address problems of this nature, for example by solving visual analogies (Reed et al., 2015). More generally however, how humans or other animals achieve this sort of high-level transfer learning is unknown, and remains a relatively unexplored topic in neuroscience. New advances on this front could provide critical insights to spur AI research toward the goal of lifelong learning in agents, and we encourage neuroscientists to engage more deeply with this question.\\n\\nAt the level of neural coding, this kind of transfer of abstract structured knowledge may rely on the formation of conceptual representations that are invariant to the objects, individuals, or scene elements that populate a sensory domain but code instead for abstract, relational information among patterns of inputs (Doumas et al., 2008). However, we currently lack direct evidence for the existence of such codes in the mammalian brain. Nevertheless, one recent report made the very interesting claim that neural codes thought to be important in the representation of allocentric (map-like) spaces might be critical for abstract reasoning in more general domains (Constantinescu et al., 2016). In the mammalian entorhinal cortex, cells encode the geometry of allocentric space with a periodic “grid” code, with receptive fields that tile the local space in a hexagonal pattern (Rowland et al., 2016). Grid codes may be an excellent candidate for organizing conceptual knowledge, because they allow state spaces to be decomposed efficiently, in a way that could support discovery of subgoals and hierarchical planning (Stachenfeld et al., 2014). Using functional neuroimaging, the researchers provide evidence for the existence of such codes while humans performed an abstract categorization task, supporting the view that periodic encoding is a generalized hallmark of human knowledge organization (Constantinescu et al., 2016). However, much further work is required to substantiate this interesting claim.\\n\\nImagination and Planning\\n\\nDespite their strong performance on goal-directed tasks, deep RL systems such as DQN operate mostly in a reactive way, learning the mapping from perceptual inputs to actions that maximize future value. This “model-free” RL is computationally inexpensive but suffers from two major drawbacks: it is relatively data inefficient, requiring large amounts of experience to derive accurate estimates, and it is inflexible, being insensitive to changes in the value of outcomes (Daw et al., 2005). By contrast, humans can more flexibly select actions based on forecasts of long-term future outcomes through simulation-based planning, which uses predictions generated from an internal model of the environment learned through experience (Daw et al., 2005, Dolan and Dayan, 2013, Tolman, 1948). Moreover, planning is not a uniquely human capacity. For example, when caching food, scrub jays consider the future conditions under which it is likely to be recovered (Raby et al., 2007), and rats use a “cognitive map” when navigating, allowing inductive inferences during wayfinding and facilitating one-shot learning behaviors in maze-like environments (Daw et al.,\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n20\\n\\n2005, Tolman, 1948). Of course, this point has not been lost on AI researchers; indeed, early planning algorithms such as Dyna (Sutton, 1991) were inspired by theories that emphasized the importance of “mental models” in generating hypothetical experiences useful for human learning (Craik, 1943). By now, a large volume of literature exists on AI planning techniques, including model-based RL methods, which seek to implement this forecast-based method of action selection. Furthermore, simulation-based planning, particularly Monte Carlo tree search (MCTS) methods, which use forward search to update a value function and/or policy (Browne et al., 2012), played a key role in recent work in which deep RL attained expert-level performance in the game of Go (Silver et al., 2016).\\n\\nAI research on planning, however, has yet to capture some of the key characteristics that give human planning abilities their power. In particular, we suggest that a general solution to this problem will require understanding how rich internal models, which in practice will have to be approximate but sufficiently accurate to support planning, can be learned through experience, without strong priors being handcrafted into the network by the experimenter. We also argue that AI research will benefit from a close reading of the related literature on how humans imagine possible scenarios, envision the future, and carry out simulation-based planning, functions that depend on a common neural substrate in the hippocampus (Doll et al., 2015, Hassabis and Maguire, 2007, Hassabis and Maguire, 2009, Schacter et al., 2012). Although imagination has an intrinsically subjective, unobservable quality, we have reason to believe that it has a conserved role in simulation-based planning across species (Hassabis and Maguire, 2009, Schacter et al., 2012). For example, when paused at a choice point, ripples of neural activity in the rat hippocampus resemble those observed during subsequent navigation of the available trajectories (“preplay”), as if the animal were “imagining” each possible alternative (Johnson and Redish, 2007, Ólafsdóttir et al., 2015, Pfeiffer and Foster, 2013). Further, recent work has suggested a similar process during non-spatial planning in humans (Doll et al., 2015, Kurth-Nelson et al., 2016). We have discussed above the ways in which the introduction of mechanisms that replay and learn offline from past experiences can improve the performance of deep RL agents such as DQN (as discussed above in Episodic Memory).\\n\\nSome encouraging initial progress toward simulation-based planning has been made using deep generative models (Eslami et al., 2016, Rezende et al., 2016a, Rezende et al., 2016b) (Figure 2). In particular, recent work has introduced new architectures that have the capacity to generate temporally consistent sequences of generated samples that reflect the geometric layout of newly experienced realistic environments (Gemici et al., 2017, Oh et al., 2015) (Figure 2E), providing a parallel to the function of the hippocampus in binding together multiple components to create an imagined experience that is spatially and temporally coherent (Hassabis and Maguire, 2007). Deep generative models thus show the potential to capture the rich dynamics of complex realistic environments, but using these models for simulation- based planning in agents remains a challenge for future work.\\n\\nInsights from neuroscience may provide guidance that facilitates the integration of simulation with control. An emerging picture from neuroscience research suggests that the hippocampus supports planning by instantiating an internal model of the environment, with goal-contingent valuation of simulated outcomes occurring in areas downstream of the hippocampus such the orbitofrontal cortex or striatum (Redish, 2016). Notably, however, the mechanisms that guide the rolling forward of an internal model of the environment in the hippocampus remain uncertain and merit future scrutiny. One possibility is that this process is initiated by the prefrontal cortex through interactions with the hippocampus. Indeed, this notion has distinct parallels with proposals from AI research that a separate controller interacts with an internal model of the environment in a bidirectional fashion, querying the model based on task-relevant goals and receiving predicted simulated states as input (Schmidhuber, 2014). Further, recent efforts to develop agents have employed architectures that instantiate a separation between controller and environmental model to effect simulation- based planning in problems involving the interaction between physical objects (Hamrick et al., 2017).\\n\\nIn enhancing agent capabilities in simulation-based planning, it will also be important to consider other salient properties of this process in humans (Hassabis and Maguire, 2007, Hassabis and Maguire, 2009). Research into human imagination emphasizes its constructive nature, with humans able to construct fictitious mental scenarios by recombining familiar elements in novel ways, necessitating compositional/disentangled representations of the form\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n21\\n\\npresent in certain generative models (Eslami et al., 2016, Higgins et al., 2016, Rezende et al., 2016a). This fits well with the notion that planning in humans involves efficient representations that support generalization and transfer, so that plans forged in one setting (e.g., going through a door to reach a room) can be leveraged in novel environments that share structure. Further, planning and mental simulation in humans are “jumpy,” bridging multiple temporal scales at a time; for example, humans seem to plan hierarchically, by considering in parallel terminal solutions, interim choice points, and piecemeal steps toward the goal (Balaguer et al., 2016, Solway et al., 2014, Huys et al., 2012). We think that ultimately these flexible, combinatorial aspects of planning will form a critical underpinning of what is perhaps the hardest challenge for AI research: to build an agent that can plan hierarchically, is truly creative, and can generate solutions to challenges that currently elude even the human mind.\\n\\nVirtual Brain Analytics\\n\\nOne rather different way in which neuroscience may serve AI is by furnishing new analytic tools for understanding computation in AI systems. Due to their complexity, the products of AI research often remain “black boxes”; we understand only poorly the nature of the computations that occur, or representations that are formed, during learning of complex tasks. However, by applying tools from neuroscience to AI systems, synthetic equivalents of single-cell recording, neuroimaging, and lesion techniques, we can gain insights into the key drivers of successful learning in AI research and increase the interpretability of these systems. We call this “virtual brain analytics.”\\n\\nRecent work has made some progress along these lines. For example, visualizing brain states through dimensionality reduction is commonplace in neuroscience, and has recently been applied to neural networks (Zahavy et al., 2016). Receptive field mapping, another standard tool in neuroscience, allows AI researchers to determine the response properties of units in a neural network. One interesting application of this approach in AI is known as activity maximization, in which a network learns to generate synthetic images by maximizing the activity of certain classes of unit (Nguyen et al., 2016, Simonyan et al., 2013). Elsewhere, neuroscience-inspired analyses of linearized networks have uncovered important principles that may be of general benefit in optimizing learning these networks, and understanding the benefits of network depth and representational structure (McClelland and Rogers, 2003, Saxe et al., 2013).\\n\\nWhile this initial progress is encouraging, more work is needed. It remains difficult to characterize the functioning of complex architectures such as networks with external memory (Graves et al., 2016). Nevertheless, AI researchers are in the unique position of having ground truth knowledge of all components of the system, together with the potential to causally manipulate individual elements, an enviable scenario from the perspective of experimental neuroscientists. As such, we encourage AI researchers to use approaches from neuroscience to explore properties of network architectures and agents through analysis, visualization, causal manipulation, not forgetting the need for carefully designed hypothesis-driven experiments (Jonas and Kording, 2017, Krakauer et al., 2017). We think that virtual brain analytics is likely to be an increasingly integral part of the pipeline of algorithmic development as the complexity of architectures increases.\\n\\nFrom AI to Neuroscience\\n\\nThus far, our review has focused primarily on the role of neuroscience in accelerating AI research rather than vice versa. Historically, however, the flow of information between neuroscience and AI has been reciprocal. Machine learning techniques have transformed the analysis of neuroimaging datasets—for example, in the multivariate analysis of fMRI and magnetoencephalographic (MEG) data (Cichy et al., 2014, Çukur et al., 2013, Kriegeskorte and Kievit, 2013)—with promise for expediting connectomic analysis (Glasser et al., 2016), among other techniques. Going further, we believe that building intelligent algorithms has the potential to offer new ideas about the underpinnings of intelligence in the brains of humans and other animals. In particular, psychologists and neuroscientists often have only quite vague notions\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n22\\n\\nof the mechanisms that underlie the concepts they study. AI research can help, by formalizing these concepts in a quantitative language and offering insights into their necessity and sufficiency (or otherwise) for intelligent behavior.\\n\\nA key illustration of this potential is provided by RL. After ideas from animal psychology helped to give birth to reinforcement learning research, key concepts from the latter fed back to inform neuroscience. In particular, the profile of neural signals observed in midbrain dopaminergic neurons in conditioning paradigms was found to bear a striking resemblance to TD-generated prediction errors, providing neural evidence that the brain implements a form of TD\\n\\nlearning (O’Doherty et al., 2003, Schultz et al., 1997). This overall narrative arc provides an excellent illustration of how the exchange of ideas between AI and neuroscience can create a “virtuous circle” advancing the objectives of both fields.\\n\\nIn another domain, work focused on enhancing the performance of CNNs has also yielded new insights into the nature of neural representations in high-level visual areas (Khaligh-Razavi and Kriegeskorte, 2014, Yamins and DiCarlo, 2016). For example, one group systematically compared the ability of more than 30 network architectures from AI to explain the structure of neural representations observed in the ventral visual stream of humans and monkeys, finding favorable evidence for deep supervised networks (Khaligh-Razavi and Kriegeskorte, 2014). Further, these deep convolutional network architectures offer a computational account of recent neurophysiological data demonstrating that the coding of category-orthogonal properties of objects (e.g., position, size) actually increases as one progresses higher up the ventral visual stream (Hong et al., 2016). While these findings are far from definitive as yet, it shows how state-of-the- art neural networks from AI can be used as plausible simulacra of biological brains, potentially providing detailed explanations of the computations occurring therein (Khaligh-Razavi and Kriegeskorte, 2014, Yamins and DiCarlo, 2016). Relatedly, properties of the LSTM architecture have provided key insights that motivated the development of working memory models that afford gating-based maintenance of task-relevant information in the prefrontal cortex (Lloyd et al., 2012, O’Reilly and Frank, 2006).\\n\\nWe also highlight two recent strands of AI research that may motivate new research in neuroscience. First, neural networks with external memory typically allow the controller to iteratively query or “hop through” the contents of memory. This mechanism is critical for reasoning over multiple supporting input statements that relate to a particular query (Sukhbaatar et al., 2015). Previous proposals in neuroscience have argued for a similar mechanism in human cognition, but any potential neural substrates, potentially in the hippocampus, remain to be described (Kumaran and McClelland, 2012). Second, recent work highlights the potential benefits of “meta-reinforcement learning,” where RL is used to optimize the weights of a recurrent network such that the latter is able to implement a second, emergent RL algorithm that is able to learn faster than the original (Duan et al., 2016, Wang et al., 2016). Intriguingly, these ideas connect with a growing neuroscience literature indicating a role for the prefrontal cortex in RL, alongside more established dopamine-based mechanisms (Schultz et al., 1997). Specifically, they indicate how a relatively slow-learning dopaminergic RL algorithm may support the emergence of a freestanding RL algorithm instantiated with the recurrent activity dynamics of the prefrontal cortex (Tsutsui et al., 2016).\\n\\nInsights from AI research are also providing novel perspectives on how the brain might implement an algorithmic parallel to backpropagation, the key mechanism that allows weights within multiple layers of a hierarchical network to be optimized toward an objective function (Hinton et al., 1986, Werbos, 1974). Backpropagation offers a powerful solution to the problem of credit assignment within deep networks, allowing efficient representations to be learned from high dimensional data (LeCun et al., 2015). However, until recently, several aspects of the backpropagation algorithm were viewed to be biologically implausible (e.g., see Bengio et al., 2015). One important factor is that backpropagation has typically been thought to require perfectly symmetric feedback and feedforward connectivity, a profile that is not observed in mammalian brains. Recent work, however, has demonstrated that this constraint can in fact be relaxed (Liao et al., 2015, Lillicrap et al., 2016). Random backward connections, even when held fixed throughout network training, are sufficient to allow the backpropagation algorithm to function effectively through a process whereby adjustment of the forward weights allows backward projections to transmit useful teaching signals\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n23\\n\\n(Lillicrap et al., 2016).\\n\\nA second core objection to the biological plausibility of backpropagation is that weight updates in multi-layered networks require access to information that is non-local (i.e., error signals generated by units many layers downstream) (for review, see Bengio et al., 2015). In contrast, plasticity in biological synapses depends primarily on local information (i.e., pre- and post-synaptic neuronal activity) (Bi and Poo, 1998). AI research has begun to address this fundamental issue. In particular, recent work has shown that hierarchical auto-encoder networks and energy-based networks (e.g., continuous Hopfield networks) (Scellier and Bengio, 2016, Whittington and Bogacz, 2017)—models that have strong connections to theoretical neuroscience ideas about predictive coding (Bastos et al., 2012)—are capable of approximating the backpropagation algorithm, based on weight updates that involve purely local information. Indeed, concrete connections have been drawn between learning in such networks and spike-timing dependent plasticity (Scellier and Bengio, 2016), a Hebbian mechanism instantiated widely across the brain (Bi and Poo, 1998). A different class of local learning rule has been shown to allow hierarchical supervised networks to generate high-level invariances characteristic of biological systems, including mirror-symmetric tuning to physically symmetric stimuli, such as faces (Leibo et al., 2017). Taken together, recent AI research offers the promise of discovering mechanisms by which the brain may implement algorithms with the functionality of backpropagation. Moreover, these developments illustrate the potential for synergistic interactions between AI and neuroscience: research aimed to develop biologically plausible forms of backpropagation have also been motivated by the search for alternative learning algorithms. Given the increasingly deep networks (e.g., >20 layer) used in AI research, factors such as the compounding of successive non- linearities pose challenges for optimization using backpropagation (Bengio et al., 2015).\\n\\nConclusions\\n\\nIn this perspective, we have reviewed some of the many ways in which neuroscience has made fundamental contributions to advancing AI research, and argued for its increasingly important relevance. In strategizing for the future exchange between the two fields, it is important to appreciate that the past contributions of neuroscience to AI have rarely involved a simple transfer of full-fledged solutions that could be directly re-implemented in machines. Rather, neuroscience has typically been useful in a subtler way, stimulating algorithmic-level questions about facets of animal learning and intelligence of interest to AI researchers and providing initial leads toward relevant mechanisms. As such, our view is that leveraging insights gained from neuroscience research will expedite progress in AI research, and this will be most effective if AI researchers actively initiate collaborations with neuroscientists to highlight key questions that could be addressed by empirical work.\\n\\nThe successful transfer of insights gained from neuroscience to the development of AI algorithms is critically dependent on the interaction between researchers working in both these fields, with insights often developing through a continual handing back and forth of ideas between fields. In the future, we hope that greater collaboration between researchers in neuroscience and AI, and the identification of a common language between the two fields (Marblestone et al., 2016), will permit a virtuous circle whereby research is accelerated through shared theoretical insights and common empirical advances. We believe that the quest to develop AI will ultimately also lead to a better understanding of our own minds and thought processes. Distilling intelligence into an algorithmic construct and comparing it to the human brain might yield insights into some of the deepest and the most enduring mysteries of the mind, such as the nature of creativity, dreams, and perhaps one day, even consciousness.\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\n24')],\n",
              " [Document(metadata={'source': '/content/the future of AI.pdf'}, page_content='CHAPTER FOURTEEN\\n\\nThe Future of Artificial Intelligence\\n\\nDavid A. Bray\\n\\n“AI can match humans into different ad-hoc teams…to fit a specific public service goal or problem set. If an emergent event or crisis occurs, AI can help identify who is available to assist with what activities—and even help coordinate swarming activities of both humans and machines to assist with the response to the event. AI can learn which humans work better on specific tasks with other humans…”\\n\\n222\\n\\nTHE FUTURE OF ARTIFICIAL INTELLIGENCE\\n\\nBy David A. Bray\\n\\nImagine being able to visit a disability claims office in a digital environment. Imagine a patent examiner equipped with digital assistants that could do the bulk of administrative work behind processing patents. Artificial intelligence (AI) may make both of these scenarios a reality. This chapter addresses the question of how advances in and adoption of AI will transform public service over the next twenty years. AI has dual meanings: • artificial intelligence • augmented intelligence, specifically how human capabilities can be improved by pairing them with machines to collectively work smarter\\n\\nMost of the benefits to government will come from a people-centered approach of pairing humans with machine learning to amplify human strengths via augmented intelligence. Such a people-centered approach means that the success of public service in the future depends on identifying beneficial ways to augment the extant human abilities of networked, cross-sector teams— who want to improve the delivery of public service—with digital assistants and learning machines to amplify the team’s strengths, mitigate any possible blind spots, and increase the capabilities of the team as a whole.\\n\\nThis chapter breaks down the possibilities into the near-term future (2020-2025), the medium-term future (2025-2030), and the long-term future (2030-2040), and then focuses on specific initiatives that are likely to be launched to employ AI to transform the public sector.\\n\\nThe Near-Term Future (2020-2025)\\n\\nThe near-term future includes using AI in specialized applications to support the information and logistics functions traditionally performed by government to provide government services. It is important to note that when deciding where to use AI, public servants determine to what degree the machines providing this assistance operate autonomously vs. semi-auton- omously.1 For the near future, most machines will probably provide support that still requires a human to act or make a decision.\\n\\nFor all the near-term future possibilities discussed below, government should implement “public review boards” that look at the diversity, consis- tency, and appropriateness of the data used. Without diverse or consistent data, the AI trained by the data may make decisions that erode public trust. Without appropriate use of data, public trust may also erode. For represen- tative government, using “public review boards” in a form akin to a random jury selection process may be one way to ensure improved oversight. Such\\n\\nThe Future of Artificial Intelligence\\n\\nactivities would also involve outreach efforts by public service organizations, to increase digital literacy and the understanding of AI and what it can do. Following are some examples of how AI will be deployed in the near future. Increased Use of AI-Supported Assistance for Individuals Seeking Government Information. Several cities already have “311” telephone lines and mobile apps to assist individuals with non-emergency city services as well as to provide information on programs, events, and activities in the city. Such public-facing services will employ AI to help individuals with their questions. Humans would still need to be in the loop for new questions where the AI does not know the answer, or instances where the AI is uncertain about the question being asked. Such AI assistance will also help government employ- ees with questions about onboarding, starting a new role, help with an exist- ing role, retiring, and other internal service queries.\\n\\nIncreased Use of AI-Supported Assistance for Talent Management and Skills Matching. AI will help community members find new jobs and tailor training to hone and improve their skills for upward mobility in their jobs. Unemployment and career assistance services will provide an AI assistant via phone or at a physical career support center. The AI assistant will serve as a personal scout for new jobs based on questions answered by the individual about their skills, abilities, and desired work. The AI assistant will also help with tailored training opportunities accessible through in-person community colleges or online. Such AI assistance for talent management and skills match- ing will also be employed internally to government itself, to help the existing government workforce find new work opportunities and tailor individual train- ing to further develop skills and abilities.\\n\\nIncreased Use of AI-Supported Review of Public Applications and Filings. Current government functions often entail detailed forms and pro- cesses to either prove or approve services to the public. Such functions include licenses, land and jurisdictional approvals, individual claims, payment processing, and travel-related documents. The current linear process of such applications is outdated, usually requiring a human to identify the right form, fill it out, and submit it—only to find that another form was needed or more information was required. Instead, AI assistants will provide more tailored support to individuals, to better understand what they are applying for and pre-review a public application or filing prior to human approval.\\n\\nIncreased Use of AI-Supported Legal, Financial and Ethics Reviews. Legal, financial, and ethics reviews often entail a rules-based approach of reviewing information submitted to ensure it comports with specified require- ments. Such reviews fit well with how AI can assist humans. An AI assistant will do the initial review, let an individual know if more information is required, and provide a preliminary result for final review by a human.\\n\\nIncreased Use of AI-Supported Detection of Fake Images, Videos, and Audio Files. It currently is possible to “clone” someone’s face to an image or video of someone else’s body. Voices also can be “cloned” to produce audio recordings that sound like someone saying something they did not say.\\n\\n223\\n\\n224\\n\\nDavid A. Bray\\n\\nDetecting such fake files requires detailed analysis and pattern matching, looking for inconsistences. An AI can support a human in detecting such irregularities.\\n\\nIncreased Use of AI-Supported Biometrics for Boarding Planes, Crossing Borders. Machines are also good at identifying the biometrics that make one individual different from another. Within sufficient training, an AI application will identify a person based on their face—and possibly other factors, such as their fingerprints or the sound of their voice. Such biometrics would allow individuals to board planes and cross borders without having to carry a physical identification card.\\n\\nA Brief History of Artificial Intelligence\\n\\nIn 1943, a young academic by the name of Herbert A. Simon received his PhD from University of Chicago with a doctoral thesis focused on administrative behavior within organizations. He wrote his thesis after co-authoring an earlier study in 1939, entitled Measuring Municipal Activities, with Clarence Ridley.2 From research into administration behaviors and municipal administration, Simon would later contribute to the first wave in the field of artificial intel- ligence, specifically problem-solving algorithms. In 1957, he partnered with Allen Newell to develop a General Problem Solver that separated information about a problem from the strategy required to solve a problem.3 For his con- tributions to the fields of artificial intelligence, information processing, decision making, and problem solving, both he and Allen Newell received the Turing Award from the Association for Computing Machinery in 1975.4\\n\\nSince then, the field of AI has experienced two more waves of innovation. Starting in the mid-1960s, the second wave of AI innovation included expert systems represented mainly as “if-then” statements instead of procedural code. The goal of such systems was to perform tasks that expert humans also could do, such as evaluate geological sites or perform medical diagnoses.5 In paral- lel, advances taught machines to solve problems, specifically to intelligently play human games, including IBM Deep Blue playing against chess masters in the late 1990s. Later, IBM Watson won against two Jeopardy! Champions in 2011. Google DeepMind’s AlphaGo won against a top-ranked world Go player in 2016.6 A Carnegie Mellon University poker AI won a 20-day tournament in 2017.7\\n\\nThe Future of Artificial Intelligence\\n\\nApproximately fifteen years into the start of the 21st century, cumulative advances in the speed, size, and scale of microprocessors and computer mem- ory reached a tipping point that triggered a third wave of AI innovation. Some of the algorithms originally envisioned by AI pioneers, such as the backpropa- gation algorithm that allows neural networks to solve problems far faster than earlier approaches to machine learning, could now be run at sufficient speeds to make the algorithms valuable to solve real-world problems.8 Machine learn- ing is a branch of AI that employs large data sets to statistically train a machine to make accurate categorizations of what something is or is not; e.g., training a machine to identify images accurately of different objects, places, or entities.9\\n\\nIncreased Use of AI-Supported Assistance for Analyzing Geospatial Data. In the next few years, an explosion of geospatial data will become available from drones for civilian purposes, private cube satellites, and sen- sors associated with the “internet of things.” AI can assist in making sense of all that information—as well as identifying patterns of importance to improve the delivery of public services. To do this appropriately, the public will need to have conversations and greater insights into what information is being col- lected and for what purposes.\\n\\nThe Medium-Term Future (2025-2030)\\n\\nThe medium-term future includes AI moving from specialized applications to embedding AI in all operations to support both the operations of govern- ment and the interpretation and decision-related functions traditionally done by government to improve public services. AI will become an essential com- ponent of all government operations in this time period.\\n\\nFor all the medium-term future possibilities discussed below, public service will need to solve growing cybersecurity challenges.10 If more public service functions are supported by AI, then any activity to alter an AI algo- rithm—or worse, the data used to train the AI—could cause the AI to make decisions that hurt people, harm property, or erode trust. A new science of understanding the resiliency, and by extension the brittleness, of AI apps to disruption by false data or other exploits will need to be developed if both the public and the public service workforce is to trust interactions with AI. Fol- lowing are some examples of AI-enabled public services.\\n\\nUse of AI-Enabled Delivery of Materials and Provision of Transportation. By 2025, engineers probably will have solved the limitation of autonomous vehicles to intelligently navigate in heavy rain or snowy conditions. This would allow public services to be paired with AI-enabled autonomous vehicles to include fire and emergency services.\\n\\nUse of AI-Enabled Robots to Offset Repetitive and Manually-Intensive Work. One of the current limitations of robots today is that most cannot grip\\n\\n225\\n\\n226\\n\\nDavid A. Bray\\n\\nobjects as well as a human. By 2025, engineers will probably have solved this limitation, making robots paired with AI a beneficial mechanism for the deliv- ery of materials to support public service. This will include using AI for civil construction efforts, disaster response, healthcare, or other public functions. Use of AI-Enabled “Tipping and Cueing” of Areas to Focus On. In a world in which more and more data is being produced by sensors connected to the “internet of everything,” by 2025 AI will have advanced to the point where it will be monitoring different data streams for patterns of interest—or irregularities—that can then cue a human expert to look at something further. The human will then take an action that would further educate the AI for additional patterns to seek. This will include helping public service experts monitoring agricultural and health conditions in a geographic area.\\n\\nUse of AI-Enabled Digital Assistants to Detect and Help Understand Biases. We all have implicit biases. Each of us have biases that we accumu- late from our past experiences, including our early childhood. Some of these biases are discriminatory, such as an implicit preference for people who look like us or a favoritism to people who are taller and exhibit other physical traits. For public service, such implicit biases should not discourage a diverse workforce that seeks to serve the public. By 2025, AI will help hold up a “digital mirror” to compare our decisions and other interactions with those of others. This can help each of us understand where our biases are and what to do so that we may become less biased. Such an activity will also start to probe the boundary between the tacit, implicit knowledge a public service expert accumulates and the explicit knowledge they can articulate and share. Use of AI-Enabled “Digital Twins” of Real-World Dynamics. Through extending the data collected from the future “internet of everything,” by 2025 AI will allow public service organizations to build models of real-world dynamics—either of actual physical assets or social interactions. Such mod- els will create highly accurate “digital twins” that would allow individuals in public service to experiment with certain scenarios in a digital environment. Individuals will also do training for crisis response and other high-intensity environments in a “digital twin” scenario, with AI providing recommendations on how to improve based on performance in the digital environment.\\n\\nThe Long-Term Future (2030-2040)\\n\\nIn 2030 and beyond, there are “farther out” ideas for the future of AI in public service. While predicting the specific future capabilities of AI is difficult, we assume advances will continue in the speed, size, and scale of micropro- cessors and computer memory to enable faster delivery of all the assisting and enabling functions of public service referenced earlier in this chapter. We can anticipate that the adoption of quantum computing, sophisticated augmented reality, and other techniques will be used to fundamentally transform the role of government to a more personalized approach in which government can respond to the unique needs of each citizen. The job of government will be\\n\\nThe Future of Artificial Intelligence\\n\\nradically changed.\\n\\nThe ability for AI to work with and help humans better act, respond, and provide public services should be fairly robust by this point. At this point, we could imagine a future where “krewes” of humans augmented with machines perform the work of public service, perhaps on a part-time basis if other pre- dictions associated with the future of work also occur by 2030.\\n\\nBy 2030, functions that used to be provided solely by government agen- cies may now be provided either through a part-time workforce or a “Public Service Corps” willing to spend some hours a week on efforts assigned to them by a coordinating public service AI. This “Public Service Corps” would embody what science-fiction author Bruce Sterling once dubbed a “krewe.”11 For a krewe, the entity of importance is not an individual per se. Rather, it is the combined abilities of a team of human individuals augmented with intelligent assistants and relevant information streams to do the work they need to do. A diverse krewe brings many different perspectives to a scenario, ideally overcoming any specific individual biases.\\n\\nIn such a futuristic scenario, several of the rote and repetitive functions currently performed by government would be performed by AI in a semi-auto- mated fashion such that applications associated with civil society activities are pre-screened and feedback provided to human applicants prior to a final human determination. Humans will still be involved for the more creative and final decision roles. The need for clerical workers or administrative workers to process applications will have gone down significantly.\\n\\nIndividuals can work part-time because the machines will do much of the work in the background. In such a civil society, choosing to work in public service is seen as a true service. Individuals may be able to work in the pri- vate sector in areas that AI assistants determine do not create conflict with their public service assignments. For humans working with AIs in krewes, it also would be important to identify mechanisms to reward a whole-of-team outcome and performance instead of solely individual actions. By working together, humans and machines, the krewe would be collectively more intel- ligent and capable than any one individual alone.\\n\\nAI can match humans into different ad-hoc teams or krewes to fit a spe- cific public service goal or problem set. If an emergent event or crisis occurs, AI can help identify who is available to assist with what activities—and even help coordinate swarming activities of both humans and machines to assist with the response to the event. AI can learn which humans work better on specific tasks with other humans, and AI may even be able to identify which robots or parts of the AI hardware might be faulty or near-failing and thus need repair.\\n\\nSuch a future would represent a major disruption to how government and public service currently function. This disruption would impact the workforce, policies, budgetary allocations, and administrative processes associated with current civil society functions. Such a future might impact military and intel- ligence functions in similar ways, with individuals who had already signed-up\\n\\n227\\n\\n228\\n\\nDavid A. Bray\\n\\nto serve being “called up” by an AI if an urgent need matching their skill set arose, for example responding to a cyber event or helping with some other national security event.\\n\\nAlternative Scenarios for the Future\\n\\nWhile “long-term” futures are difficult to envision, it is possible to set forth two contrasting scenarios for the future of AI in public service.\\n\\nScenario One: An Optimistic View of the Future In order to achieve the vision of “A Public Service ‘Of the People, By the People, For the People,’” government workers will need to overcome budgetary challenges, potentially restrictive policies, ossifying processes, aging legacy IT systems, and skepticism to the point of strong distrust of the activities of government. With strong support, both from the public and elected political leaders, our representative government will be able to cross the chasm between how government currently operates and the ways in which public service could be dramatically transformed and deliver vastly better services and results to the public in 2040.\\n\\nWhile this chapter presents potential milestones for where AI in public service could go, there will need to be experiments to gain expertise on the best way to align policies, people, processes, and technology to achieve desired goals. Unlike the venture capital community in the private sector, public service oper- ates with money from taxpayers who have a right to expect that their money is spent wisely. This can create an environment in which maintaining the status quo, instead of attempting to embrace AI, may slow or prevent a government from achieving the benefits of AI.\\n\\nThe public will also rightfully need to be informed about what AI and algorithms do and how they are being used. Transparency in these activities will be key to engender public trust. Public discussions on what data should be used to train and inform AI activities will need to occur. A workforce savvy enough to keep up with both the technologies associated with AI—and more importantly the civil, legal, and people-centered impacts of such technologies in public service—will need to be recruited and retained.\\n\\nSafe spaces to learn and explore how AI can improve public service—and then to translate these activities into public service-wide scaled activities—will need to be put in place.12 Without safe spaces and possibly high-priority goals, anything that appears to have gone wrong or not worked on the first try may be politicized and prevent representative governments from being able to adapt to the rapidly accelerating age of AI. For public service to become more agile and\\n\\nThe Future of Artificial Intelligence\\n\\nresilient, the barriers will come not from technology. Rather, the barriers will be human-centered, coming from a risk-averse political culture unwilling to make mistakes in areas where it is okay to make mistakes (i.e., the mistakes do not harm people or property), learning, adapting, and improving.\\n\\nScenario Two: A Pessimistic View of the Future An alternative, cautionary note for the future of AI and public service is one in which AI is used by government, well-intended or not, to monitor the activities of individuals. Instead of empowering individuals, AI is used to sort and filter behaviors that the government does not permit. No insight into what AI and its algorithms are doing for the government is shared with the public, and the public does not know that they each have different risk, credit, and behavioral scores that influence what they can and cannot do in society.\\n\\nSuch a scenario would be a pessimistic one in which people are dehumanized and disconnected from engaging in civil society. Distrust in public service is heightened and no one feels like they can help make a difference. While the foreseen uses of AI discussed in this chapter seek to prevent such a scenario, the cautionary note that it could occur is worth remembering – if only to emphasize why a more people-centered “better way forward” is needed for the future of AI and public service ahead.\\n\\nAs we embrace the future of AI in public service, we must recognize that AI technologies will reflect the choices we humans make about how to use it, whom to include, and how to ensure the diversity, consistency, and appro- priateness of AI’s activities within civil societies. Since we are human, not all decisions made initially will be perfect. However, with an environment that encourages informed experimentation and appropriate safeguards to protect the public, we can course-correct and over time improve how civil society operates for the future ahead.\\n\\nDavid A. Bray is executive director for the People-Centered internet coalition, Chief Strategy Officer at the geospatial company MapLarge, and gives guest lectures at Singularity and Harvard Universities on leadership in a networked world and how we might encourage a more people-centered internet for the future.\\n\\n229\\n\\n230\\n\\nDavid A. Bray\\n\\nEndnotes 1\\n\\nPartnership for Public Service, The Future Has Begun: Using Artificial Intelligence to Transform Government, IBM Center for The Business of Government, 2017. C. E. Ridley and H. A. Simon, Measuring Municipal Activities (Chicago: International City Managers’ Association, 1938).\\n\\n2\\n\\n3 H. A. Simon and A. Newell, “Human problem solving: The state of the theory in 1970,”\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\nAmerican Psychologist 26, No. 2 (1971). “Herbert (“Herb”) Alexander Simon,” A.M. Turing Award, accessed May 10, 2018, https:// amturing.acm.org/award_winners/simon_1031467.cfm “Expert Systems,” accessed May 10, 2018, https://people.cs.clemson.edu/~goddard/ texts/cpsc8100/chapA7.pdf “Human vs Machine: Five epic fights against AI,” New Scientist, accessed May 10, 2018, https://www.newscientist.com/article/2133146-human-vs-machine-five-epic-fights- against-ai/ “AI just won a poker tournament against professional players,” New Scientist, accessed May 10, 2018, https://www.newscientist.com/article/2119815-ai-just-won-a-poker-tour- nament-against-professional-players/ “How the backpropagation algorithm works,” Neural Networks and Deep Learning, accessed May 10, 2018, http://neuralnetworksanddeeplearning.com/chap2.html\\n\\n9 N.M. Nasrabadi, “Pattern Recognition and Machine Learning,” Journal of Electronic Imag-\\n\\ning, 16, No. 4, (2007).\\n\\n10 U.S. Government Accountability Office, Artificial Intelligence: Emerging Opportunities,\\n\\nChallenges, and Implications, GAO-18-142SP, (2018).\\n\\n11 B. Sterling, Distraction (New York City: Spectra, 1999). 12 “Do We Need A “Civilian ARPA” for AI?,” Trajectory, accessed May 10, 2018, http://trajec-\\n\\ntorymagazine.com/do-we-need-a-civilian-arpa-for-ai/')]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Yw7wd7mUOHfa"
      },
      "outputs": [],
      "source": [
        "# Split contents into text chunks\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1700,\n",
        "    chunk_overlap=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn84d1foPMo_",
        "outputId": "66eea068-4573-4d7d-b2b5-fb00bd45ffbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1748, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3035, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2181, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2096, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3114, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1782, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3249, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3291, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1734, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1839, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2232, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2236, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3263, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1850, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2440, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2364, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1703, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2258, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2162, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2090, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1820, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1881, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1892, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1720, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1730, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1710, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2171, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1738, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3446, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3459, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3363, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1898, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1791, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1727, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2018, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1744, which is longer than the specified 1700\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2065, which is longer than the specified 1700\n"
          ]
        }
      ],
      "source": [
        "# Gather all text chunks into a list\n",
        "texts = []\n",
        "for doc in documents:\n",
        "    text_chunks = text_splitter.split_documents(doc)\n",
        "    texts.extend(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J0v8VJjUP1Lj",
        "outputId": "d4cb7803-cf58-40e8-da97-f3396370656e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Undergraduate Topics in Computer Science\n",
            "\n",
            "Wolfgang Ertel\n",
            "\n",
            "Introduction to Artificial Intelligence\n",
            "\n",
            "Second Edition\n",
            "\n",
            "Undergraduate Topics in Computer Science\n",
            "\n",
            "Series editor Ian Mackie\n",
            "\n",
            "Advisory Board Samson Abramsky, University of Oxford, Oxford, UK Karin Breitman, Pontiﬁcal Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil Chris Hankin, Imperial College London, London, UK Dexter Kozen, Cornell University, Ithaca, USA Andrew Pitts, University of Cambridge, Cambridge, UK Hanne Riis Nielson, Technical University of Denmark, Kongens Lyngby, Denmark Steven Skiena, Stony Brook University, Stony Brook, USA Iain Stewart, University of Durham, Durham, UK\n",
            "\n",
            "Undergraduate Topics in Computer Science (UTiCS) delivers high-quality instruc- tional content for undergraduates studying in all areas of computing and information science. From core foundational and theoretical material to ﬁnal-year topics and applications, UTiCS books take a fresh, concise, and modern approach and are ideal for self-study or for a one- or two-semester course. The texts are all authored by established experts in their ﬁelds, reviewed by an international advisory board, and contain numerous examples and problems. Many include fully worked solutions.\n",
            "\n",
            "More information about this series at http://www.springer.com/series/7592\n",
            "\n",
            "Wolfgang Ertel\n",
            "\n",
            "Introduction to Artificial Intelligence\n",
            "\n",
            "Second Edition\n",
            "\n",
            "Translated by Nathanael Black With illustrations by Florian Mast\n",
            "\n",
            "123\n",
            "\n",
            "Wolfgang Ertel Hochschule Ravensburg-Weingarten Weingarten Germany\n",
            "\n",
            "ISSN 1863-7310 Undergraduate Topics in Computer Science ISBN 978-3-319-58486-7 DOI 10.1007/978-3-319-58487-4\n",
            "\n",
            "ISSN 2197-1781\n",
            "\n",
            "(electronic)\n",
            "\n",
            "ISBN 978-3-319-58487-4\n",
            "\n",
            "(eBook)' metadata={'source': '/content/Introduction to Artificial Intelligence ( PDFDrive ).pdf'}\n",
            "-----------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------\n",
            "page_content='ISSN 1863-7310 Undergraduate Topics in Computer Science ISBN 978-3-319-58486-7 DOI 10.1007/978-3-319-58487-4\n",
            "\n",
            "ISSN 2197-1781\n",
            "\n",
            "(electronic)\n",
            "\n",
            "ISBN 978-3-319-58487-4\n",
            "\n",
            "(eBook)\n",
            "\n",
            "Library of Congress Control Number: 2017943187\n",
            "\n",
            "1st edition: © Springer-Verlag London Limited 2011 2nd edition: © Springer International Publishing AG 2017 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.\n",
            "\n",
            "trademarks, service marks, etc.\n",
            "\n",
            "Printed on acid-free paper' metadata={'source': '/content/Introduction to Artificial Intelligence ( PDFDrive ).pdf'}\n"
          ]
        }
      ],
      "source": [
        "# Get the first 5 text chunks\n",
        "print(texts[0])\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "print(texts[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "ab1229400c0b413998af7b54d77ff6b5",
            "003f0d9528474b7391b46ef3aa0d4c4d",
            "260772fbe5d649cab41b82b1993dda3d",
            "31ab80fb5e0a4274935e7ce66f9af845",
            "fd79391df61945e694a06b9c0a48dbc9",
            "4ec94254ad384db4be196f5a7accf14b",
            "8adccb78528d46dfacc37bf41d87a648",
            "445ceef5b82e45aeb3e1fb48837c898d",
            "1f18f322203d47be93b4238699026780",
            "2bcd1181f5d0435abc64adb596748a06",
            "191617780c344097b23419de401485f0",
            "bdfc5221e5914eaabd0404c9037ff6fd",
            "18f085bd319e40ec8f3bd33eb54b01bc",
            "99d2f82a21d047709c69e73528601241",
            "4ac8ed21286f4dd3a9d6ceef25d1cafc",
            "847c8f53440f49c9b8ae65ec941ee06c",
            "f985a2c117bf4a60a6303673dfe73c90",
            "5a6ed391e88146a9a164082d05c93461",
            "250a392a2afd475db0a21e9dcdfd83ec",
            "9ad72c36692044c1b76650483c30604e",
            "91d329b6e13343009072ee2d38e61700",
            "de60a03eef4f48b1884ae44a3695bf97",
            "d0b4655868ac480abf9b029a82c65d4c",
            "931fcbd09fc54d548879f7f86b802406",
            "a94d4992fb984d10abad4b1933b670d4",
            "05235bbddacc407c8eb672d24f03fd89",
            "47fa1dd63ce1471ab8a066269bd3b2a6",
            "f8aacb7a0b3b400e95d7a7e7cc1e9f96",
            "64ae150489144acc8f4f444a25ed49a2",
            "74292b1d1f784959ae71838aa9c8a31b",
            "2015ee083bb5481cbf7dc2759277a813",
            "ec526829ab034b45af763e651edfbd4b",
            "f86fb4efeb8844989f1686d803ea6783",
            "79b0ba3a25964fae860e17290054a4c3",
            "ac4ce096afe343b9a732d3ff80d5878a",
            "8734f74eced446c6b315a6271b34b7fe",
            "765ca2da86854417bd736853ba3cfff5",
            "89eb10e5ad894baead243e365569df95",
            "3b5792cf4ac840fb869e7cf72a99f43d",
            "554d6db0a245428db8c40fdf42bdda70",
            "f465b6fe4ece4ca29213d401a6acc60a",
            "c784aaaa4ba94944bd95862ec62dfbc5",
            "13d3713f6cc34eca96c6f675dbce7f1b",
            "ae36f6b1ca9d451080de83034df4f0c1",
            "bf3f03367cb044d4818c4d4668ccce79",
            "149ed0484bdf472bb4df628c070491a2",
            "253243cebd0e49249e53f27befdd6328",
            "760d5a7f6c7e4598ad9a7d8191ae7f16",
            "b6f6ad4f20b34fbc9078a3a67589c540",
            "f9587751111848af99842bf24249ebe3",
            "2a904c9419c04d13b19fefe1bf8dafc9",
            "18ee20039afd4c4380ae9c779020a60c",
            "4e2bba50bdfb43caaaaa0c33dcb23ea8",
            "3b135ce2c8a54979924e42734de89b4d",
            "88b340aec449464da4722f88baf96946",
            "55709f3cd0b542b5a202e948b1e29c66",
            "719a1907d4064be19bbf0739789c7c5f",
            "f13997759e3c4c46ae4663723d5af111",
            "b07484423e05437d9453a6fb8dccfd44",
            "50c4ce5659db43f9b871e08faf770ee2",
            "7d9489cae36644fdbadb174c0488cdef",
            "64b196f76dec426baadfc49d422ba627",
            "aedd84f865a1461d82ce1be9b9c35456",
            "70c9d647a7fa4309bc31bd00033af486",
            "5ccfd9d7ac4a4755a29faeac9576c8e7",
            "cd6794c43e1846a9a7fb0c2274dbd868",
            "c0ba1abedd53413bb3cc74c73011e3b7",
            "187f45a836a649789029958c4a1678f0",
            "8da76c5493864b17b645cc0a0d0685e8",
            "a35015a53c53493c8c000b7a1d2e4ab1",
            "61fced29bb004890bd20eede31eaacb1",
            "752a370b7a554e0fa70160025f7c8b99",
            "2320dc6aae284dd3b8282d9a0fd8c511",
            "dd8877a8c3d04260a85b82409e5bcb45",
            "c209d4e2151c424b9a7de516b75ee7a5",
            "375863230f5542c0870680bf52753ed6",
            "e071aceb04684bf3895c2cd2f080a2f9",
            "8a59cc9e28f14473a913c7e8d4805ae3",
            "c2e32e81634b4af98eaf58ceec65147f",
            "630e4066e14546f692b7594b4cb292e7",
            "4ce44dd4ce2c4176af95c1bde8cbc650",
            "31a93bdc0a9942e8a3750a2175700b20",
            "0bb8ea1e5b03464b91e97a6b39e75397",
            "1a8ad51d2b2e4cad925f68a28c05d25a",
            "20555e5328d14a6692ea6b2afd56d70d",
            "2042fface84149a5aa4ec3e8b9cef26f",
            "753d0397314b46b5982ffb269b29f9c0",
            "9d38268badd44286a17d181a6321fb7c",
            "12afbfe865034484bdfd594326b8d46c",
            "a715891e7fd74ffab2ee91279cd9a6af",
            "b126b52a3ff04d9ea5f12eea601d964c",
            "b93dae6467c642898fe7c7b454509d44",
            "a4befaaa2bdd40818e05e72df596bcb3",
            "f173b94636f440c68743574c73dafd26",
            "18383879cb784814be31c2ba3a7ab16e",
            "bf3266a156534736b6f3d9315d7272c8",
            "18458cc888e54e63a48b598584472ae8",
            "3b2fa3daf7184e638da823900a436981",
            "87dc1828b1d94028ac011302425e5de7",
            "57300de398c245ce975a34976fd7610d",
            "55ddfcc1e1994e04b45752151c9ce0d7",
            "b01ba078301546d38d15ca2cd8087ab8",
            "1520f7a3a1404d56b162a22599fc352a",
            "38ad05af83f64b3580ae1a29868f6c78",
            "e8c12384729d46e8bf8fb0ce785cab1b",
            "1a1d413fdacc4cbeb13dfd9520b66f17",
            "b18313141f4b495e974a680fc391c645",
            "2a674b3829af43b6843c08c812f76212",
            "fd53f41a327e4e3aa1ffe3b57c59f68a",
            "81b5e2884b6841a9a7c4a16069cb3d73",
            "f2da6e7e91624f6d8ceb67f756ec958a",
            "1e2f12a64f6d4baa87e6730ab51d5147",
            "cdba425968f24ba3a97c6af0020ba07f",
            "7279288590754c3d989478fab07b2fd3",
            "d11de2ca75f047078967d78e2f36e273",
            "9796404d187b460b8bceb9c58b7571c8",
            "b371b2fc8e2b4d7ebdab7bf4d878e4e0",
            "d80552cf525947b4a707cddb483d6262",
            "4e16d59ed64d4a86b03e57f95c664e4b",
            "3f72b03e8f424c65ab6fa80dbd9826dc",
            "ebc15dd17ef0449c8f42187d51487acb"
          ]
        },
        "collapsed": true,
        "id": "YsCwQ0FYP5sg",
        "outputId": "161e56cf-0d12-4891-9e0f-ea4381084082"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab1229400c0b413998af7b54d77ff6b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdfc5221e5914eaabd0404c9037ff6fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0b4655868ac480abf9b029a82c65d4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79b0ba3a25964fae860e17290054a4c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf3f03367cb044d4818c4d4668ccce79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55709f3cd0b542b5a202e948b1e29c66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0ba1abedd53413bb3cc74c73011e3b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a59cc9e28f14473a913c7e8d4805ae3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12afbfe865034484bdfd594326b8d46c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57300de398c245ce975a34976fd7610d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2da6e7e91624f6d8ceb67f756ec958a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Instantiate embedding\n",
        "embedding = HuggingFaceEmbeddings()\n",
        "\n",
        "# Create variable for vector database folder\n",
        "persist_directory = \"/content/chroma_db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q7iVunrdST26"
      },
      "outputs": [],
      "source": [
        "# Create variable for vector database\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=texts,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r0TROIrwTbNN"
      },
      "outputs": [],
      "source": [
        "# Create retriever to retrieve information from our vector database\n",
        "retriever = vectordb.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rV6TDoIFW0mi"
      },
      "outputs": [],
      "source": [
        "# Create a llm from groq\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    temperature=0.5,\n",
        "    groq_api_key=groq_api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "apAzUfZhYEAo"
      },
      "outputs": [],
      "source": [
        "# Create a conversational chain\n",
        "conv_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO_Kd6GNZAPI",
        "outputId": "b7670ad9-e6d3-490c-a049-5da5219b6151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: According to the provided context, there is no single definition of Artificial Intelligence (AI) that is universally accepted by practitioners. However, some common definitions include:\n",
            "\n",
            "* A computerized system that exhibits behavior that is commonly thought of as requiring intelligence.\n",
            "* A system capable of rationally solving complex problems or taking appropriate actions to achieve its goals in whatever real-world circumstances it encounters.\n",
            "* A set of technologies that try to imitate or augment human intelligence.\n",
            "\n",
            "Experts also agree that AI is a broad field that includes various subfields, such as machine learning, natural language processing, and neural networks. The goal of AI is to develop machines that behave as though they were intelligent, as defined by John McCarthy, one of the pioneers of AI, in 1955.\n",
            "Source Document: [Document(metadata={'source': '/content/preparing for future of ai.pdf'}, page_content='Simultaneously, industry has been increasing its investment in AI. In 2016, Google Chief Executive Officer (CEO) Sundar Pichai said, “Machine learning [a subfield of AI] is a core, transformative way by which we’re rethinking how we’re doing everything. We are thoughtfully applying it across all our products, be it search, ads, YouTube, or Play. And we’re in early days, but you will see us\\u200a—\\u200ain a systematic way\\u200a—\\u200aapply machine learning in all these areas.”7 This view of AI broadly impacting how software is created and delivered was widely shared by CEOs in the technology industry, including Ginni Rometty of IBM, who has said that her organization is betting the company on AI.8\\n\\nWhat is Artificial Intelligence?\\n\\nThere is no single definition of AI that is universally accepted by practitioners. Some define AI loosely as a computerized system that exhibits behavior that is commonly thought of as requiring intelligence. Others define AI as a system capable of rationally solving complex problems or taking appropriate actions to achieve its goals in whatever real world circumstances it encounters.\\n\\nExperts offer differing taxonomies of AI problems and solutions. A popular AI textbook9 used the following taxonomy: (1) systems that think like humans (e.g., cognitive architectures and neural networks); (2) systems that act like humans (e.g., pass the Turing test via natural language processing; knowledge representation, automated reasoning, and learning), (3) systems that think rationally (e.g.,'), Document(metadata={'source': '/content/preparing for future of ai.pdf'}, page_content='General AI (sometimes called Artificial General Intelligence, or AGI) refers to a notional future AI system that exhibits apparently intelligent behavior at least as advanced as a person across the full range of cognitive tasks. A broad chasm seems to separate today’s Narrow AI from the much more difficult challenge of General AI. Attempts to reach General AI by expanding Narrow AI solutions have made little headway over many decades of research. The current consensus of the private-sector expert community, with which the NSTC Committee on Technology concurs, is that General AI will not be achieved for at least decades.14\\n\\n10 Frank Chen, “AI, Deep Learning, and Machine Learning: A Primer,” Andreessen Horowitz, June 10, 2016, http://a16z.com/2016/06/10/ai-deep-learning-machines.\\n\\n11 Pedro Domingos, The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World (New York, New York: Basic Books, 2015).\\n\\n12 Narrow AI is not a single technical approach, but rather a set of discrete problems whose solutions rely on a toolkit of AI methods along with some problem-specific algorithms. The diversity of Narrow AI problems and solutions, and the apparent need to develop specific methods for each Narrow AI application, has made it infeasible to “generalize” a single Narrow AI solution to produce intelligent behavior of general applicability.\\n\\n13 Mike Purdy and Paul Daugherty, “Why Artificial Intelligence is the Future of Growth,” Accenture, 2016, https://www.accenture.com/us-en/_acnmedia/PDF-33/Accenture-Why-AI-is-the-Future-of-Growth.pdf.'), Document(metadata={'source': '/content/Introduction to Artificial Intelligence ( PDFDrive ).pdf'}, page_content='Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .\\n\\nContents\\n\\n339\\n\\n351\\n\\nIntroduction\\n\\n1.1 What Is Artificial Intelligence?\\n\\nThe term artiﬁcial intelligence stirs emotions. For one thing there is our fascination with intelligence, which seemingly imparts to us humans a special place among life forms. Questions arise such as “What is intelligence?”, “How can one measure intelligence?” or “How does the brain work?”. All these questions are meaningful when trying to understand artiﬁcial intelligence. However, the central question for the engineer, especially for the computer scientist, is the question of the intelligent machine that behaves like a person, showing intelligent behavior.\\n\\nThe attribute artiﬁcial might awaken much different associations. It brings up fears of intelligent cyborgs. It recalls images from science ﬁction novels. It raises the question of whether our highest good, the soul, is something we should try to understand, model, or even reconstruct.\\n\\nWith such different offhand interpretations, it becomes difﬁcult to deﬁne the term artiﬁcial intelligence or AI simply and robustly. Nevertheless I would like to try, using examples and historical deﬁnitions, to characterize the ﬁeld of AI. In 1955, John McCarthy, one of the pioneers of AI, was the ﬁrst to deﬁne the term artiﬁcial intelligence, roughly as follows:\\n\\nThe goal of AI is to develop machines that behave as though they were intelligent.'), Document(metadata={'source': '/content/future of A.I intelligence.pdf'}, page_content='Tronc is not the only company to enthusiastically embrace the term “artificial intelligence.” A.I. is hot, and every company worth its stock price is talking about how this magical potion will change everything. Even Macy’s recently announced that it was testing an I.B.M. artificial-intelligence tool in ten of its department stores, in order to bring back 9\\n\\nYU Ideas - A Project of the Office of the President - Yeshiva University – yu.edu/yuideas\\n\\ncustomers who are abandoning traditional retail in favor of online shopping.\\n\\nMuch like “the cloud,” “big data,” and “machine learning” before it, the term “artificial intelligence” has been hijacked by marketers and advertising copywriters. A lot of what people are calling “artificial intelligence” is really data analytics—in other words, business as usual. If the hype leaves you asking “What is A.I., really?,” don’t worry, you’re not alone. I asked various experts to define the term and got different answers. The only thing they all seem to agree on is that artificial intelligence is a set of technologies that try to imitate or augment human intelligence. To me, the emphasis is on augmentation, in which intelligent software helps us interact and deal with the increasingly digital world we live in.')]\n"
          ]
        }
      ],
      "source": [
        "# Invoke the conversational chain to ask our question and get a response\n",
        "question = \"What is ai?\"\n",
        "response = conv_chain.invoke({\"question\": question, \"chat_history\": []})\n",
        "print(f\"Answer: {response['answer']}\")\n",
        "print(f\"Source Document: {response['source_documents']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PYVBKE-jaBiI"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "# Function to search and recall from memory\n",
        "def recall_from_memory(memory, query):\n",
        "    for entry in memory:\n",
        "        if query.lower() in entry[\"question\"].lower():\n",
        "            return f\"You mentioned this before: {entry['question']}\\nResponse: {entry['response']}\"\n",
        "\n",
        "    # Provide feedback if nothing is found\n",
        "    return f\"I don't recall anything specific about '{query}'. Could you please provide more details or rephrase your question?\"\n",
        "\n",
        "# Create function to process the user question with memory and history\n",
        "def process_question(user_question, history, memory=None):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        if history is None:\n",
        "            history = []\n",
        "\n",
        "        if memory is None:\n",
        "            memory = []\n",
        "\n",
        "        # Prepare chat_history in the format expected by conv_chain\n",
        "        chat_history = [(h[0], h[1].split(\"\\n\\nResponse time:\")[0]) for h in history]\n",
        "\n",
        "        # Debug print\n",
        "        print(f\"Processing question: {user_question}\")\n",
        "        print(f\"Chat history: {chat_history}\")\n",
        "\n",
        "        # Check if the user is asking for a recall of past conversations\n",
        "        if \"recall\" in user_question.lower() or \"remember\" in user_question.lower():\n",
        "            recall_query = user_question.replace(\"recall\", \"\").replace(\"remember\", \"\").strip()\n",
        "            if recall_query:\n",
        "                response = recall_from_memory(memory, recall_query)\n",
        "            else:\n",
        "                response = \"Please specify what you'd like me to recall.\"\n",
        "\n",
        "        # Custom response for ownership/founder-related questions\n",
        "        elif \"founder\" in user_question.lower() or \"builder\" in user_question.lower() or \"owner\" in user_question.lower():\n",
        "            response = (\n",
        "                \"This chat assistant was built and is maintained by Joel Tamakloe, a data scientist and AI enthusiast. \"\n",
        "                \"Joel's background includes extensive experience in building AI-powered applications and solving real-world problems using data. \"\n",
        "                \"This assistant was created to make information about artificial intelligence more accessible and to assist users in exploring AI concepts interactively. \"\n",
        "                \"It is powered by advanced AI models like LLaMA and uses cutting-edge tools such as Hugging Face for embedding and Chroma for vector storage. \"\n",
        "                \"Currently, it's in the testing phase with a focus on AI-related topics, aiming to improve its capabilities and expand into educational and business applications in the future.\"\n",
        "            )\n",
        "        # Custom response for personal questions\n",
        "        elif \"who are you\" in user_question.lower() or \"about you\" in user_question.lower() or \"yourself\" in user_question.lower():\n",
        "            response = (\n",
        "                \"I am an AI-powered chat assistant designed to assist users with exploring and learning about artificial intelligence and related topics. \"\n",
        "                \"My purpose is to provide an intuitive way for users to interact with AI and gain insights on topics related to artificial intelligence. \"\n",
        "                \"I use advanced tools and technologies like the LLaMA model, a powerful large language model, to process natural language queries, and Chroma, a vector database management system, to efficiently store and retrieve information. \"\n",
        "                \"In the future, I plan to expand my abilities to cover more topics, improve response accuracy, and possibly integrate video and voice interaction for a more dynamic user experience. \"\n",
        "                \"Ask me anything about A.I. I am happy to help! 😊\"\n",
        "            )\n",
        "        # Custom response for greetings\n",
        "        elif \"hello\" in user_question.lower() or \"hi\" in user_question.lower():\n",
        "            response = \"Hello! How can I assist you today?\"\n",
        "\n",
        "        # Invoke conv_chain with both the question and chat_history for all other questions\n",
        "        else:\n",
        "            response = conv_chain.invoke({\"question\": user_question, \"chat_history\": chat_history})\n",
        "\n",
        "            if isinstance(response, dict) and 'answer' in response:\n",
        "                response = response['answer']\n",
        "\n",
        "            # Add a conversational prompt occasionally\n",
        "            if random.random() < 0.3:  # 30% chance to add small talk\n",
        "                response += random.choice([\n",
        "                    \"By the way, feel free to ask me anything else or even chat casually!\",\n",
        "                    \"Let me know if you need more information or if you'd like to discuss something else! 😊\",\n",
        "                    \"Is there anything else you're curious about? I'm happy to help!\",\n",
        "                    \"If you have more questions or just want to chat, I'm here!\",\n",
        "                    \"Don't hesitate to ask me anything. I'm here to assist you!\",\n",
        "                    \"Got any other interesting topics in mind? Let's chat!\",\n",
        "                    \"Feel free to share more or ask anything else that's on your mind!\"\n",
        "                ])\n",
        "\n",
        "        # Measure the response time\n",
        "        end_time = time.time()\n",
        "        response_time = f\"Response time: {end_time - start_time:.2f} seconds.\"\n",
        "\n",
        "        # Combine the response and the response time\n",
        "        full_response = f\"{response}\\n\\n{response_time}\"\n",
        "\n",
        "        # Update the history\n",
        "        history.append((user_question, full_response))\n",
        "\n",
        "        # Debug print\n",
        "        print(f\"Processed successfully. Response: {full_response}\")\n",
        "\n",
        "        return history, memory, full_response\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred: {str(e)}\"\n",
        "        print(error_message)\n",
        "        return history, memory, error_message\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "vTmqEehfe_ce",
        "outputId": "3adb1d05-c9b5-4585-ef43-d97a8ed58af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://6441e236269fe0e0d8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6441e236269fe0e0d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Setup the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_question,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Type your question here...\"),\n",
        "        gr.State()\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Chatbot(),\n",
        "        gr.State(),\n",
        "        gr.Textbox(label=\"Latest Answer\")\n",
        "    ],\n",
        "    title=\"Adika Chat Assistant\",\n",
        "    description=\"Ask any question about Artificial Intelligence.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dBaOvgBgbVM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab1229400c0b413998af7b54d77ff6b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_003f0d9528474b7391b46ef3aa0d4c4d",
              "IPY_MODEL_260772fbe5d649cab41b82b1993dda3d",
              "IPY_MODEL_31ab80fb5e0a4274935e7ce66f9af845"
            ],
            "layout": "IPY_MODEL_fd79391df61945e694a06b9c0a48dbc9"
          }
        },
        "003f0d9528474b7391b46ef3aa0d4c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec94254ad384db4be196f5a7accf14b",
            "placeholder": "​",
            "style": "IPY_MODEL_8adccb78528d46dfacc37bf41d87a648",
            "value": "modules.json: 100%"
          }
        },
        "260772fbe5d649cab41b82b1993dda3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_445ceef5b82e45aeb3e1fb48837c898d",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f18f322203d47be93b4238699026780",
            "value": 349
          }
        },
        "31ab80fb5e0a4274935e7ce66f9af845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bcd1181f5d0435abc64adb596748a06",
            "placeholder": "​",
            "style": "IPY_MODEL_191617780c344097b23419de401485f0",
            "value": " 349/349 [00:00&lt;00:00, 14.9kB/s]"
          }
        },
        "fd79391df61945e694a06b9c0a48dbc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec94254ad384db4be196f5a7accf14b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8adccb78528d46dfacc37bf41d87a648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "445ceef5b82e45aeb3e1fb48837c898d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f18f322203d47be93b4238699026780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bcd1181f5d0435abc64adb596748a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "191617780c344097b23419de401485f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdfc5221e5914eaabd0404c9037ff6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18f085bd319e40ec8f3bd33eb54b01bc",
              "IPY_MODEL_99d2f82a21d047709c69e73528601241",
              "IPY_MODEL_4ac8ed21286f4dd3a9d6ceef25d1cafc"
            ],
            "layout": "IPY_MODEL_847c8f53440f49c9b8ae65ec941ee06c"
          }
        },
        "18f085bd319e40ec8f3bd33eb54b01bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f985a2c117bf4a60a6303673dfe73c90",
            "placeholder": "​",
            "style": "IPY_MODEL_5a6ed391e88146a9a164082d05c93461",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "99d2f82a21d047709c69e73528601241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_250a392a2afd475db0a21e9dcdfd83ec",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ad72c36692044c1b76650483c30604e",
            "value": 116
          }
        },
        "4ac8ed21286f4dd3a9d6ceef25d1cafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91d329b6e13343009072ee2d38e61700",
            "placeholder": "​",
            "style": "IPY_MODEL_de60a03eef4f48b1884ae44a3695bf97",
            "value": " 116/116 [00:00&lt;00:00, 6.88kB/s]"
          }
        },
        "847c8f53440f49c9b8ae65ec941ee06c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f985a2c117bf4a60a6303673dfe73c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a6ed391e88146a9a164082d05c93461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "250a392a2afd475db0a21e9dcdfd83ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad72c36692044c1b76650483c30604e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91d329b6e13343009072ee2d38e61700": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de60a03eef4f48b1884ae44a3695bf97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0b4655868ac480abf9b029a82c65d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_931fcbd09fc54d548879f7f86b802406",
              "IPY_MODEL_a94d4992fb984d10abad4b1933b670d4",
              "IPY_MODEL_05235bbddacc407c8eb672d24f03fd89"
            ],
            "layout": "IPY_MODEL_47fa1dd63ce1471ab8a066269bd3b2a6"
          }
        },
        "931fcbd09fc54d548879f7f86b802406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8aacb7a0b3b400e95d7a7e7cc1e9f96",
            "placeholder": "​",
            "style": "IPY_MODEL_64ae150489144acc8f4f444a25ed49a2",
            "value": "README.md: 100%"
          }
        },
        "a94d4992fb984d10abad4b1933b670d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74292b1d1f784959ae71838aa9c8a31b",
            "max": 10621,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2015ee083bb5481cbf7dc2759277a813",
            "value": 10621
          }
        },
        "05235bbddacc407c8eb672d24f03fd89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec526829ab034b45af763e651edfbd4b",
            "placeholder": "​",
            "style": "IPY_MODEL_f86fb4efeb8844989f1686d803ea6783",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 693kB/s]"
          }
        },
        "47fa1dd63ce1471ab8a066269bd3b2a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8aacb7a0b3b400e95d7a7e7cc1e9f96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64ae150489144acc8f4f444a25ed49a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74292b1d1f784959ae71838aa9c8a31b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2015ee083bb5481cbf7dc2759277a813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec526829ab034b45af763e651edfbd4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f86fb4efeb8844989f1686d803ea6783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79b0ba3a25964fae860e17290054a4c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac4ce096afe343b9a732d3ff80d5878a",
              "IPY_MODEL_8734f74eced446c6b315a6271b34b7fe",
              "IPY_MODEL_765ca2da86854417bd736853ba3cfff5"
            ],
            "layout": "IPY_MODEL_89eb10e5ad894baead243e365569df95"
          }
        },
        "ac4ce096afe343b9a732d3ff80d5878a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b5792cf4ac840fb869e7cf72a99f43d",
            "placeholder": "​",
            "style": "IPY_MODEL_554d6db0a245428db8c40fdf42bdda70",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "8734f74eced446c6b315a6271b34b7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f465b6fe4ece4ca29213d401a6acc60a",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c784aaaa4ba94944bd95862ec62dfbc5",
            "value": 53
          }
        },
        "765ca2da86854417bd736853ba3cfff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13d3713f6cc34eca96c6f675dbce7f1b",
            "placeholder": "​",
            "style": "IPY_MODEL_ae36f6b1ca9d451080de83034df4f0c1",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.74kB/s]"
          }
        },
        "89eb10e5ad894baead243e365569df95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5792cf4ac840fb869e7cf72a99f43d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "554d6db0a245428db8c40fdf42bdda70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f465b6fe4ece4ca29213d401a6acc60a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c784aaaa4ba94944bd95862ec62dfbc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13d3713f6cc34eca96c6f675dbce7f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae36f6b1ca9d451080de83034df4f0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf3f03367cb044d4818c4d4668ccce79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_149ed0484bdf472bb4df628c070491a2",
              "IPY_MODEL_253243cebd0e49249e53f27befdd6328",
              "IPY_MODEL_760d5a7f6c7e4598ad9a7d8191ae7f16"
            ],
            "layout": "IPY_MODEL_b6f6ad4f20b34fbc9078a3a67589c540"
          }
        },
        "149ed0484bdf472bb4df628c070491a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9587751111848af99842bf24249ebe3",
            "placeholder": "​",
            "style": "IPY_MODEL_2a904c9419c04d13b19fefe1bf8dafc9",
            "value": "config.json: 100%"
          }
        },
        "253243cebd0e49249e53f27befdd6328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18ee20039afd4c4380ae9c779020a60c",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e2bba50bdfb43caaaaa0c33dcb23ea8",
            "value": 571
          }
        },
        "760d5a7f6c7e4598ad9a7d8191ae7f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b135ce2c8a54979924e42734de89b4d",
            "placeholder": "​",
            "style": "IPY_MODEL_88b340aec449464da4722f88baf96946",
            "value": " 571/571 [00:00&lt;00:00, 36.9kB/s]"
          }
        },
        "b6f6ad4f20b34fbc9078a3a67589c540": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9587751111848af99842bf24249ebe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a904c9419c04d13b19fefe1bf8dafc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18ee20039afd4c4380ae9c779020a60c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2bba50bdfb43caaaaa0c33dcb23ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b135ce2c8a54979924e42734de89b4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88b340aec449464da4722f88baf96946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55709f3cd0b542b5a202e948b1e29c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_719a1907d4064be19bbf0739789c7c5f",
              "IPY_MODEL_f13997759e3c4c46ae4663723d5af111",
              "IPY_MODEL_b07484423e05437d9453a6fb8dccfd44"
            ],
            "layout": "IPY_MODEL_50c4ce5659db43f9b871e08faf770ee2"
          }
        },
        "719a1907d4064be19bbf0739789c7c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d9489cae36644fdbadb174c0488cdef",
            "placeholder": "​",
            "style": "IPY_MODEL_64b196f76dec426baadfc49d422ba627",
            "value": "model.safetensors: 100%"
          }
        },
        "f13997759e3c4c46ae4663723d5af111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aedd84f865a1461d82ce1be9b9c35456",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70c9d647a7fa4309bc31bd00033af486",
            "value": 437971872
          }
        },
        "b07484423e05437d9453a6fb8dccfd44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ccfd9d7ac4a4755a29faeac9576c8e7",
            "placeholder": "​",
            "style": "IPY_MODEL_cd6794c43e1846a9a7fb0c2274dbd868",
            "value": " 438M/438M [00:06&lt;00:00, 160MB/s]"
          }
        },
        "50c4ce5659db43f9b871e08faf770ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d9489cae36644fdbadb174c0488cdef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64b196f76dec426baadfc49d422ba627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aedd84f865a1461d82ce1be9b9c35456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c9d647a7fa4309bc31bd00033af486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ccfd9d7ac4a4755a29faeac9576c8e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd6794c43e1846a9a7fb0c2274dbd868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0ba1abedd53413bb3cc74c73011e3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_187f45a836a649789029958c4a1678f0",
              "IPY_MODEL_8da76c5493864b17b645cc0a0d0685e8",
              "IPY_MODEL_a35015a53c53493c8c000b7a1d2e4ab1"
            ],
            "layout": "IPY_MODEL_61fced29bb004890bd20eede31eaacb1"
          }
        },
        "187f45a836a649789029958c4a1678f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752a370b7a554e0fa70160025f7c8b99",
            "placeholder": "​",
            "style": "IPY_MODEL_2320dc6aae284dd3b8282d9a0fd8c511",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8da76c5493864b17b645cc0a0d0685e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd8877a8c3d04260a85b82409e5bcb45",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c209d4e2151c424b9a7de516b75ee7a5",
            "value": 363
          }
        },
        "a35015a53c53493c8c000b7a1d2e4ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_375863230f5542c0870680bf52753ed6",
            "placeholder": "​",
            "style": "IPY_MODEL_e071aceb04684bf3895c2cd2f080a2f9",
            "value": " 363/363 [00:00&lt;00:00, 22.3kB/s]"
          }
        },
        "61fced29bb004890bd20eede31eaacb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "752a370b7a554e0fa70160025f7c8b99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2320dc6aae284dd3b8282d9a0fd8c511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd8877a8c3d04260a85b82409e5bcb45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c209d4e2151c424b9a7de516b75ee7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "375863230f5542c0870680bf52753ed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e071aceb04684bf3895c2cd2f080a2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a59cc9e28f14473a913c7e8d4805ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2e32e81634b4af98eaf58ceec65147f",
              "IPY_MODEL_630e4066e14546f692b7594b4cb292e7",
              "IPY_MODEL_4ce44dd4ce2c4176af95c1bde8cbc650"
            ],
            "layout": "IPY_MODEL_31a93bdc0a9942e8a3750a2175700b20"
          }
        },
        "c2e32e81634b4af98eaf58ceec65147f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bb8ea1e5b03464b91e97a6b39e75397",
            "placeholder": "​",
            "style": "IPY_MODEL_1a8ad51d2b2e4cad925f68a28c05d25a",
            "value": "vocab.txt: 100%"
          }
        },
        "630e4066e14546f692b7594b4cb292e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20555e5328d14a6692ea6b2afd56d70d",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2042fface84149a5aa4ec3e8b9cef26f",
            "value": 231536
          }
        },
        "4ce44dd4ce2c4176af95c1bde8cbc650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_753d0397314b46b5982ffb269b29f9c0",
            "placeholder": "​",
            "style": "IPY_MODEL_9d38268badd44286a17d181a6321fb7c",
            "value": " 232k/232k [00:00&lt;00:00, 8.19MB/s]"
          }
        },
        "31a93bdc0a9942e8a3750a2175700b20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb8ea1e5b03464b91e97a6b39e75397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a8ad51d2b2e4cad925f68a28c05d25a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20555e5328d14a6692ea6b2afd56d70d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2042fface84149a5aa4ec3e8b9cef26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "753d0397314b46b5982ffb269b29f9c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d38268badd44286a17d181a6321fb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12afbfe865034484bdfd594326b8d46c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a715891e7fd74ffab2ee91279cd9a6af",
              "IPY_MODEL_b126b52a3ff04d9ea5f12eea601d964c",
              "IPY_MODEL_b93dae6467c642898fe7c7b454509d44"
            ],
            "layout": "IPY_MODEL_a4befaaa2bdd40818e05e72df596bcb3"
          }
        },
        "a715891e7fd74ffab2ee91279cd9a6af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f173b94636f440c68743574c73dafd26",
            "placeholder": "​",
            "style": "IPY_MODEL_18383879cb784814be31c2ba3a7ab16e",
            "value": "tokenizer.json: 100%"
          }
        },
        "b126b52a3ff04d9ea5f12eea601d964c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf3266a156534736b6f3d9315d7272c8",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18458cc888e54e63a48b598584472ae8",
            "value": 466021
          }
        },
        "b93dae6467c642898fe7c7b454509d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b2fa3daf7184e638da823900a436981",
            "placeholder": "​",
            "style": "IPY_MODEL_87dc1828b1d94028ac011302425e5de7",
            "value": " 466k/466k [00:00&lt;00:00, 15.0MB/s]"
          }
        },
        "a4befaaa2bdd40818e05e72df596bcb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f173b94636f440c68743574c73dafd26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18383879cb784814be31c2ba3a7ab16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf3266a156534736b6f3d9315d7272c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18458cc888e54e63a48b598584472ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b2fa3daf7184e638da823900a436981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87dc1828b1d94028ac011302425e5de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57300de398c245ce975a34976fd7610d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55ddfcc1e1994e04b45752151c9ce0d7",
              "IPY_MODEL_b01ba078301546d38d15ca2cd8087ab8",
              "IPY_MODEL_1520f7a3a1404d56b162a22599fc352a"
            ],
            "layout": "IPY_MODEL_38ad05af83f64b3580ae1a29868f6c78"
          }
        },
        "55ddfcc1e1994e04b45752151c9ce0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8c12384729d46e8bf8fb0ce785cab1b",
            "placeholder": "​",
            "style": "IPY_MODEL_1a1d413fdacc4cbeb13dfd9520b66f17",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "b01ba078301546d38d15ca2cd8087ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b18313141f4b495e974a680fc391c645",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a674b3829af43b6843c08c812f76212",
            "value": 239
          }
        },
        "1520f7a3a1404d56b162a22599fc352a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd53f41a327e4e3aa1ffe3b57c59f68a",
            "placeholder": "​",
            "style": "IPY_MODEL_81b5e2884b6841a9a7c4a16069cb3d73",
            "value": " 239/239 [00:00&lt;00:00, 14.3kB/s]"
          }
        },
        "38ad05af83f64b3580ae1a29868f6c78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8c12384729d46e8bf8fb0ce785cab1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a1d413fdacc4cbeb13dfd9520b66f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b18313141f4b495e974a680fc391c645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a674b3829af43b6843c08c812f76212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd53f41a327e4e3aa1ffe3b57c59f68a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81b5e2884b6841a9a7c4a16069cb3d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2da6e7e91624f6d8ceb67f756ec958a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e2f12a64f6d4baa87e6730ab51d5147",
              "IPY_MODEL_cdba425968f24ba3a97c6af0020ba07f",
              "IPY_MODEL_7279288590754c3d989478fab07b2fd3"
            ],
            "layout": "IPY_MODEL_d11de2ca75f047078967d78e2f36e273"
          }
        },
        "1e2f12a64f6d4baa87e6730ab51d5147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9796404d187b460b8bceb9c58b7571c8",
            "placeholder": "​",
            "style": "IPY_MODEL_b371b2fc8e2b4d7ebdab7bf4d878e4e0",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "cdba425968f24ba3a97c6af0020ba07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d80552cf525947b4a707cddb483d6262",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e16d59ed64d4a86b03e57f95c664e4b",
            "value": 190
          }
        },
        "7279288590754c3d989478fab07b2fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f72b03e8f424c65ab6fa80dbd9826dc",
            "placeholder": "​",
            "style": "IPY_MODEL_ebc15dd17ef0449c8f42187d51487acb",
            "value": " 190/190 [00:00&lt;00:00, 10.5kB/s]"
          }
        },
        "d11de2ca75f047078967d78e2f36e273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9796404d187b460b8bceb9c58b7571c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b371b2fc8e2b4d7ebdab7bf4d878e4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d80552cf525947b4a707cddb483d6262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e16d59ed64d4a86b03e57f95c664e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f72b03e8f424c65ab6fa80dbd9826dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc15dd17ef0449c8f42187d51487acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}