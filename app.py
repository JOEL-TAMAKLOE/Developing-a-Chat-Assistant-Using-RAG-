# -*- coding: utf-8 -*-
"""DEVELOPING_AI_CHAT_ASSISTANT_USING_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11uuBymrlkHq4NaSZn5VtGtUXh7RXzlsQ
"""

# Install relevant libraries
!pip install chromadb==0.5.5 \
    langchain-chroma==0.1.2 \
    langchain==0.2.11 \
    langchain-community==0.2.10 \
    langchain-text-splitters==0.2.2 \
    langchain-groq==0.1.6 \
    transformers==4.43.2 \
    sentence-transformers==2.7.0 \
    unstructured==0.15.0 \
    "unstructured[pdf]==0.15.0" \
    gradio \
    pydantic-settings

#importing relevant libraries
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyPDFLoader
import gradio as gr
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
import time
import random

#for handling PDF files in the pdf2image library.
!apt-get install -y poppler-utils

from config import settings
import warnings
warnings.filterwarnings('ignore')

# Create a variable for your api key
groq_api_key = settings.groq_api_key

from google.colab import files
import os

# Create a folder for uploaded PDFs
os.makedirs("pdfs", exist_ok=True)

# Upload PDFs
uploaded_files = files.upload()

# Save them into "pdfs" folder
for filename in uploaded_files.keys():
    os.rename(filename, os.path.join("pdfs", filename))

print(f"Uploaded {len(uploaded_files)} PDF(s) successfully.")

#Load PDFs into LangChain
all_docs = []

# Loop through PDFs in the folder
for pdf_file in os.listdir("pdfs"):
    if pdf_file.endswith(".pdf"):
        loader = PyPDFLoader(os.path.join("pdfs", pdf_file))
        docs = loader.load()
        all_docs.extend(docs)

print(f"Loaded {len(all_docs)} pages from all PDFs.")

all_docs

#Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(all_docs)

print(f"Split into {len(chunks)} chunks.")

#Get the first 5 text chunks
print(chunks[0])
print("-----------------------------------------------------------------------")
print("-----------------------------------------------------------------------")
print(chunks[5])

# Instantiate embedding
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

#instantiate vectorstore
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="chroma_db"
)

from pickle import TRUE
from langchain_community.vectorstores import Chroma
# ===== SETTINGS =====
persist_directory = "chroma_db"
first_run = True  # üîπ Change to True only when adding new PDFs

# ===== EMBEDDINGS =====
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

if first_run:
    # 1Ô∏è‚É£ Create & Save Vectors
    # Make sure `chunks` is already defined from your document loader + text splitter
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    vectorstore.persist()
    print("‚úÖ Vector DB created and saved.")
else:
    # 2Ô∏è‚É£ Load Existing Vectors
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    print("‚úÖ Vector DB loaded from disk.")

# ===== RETRIEVER =====
retriever = vectorstore.as_retriever()

# set-up retriever to retrieve information from our vector database
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# Create a llm from groq
llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.5,
    groq_api_key=groq_api_key
)

# Create a conversational chain
conv_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# Invoke the conversational chain to ask our question and get a response
question = "What is the impact of ai?"
response = conv_chain.invoke({"question": question, "chat_history": []})
print(f"Answer: {response['answer']}")
print(f"Source Document: {response['source_documents']}")

# Create function to process the user question with other necessary information
def process_question(user_question, history):
    try:
        start_time = time.time()

        if history is None:
            history = []

        # Prepare chat_history in the format expected by conv_chain
        chat_history = [(h[0], h[1].split("\n\nResponse time:")[0]) for h in history]

        # Debug print
        print(f"Processing question: {user_question}")
        print(f"Chat history: {chat_history}")

        # Custom response for ownership/founder-related questions
        if "founder" in user_question.lower() or "builder" in user_question.lower() or "owner" in user_question.lower():
            response = (
                "This chat assistant was built and is maintained by Mr. Joel Tamakloe, a data scientist and an AI and Cybersecurity enthusiast. "
                "Joel's background includes extensive experience in building AI-powered applications and solving real-world problems using data. "
                "This assistant was created to make information about artificial intelligence more accessible and to assist users in exploring AI concepts interactively. "
                "It is powered by advanced AI models like LLaMA and uses cutting-edge tools such as Hugging Face for embedding and Chroma for vector storage. "
                "Currently, it's in the testing phase with a focus on AI-related topics, aiming to improve its capabilities and expand into educational and business applications in the future."
            )

        # Custom response for personal questions like "Tell me about yourself"
        elif "who are you" in user_question.lower() or "myself" in user_question.lower() or "yourself" in user_question.lower():
            response = (
                "I am an AI-powered chat assistant designed to assist users with exploring and learning about artificial intelligence and related topics. "
                "My purpose is to provide an intuitive way for users to interact with AI and gain insights on topics related to artificial intelligence. "
                "I use advanced tools and technologies like the LLaMA model, a powerful large language model, to process natural language queries, and Chroma, a vector database management system, to efficiently store and retrieve information. "
                "In the future, I plan to expand my abilities to cover more topics, improve response accuracy, and possibly integrate video and voice interaction for a more dynamic user experience. "
                "Ask me anything about A.I. I am happy to help! üòä"
            )

        # Custom response for greetings like "Hello"
        elif "hello" in user_question.lower() or "hi" in user_question.lower():
            response = "Hello! How can I assist you today?"

        # Invoke conv_chain with both the question and chat_history for all other questions
        else:
            response = conv_chain.invoke({"question": user_question, "chat_history": chat_history})

            # If response is a dict, extract the actual response text
            if isinstance(response, dict) and 'answer' in response:
                response = response['answer']

            # Add a conversational prompt occasionally
            if random.random() < 0.3:  # 30% chance to add small talk
                response += "\n\nBy the way, feel free to ask me anything else or even chat casually!"

        # Measure the response time
        end_time = time.time()
        response_time = f"Response time: {end_time - start_time:.2f} seconds."

        # Combine the response and the response time
        full_response = f"{response}\n\n{response_time}"

        # Update the history
        history.append((user_question, full_response))

        # Debug print
        print(f"Processed successfully. Response: {full_response}")

        return history,history, full_response

    except Exception as e:
        error_message = f"An error occurred: {str(e)}"
        print(error_message)
        return history, history, error_message

# Setup the Gradio interface
iface = gr.Interface(
    fn=process_question,
    inputs=[
        gr.Textbox(lines=2, placeholder="Type your question here..."),
        gr.State()
    ],
    outputs=[
        gr.Chatbot(),
        gr.State(),
        gr.Textbox(label="Latest Answer")
    ],
    title="Joel A.I Chat Assistant",
    description="Ask any question about Artificial Intelligence."
)

# Launch the interface
iface.launch(share=True)

